{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c314291b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from einops import rearrange\n",
    "from einops.layers.torch import Rearrange\n",
    "torch.cuda.is_available()\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d94f35ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db2c3767",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89623b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(\n",
    "            0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(\n",
    "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        hidden_dim = int(inp * 4)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.proj(x) + self.attn(x)\n",
    "        else:\n",
    "            x = x + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "aba01529",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((3,2500,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2cd1a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Attention(10, 20, (50,50), heads=2, dim_head=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "075c1963",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2500, 20])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8a08ed43",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = Transformer(260, 520, (30, 20), heads=13, dim_head=20, downsample=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d3c72b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.randn((30, 260, 30, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9fd0aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 520, 30, 20])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T(y).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1a152936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_3x3_bn(inp, oup, image_size, downsample=False):\n",
    "    stride = 1 if downsample == False else 2\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n",
    "        nn.BatchNorm2d(oup),\n",
    "        nn.GELU()\n",
    "    )\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn, norm):\n",
    "        super().__init__()\n",
    "        self.norm = norm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class SE(nn.Module):\n",
    "    def __init__(self, inp, oup, expansion=0.25):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(oup, int(inp * expansion), bias=False),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(int(inp * expansion), oup, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.avg_pool(x).view(b, c)\n",
    "        y = self.fc(y).view(b, c, 1, 1)\n",
    "        return x * y\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class MBConv(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, downsample=False, expansion=4):\n",
    "        super().__init__()\n",
    "        self.downsample = downsample\n",
    "        stride = 1 if self.downsample == False else 2\n",
    "        hidden_dim = int(inp * expansion)\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        if expansion == 1:\n",
    "            self.conv = nn.Sequential(\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, stride,\n",
    "                          1, groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        else:\n",
    "            self.conv = nn.Sequential(\n",
    "                # pw\n",
    "                # down-sample in the first conv\n",
    "                nn.Conv2d(inp, hidden_dim, 1, stride, 0, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                # dw\n",
    "                nn.Conv2d(hidden_dim, hidden_dim, 3, 1, 1,\n",
    "                          groups=hidden_dim, bias=False),\n",
    "                nn.BatchNorm2d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                SE(inp, hidden_dim),\n",
    "                # pw-linear\n",
    "                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "                nn.BatchNorm2d(oup),\n",
    "            )\n",
    "        \n",
    "        self.conv = PreNorm(inp, self.conv, nn.BatchNorm2d)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            return self.proj(self.pool(x)) + self.conv(x)\n",
    "        else:\n",
    "            return x + self.conv(x)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, dropout=0.):\n",
    "        super().__init__()\n",
    "        inner_dim = dim_head * heads\n",
    "        project_out = not (heads == 1 and dim_head == inp)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "\n",
    "        self.heads = heads\n",
    "        self.scale = dim_head ** -0.5\n",
    "\n",
    "        # parameter table of relative position bias\n",
    "        self.relative_bias_table = nn.Parameter(\n",
    "            torch.zeros((2 * self.ih - 1) * (2 * self.iw - 1), heads))\n",
    "\n",
    "        coords = torch.meshgrid((torch.arange(self.ih), torch.arange(self.iw)))\n",
    "        coords = torch.flatten(torch.stack(coords), 1)\n",
    "        relative_coords = coords[:, :, None] - coords[:, None, :]\n",
    "\n",
    "        relative_coords[0] += self.ih - 1\n",
    "        relative_coords[1] += self.iw - 1\n",
    "        relative_coords[0] *= 2 * self.iw - 1\n",
    "        relative_coords = rearrange(relative_coords, 'c h w -> h w c')\n",
    "        relative_index = relative_coords.sum(-1).flatten().unsqueeze(1)\n",
    "        self.register_buffer(\"relative_index\", relative_index)\n",
    "\n",
    "        self.attend = nn.Softmax(dim=-1)\n",
    "        self.to_qkv = nn.Linear(inp, inner_dim * 3, bias=False)\n",
    "\n",
    "        self.to_out = nn.Sequential(\n",
    "            nn.Linear(inner_dim, oup),\n",
    "            nn.Dropout(dropout)\n",
    "        ) if project_out else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.to_qkv(x).chunk(3, dim=-1)\n",
    "        q, k, v = map(lambda t: rearrange(\n",
    "            t, 'b n (h d) -> b h n d', h=self.heads), qkv)\n",
    "\n",
    "        dots = torch.matmul(q, k.transpose(-1, -2)) * self.scale\n",
    "\n",
    "        # Use \"gather\" for more efficiency on GPUs\n",
    "        relative_bias = self.relative_bias_table.gather(\n",
    "            0, self.relative_index.repeat(1, self.heads))\n",
    "        relative_bias = rearrange(\n",
    "            relative_bias, '(h w) c -> 1 c h w', h=self.ih*self.iw, w=self.ih*self.iw)\n",
    "        dots = dots + relative_bias\n",
    "\n",
    "        attn = self.attend(dots)\n",
    "        out = torch.matmul(attn, v)\n",
    "        out = rearrange(out, 'b h n d -> b n (h d)')\n",
    "        out = self.to_out(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, inp, oup, image_size, heads=8, dim_head=32, downsample=False, dropout=0.):\n",
    "        super().__init__()\n",
    "        print(image_size)\n",
    "        hidden_dim = int(inp * 4)\n",
    "\n",
    "        self.ih, self.iw = image_size\n",
    "        self.downsample = downsample\n",
    "\n",
    "        if self.downsample:\n",
    "            self.pool1 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.pool2 = nn.MaxPool2d(3, 2, 1)\n",
    "            self.proj = nn.Conv2d(inp, oup, 1, 1, 0, bias=False)\n",
    "\n",
    "        self.attn = Attention(inp, oup, image_size, heads, dim_head, dropout)\n",
    "        self.ff = FeedForward(oup, hidden_dim, dropout)\n",
    "\n",
    "        self.attn = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(inp, self.attn, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            Rearrange('b c ih iw -> b (ih iw) c'),\n",
    "            PreNorm(oup, self.ff, nn.LayerNorm),\n",
    "            Rearrange('b (ih iw) c -> b c ih iw', ih=self.ih, iw=self.iw)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.downsample:\n",
    "            x = self.proj(self.pool1(x)) + self.attn(self.pool2(x))\n",
    "        else:\n",
    "            print(1,x.shape)\n",
    "            print(2,self.attn(x).shape)\n",
    "            x = x + self.attn(x)\n",
    "        x = x + self.ff(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class CoAtNet(nn.Module):\n",
    "    def __init__(self, image_size, in_channels, num_blocks, channels, num_classes=1000, block_types=['C', 'C', 'T', 'T']):\n",
    "        super().__init__()\n",
    "        ih, iw = image_size\n",
    "        block = {'C': MBConv, 'T': Transformer}\n",
    "\n",
    "        self.s0 = self._make_layer(\n",
    "            conv_3x3_bn, in_channels, channels[0], num_blocks[0], (ih // 2, iw // 2))\n",
    "        self.s1 = self._make_layer(\n",
    "            block[block_types[0]], channels[0], channels[1], num_blocks[1], (ih // 4, iw // 4))\n",
    "        self.s2 = self._make_layer(\n",
    "            block[block_types[1]], channels[1], channels[2], num_blocks[2], (ih // 8, iw // 8))\n",
    "        self.s3 = self._make_layer(\n",
    "            block[block_types[2]], channels[2], channels[3], num_blocks[3], (ih // 16, iw // 16))\n",
    "        self.s4 = self._make_layer(\n",
    "            block[block_types[3]], channels[3], channels[4], num_blocks[4], (ih // 32, iw // 32))\n",
    "\n",
    "        self.pool = nn.AvgPool2d(ih // 32, 1)\n",
    "        self.fc = nn.Linear(channels[-1], num_classes, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.s0(x)\n",
    "        x = self.s1(x)\n",
    "        x = self.s2(x)\n",
    "        x = self.s3(x)\n",
    "        x = self.s4(x)\n",
    "\n",
    "        x = self.pool(x).view(-1, x.shape[1])\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def _make_layer(self, block, inp, oup, depth, image_size):\n",
    "        layers = nn.ModuleList([])\n",
    "        for i in range(depth):\n",
    "            if i == 0:\n",
    "                layers.append(block(inp, oup, image_size, downsample=True))\n",
    "            else:\n",
    "                layers.append(block(oup, oup, image_size))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "def coatnet_0():\n",
    "    num_blocks = [2, 2, 3, 5, 2]            # L\n",
    "    channels = [64, 96, 192, 384, 768]      # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_1():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [64, 96, 192, 384, 768]      # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_2():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [128, 128, 256, 512, 1026]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_3():\n",
    "    num_blocks = [2, 2, 6, 14, 2]           # L\n",
    "    channels = [192, 192, 384, 768, 1536]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def coatnet_4():\n",
    "    num_blocks = [2, 2, 12, 28, 2]          # L\n",
    "    channels = [192, 192, 384, 768, 1536]   # D\n",
    "    return CoAtNet((224, 224), 3, num_blocks, channels, num_classes=1000)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9c859eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 14)\n",
      "(14, 14)\n",
      "(14, 14)\n",
      "(14, 14)\n",
      "(14, 14)\n",
      "(7, 7)\n",
      "(7, 7)\n",
      "1 torch.Size([1, 384, 14, 14])\n",
      "2 torch.Size([1, 384, 14, 14])\n",
      "1 torch.Size([1, 384, 14, 14])\n",
      "2 torch.Size([1, 384, 14, 14])\n",
      "1 torch.Size([1, 384, 14, 14])\n",
      "2 torch.Size([1, 384, 14, 14])\n",
      "1 torch.Size([1, 384, 14, 14])\n",
      "2 torch.Size([1, 384, 14, 14])\n",
      "1 torch.Size([1, 768, 7, 7])\n",
      "2 torch.Size([1, 768, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "img = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "net = coatnet_0()\n",
    "out = net(img)\n",
    "# print(out.shape, count_parameters(net))\n",
    "\n",
    "# net = coatnet_1()\n",
    "# out = net(img)\n",
    "# print(out.shape, count_parameters(net))\n",
    "\n",
    "# net = coatnet_2()\n",
    "# out = net(img)\n",
    "# print(out.shape, count_parameters(net))\n",
    "\n",
    "# net = coatnet_3()\n",
    "# out = net(img)\n",
    "# print(out.shape, count_parameters(net))\n",
    "\n",
    "# net = coatnet_4()\n",
    "# out = net(img)\n",
    "# print(out.shape, count_parameters(net))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d34dfcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.ones(5,3,10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2e471af",
   "metadata": {},
   "outputs": [],
   "source": [
    "C2 = nn.Conv2d(3,6,(3,3), groups=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a728545f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([6, 1, 3, 3])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6948e69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5633, 0.5709, 0.3618, 0.4083, 0.7061, 0.6933],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2.weight[0].sum()+C2.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d1e2342f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          ...,\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633]],\n",
       "\n",
       "         [[-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          ...,\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354]],\n",
       "\n",
       "         [[ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          ...,\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606]],\n",
       "\n",
       "         [[ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          ...,\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989]],\n",
       "\n",
       "         [[ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          ...,\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446]],\n",
       "\n",
       "         [[ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          ...,\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          ...,\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633]],\n",
       "\n",
       "         [[-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          ...,\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354]],\n",
       "\n",
       "         [[ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          ...,\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606]],\n",
       "\n",
       "         [[ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          ...,\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989]],\n",
       "\n",
       "         [[ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          ...,\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446]],\n",
       "\n",
       "         [[ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          ...,\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          ...,\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633]],\n",
       "\n",
       "         [[-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          ...,\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354]],\n",
       "\n",
       "         [[ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          ...,\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606]],\n",
       "\n",
       "         [[ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          ...,\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989]],\n",
       "\n",
       "         [[ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          ...,\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446]],\n",
       "\n",
       "         [[ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          ...,\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          ...,\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633]],\n",
       "\n",
       "         [[-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          ...,\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354]],\n",
       "\n",
       "         [[ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          ...,\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606]],\n",
       "\n",
       "         [[ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          ...,\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989]],\n",
       "\n",
       "         [[ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          ...,\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446]],\n",
       "\n",
       "         [[ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          ...,\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804]]],\n",
       "\n",
       "\n",
       "        [[[ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          ...,\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633],\n",
       "          [ 0.5633,  0.5633,  0.5633,  ...,  0.5633,  0.5633,  0.5633]],\n",
       "\n",
       "         [[-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          ...,\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354],\n",
       "          [-0.2354, -0.2354, -0.2354,  ..., -0.2354, -0.2354, -0.2354]],\n",
       "\n",
       "         [[ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          ...,\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606],\n",
       "          [ 0.0606,  0.0606,  0.0606,  ...,  0.0606,  0.0606,  0.0606]],\n",
       "\n",
       "         [[ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          ...,\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989],\n",
       "          [ 0.4989,  0.4989,  0.4989,  ...,  0.4989,  0.4989,  0.4989]],\n",
       "\n",
       "         [[ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          ...,\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446],\n",
       "          [ 0.5446,  0.5446,  0.5446,  ...,  0.5446,  0.5446,  0.5446]],\n",
       "\n",
       "         [[ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          ...,\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804],\n",
       "          [ 0.9804,  0.9804,  0.9804,  ...,  0.9804,  0.9804,  0.9804]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C2(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f5166d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "C1 = nn.Conv1d(3,6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d5a1dbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 6, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1 = torch.randn(5,3,10)\n",
    "C1(z1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "11429c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "b50da224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7efd65f33550>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_epochs = 3\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "034b8b66",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/files'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[0;32m----> 2\u001b[0m   \u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdatasets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMNIST\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/files/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                               \u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.1307\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.3081\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                             \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      8\u001b[0m   batch_size\u001b[38;5;241m=\u001b[39mbatch_size_train, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/torchvision/datasets/mnist.py:99\u001b[0m, in \u001b[0;36mMNIST.__init__\u001b[0;34m(self, root, train, transform, target_transform, download)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found. You can use download=True to download it\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/site-packages/torchvision/datasets/mnist.py:179\u001b[0m, in \u001b[0;36mMNIST.download\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_exists():\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# download files\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m filename, md5 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresources:\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/os.py:215\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head \u001b[38;5;129;01mand\u001b[39;00m tail \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39mexists(head):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexist_ok\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileExistsError\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;66;03m# Defeats race condition when another thread created the path\u001b[39;00m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/scratch/mfeng/anaconda/envs/atd2022/lib/python3.9/os.py:225\u001b[0m, in \u001b[0;36mmakedirs\u001b[0;34m(name, mode, exist_ok)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 225\u001b[0m     \u001b[43mmkdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     \u001b[38;5;66;03m# Cannot rely on checking for EEXIST, since the operating system\u001b[39;00m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# could give priority to other errors like EACCES or EROFS\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m exist_ok \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path\u001b[38;5;241m.\u001b[39misdir(name):\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/files'"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b15c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd2022",
   "language": "python",
   "name": "atd2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
