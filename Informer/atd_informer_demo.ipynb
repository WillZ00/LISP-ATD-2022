{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1a5591",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92827dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import atd2022\n",
    "import torch\n",
    "from atd_informer import atd_informer\n",
    "from utils.tools import dotdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dd46b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'atd_informer.atd_informer' from '/Users/will/Desktop/LISP-ATD-2022/Informer/atd_informer/atd_informer.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(atd_informer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9b8a5",
   "metadata": {},
   "source": [
    "# Model Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86b3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "\n",
    "args.enc_in = 5200 # encoder input size\n",
    "args.dec_in = 5200 # decoder input size\n",
    "args.c_out = 5200 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'w'\n",
    "args.inverse=False\n",
    "args.timeenc=0\n",
    "args.checkpoints = \"/Users/will/Desktop/tmp\"\n",
    "\n",
    "\n",
    "\n",
    "args.seq_len=8\n",
    "args.label_len=4\n",
    "args.pred_len=2\n",
    "\n",
    "\n",
    "args.batch_size = 1\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False\n",
    "\n",
    "args.itr=1\n",
    "args.train_epochs=5\n",
    "args.patience=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64fedfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "informer = atd_informer.ATD_Informer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fb2660",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6a523c5",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38fc8f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl8_ll4_pl2_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 171\n",
      "val 34\n",
      "test 34\n",
      "171\n",
      "34\n",
      "\titers: 100, epoch: 1 | loss: 5477415.5000000\n",
      "\tspeed: 0.2054s/iter; left time: 155.2822s\n",
      "Epoch: 1 cost time: 34.54617977142334\n",
      "Epoch: 1, Steps: 171 | Train Loss: 3916993.1747076 Vali Loss: 3477570.5000000 Test Loss: 3477570.5000000\n",
      "Validation loss decreased (inf --> 3477570.500000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      "\titers: 100, epoch: 2 | loss: 5497477.5000000\n",
      "\tspeed: 0.3708s/iter; left time: 216.9103s\n",
      "Epoch: 2 cost time: 38.688735246658325\n",
      "Epoch: 2, Steps: 171 | Train Loss: 3912589.6798246 Vali Loss: 3473233.7500000 Test Loss: 3473234.0000000\n",
      "Validation loss decreased (3477570.500000 --> 3473233.750000).  Saving model ...\n",
      "Updating learning rate to 5e-05\n",
      "\titers: 100, epoch: 3 | loss: 4564449.5000000\n",
      "\tspeed: 0.3766s/iter; left time: 155.8961s\n",
      "Epoch: 3 cost time: 34.6571409702301\n",
      "Epoch: 3, Steps: 171 | Train Loss: 3909084.7858187 Vali Loss: 3470986.7500000 Test Loss: 3470986.5000000\n",
      "Validation loss decreased (3473233.750000 --> 3470986.750000).  Saving model ...\n",
      "Updating learning rate to 2.5e-05\n",
      "\titers: 100, epoch: 4 | loss: 1479022.1250000\n",
      "\tspeed: 0.4038s/iter; left time: 98.1256s\n",
      "Epoch: 4 cost time: 37.99622321128845\n",
      "Epoch: 4, Steps: 171 | Train Loss: 3907298.2843567 Vali Loss: 3469865.5000000 Test Loss: 3469865.5000000\n",
      "Validation loss decreased (3470986.750000 --> 3469865.500000).  Saving model ...\n",
      "Updating learning rate to 1.25e-05\n",
      "\titers: 100, epoch: 5 | loss: 2808506.0000000\n",
      "\tspeed: 0.3932s/iter; left time: 28.3075s\n",
      "Epoch: 5 cost time: 36.17717909812927\n",
      "Epoch: 5, Steps: 171 | Train Loss: 3906405.6754386 Vali Loss: 3469302.0000000 Test Loss: 3469302.0000000\n",
      "Validation loss decreased (3469865.500000 --> 3469302.000000).  Saving model ...\n",
      "Updating learning rate to 6.25e-06\n",
      ">>>>>>>testing : None_None_ftNone_sl8_ll4_pl2_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "test 34\n",
      "test shape: (34, 1, 2, 5200) (34, 1, 2, 5200)\n",
      "test shape: (34, 2, 5200) (34, 2, 5200)\n",
      "mse:3469301.25, mae:208.78269958496094\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    # set experiments\n",
    "    exp = informer(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a47a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b885cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd2022",
   "language": "python",
   "name": "atd2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
