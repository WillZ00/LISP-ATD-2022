{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1a5591",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92827dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import atd2022\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from atd_informer import atd_informer\n",
    "from utils.tools import dotdict\n",
    "from atd_wrapper import InformerForcaster\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dd46b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'atd_informer.atd_informer' from '/scratch/wzong/LISP-ATD-2022/Informer/atd_informer/atd_informer.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(atd_informer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9b8a5",
   "metadata": {},
   "source": [
    "# Model Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86b3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "\n",
    "args.enc_in = 20 # encoder input size\n",
    "args.dec_in = 20 # decoder input size\n",
    "args.c_out = 20 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'w'\n",
    "args.inverse=False\n",
    "args.timeenc=0\n",
    "\n",
    "args.cols=1\n",
    "args.checkpoints = \"/Users/will/Desktop/tmp\"\n",
    "\n",
    "\n",
    "\n",
    "args.seq_len=5\n",
    "args.label_len=3\n",
    "args.pred_len=1\n",
    "\n",
    "\n",
    "args.batch_size = 1\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False\n",
    "\n",
    "args.itr=1\n",
    "args.train_epochs=1\n",
    "args.patience=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64fedfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "informer = atd_informer.ATD_Informer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a523c5",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d38fc8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m setting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ft\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ll\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dm\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_nh\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_el\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_df\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_at\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_fc\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_eb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dt\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_mx\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mmodel, args\u001b[38;5;241m.\u001b[39mdata, args\u001b[38;5;241m.\u001b[39mfeatures, \n\u001b[1;32m      4\u001b[0m             args\u001b[38;5;241m.\u001b[39mseq_len, args\u001b[38;5;241m.\u001b[39mlabel_len, args\u001b[38;5;241m.\u001b[39mpred_len,\n\u001b[1;32m      5\u001b[0m             args\u001b[38;5;241m.\u001b[39md_model, args\u001b[38;5;241m.\u001b[39mn_heads, args\u001b[38;5;241m.\u001b[39me_layers, args\u001b[38;5;241m.\u001b[39md_layers, args\u001b[38;5;241m.\u001b[39md_ff, args\u001b[38;5;241m.\u001b[39mattn, args\u001b[38;5;241m.\u001b[39mfactor, args\u001b[38;5;241m.\u001b[39membed, args\u001b[38;5;241m.\u001b[39mdistil, args\u001b[38;5;241m.\u001b[39mmix, args\u001b[38;5;241m.\u001b[39mdes, ii)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43minformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    # set experiments\n",
    "    exp = informer(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a47a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdde6edb",
   "metadata": {},
   "source": [
    "# Generate Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c023c",
   "metadata": {},
   "source": [
    "#setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                                                                                                     args.seq_len, args.label_len, args.pred_len,\n",
    "                                                                                                     args.d_model, args.n_heads, args.e_layers, args.d_layers,\n",
    "                                                                                                     args.d_ff, args.attn, args.factor, args.embed, args.distil, \n",
    "                                                                                                     args.mix, args.des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a6966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting= \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64271710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "check data_x shape (8, 20)\n",
      "check data_y shape (8, 20)\n",
      "data_x [[ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 7  6  7 49  6  0  1  2  2  2  6  0  1  0  0  0  2  0  4  0]]\n",
      "data_y [[ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 7  6  7 49  6  0  1  2  2  2  6  0  1  0  0  0  2  0  4  0]]\n",
      "x,y [[ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]] [[ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]] [[ 0.06666667  0.46153846]\n",
      " [ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]] [[-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]]\n",
      "x,y [[ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]] [[13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]] [[ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]] [[-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]]\n",
      "x,y [[ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]] [[14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]] [[-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]] [[-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]\n",
      " [-0.36666667 -0.40384615]]\n",
      "x,y [[13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 7  6  7 49  6  0  1  2  2  2  6  0  1  0  0  0  2  0  4  0]] [[ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 7  6  7 49  6  0  1  2  2  2  6  0  1  0  0  0  2  0  4  0]] [[-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]\n",
      " [-0.36666667 -0.40384615]] [[ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]\n",
      " [-0.36666667 -0.40384615]]\n",
      "(4, 1, 1, 20)\n",
      "[[[[ 6.8321867e+00  4.1660962e+00  5.5001898e+00  1.3873180e+01\n",
      "     4.5615072e+00  1.6799215e+00  1.7307360e+00  1.9218162e+00\n",
      "     1.6953726e+00  1.4328616e+00  1.7674569e+00  1.1856138e+00\n",
      "     7.7103615e-01  3.1425217e-01  2.0257424e-01  6.5961373e-01\n",
      "     1.4129713e+00  3.5161200e-01  1.3668158e+00 -7.2176936e-03]]]\n",
      "\n",
      "\n",
      " [[[ 6.8742909e+00  3.9449117e+00  5.3744726e+00  1.3874529e+01\n",
      "     4.5440998e+00  1.5384877e+00  1.7218031e+00  1.8847367e+00\n",
      "     1.8090762e+00  1.4132628e+00  1.6957563e+00  1.2538692e+00\n",
      "     8.0295253e-01  3.5027868e-01  2.6355246e-01  6.1726761e-01\n",
      "     1.4184737e+00  3.3506823e-01  1.2761524e+00 -3.8309142e-02]]]\n",
      "\n",
      "\n",
      " [[[ 6.8484159e+00  4.1268854e+00  5.3488512e+00  1.3879792e+01\n",
      "     4.6213698e+00  1.5509696e+00  1.6716541e+00  2.0483508e+00\n",
      "     1.7572607e+00  1.5358107e+00  1.7793263e+00  1.2569684e+00\n",
      "     8.8902807e-01  4.5104560e-01  1.3242824e-01  6.5073293e-01\n",
      "     1.2748759e+00  3.2379743e-01  1.2665429e+00 -1.5023375e-02]]]\n",
      "\n",
      "\n",
      " [[[ 6.8119202e+00  4.1185937e+00  5.2671285e+00  1.3943764e+01\n",
      "     4.6129217e+00  1.5520456e+00  1.6060793e+00  2.0029035e+00\n",
      "     1.7593231e+00  1.5158809e+00  1.7232834e+00  1.2177167e+00\n",
      "     8.4486645e-01  4.2140117e-01  1.7160924e-01  5.9498185e-01\n",
      "     1.2837940e+00  3.4905431e-01  1.2570684e+00 -6.2916927e-02]]]]\n"
     ]
    }
   ],
   "source": [
    "exp.predict(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28d869e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aeba6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6.8321867e+00,  4.1660962e+00,  5.5001898e+00,  1.3873180e+01,\n",
       "          4.5615072e+00,  1.6799215e+00,  1.7307360e+00,  1.9218162e+00,\n",
       "          1.6953726e+00,  1.4328616e+00,  1.7674569e+00,  1.1856138e+00,\n",
       "          7.7103615e-01,  3.1425217e-01,  2.0257424e-01,  6.5961373e-01,\n",
       "          1.4129713e+00,  3.5161200e-01,  1.3668158e+00, -7.2176936e-03]],\n",
       "\n",
       "       [[ 6.8742909e+00,  3.9449117e+00,  5.3744726e+00,  1.3874529e+01,\n",
       "          4.5440998e+00,  1.5384877e+00,  1.7218031e+00,  1.8847367e+00,\n",
       "          1.8090762e+00,  1.4132628e+00,  1.6957563e+00,  1.2538692e+00,\n",
       "          8.0295253e-01,  3.5027868e-01,  2.6355246e-01,  6.1726761e-01,\n",
       "          1.4184737e+00,  3.3506823e-01,  1.2761524e+00, -3.8309142e-02]],\n",
       "\n",
       "       [[ 6.8484159e+00,  4.1268854e+00,  5.3488512e+00,  1.3879792e+01,\n",
       "          4.6213698e+00,  1.5509696e+00,  1.6716541e+00,  2.0483508e+00,\n",
       "          1.7572607e+00,  1.5358107e+00,  1.7793263e+00,  1.2569684e+00,\n",
       "          8.8902807e-01,  4.5104560e-01,  1.3242824e-01,  6.5073293e-01,\n",
       "          1.2748759e+00,  3.2379743e-01,  1.2665429e+00, -1.5023375e-02]],\n",
       "\n",
       "       [[ 6.8119202e+00,  4.1185937e+00,  5.2671285e+00,  1.3943764e+01,\n",
       "          4.6129217e+00,  1.5520456e+00,  1.6060793e+00,  2.0029035e+00,\n",
       "          1.7593231e+00,  1.5158809e+00,  1.7232834e+00,  1.2177167e+00,\n",
       "          8.4486645e-01,  4.2140117e-01,  1.7160924e-01,  5.9498185e-01,\n",
       "          1.2837940e+00,  3.4905431e-01,  1.2570684e+00, -6.2916927e-02]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53aa32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d04631",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96df20b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9601b25",
   "metadata": {},
   "source": [
    "# Testing ATD Protocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2273e513",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = atd2022.io.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b24828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = truth[[\"AA\", \"AC\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa970dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Event</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-12-30/2014-01-05</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-06/2014-01-12</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-13/2014-01-19</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-20/2014-01-26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2014-01-27/2014-02-02</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Region                AA                                                      \\\n",
       "Event                 01 02  03  04 05   06 07 08 09 10 11 12 13 14 15 16 17   \n",
       "2013-12-30/2014-01-05  0  0   0   6  0    0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2014-01-06/2014-01-12  2  0   0  11  0    1  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2014-01-13/2014-01-19  0  1   3   7  1    0  0  0  0  0  1  0  0  0  0  0  0   \n",
       "2014-01-20/2014-01-26  0  1   0   4  0    0  0  2  0  0  0  0  0  0  0  1  0   \n",
       "2014-01-27/2014-02-02  6  1  59   3  0  103  0  4  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                         AC                                             \\\n",
       "Event                 18 19 20 01 02 03  04 05 06 07 08 09 10 11 12 13 14 15   \n",
       "2013-12-30/2014-01-05  0  0  0  4  4  6   9  0  0  1  1  1  0  1  0  0  0  0   \n",
       "2014-01-06/2014-01-12  0  0  0  5  0  8  34  2  1  1  0  0  0  0  2  0  0  0   \n",
       "2014-01-13/2014-01-19  0  1  0  1  0  4   3  0  2  0  0  0  0  0  0  0  0  0   \n",
       "2014-01-20/2014-01-26  0  0  0  3  0  6   1  1  0  0  0  0  0  2  0  0  0  0   \n",
       "2014-01-27/2014-02-02  0  0  0  8  2  2  47  6  4  3  0  0  0  0  2  1  0  0   \n",
       "\n",
       "Region                                 \n",
       "Event                 16  17 18 19 20  \n",
       "2013-12-30/2014-01-05  0  11  0  0  0  \n",
       "2014-01-06/2014-01-12  0   4  0  3  0  \n",
       "2014-01-13/2014-01-19  0   1  1  1  0  \n",
       "2014-01-20/2014-01-26  0   0  0  3  0  \n",
       "2014-01-27/2014-02-02  0   3  0  0  0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff8da9cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Event</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-08/2018-01-14</th>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15/2018-01-21</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22/2018-01-28</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29/2018-02-04</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>46</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05/2018-02-11</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>64</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Region                 AA                                                     \\\n",
       "Event                  01 02  03  04 05  06 07 08 09 10 11 12 13 14 15 16 17   \n",
       "2018-01-08/2018-01-14  13  9  20  85  4   2  0  2  8  0  0  0  0  0  0  1  8   \n",
       "2018-01-15/2018-01-21  14  5   8  10  1   0  0  1  0  1  0  1  0  0  0  0  3   \n",
       "2018-01-22/2018-01-28   5  2   0  23  3   2  5  1  0  0  2  0  0  0  0  3  4   \n",
       "2018-01-29/2018-02-04   3  2   7  37  6  10  4  7  0  1  1  1  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11   7  6   7  49  6   0  1  2  2  2  6  0  1  0  0  0  2   \n",
       "\n",
       "Region                          AC                                           \\\n",
       "Event                 18 19 20  01  02  03   04  05 06  07  08 09 10  11 12   \n",
       "2018-01-08/2018-01-14  0  0  0  34   9  14  107   3  2   6   3  0  1   8  7   \n",
       "2018-01-15/2018-01-21  0  2  0  36  11  13   67  18  6   7  12  7  5  20  2   \n",
       "2018-01-22/2018-01-28  0  4  0  24  13   9   40  12  9   4   5  1  3  21  5   \n",
       "2018-01-29/2018-02-04  0  0  0  21  13  12   46  17  6  10   6  1  4  17  1   \n",
       "2018-02-05/2018-02-11  0  4  0  32  12  23   64  13  8   7  28  4  4  12  3   \n",
       "\n",
       "Region                                            \n",
       "Event                  13 14 15 16  17 18  19 20  \n",
       "2018-01-08/2018-01-14  16  1  0  2   3  0  18  0  \n",
       "2018-01-15/2018-01-21   3  0  0  0   1  0   6  0  \n",
       "2018-01-22/2018-01-28   1  1  0  1   2  0   6  0  \n",
       "2018-01-29/2018-02-04   2  1  0  0   9  0   4  0  \n",
       "2018-02-05/2018-02-11   4  0  0  0  15  0   1  0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33ef5ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "\n",
    "args.enc_in = 20 # encoder input size\n",
    "args.dec_in = 20 # decoder input size\n",
    "args.c_out = 20 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 4 # num of encoder layers\n",
    "args.d_layers = 3 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'w'\n",
    "args.inverse=False\n",
    "args.timeenc=0\n",
    "\n",
    "\n",
    "args.use_gpu=True\n",
    "\n",
    "#args.cols=1\n",
    "args.checkpoints = \"/scratch/wzong/atd_tmp\"\n",
    "\n",
    "\n",
    "\n",
    "args.seq_len=96\n",
    "args.label_len=48\n",
    "args.pred_len=1\n",
    "\n",
    "\n",
    "args.batch_size = 64\n",
    "args.learning_rate = 0.00001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False\n",
    "\n",
    "args.itr=20\n",
    "args.train_epochs=250\n",
    "args.patience=25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "75230c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = InformerForcaster(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14137967",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d335f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.14631247520446777\n",
      "Epoch: 1, Steps: 1 | Train Loss: 119.8671188 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.12975621223449707\n",
      "Epoch: 2, Steps: 1 | Train Loss: 118.4760971 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.1277313232421875\n",
      "Epoch: 3, Steps: 1 | Train Loss: 116.5049973 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.12137675285339355\n",
      "Epoch: 4, Steps: 1 | Train Loss: 115.8043213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.12077164649963379\n",
      "Epoch: 5, Steps: 1 | Train Loss: 115.8476791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.12037873268127441\n",
      "Epoch: 6, Steps: 1 | Train Loss: 115.4014893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11705970764160156\n",
      "Epoch: 7, Steps: 1 | Train Loss: 115.5211334 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.11722922325134277\n",
      "Epoch: 8, Steps: 1 | Train Loss: 115.9255981 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11551833152770996\n",
      "Epoch: 9, Steps: 1 | Train Loss: 115.7472458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.12500309944152832\n",
      "Epoch: 10, Steps: 1 | Train Loss: 115.2314453 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.12538576126098633\n",
      "Epoch: 11, Steps: 1 | Train Loss: 116.0759659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11686110496520996\n",
      "Epoch: 12, Steps: 1 | Train Loss: 115.2780304 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.1267557144165039\n",
      "Epoch: 13, Steps: 1 | Train Loss: 116.1205597 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.12801384925842285\n",
      "Epoch: 14, Steps: 1 | Train Loss: 115.3950195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.12350082397460938\n",
      "Epoch: 15, Steps: 1 | Train Loss: 115.8731461 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.11937999725341797\n",
      "Epoch: 16, Steps: 1 | Train Loss: 115.7834244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.11597156524658203\n",
      "Epoch: 17, Steps: 1 | Train Loss: 115.1209641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11522650718688965\n",
      "Epoch: 18, Steps: 1 | Train Loss: 115.4510498 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.11529064178466797\n",
      "Epoch: 19, Steps: 1 | Train Loss: 115.7076645 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.11443185806274414\n",
      "Epoch: 20, Steps: 1 | Train Loss: 115.0332413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11758279800415039\n",
      "Epoch: 21, Steps: 1 | Train Loss: 115.3796310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.11591100692749023\n",
      "Epoch: 22, Steps: 1 | Train Loss: 115.5801163 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11445045471191406\n",
      "Epoch: 23, Steps: 1 | Train Loss: 115.5632477 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11605024337768555\n",
      "Epoch: 24, Steps: 1 | Train Loss: 115.1094284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11420989036560059\n",
      "Epoch: 25, Steps: 1 | Train Loss: 115.8143845 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11308765411376953\n",
      "Epoch: 26, Steps: 1 | Train Loss: 115.0428238 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.12766766548156738\n",
      "Epoch: 27, Steps: 1 | Train Loss: 115.4167709 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.11988353729248047\n",
      "Epoch: 28, Steps: 1 | Train Loss: 115.3962173 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.11625146865844727\n",
      "Epoch: 29, Steps: 1 | Train Loss: 115.5870361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11561775207519531\n",
      "Epoch: 30, Steps: 1 | Train Loss: 115.0838165 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11580467224121094\n",
      "Epoch: 31, Steps: 1 | Train Loss: 115.3544922 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.1157846450805664\n",
      "Epoch: 32, Steps: 1 | Train Loss: 115.3201675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11628150939941406\n",
      "Epoch: 33, Steps: 1 | Train Loss: 115.4429550 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.12781000137329102\n",
      "Epoch: 34, Steps: 1 | Train Loss: 115.8431549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.12993597984313965\n",
      "Epoch: 35, Steps: 1 | Train Loss: 115.5045166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.12912487983703613\n",
      "Epoch: 36, Steps: 1 | Train Loss: 115.2462692 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.11925888061523438\n",
      "Epoch: 37, Steps: 1 | Train Loss: 115.5696793 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11663985252380371\n",
      "Epoch: 38, Steps: 1 | Train Loss: 115.5295639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.115814208984375\n",
      "Epoch: 39, Steps: 1 | Train Loss: 116.0848160 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.11661982536315918\n",
      "Epoch: 40, Steps: 1 | Train Loss: 115.1811523 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.11613917350769043\n",
      "Epoch: 41, Steps: 1 | Train Loss: 115.4446182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.1330118179321289\n",
      "Epoch: 42, Steps: 1 | Train Loss: 115.7110367 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.13024044036865234\n",
      "Epoch: 43, Steps: 1 | Train Loss: 115.6551895 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11713695526123047\n",
      "Epoch: 44, Steps: 1 | Train Loss: 115.5291519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.1173555850982666\n",
      "Epoch: 45, Steps: 1 | Train Loss: 114.9953995 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11980676651000977\n",
      "Epoch: 46, Steps: 1 | Train Loss: 115.2151489 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.10954713821411133\n",
      "Epoch: 47, Steps: 1 | Train Loss: 115.5062408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.11629128456115723\n",
      "Epoch: 48, Steps: 1 | Train Loss: 115.1563034 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.11661291122436523\n",
      "Epoch: 49, Steps: 1 | Train Loss: 115.4856720 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.12957310676574707\n",
      "Epoch: 50, Steps: 1 | Train Loss: 115.9105225 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11800909042358398\n",
      "Epoch: 51, Steps: 1 | Train Loss: 115.1680679 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11665868759155273\n",
      "Epoch: 52, Steps: 1 | Train Loss: 115.8556900 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.11557817459106445\n",
      "Epoch: 53, Steps: 1 | Train Loss: 115.9182663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.11709332466125488\n",
      "Epoch: 54, Steps: 1 | Train Loss: 115.3723755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.11583828926086426\n",
      "Epoch: 55, Steps: 1 | Train Loss: 115.2357407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1169130802154541\n",
      "Epoch: 56, Steps: 1 | Train Loss: 115.3625488 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.12563395500183105\n",
      "Epoch: 57, Steps: 1 | Train Loss: 115.3243561 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.11719727516174316\n",
      "Epoch: 58, Steps: 1 | Train Loss: 115.2209244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11626267433166504\n",
      "Epoch: 59, Steps: 1 | Train Loss: 115.1009750 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11560606956481934\n",
      "Epoch: 60, Steps: 1 | Train Loss: 115.4460220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.1152658462524414\n",
      "Epoch: 61, Steps: 1 | Train Loss: 115.8338394 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.1161184310913086\n",
      "Epoch: 62, Steps: 1 | Train Loss: 115.5335464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11681795120239258\n",
      "Epoch: 63, Steps: 1 | Train Loss: 115.0359726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.12729692459106445\n",
      "Epoch: 64, Steps: 1 | Train Loss: 115.8391037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11742711067199707\n",
      "Epoch: 65, Steps: 1 | Train Loss: 116.0304108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.12105488777160645\n",
      "Epoch: 66, Steps: 1 | Train Loss: 115.5611572 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11640143394470215\n",
      "Epoch: 67, Steps: 1 | Train Loss: 115.0397949 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.11643838882446289\n",
      "Epoch: 68, Steps: 1 | Train Loss: 115.3186798 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.1360607147216797\n",
      "Epoch: 69, Steps: 1 | Train Loss: 115.6872482 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11084151268005371\n",
      "Epoch: 70, Steps: 1 | Train Loss: 115.2064133 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.13103699684143066\n",
      "Epoch: 71, Steps: 1 | Train Loss: 115.2338257 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.11752104759216309\n",
      "Epoch: 72, Steps: 1 | Train Loss: 115.4499893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11641049385070801\n",
      "Epoch: 73, Steps: 1 | Train Loss: 115.4921646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.11611485481262207\n",
      "Epoch: 74, Steps: 1 | Train Loss: 116.0053711 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.1159827709197998\n",
      "Epoch: 75, Steps: 1 | Train Loss: 115.5600357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11622834205627441\n",
      "Epoch: 76, Steps: 1 | Train Loss: 115.7539673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.1162114143371582\n",
      "Epoch: 77, Steps: 1 | Train Loss: 115.1549835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.12424898147583008\n",
      "Epoch: 78, Steps: 1 | Train Loss: 115.1345825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.11568140983581543\n",
      "Epoch: 79, Steps: 1 | Train Loss: 115.8540039 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.11388969421386719\n",
      "Epoch: 80, Steps: 1 | Train Loss: 114.8754425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.11236310005187988\n",
      "Epoch: 81, Steps: 1 | Train Loss: 115.2121811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11493468284606934\n",
      "Epoch: 82, Steps: 1 | Train Loss: 115.7948761 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.1172935962677002\n",
      "Epoch: 83, Steps: 1 | Train Loss: 115.6166000 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.12309980392456055\n",
      "Epoch: 84, Steps: 1 | Train Loss: 116.0251846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.11701583862304688\n",
      "Epoch: 85, Steps: 1 | Train Loss: 115.5848618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.1298985481262207\n",
      "Epoch: 86, Steps: 1 | Train Loss: 115.4800034 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.11977314949035645\n",
      "Epoch: 87, Steps: 1 | Train Loss: 115.9957275 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11707830429077148\n",
      "Epoch: 88, Steps: 1 | Train Loss: 115.0915298 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11790776252746582\n",
      "Epoch: 89, Steps: 1 | Train Loss: 115.5020294 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11702418327331543\n",
      "Epoch: 90, Steps: 1 | Train Loss: 115.0725250 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.11876606941223145\n",
      "Epoch: 91, Steps: 1 | Train Loss: 115.3049850 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.11942362785339355\n",
      "Epoch: 92, Steps: 1 | Train Loss: 115.6631393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.1184227466583252\n",
      "Epoch: 93, Steps: 1 | Train Loss: 115.4367218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.1315934658050537\n",
      "Epoch: 94, Steps: 1 | Train Loss: 115.4839706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.12319779396057129\n",
      "Epoch: 95, Steps: 1 | Train Loss: 115.2152634 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.11838531494140625\n",
      "Epoch: 96, Steps: 1 | Train Loss: 115.7520294 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.11985349655151367\n",
      "Epoch: 97, Steps: 1 | Train Loss: 115.1044693 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.11608457565307617\n",
      "Epoch: 98, Steps: 1 | Train Loss: 115.6442184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.1149284839630127\n",
      "Epoch: 99, Steps: 1 | Train Loss: 115.7084961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11525797843933105\n",
      "Epoch: 100, Steps: 1 | Train Loss: 115.7450790 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.11630630493164062\n",
      "Epoch: 101, Steps: 1 | Train Loss: 115.7054062 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.12045931816101074\n",
      "Epoch: 102, Steps: 1 | Train Loss: 115.0543213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.11555194854736328\n",
      "Epoch: 103, Steps: 1 | Train Loss: 115.2672653 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.13025188446044922\n",
      "Epoch: 104, Steps: 1 | Train Loss: 115.2462921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.12275838851928711\n",
      "Epoch: 105, Steps: 1 | Train Loss: 115.8702774 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.11759567260742188\n",
      "Epoch: 106, Steps: 1 | Train Loss: 115.6335602 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.11444854736328125\n",
      "Epoch: 107, Steps: 1 | Train Loss: 115.2264175 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.11332559585571289\n",
      "Epoch: 108, Steps: 1 | Train Loss: 115.5738754 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.12615728378295898\n",
      "Epoch: 109, Steps: 1 | Train Loss: 115.1070099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.1264636516571045\n",
      "Epoch: 110, Steps: 1 | Train Loss: 115.2304230 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.131150484085083\n",
      "Epoch: 111, Steps: 1 | Train Loss: 115.7197266 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.12993836402893066\n",
      "Epoch: 112, Steps: 1 | Train Loss: 115.4112549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11707162857055664\n",
      "Epoch: 113, Steps: 1 | Train Loss: 115.7557907 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.11343979835510254\n",
      "Epoch: 114, Steps: 1 | Train Loss: 115.4583130 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11373591423034668\n",
      "Epoch: 115, Steps: 1 | Train Loss: 116.0507126 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11362934112548828\n",
      "Epoch: 116, Steps: 1 | Train Loss: 115.2500000 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.11468815803527832\n",
      "Epoch: 117, Steps: 1 | Train Loss: 115.6191940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.11364960670471191\n",
      "Epoch: 118, Steps: 1 | Train Loss: 115.4805832 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.12667202949523926\n",
      "Epoch: 119, Steps: 1 | Train Loss: 116.0899887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1229398250579834\n",
      "Epoch: 120, Steps: 1 | Train Loss: 114.9634933 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11839532852172852\n",
      "Epoch: 121, Steps: 1 | Train Loss: 115.6694260 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11493420600891113\n",
      "Epoch: 122, Steps: 1 | Train Loss: 115.1479492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.11353325843811035\n",
      "Epoch: 123, Steps: 1 | Train Loss: 115.2991714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11444354057312012\n",
      "Epoch: 124, Steps: 1 | Train Loss: 115.5790405 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.11452889442443848\n",
      "Epoch: 125, Steps: 1 | Train Loss: 115.6615219 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.11362552642822266\n",
      "Epoch: 126, Steps: 1 | Train Loss: 115.3670425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11338973045349121\n",
      "Epoch: 127, Steps: 1 | Train Loss: 114.8256378 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.11371922492980957\n",
      "Epoch: 128, Steps: 1 | Train Loss: 115.1144791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.1357128620147705\n",
      "Epoch: 129, Steps: 1 | Train Loss: 115.9313583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.1272571086883545\n",
      "Epoch: 130, Steps: 1 | Train Loss: 115.3551407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11411237716674805\n",
      "Epoch: 131, Steps: 1 | Train Loss: 115.4587402 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.1135396957397461\n",
      "Epoch: 132, Steps: 1 | Train Loss: 115.7774200 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11341094970703125\n",
      "Epoch: 133, Steps: 1 | Train Loss: 115.6457672 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.11424994468688965\n",
      "Epoch: 134, Steps: 1 | Train Loss: 115.4013596 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.11246418952941895\n",
      "Epoch: 135, Steps: 1 | Train Loss: 114.9334946 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.11504840850830078\n",
      "Epoch: 136, Steps: 1 | Train Loss: 115.5193710 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.1136474609375\n",
      "Epoch: 137, Steps: 1 | Train Loss: 115.0179825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.12604498863220215\n",
      "Epoch: 138, Steps: 1 | Train Loss: 115.5803757 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.12879300117492676\n",
      "Epoch: 139, Steps: 1 | Train Loss: 115.7597046 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.11725616455078125\n",
      "Epoch: 140, Steps: 1 | Train Loss: 115.7884750 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.11575770378112793\n",
      "Epoch: 141, Steps: 1 | Train Loss: 115.7011337 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11471223831176758\n",
      "Epoch: 142, Steps: 1 | Train Loss: 115.9819336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.1156163215637207\n",
      "Epoch: 143, Steps: 1 | Train Loss: 116.0526886 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11418008804321289\n",
      "Epoch: 144, Steps: 1 | Train Loss: 115.2169952 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.11477327346801758\n",
      "Epoch: 145, Steps: 1 | Train Loss: 115.6951218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.11986613273620605\n",
      "Epoch: 146, Steps: 1 | Train Loss: 115.6356964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.11610913276672363\n",
      "Epoch: 147, Steps: 1 | Train Loss: 114.7188263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11944293975830078\n",
      "Epoch: 148, Steps: 1 | Train Loss: 116.0895004 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11959624290466309\n",
      "Epoch: 149, Steps: 1 | Train Loss: 115.9155426 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.11683511734008789\n",
      "Epoch: 150, Steps: 1 | Train Loss: 115.5391159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.11462020874023438\n",
      "Epoch: 151, Steps: 1 | Train Loss: 115.2061691 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.11478853225708008\n",
      "Epoch: 152, Steps: 1 | Train Loss: 116.2322998 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10468125343322754\n",
      "Epoch: 153, Steps: 1 | Train Loss: 115.6717529 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10720086097717285\n",
      "Epoch: 154, Steps: 1 | Train Loss: 115.8144302 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10668683052062988\n",
      "Epoch: 155, Steps: 1 | Train Loss: 115.6170807 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.12619733810424805\n",
      "Epoch: 156, Steps: 1 | Train Loss: 115.1161423 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.12874841690063477\n",
      "Epoch: 157, Steps: 1 | Train Loss: 115.3663254 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.12906503677368164\n",
      "Epoch: 158, Steps: 1 | Train Loss: 115.6278839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.1273653507232666\n",
      "Epoch: 159, Steps: 1 | Train Loss: 116.0233536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.12098169326782227\n",
      "Epoch: 160, Steps: 1 | Train Loss: 115.8384552 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.11540031433105469\n",
      "Epoch: 161, Steps: 1 | Train Loss: 115.7205811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10566258430480957\n",
      "Epoch: 162, Steps: 1 | Train Loss: 115.3919830 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.11310267448425293\n",
      "Epoch: 163, Steps: 1 | Train Loss: 115.9333267 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.11586594581604004\n",
      "Epoch: 164, Steps: 1 | Train Loss: 115.6238022 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.11391806602478027\n",
      "Epoch: 165, Steps: 1 | Train Loss: 115.1889191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.12513136863708496\n",
      "Epoch: 166, Steps: 1 | Train Loss: 116.0535431 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11549878120422363\n",
      "Epoch: 167, Steps: 1 | Train Loss: 114.5235825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11456632614135742\n",
      "Epoch: 168, Steps: 1 | Train Loss: 115.3565598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11517333984375\n",
      "Epoch: 169, Steps: 1 | Train Loss: 114.9070053 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.1154320240020752\n",
      "Epoch: 170, Steps: 1 | Train Loss: 115.2169952 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.1155850887298584\n",
      "Epoch: 171, Steps: 1 | Train Loss: 114.9006577 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.12205147743225098\n",
      "Epoch: 172, Steps: 1 | Train Loss: 115.3298569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.12806272506713867\n",
      "Epoch: 173, Steps: 1 | Train Loss: 114.9709091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.12944269180297852\n",
      "Epoch: 174, Steps: 1 | Train Loss: 115.0417404 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.1230318546295166\n",
      "Epoch: 175, Steps: 1 | Train Loss: 115.8186569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.11537957191467285\n",
      "Epoch: 176, Steps: 1 | Train Loss: 115.4074249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.11317300796508789\n",
      "Epoch: 177, Steps: 1 | Train Loss: 115.4185944 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11420989036560059\n",
      "Epoch: 178, Steps: 1 | Train Loss: 116.1653366 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11476778984069824\n",
      "Epoch: 179, Steps: 1 | Train Loss: 116.0860977 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11401987075805664\n",
      "Epoch: 180, Steps: 1 | Train Loss: 115.6329956 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.1148378849029541\n",
      "Epoch: 181, Steps: 1 | Train Loss: 115.2078629 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11511659622192383\n",
      "Epoch: 182, Steps: 1 | Train Loss: 115.8438263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.12851285934448242\n",
      "Epoch: 183, Steps: 1 | Train Loss: 116.2058334 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.1524817943572998\n",
      "Epoch: 184, Steps: 1 | Train Loss: 115.2814713 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.15471744537353516\n",
      "Epoch: 185, Steps: 1 | Train Loss: 115.2810059 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.12833023071289062\n",
      "Epoch: 186, Steps: 1 | Train Loss: 115.0439835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.12305521965026855\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.7599487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.12338852882385254\n",
      "Epoch: 188, Steps: 1 | Train Loss: 115.4893341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.12442445755004883\n",
      "Epoch: 189, Steps: 1 | Train Loss: 115.3326645 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12227988243103027\n",
      "Epoch: 190, Steps: 1 | Train Loss: 115.4545288 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.12602734565734863\n",
      "Epoch: 191, Steps: 1 | Train Loss: 115.0947647 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.137437105178833\n",
      "Epoch: 192, Steps: 1 | Train Loss: 115.2128067 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.13799810409545898\n",
      "Epoch: 193, Steps: 1 | Train Loss: 115.6073990 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.1299586296081543\n",
      "Epoch: 194, Steps: 1 | Train Loss: 115.5190430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.12428426742553711\n",
      "Epoch: 195, Steps: 1 | Train Loss: 114.9334030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11598038673400879\n",
      "Epoch: 196, Steps: 1 | Train Loss: 115.6263657 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11575865745544434\n",
      "Epoch: 197, Steps: 1 | Train Loss: 115.5271759 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11512327194213867\n",
      "Epoch: 198, Steps: 1 | Train Loss: 115.3713150 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.1135413646697998\n",
      "Epoch: 199, Steps: 1 | Train Loss: 116.0568466 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11412525177001953\n",
      "Epoch: 200, Steps: 1 | Train Loss: 115.1025620 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11382389068603516\n",
      "Epoch: 201, Steps: 1 | Train Loss: 115.5789719 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.11377668380737305\n",
      "Epoch: 202, Steps: 1 | Train Loss: 115.5601425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.12680578231811523\n",
      "Epoch: 203, Steps: 1 | Train Loss: 115.6137466 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.12357068061828613\n",
      "Epoch: 204, Steps: 1 | Train Loss: 115.5555954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.11499309539794922\n",
      "Epoch: 205, Steps: 1 | Train Loss: 114.7302780 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.12442159652709961\n",
      "Epoch: 206, Steps: 1 | Train Loss: 115.3713150 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.1157984733581543\n",
      "Epoch: 207, Steps: 1 | Train Loss: 115.7687759 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.1140437126159668\n",
      "Epoch: 208, Steps: 1 | Train Loss: 115.1910782 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11531734466552734\n",
      "Epoch: 209, Steps: 1 | Train Loss: 115.4237671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11849498748779297\n",
      "Epoch: 210, Steps: 1 | Train Loss: 116.1814346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.12869882583618164\n",
      "Epoch: 211, Steps: 1 | Train Loss: 115.6928253 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.1267833709716797\n",
      "Epoch: 212, Steps: 1 | Train Loss: 116.3210602 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.12758398056030273\n",
      "Epoch: 213, Steps: 1 | Train Loss: 115.7397003 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11862802505493164\n",
      "Epoch: 214, Steps: 1 | Train Loss: 116.3296051 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11563801765441895\n",
      "Epoch: 215, Steps: 1 | Train Loss: 116.1848907 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11353421211242676\n",
      "Epoch: 216, Steps: 1 | Train Loss: 115.7515488 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11452889442443848\n",
      "Epoch: 217, Steps: 1 | Train Loss: 115.3078613 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.11423015594482422\n",
      "Epoch: 218, Steps: 1 | Train Loss: 114.6894913 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.1145026683807373\n",
      "Epoch: 219, Steps: 1 | Train Loss: 115.6409836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11209702491760254\n",
      "Epoch: 220, Steps: 1 | Train Loss: 115.9925766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11638283729553223\n",
      "Epoch: 221, Steps: 1 | Train Loss: 115.5295944 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.1245884895324707\n",
      "Epoch: 222, Steps: 1 | Train Loss: 115.2505112 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11603665351867676\n",
      "Epoch: 223, Steps: 1 | Train Loss: 115.3491821 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.13178253173828125\n",
      "Epoch: 224, Steps: 1 | Train Loss: 114.9910278 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11909794807434082\n",
      "Epoch: 225, Steps: 1 | Train Loss: 115.4641953 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.12023353576660156\n",
      "Epoch: 226, Steps: 1 | Train Loss: 115.5035172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11653399467468262\n",
      "Epoch: 227, Steps: 1 | Train Loss: 115.2683487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11463379859924316\n",
      "Epoch: 228, Steps: 1 | Train Loss: 114.8742828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11466217041015625\n",
      "Epoch: 229, Steps: 1 | Train Loss: 115.5433731 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.1286768913269043\n",
      "Epoch: 230, Steps: 1 | Train Loss: 115.6730728 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.12927651405334473\n",
      "Epoch: 231, Steps: 1 | Train Loss: 115.1265869 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.11689949035644531\n",
      "Epoch: 232, Steps: 1 | Train Loss: 115.2670517 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11494708061218262\n",
      "Epoch: 233, Steps: 1 | Train Loss: 115.6925430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11547493934631348\n",
      "Epoch: 234, Steps: 1 | Train Loss: 115.6329880 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11441540718078613\n",
      "Epoch: 235, Steps: 1 | Train Loss: 115.1649094 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11850595474243164\n",
      "Epoch: 236, Steps: 1 | Train Loss: 115.5655518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11735653877258301\n",
      "Epoch: 237, Steps: 1 | Train Loss: 115.6846237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11476874351501465\n",
      "Epoch: 238, Steps: 1 | Train Loss: 115.2168198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.1296226978302002\n",
      "Epoch: 239, Steps: 1 | Train Loss: 115.5802002 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.1291213035583496\n",
      "Epoch: 240, Steps: 1 | Train Loss: 114.9543381 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.11895322799682617\n",
      "Epoch: 241, Steps: 1 | Train Loss: 115.7360992 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.1194159984588623\n",
      "Epoch: 242, Steps: 1 | Train Loss: 115.4443893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.11722278594970703\n",
      "Epoch: 243, Steps: 1 | Train Loss: 115.2919846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.11656975746154785\n",
      "Epoch: 244, Steps: 1 | Train Loss: 115.8168716 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.12054920196533203\n",
      "Epoch: 245, Steps: 1 | Train Loss: 115.6562271 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11958169937133789\n",
      "Epoch: 246, Steps: 1 | Train Loss: 115.5425339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.12780022621154785\n",
      "Epoch: 247, Steps: 1 | Train Loss: 115.6575851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.12842917442321777\n",
      "Epoch: 248, Steps: 1 | Train Loss: 115.7375488 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11595416069030762\n",
      "Epoch: 249, Steps: 1 | Train Loss: 116.0355759 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11596250534057617\n",
      "Epoch: 250, Steps: 1 | Train Loss: 115.5979614 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11291170120239258\n",
      "Epoch: 1, Steps: 1 | Train Loss: 117.9901657 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10674786567687988\n",
      "Epoch: 2, Steps: 1 | Train Loss: 116.7709961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10735797882080078\n",
      "Epoch: 3, Steps: 1 | Train Loss: 115.0829697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10685610771179199\n",
      "Epoch: 4, Steps: 1 | Train Loss: 114.9516983 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10703063011169434\n",
      "Epoch: 5, Steps: 1 | Train Loss: 114.9552231 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.1062157154083252\n",
      "Epoch: 6, Steps: 1 | Train Loss: 114.7576447 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10711002349853516\n",
      "Epoch: 7, Steps: 1 | Train Loss: 114.6727295 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.12119174003601074\n",
      "Epoch: 8, Steps: 1 | Train Loss: 114.0005264 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11447525024414062\n",
      "Epoch: 9, Steps: 1 | Train Loss: 113.9483871 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.1089792251586914\n",
      "Epoch: 10, Steps: 1 | Train Loss: 113.7927246 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10913872718811035\n",
      "Epoch: 11, Steps: 1 | Train Loss: 114.1755905 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11095571517944336\n",
      "Epoch: 12, Steps: 1 | Train Loss: 114.1564255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.12163162231445312\n",
      "Epoch: 13, Steps: 1 | Train Loss: 114.1245117 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10684728622436523\n",
      "Epoch: 14, Steps: 1 | Train Loss: 113.9714508 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10663723945617676\n",
      "Epoch: 15, Steps: 1 | Train Loss: 114.3607407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10697221755981445\n",
      "Epoch: 16, Steps: 1 | Train Loss: 114.4954987 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.10860157012939453\n",
      "Epoch: 17, Steps: 1 | Train Loss: 114.7313843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10727763175964355\n",
      "Epoch: 18, Steps: 1 | Train Loss: 114.3882828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10701155662536621\n",
      "Epoch: 19, Steps: 1 | Train Loss: 114.4345932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.11074137687683105\n",
      "Epoch: 20, Steps: 1 | Train Loss: 114.3369522 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.10874032974243164\n",
      "Epoch: 21, Steps: 1 | Train Loss: 113.8389893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10803890228271484\n",
      "Epoch: 22, Steps: 1 | Train Loss: 114.5210953 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.10758590698242188\n",
      "Epoch: 23, Steps: 1 | Train Loss: 114.0795212 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.10678386688232422\n",
      "Epoch: 24, Steps: 1 | Train Loss: 114.7402954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.10690593719482422\n",
      "Epoch: 25, Steps: 1 | Train Loss: 114.3236618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.10687088966369629\n",
      "Epoch: 26, Steps: 1 | Train Loss: 114.0675583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.10712742805480957\n",
      "Epoch: 27, Steps: 1 | Train Loss: 114.6828842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.10790872573852539\n",
      "Epoch: 28, Steps: 1 | Train Loss: 114.3035049 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10725092887878418\n",
      "Epoch: 29, Steps: 1 | Train Loss: 114.1640015 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.10690832138061523\n",
      "Epoch: 30, Steps: 1 | Train Loss: 114.3284302 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.12158513069152832\n",
      "Epoch: 31, Steps: 1 | Train Loss: 113.7857666 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.11390948295593262\n",
      "Epoch: 32, Steps: 1 | Train Loss: 114.3362579 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10941267013549805\n",
      "Epoch: 33, Steps: 1 | Train Loss: 114.1691513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10840988159179688\n",
      "Epoch: 34, Steps: 1 | Train Loss: 114.0107193 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10751581192016602\n",
      "Epoch: 35, Steps: 1 | Train Loss: 114.3188705 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10709476470947266\n",
      "Epoch: 36, Steps: 1 | Train Loss: 113.7717209 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10771679878234863\n",
      "Epoch: 37, Steps: 1 | Train Loss: 114.1593781 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10705924034118652\n",
      "Epoch: 38, Steps: 1 | Train Loss: 114.4050903 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10901904106140137\n",
      "Epoch: 39, Steps: 1 | Train Loss: 113.9824448 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10764718055725098\n",
      "Epoch: 40, Steps: 1 | Train Loss: 114.9342270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10710978507995605\n",
      "Epoch: 41, Steps: 1 | Train Loss: 113.7866974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10692381858825684\n",
      "Epoch: 42, Steps: 1 | Train Loss: 114.0241928 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.1209266185760498\n",
      "Epoch: 43, Steps: 1 | Train Loss: 114.5146484 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11131811141967773\n",
      "Epoch: 44, Steps: 1 | Train Loss: 114.5083618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10811352729797363\n",
      "Epoch: 45, Steps: 1 | Train Loss: 114.7543259 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.1078648567199707\n",
      "Epoch: 46, Steps: 1 | Train Loss: 113.5620270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.10701727867126465\n",
      "Epoch: 47, Steps: 1 | Train Loss: 114.2801056 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.10666847229003906\n",
      "Epoch: 48, Steps: 1 | Train Loss: 113.6367111 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.10753154754638672\n",
      "Epoch: 49, Steps: 1 | Train Loss: 113.7424850 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11010026931762695\n",
      "Epoch: 50, Steps: 1 | Train Loss: 114.7667465 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.10517740249633789\n",
      "Epoch: 51, Steps: 1 | Train Loss: 114.1755142 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10997772216796875\n",
      "Epoch: 52, Steps: 1 | Train Loss: 113.8803482 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.1091921329498291\n",
      "Epoch: 53, Steps: 1 | Train Loss: 114.0936508 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10985445976257324\n",
      "Epoch: 54, Steps: 1 | Train Loss: 114.1130981 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.12593770027160645\n",
      "Epoch: 55, Steps: 1 | Train Loss: 114.8313217 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.11557459831237793\n",
      "Epoch: 56, Steps: 1 | Train Loss: 114.3026123 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.1119699478149414\n",
      "Epoch: 57, Steps: 1 | Train Loss: 114.1670303 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10992956161499023\n",
      "Epoch: 58, Steps: 1 | Train Loss: 114.2199707 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.1102442741394043\n",
      "Epoch: 59, Steps: 1 | Train Loss: 114.0450668 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10676407814025879\n",
      "Epoch: 60, Steps: 1 | Train Loss: 114.7097168 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.11655569076538086\n",
      "Epoch: 61, Steps: 1 | Train Loss: 113.9881363 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11717605590820312\n",
      "Epoch: 62, Steps: 1 | Train Loss: 114.3138428 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11820006370544434\n",
      "Epoch: 63, Steps: 1 | Train Loss: 114.6784897 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.11895966529846191\n",
      "Epoch: 64, Steps: 1 | Train Loss: 114.1740036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.1073451042175293\n",
      "Epoch: 65, Steps: 1 | Train Loss: 114.5274887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11406302452087402\n",
      "Epoch: 66, Steps: 1 | Train Loss: 114.3834152 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.10898852348327637\n",
      "Epoch: 67, Steps: 1 | Train Loss: 113.8048477 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.1083369255065918\n",
      "Epoch: 68, Steps: 1 | Train Loss: 114.2597885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.11422395706176758\n",
      "Epoch: 69, Steps: 1 | Train Loss: 113.8595963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.1162412166595459\n",
      "Epoch: 70, Steps: 1 | Train Loss: 114.0374985 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.12261533737182617\n",
      "Epoch: 71, Steps: 1 | Train Loss: 114.1268463 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10836005210876465\n",
      "Epoch: 72, Steps: 1 | Train Loss: 114.0033798 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.1070857048034668\n",
      "Epoch: 73, Steps: 1 | Train Loss: 113.9587173 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.10700654983520508\n",
      "Epoch: 74, Steps: 1 | Train Loss: 113.9018173 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.12298274040222168\n",
      "Epoch: 75, Steps: 1 | Train Loss: 113.9104233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.10891103744506836\n",
      "Epoch: 76, Steps: 1 | Train Loss: 114.4136963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.10770893096923828\n",
      "Epoch: 77, Steps: 1 | Train Loss: 114.4080429 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.1082301139831543\n",
      "Epoch: 78, Steps: 1 | Train Loss: 114.5810776 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10896944999694824\n",
      "Epoch: 79, Steps: 1 | Train Loss: 113.9342270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10747909545898438\n",
      "Epoch: 80, Steps: 1 | Train Loss: 114.0139999 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10769128799438477\n",
      "Epoch: 81, Steps: 1 | Train Loss: 113.8987656 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11867737770080566\n",
      "Epoch: 82, Steps: 1 | Train Loss: 114.6866455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.11254048347473145\n",
      "Epoch: 83, Steps: 1 | Train Loss: 114.0046310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.10930156707763672\n",
      "Epoch: 84, Steps: 1 | Train Loss: 114.0399704 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.11006474494934082\n",
      "Epoch: 85, Steps: 1 | Train Loss: 113.9956284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.10883426666259766\n",
      "Epoch: 86, Steps: 1 | Train Loss: 114.2054214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10832476615905762\n",
      "Epoch: 87, Steps: 1 | Train Loss: 113.7827377 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.10756158828735352\n",
      "Epoch: 88, Steps: 1 | Train Loss: 113.8653717 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10763382911682129\n",
      "Epoch: 89, Steps: 1 | Train Loss: 114.0912857 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10790228843688965\n",
      "Epoch: 90, Steps: 1 | Train Loss: 113.8912125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10755610466003418\n",
      "Epoch: 91, Steps: 1 | Train Loss: 114.1719284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10727906227111816\n",
      "Epoch: 92, Steps: 1 | Train Loss: 114.7007980 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.12081265449523926\n",
      "Epoch: 93, Steps: 1 | Train Loss: 114.2507553 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.12067675590515137\n",
      "Epoch: 94, Steps: 1 | Train Loss: 114.0671158 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.12129569053649902\n",
      "Epoch: 95, Steps: 1 | Train Loss: 113.9109497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.12075519561767578\n",
      "Epoch: 96, Steps: 1 | Train Loss: 114.0479507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.11429381370544434\n",
      "Epoch: 97, Steps: 1 | Train Loss: 114.5162125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10975193977355957\n",
      "Epoch: 98, Steps: 1 | Train Loss: 113.9346924 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10860085487365723\n",
      "Epoch: 99, Steps: 1 | Train Loss: 114.9564438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10780572891235352\n",
      "Epoch: 100, Steps: 1 | Train Loss: 113.9184113 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10756659507751465\n",
      "Epoch: 101, Steps: 1 | Train Loss: 114.0217285 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.1073617935180664\n",
      "Epoch: 102, Steps: 1 | Train Loss: 114.7695465 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10723233222961426\n",
      "Epoch: 103, Steps: 1 | Train Loss: 114.5582047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10723376274108887\n",
      "Epoch: 104, Steps: 1 | Train Loss: 114.7232666 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10716986656188965\n",
      "Epoch: 105, Steps: 1 | Train Loss: 114.2273178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10844182968139648\n",
      "Epoch: 106, Steps: 1 | Train Loss: 114.1482468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10833024978637695\n",
      "Epoch: 107, Steps: 1 | Train Loss: 114.0432968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10817337036132812\n",
      "Epoch: 108, Steps: 1 | Train Loss: 113.6510010 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.12144732475280762\n",
      "Epoch: 109, Steps: 1 | Train Loss: 113.7652206 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.12123298645019531\n",
      "Epoch: 110, Steps: 1 | Train Loss: 114.9263687 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11736106872558594\n",
      "Epoch: 111, Steps: 1 | Train Loss: 113.4620285 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.11059689521789551\n",
      "Epoch: 112, Steps: 1 | Train Loss: 114.0588379 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10867810249328613\n",
      "Epoch: 113, Steps: 1 | Train Loss: 114.2988052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.1084146499633789\n",
      "Epoch: 114, Steps: 1 | Train Loss: 113.9873276 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10739755630493164\n",
      "Epoch: 115, Steps: 1 | Train Loss: 113.9010010 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10724043846130371\n",
      "Epoch: 116, Steps: 1 | Train Loss: 113.8502808 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10718631744384766\n",
      "Epoch: 117, Steps: 1 | Train Loss: 114.4998932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10721945762634277\n",
      "Epoch: 118, Steps: 1 | Train Loss: 113.8815536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10785412788391113\n",
      "Epoch: 119, Steps: 1 | Train Loss: 114.9749527 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.10805344581604004\n",
      "Epoch: 120, Steps: 1 | Train Loss: 114.1952438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.10707283020019531\n",
      "Epoch: 121, Steps: 1 | Train Loss: 113.9113312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10721325874328613\n",
      "Epoch: 122, Steps: 1 | Train Loss: 113.5343552 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.1071023941040039\n",
      "Epoch: 123, Steps: 1 | Train Loss: 114.9452057 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.12129354476928711\n",
      "Epoch: 124, Steps: 1 | Train Loss: 114.1663971 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.12083935737609863\n",
      "Epoch: 125, Steps: 1 | Train Loss: 114.2341537 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.12075686454772949\n",
      "Epoch: 126, Steps: 1 | Train Loss: 114.5131378 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11609768867492676\n",
      "Epoch: 127, Steps: 1 | Train Loss: 114.3102798 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.10995149612426758\n",
      "Epoch: 128, Steps: 1 | Train Loss: 114.5692673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10860538482666016\n",
      "Epoch: 129, Steps: 1 | Train Loss: 113.9215469 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10810542106628418\n",
      "Epoch: 130, Steps: 1 | Train Loss: 114.3125381 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.1079413890838623\n",
      "Epoch: 131, Steps: 1 | Train Loss: 113.8422394 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10756611824035645\n",
      "Epoch: 132, Steps: 1 | Train Loss: 114.7408829 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.10715579986572266\n",
      "Epoch: 133, Steps: 1 | Train Loss: 114.1573029 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10728573799133301\n",
      "Epoch: 134, Steps: 1 | Train Loss: 114.5575180 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10776352882385254\n",
      "Epoch: 135, Steps: 1 | Train Loss: 114.7093506 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10776376724243164\n",
      "Epoch: 136, Steps: 1 | Train Loss: 114.0724030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.10755372047424316\n",
      "Epoch: 137, Steps: 1 | Train Loss: 114.0653839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.1074221134185791\n",
      "Epoch: 138, Steps: 1 | Train Loss: 114.1911392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.1070864200592041\n",
      "Epoch: 139, Steps: 1 | Train Loss: 114.0849228 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.1102290153503418\n",
      "Epoch: 140, Steps: 1 | Train Loss: 114.3114395 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.10840535163879395\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.3375473 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.10781407356262207\n",
      "Epoch: 142, Steps: 1 | Train Loss: 114.2168732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.10688114166259766\n",
      "Epoch: 143, Steps: 1 | Train Loss: 114.4312286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.1072838306427002\n",
      "Epoch: 144, Steps: 1 | Train Loss: 114.3873520 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10784411430358887\n",
      "Epoch: 145, Steps: 1 | Train Loss: 114.7172470 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10769820213317871\n",
      "Epoch: 146, Steps: 1 | Train Loss: 113.7415161 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10871124267578125\n",
      "Epoch: 147, Steps: 1 | Train Loss: 114.2577896 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.12284612655639648\n",
      "Epoch: 148, Steps: 1 | Train Loss: 114.4245224 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.1210176944732666\n",
      "Epoch: 149, Steps: 1 | Train Loss: 114.1038361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.11185169219970703\n",
      "Epoch: 150, Steps: 1 | Train Loss: 113.6326294 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10978031158447266\n",
      "Epoch: 151, Steps: 1 | Train Loss: 114.7046661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10856103897094727\n",
      "Epoch: 152, Steps: 1 | Train Loss: 114.1718979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.1076507568359375\n",
      "Epoch: 153, Steps: 1 | Train Loss: 114.0754395 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10739970207214355\n",
      "Epoch: 154, Steps: 1 | Train Loss: 114.2473984 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.1075279712677002\n",
      "Epoch: 155, Steps: 1 | Train Loss: 113.8320694 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10770010948181152\n",
      "Epoch: 156, Steps: 1 | Train Loss: 114.3448639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10753011703491211\n",
      "Epoch: 157, Steps: 1 | Train Loss: 113.8862076 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10756731033325195\n",
      "Epoch: 158, Steps: 1 | Train Loss: 114.1326447 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10828113555908203\n",
      "Epoch: 159, Steps: 1 | Train Loss: 114.3326569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.12321352958679199\n",
      "Epoch: 160, Steps: 1 | Train Loss: 114.1945801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.11258769035339355\n",
      "Epoch: 161, Steps: 1 | Train Loss: 114.1142197 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10947585105895996\n",
      "Epoch: 162, Steps: 1 | Train Loss: 114.1353760 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10871672630310059\n",
      "Epoch: 163, Steps: 1 | Train Loss: 114.4692917 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10771512985229492\n",
      "Epoch: 164, Steps: 1 | Train Loss: 114.2913818 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10812902450561523\n",
      "Epoch: 165, Steps: 1 | Train Loss: 114.5003433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.1073770523071289\n",
      "Epoch: 166, Steps: 1 | Train Loss: 114.6857834 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10724902153015137\n",
      "Epoch: 167, Steps: 1 | Train Loss: 114.0287857 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.10757708549499512\n",
      "Epoch: 168, Steps: 1 | Train Loss: 113.8865585 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.10738396644592285\n",
      "Epoch: 169, Steps: 1 | Train Loss: 114.5516968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.10731816291809082\n",
      "Epoch: 170, Steps: 1 | Train Loss: 114.1159439 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.10730171203613281\n",
      "Epoch: 171, Steps: 1 | Train Loss: 114.5859528 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.1215982437133789\n",
      "Epoch: 172, Steps: 1 | Train Loss: 114.7641220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11747002601623535\n",
      "Epoch: 173, Steps: 1 | Train Loss: 113.8398819 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10999417304992676\n",
      "Epoch: 174, Steps: 1 | Train Loss: 114.2192917 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10855722427368164\n",
      "Epoch: 175, Steps: 1 | Train Loss: 114.7252426 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.10785603523254395\n",
      "Epoch: 176, Steps: 1 | Train Loss: 114.0045547 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10753917694091797\n",
      "Epoch: 177, Steps: 1 | Train Loss: 113.7081833 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10746312141418457\n",
      "Epoch: 178, Steps: 1 | Train Loss: 113.6972275 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10779356956481934\n",
      "Epoch: 179, Steps: 1 | Train Loss: 114.4197922 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10758399963378906\n",
      "Epoch: 180, Steps: 1 | Train Loss: 114.3737564 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.10734415054321289\n",
      "Epoch: 181, Steps: 1 | Train Loss: 114.1324005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10745859146118164\n",
      "Epoch: 182, Steps: 1 | Train Loss: 114.2543259 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10925602912902832\n",
      "Epoch: 183, Steps: 1 | Train Loss: 113.9780045 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.1212303638458252\n",
      "Epoch: 184, Steps: 1 | Train Loss: 113.6892853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.12195682525634766\n",
      "Epoch: 185, Steps: 1 | Train Loss: 114.5082932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11218476295471191\n",
      "Epoch: 186, Steps: 1 | Train Loss: 114.2443619 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10994362831115723\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.1464996 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10763859748840332\n",
      "Epoch: 188, Steps: 1 | Train Loss: 114.2320938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.10791206359863281\n",
      "Epoch: 189, Steps: 1 | Train Loss: 114.4396896 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10763239860534668\n",
      "Epoch: 190, Steps: 1 | Train Loss: 113.9761963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.1077420711517334\n",
      "Epoch: 191, Steps: 1 | Train Loss: 114.2829742 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10783100128173828\n",
      "Epoch: 192, Steps: 1 | Train Loss: 114.7138214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.1074683666229248\n",
      "Epoch: 193, Steps: 1 | Train Loss: 113.9215088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.12208819389343262\n",
      "Epoch: 194, Steps: 1 | Train Loss: 113.8000259 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11215925216674805\n",
      "Epoch: 195, Steps: 1 | Train Loss: 113.6772385 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.10898041725158691\n",
      "Epoch: 196, Steps: 1 | Train Loss: 114.3281479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.10811758041381836\n",
      "Epoch: 197, Steps: 1 | Train Loss: 114.1368408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.10930514335632324\n",
      "Epoch: 198, Steps: 1 | Train Loss: 113.8556519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10796117782592773\n",
      "Epoch: 199, Steps: 1 | Train Loss: 113.9359131 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10759186744689941\n",
      "Epoch: 200, Steps: 1 | Train Loss: 113.8673477 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10736203193664551\n",
      "Epoch: 201, Steps: 1 | Train Loss: 114.0523682 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10747456550598145\n",
      "Epoch: 202, Steps: 1 | Train Loss: 113.4920425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10979151725769043\n",
      "Epoch: 203, Steps: 1 | Train Loss: 114.2169571 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.1072988510131836\n",
      "Epoch: 204, Steps: 1 | Train Loss: 114.1383591 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12108182907104492\n",
      "Epoch: 205, Steps: 1 | Train Loss: 113.8156281 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.1217949390411377\n",
      "Epoch: 206, Steps: 1 | Train Loss: 114.1089096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11099505424499512\n",
      "Epoch: 207, Steps: 1 | Train Loss: 114.3123779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.10925889015197754\n",
      "Epoch: 208, Steps: 1 | Train Loss: 114.4905167 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10828042030334473\n",
      "Epoch: 209, Steps: 1 | Train Loss: 114.2061157 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11061859130859375\n",
      "Epoch: 210, Steps: 1 | Train Loss: 113.7417145 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.11096906661987305\n",
      "Epoch: 211, Steps: 1 | Train Loss: 114.1763229 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11049151420593262\n",
      "Epoch: 212, Steps: 1 | Train Loss: 114.3768539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11090803146362305\n",
      "Epoch: 213, Steps: 1 | Train Loss: 114.3305817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11004781723022461\n",
      "Epoch: 214, Steps: 1 | Train Loss: 114.3612671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11061453819274902\n",
      "Epoch: 215, Steps: 1 | Train Loss: 113.7335739 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.12425398826599121\n",
      "Epoch: 216, Steps: 1 | Train Loss: 114.2473068 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.12397527694702148\n",
      "Epoch: 217, Steps: 1 | Train Loss: 114.3022995 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.11372613906860352\n",
      "Epoch: 218, Steps: 1 | Train Loss: 114.0458908 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.11237668991088867\n",
      "Epoch: 219, Steps: 1 | Train Loss: 114.0671616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11164212226867676\n",
      "Epoch: 220, Steps: 1 | Train Loss: 114.0523682 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11090540885925293\n",
      "Epoch: 221, Steps: 1 | Train Loss: 114.8290405 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10818028450012207\n",
      "Epoch: 222, Steps: 1 | Train Loss: 114.3194504 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10995268821716309\n",
      "Epoch: 223, Steps: 1 | Train Loss: 114.1190948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11066532135009766\n",
      "Epoch: 224, Steps: 1 | Train Loss: 114.0594025 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.10931873321533203\n",
      "Epoch: 225, Steps: 1 | Train Loss: 114.2578354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11068201065063477\n",
      "Epoch: 226, Steps: 1 | Train Loss: 113.8767624 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11067843437194824\n",
      "Epoch: 227, Steps: 1 | Train Loss: 113.8102036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.12531018257141113\n",
      "Epoch: 228, Steps: 1 | Train Loss: 114.7648239 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11881136894226074\n",
      "Epoch: 229, Steps: 1 | Train Loss: 113.8584747 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11137080192565918\n",
      "Epoch: 230, Steps: 1 | Train Loss: 114.0951767 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.10816669464111328\n",
      "Epoch: 231, Steps: 1 | Train Loss: 114.2553101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10900712013244629\n",
      "Epoch: 232, Steps: 1 | Train Loss: 113.7754898 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.10743474960327148\n",
      "Epoch: 233, Steps: 1 | Train Loss: 114.6678238 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.10741996765136719\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.9570312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.1071772575378418\n",
      "Epoch: 235, Steps: 1 | Train Loss: 114.1861115 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10697102546691895\n",
      "Epoch: 236, Steps: 1 | Train Loss: 114.5763779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10691571235656738\n",
      "Epoch: 237, Steps: 1 | Train Loss: 114.7886124 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10703158378601074\n",
      "Epoch: 238, Steps: 1 | Train Loss: 114.2844009 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10791707038879395\n",
      "Epoch: 239, Steps: 1 | Train Loss: 114.2626953 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10802960395812988\n",
      "Epoch: 240, Steps: 1 | Train Loss: 114.3701553 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.12067151069641113\n",
      "Epoch: 241, Steps: 1 | Train Loss: 114.1443634 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.1148378849029541\n",
      "Epoch: 242, Steps: 1 | Train Loss: 114.5658951 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10942840576171875\n",
      "Epoch: 243, Steps: 1 | Train Loss: 113.9610138 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10860538482666016\n",
      "Epoch: 244, Steps: 1 | Train Loss: 113.9488297 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.10735321044921875\n",
      "Epoch: 245, Steps: 1 | Train Loss: 114.1177750 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.1070549488067627\n",
      "Epoch: 246, Steps: 1 | Train Loss: 114.3467941 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.1074068546295166\n",
      "Epoch: 247, Steps: 1 | Train Loss: 114.1807861 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10728621482849121\n",
      "Epoch: 248, Steps: 1 | Train Loss: 113.8165512 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10704803466796875\n",
      "Epoch: 249, Steps: 1 | Train Loss: 114.6908951 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10667991638183594\n",
      "Epoch: 250, Steps: 1 | Train Loss: 113.8816681 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_2>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.28745388984680176\n",
      "Epoch: 1, Steps: 1 | Train Loss: 119.3455200 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.1123349666595459\n",
      "Epoch: 2, Steps: 1 | Train Loss: 117.9000244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11110687255859375\n",
      "Epoch: 3, Steps: 1 | Train Loss: 115.6985016 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.11043310165405273\n",
      "Epoch: 4, Steps: 1 | Train Loss: 114.5686188 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11018991470336914\n",
      "Epoch: 5, Steps: 1 | Train Loss: 114.7305069 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11080622673034668\n",
      "Epoch: 6, Steps: 1 | Train Loss: 114.3084106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11143207550048828\n",
      "Epoch: 7, Steps: 1 | Train Loss: 114.1016159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10845828056335449\n",
      "Epoch: 8, Steps: 1 | Train Loss: 114.2615509 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10794758796691895\n",
      "Epoch: 9, Steps: 1 | Train Loss: 114.2526474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10769152641296387\n",
      "Epoch: 10, Steps: 1 | Train Loss: 114.0647202 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.1086118221282959\n",
      "Epoch: 11, Steps: 1 | Train Loss: 114.2226105 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10836172103881836\n",
      "Epoch: 12, Steps: 1 | Train Loss: 113.9038849 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10827779769897461\n",
      "Epoch: 13, Steps: 1 | Train Loss: 114.2347565 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.12161970138549805\n",
      "Epoch: 14, Steps: 1 | Train Loss: 114.1169586 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.12184786796569824\n",
      "Epoch: 15, Steps: 1 | Train Loss: 113.8973618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12606430053710938\n",
      "Epoch: 16, Steps: 1 | Train Loss: 113.9077911 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.12917113304138184\n",
      "Epoch: 17, Steps: 1 | Train Loss: 114.0870972 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.13028788566589355\n",
      "Epoch: 18, Steps: 1 | Train Loss: 113.6883698 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.1247103214263916\n",
      "Epoch: 19, Steps: 1 | Train Loss: 113.9988327 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.12061905860900879\n",
      "Epoch: 20, Steps: 1 | Train Loss: 113.8682861 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11766576766967773\n",
      "Epoch: 21, Steps: 1 | Train Loss: 114.1705322 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.12060141563415527\n",
      "Epoch: 22, Steps: 1 | Train Loss: 114.1946182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11948442459106445\n",
      "Epoch: 23, Steps: 1 | Train Loss: 113.9601440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11720156669616699\n",
      "Epoch: 24, Steps: 1 | Train Loss: 113.8523178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.12026143074035645\n",
      "Epoch: 25, Steps: 1 | Train Loss: 114.1726837 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11853837966918945\n",
      "Epoch: 26, Steps: 1 | Train Loss: 113.6595688 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11905884742736816\n",
      "Epoch: 27, Steps: 1 | Train Loss: 113.8437042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.12118864059448242\n",
      "Epoch: 28, Steps: 1 | Train Loss: 113.8818359 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.11738753318786621\n",
      "Epoch: 29, Steps: 1 | Train Loss: 113.8314209 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.13092875480651855\n",
      "Epoch: 30, Steps: 1 | Train Loss: 114.4374542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.12193417549133301\n",
      "Epoch: 31, Steps: 1 | Train Loss: 113.9271011 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.11876201629638672\n",
      "Epoch: 32, Steps: 1 | Train Loss: 114.1599121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11407709121704102\n",
      "Epoch: 33, Steps: 1 | Train Loss: 113.8248291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.11235618591308594\n",
      "Epoch: 34, Steps: 1 | Train Loss: 114.4033813 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.11140775680541992\n",
      "Epoch: 35, Steps: 1 | Train Loss: 113.7578964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.11222100257873535\n",
      "Epoch: 36, Steps: 1 | Train Loss: 114.1302872 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10749697685241699\n",
      "Epoch: 37, Steps: 1 | Train Loss: 113.8479004 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10753726959228516\n",
      "Epoch: 38, Steps: 1 | Train Loss: 114.1324005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10731339454650879\n",
      "Epoch: 39, Steps: 1 | Train Loss: 114.1021957 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10706353187561035\n",
      "Epoch: 40, Steps: 1 | Train Loss: 113.6820908 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10709857940673828\n",
      "Epoch: 41, Steps: 1 | Train Loss: 113.6418686 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10727214813232422\n",
      "Epoch: 42, Steps: 1 | Train Loss: 113.6661301 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.12060976028442383\n",
      "Epoch: 43, Steps: 1 | Train Loss: 113.6301270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.1224982738494873\n",
      "Epoch: 44, Steps: 1 | Train Loss: 113.6487961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.12200117111206055\n",
      "Epoch: 45, Steps: 1 | Train Loss: 113.9941559 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.12342190742492676\n",
      "Epoch: 46, Steps: 1 | Train Loss: 114.3717270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.12396526336669922\n",
      "Epoch: 47, Steps: 1 | Train Loss: 113.9731369 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.12044143676757812\n",
      "Epoch: 48, Steps: 1 | Train Loss: 113.9358902 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.1173858642578125\n",
      "Epoch: 49, Steps: 1 | Train Loss: 113.8814850 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11033225059509277\n",
      "Epoch: 50, Steps: 1 | Train Loss: 113.9097214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11033129692077637\n",
      "Epoch: 51, Steps: 1 | Train Loss: 113.9288864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10781335830688477\n",
      "Epoch: 52, Steps: 1 | Train Loss: 114.6039352 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.11080718040466309\n",
      "Epoch: 53, Steps: 1 | Train Loss: 113.8712387 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.11155986785888672\n",
      "Epoch: 54, Steps: 1 | Train Loss: 114.1639938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.1004488468170166\n",
      "Epoch: 55, Steps: 1 | Train Loss: 114.0806808 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1002047061920166\n",
      "Epoch: 56, Steps: 1 | Train Loss: 114.0378647 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10065364837646484\n",
      "Epoch: 57, Steps: 1 | Train Loss: 114.3773727 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10139918327331543\n",
      "Epoch: 58, Steps: 1 | Train Loss: 114.1959457 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.10901379585266113\n",
      "Epoch: 59, Steps: 1 | Train Loss: 113.7806168 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10864853858947754\n",
      "Epoch: 60, Steps: 1 | Train Loss: 113.7940674 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.10807991027832031\n",
      "Epoch: 61, Steps: 1 | Train Loss: 113.8220749 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.10791635513305664\n",
      "Epoch: 62, Steps: 1 | Train Loss: 113.6525650 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.10764479637145996\n",
      "Epoch: 63, Steps: 1 | Train Loss: 114.3325958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.1212313175201416\n",
      "Epoch: 64, Steps: 1 | Train Loss: 114.0156860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11738157272338867\n",
      "Epoch: 65, Steps: 1 | Train Loss: 113.8876266 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11045098304748535\n",
      "Epoch: 66, Steps: 1 | Train Loss: 114.3876495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.10890483856201172\n",
      "Epoch: 67, Steps: 1 | Train Loss: 114.1978302 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.1083223819732666\n",
      "Epoch: 68, Steps: 1 | Train Loss: 114.3034439 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10759711265563965\n",
      "Epoch: 69, Steps: 1 | Train Loss: 114.0166550 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.10780191421508789\n",
      "Epoch: 70, Steps: 1 | Train Loss: 114.2271957 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10764265060424805\n",
      "Epoch: 71, Steps: 1 | Train Loss: 114.2753220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10847973823547363\n",
      "Epoch: 72, Steps: 1 | Train Loss: 114.0520859 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.10799312591552734\n",
      "Epoch: 73, Steps: 1 | Train Loss: 114.1198959 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.10793423652648926\n",
      "Epoch: 74, Steps: 1 | Train Loss: 114.4381332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11749863624572754\n",
      "Epoch: 75, Steps: 1 | Train Loss: 113.9453125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.10988259315490723\n",
      "Epoch: 76, Steps: 1 | Train Loss: 113.9372940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.10851454734802246\n",
      "Epoch: 77, Steps: 1 | Train Loss: 113.8948898 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10803818702697754\n",
      "Epoch: 78, Steps: 1 | Train Loss: 113.8162384 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10880184173583984\n",
      "Epoch: 79, Steps: 1 | Train Loss: 113.8764191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10807943344116211\n",
      "Epoch: 80, Steps: 1 | Train Loss: 113.9980240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10842037200927734\n",
      "Epoch: 81, Steps: 1 | Train Loss: 113.8946762 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.10730624198913574\n",
      "Epoch: 82, Steps: 1 | Train Loss: 114.1260986 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.10738444328308105\n",
      "Epoch: 83, Steps: 1 | Train Loss: 114.2894058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11703705787658691\n",
      "Epoch: 84, Steps: 1 | Train Loss: 114.1071167 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.10982489585876465\n",
      "Epoch: 85, Steps: 1 | Train Loss: 113.9583054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.10752606391906738\n",
      "Epoch: 86, Steps: 1 | Train Loss: 113.8811035 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10753726959228516\n",
      "Epoch: 87, Steps: 1 | Train Loss: 113.9225464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.10750174522399902\n",
      "Epoch: 88, Steps: 1 | Train Loss: 114.1493759 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10753774642944336\n",
      "Epoch: 89, Steps: 1 | Train Loss: 114.1397018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.12019038200378418\n",
      "Epoch: 90, Steps: 1 | Train Loss: 114.1242218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.1138601303100586\n",
      "Epoch: 91, Steps: 1 | Train Loss: 114.0404434 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.1096799373626709\n",
      "Epoch: 92, Steps: 1 | Train Loss: 114.4305649 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.108245849609375\n",
      "Epoch: 93, Steps: 1 | Train Loss: 113.8992310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10768461227416992\n",
      "Epoch: 94, Steps: 1 | Train Loss: 113.7372208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10772919654846191\n",
      "Epoch: 95, Steps: 1 | Train Loss: 113.9882431 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10746598243713379\n",
      "Epoch: 96, Steps: 1 | Train Loss: 114.0575485 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.12610244750976562\n",
      "Epoch: 97, Steps: 1 | Train Loss: 114.3120651 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.11659717559814453\n",
      "Epoch: 98, Steps: 1 | Train Loss: 113.8734665 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10736536979675293\n",
      "Epoch: 99, Steps: 1 | Train Loss: 113.8953247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11558747291564941\n",
      "Epoch: 100, Steps: 1 | Train Loss: 114.0285873 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10956478118896484\n",
      "Epoch: 101, Steps: 1 | Train Loss: 113.7672958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.1083219051361084\n",
      "Epoch: 102, Steps: 1 | Train Loss: 113.9927979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.1078636646270752\n",
      "Epoch: 103, Steps: 1 | Train Loss: 114.2195587 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10774612426757812\n",
      "Epoch: 104, Steps: 1 | Train Loss: 114.4267502 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10770869255065918\n",
      "Epoch: 105, Steps: 1 | Train Loss: 114.0103302 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10770726203918457\n",
      "Epoch: 106, Steps: 1 | Train Loss: 114.1515732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10749197006225586\n",
      "Epoch: 107, Steps: 1 | Train Loss: 114.3601456 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10711860656738281\n",
      "Epoch: 108, Steps: 1 | Train Loss: 114.5534668 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11035919189453125\n",
      "Epoch: 109, Steps: 1 | Train Loss: 114.5686646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.10974431037902832\n",
      "Epoch: 110, Steps: 1 | Train Loss: 114.0563507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.10729718208312988\n",
      "Epoch: 111, Steps: 1 | Train Loss: 114.0631485 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.13246631622314453\n",
      "Epoch: 112, Steps: 1 | Train Loss: 113.9546661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11385273933410645\n",
      "Epoch: 113, Steps: 1 | Train Loss: 113.6920166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.12312889099121094\n",
      "Epoch: 114, Steps: 1 | Train Loss: 113.9319611 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.13871216773986816\n",
      "Epoch: 115, Steps: 1 | Train Loss: 114.3814468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.12156343460083008\n",
      "Epoch: 116, Steps: 1 | Train Loss: 113.8564453 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.1229093074798584\n",
      "Epoch: 117, Steps: 1 | Train Loss: 114.2652817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.12668824195861816\n",
      "Epoch: 118, Steps: 1 | Train Loss: 114.4516830 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.13216447830200195\n",
      "Epoch: 119, Steps: 1 | Train Loss: 114.1468506 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.12641119956970215\n",
      "Epoch: 120, Steps: 1 | Train Loss: 114.0718155 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11465764045715332\n",
      "Epoch: 121, Steps: 1 | Train Loss: 114.3435593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10775446891784668\n",
      "Epoch: 122, Steps: 1 | Train Loss: 114.3121567 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.1077122688293457\n",
      "Epoch: 123, Steps: 1 | Train Loss: 114.2732925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.10731959342956543\n",
      "Epoch: 124, Steps: 1 | Train Loss: 113.5145416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.10722923278808594\n",
      "Epoch: 125, Steps: 1 | Train Loss: 114.1685791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.10739517211914062\n",
      "Epoch: 126, Steps: 1 | Train Loss: 114.0948486 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.10777974128723145\n",
      "Epoch: 127, Steps: 1 | Train Loss: 113.7069092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12099814414978027\n",
      "Epoch: 128, Steps: 1 | Train Loss: 114.2479858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.12206625938415527\n",
      "Epoch: 129, Steps: 1 | Train Loss: 113.8998566 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11292815208435059\n",
      "Epoch: 130, Steps: 1 | Train Loss: 114.0382004 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.10905933380126953\n",
      "Epoch: 131, Steps: 1 | Train Loss: 114.0784683 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10861492156982422\n",
      "Epoch: 132, Steps: 1 | Train Loss: 113.7758560 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.10781455039978027\n",
      "Epoch: 133, Steps: 1 | Train Loss: 113.7115021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10782575607299805\n",
      "Epoch: 134, Steps: 1 | Train Loss: 113.7910080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10726666450500488\n",
      "Epoch: 135, Steps: 1 | Train Loss: 114.3142624 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.1073465347290039\n",
      "Epoch: 136, Steps: 1 | Train Loss: 114.1894531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.10710000991821289\n",
      "Epoch: 137, Steps: 1 | Train Loss: 113.9683380 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.10738253593444824\n",
      "Epoch: 138, Steps: 1 | Train Loss: 113.8613281 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.10833573341369629\n",
      "Epoch: 139, Steps: 1 | Train Loss: 114.3971939 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10765695571899414\n",
      "Epoch: 140, Steps: 1 | Train Loss: 114.1153183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.12090778350830078\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.0723419 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.12072467803955078\n",
      "Epoch: 142, Steps: 1 | Train Loss: 113.7153244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11940217018127441\n",
      "Epoch: 143, Steps: 1 | Train Loss: 114.2851791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11533236503601074\n",
      "Epoch: 144, Steps: 1 | Train Loss: 113.8493195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10948777198791504\n",
      "Epoch: 145, Steps: 1 | Train Loss: 114.1181030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10835623741149902\n",
      "Epoch: 146, Steps: 1 | Train Loss: 114.0249252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10772418975830078\n",
      "Epoch: 147, Steps: 1 | Train Loss: 114.1316452 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10730147361755371\n",
      "Epoch: 148, Steps: 1 | Train Loss: 114.2328644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10712385177612305\n",
      "Epoch: 149, Steps: 1 | Train Loss: 114.0098877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.1076822280883789\n",
      "Epoch: 150, Steps: 1 | Train Loss: 114.1234665 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10684633255004883\n",
      "Epoch: 151, Steps: 1 | Train Loss: 114.0974960 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10687565803527832\n",
      "Epoch: 152, Steps: 1 | Train Loss: 114.0857468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.1068274974822998\n",
      "Epoch: 153, Steps: 1 | Train Loss: 114.1537247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10698819160461426\n",
      "Epoch: 154, Steps: 1 | Train Loss: 114.1573410 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.12091994285583496\n",
      "Epoch: 155, Steps: 1 | Train Loss: 113.9530563 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11945939064025879\n",
      "Epoch: 156, Steps: 1 | Train Loss: 113.8444366 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.11393117904663086\n",
      "Epoch: 157, Steps: 1 | Train Loss: 114.2000961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10934233665466309\n",
      "Epoch: 158, Steps: 1 | Train Loss: 114.2165680 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10849356651306152\n",
      "Epoch: 159, Steps: 1 | Train Loss: 114.1362305 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.10761356353759766\n",
      "Epoch: 160, Steps: 1 | Train Loss: 114.4254379 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10729527473449707\n",
      "Epoch: 161, Steps: 1 | Train Loss: 114.2923126 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10723996162414551\n",
      "Epoch: 162, Steps: 1 | Train Loss: 114.4958267 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.106842041015625\n",
      "Epoch: 163, Steps: 1 | Train Loss: 113.8551025 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10701894760131836\n",
      "Epoch: 164, Steps: 1 | Train Loss: 114.0564957 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10646986961364746\n",
      "Epoch: 165, Steps: 1 | Train Loss: 113.9702148 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.1069631576538086\n",
      "Epoch: 166, Steps: 1 | Train Loss: 113.8684311 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10686445236206055\n",
      "Epoch: 167, Steps: 1 | Train Loss: 114.1044235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.10678911209106445\n",
      "Epoch: 168, Steps: 1 | Train Loss: 113.8591232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.10720229148864746\n",
      "Epoch: 169, Steps: 1 | Train Loss: 113.8361206 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.10699868202209473\n",
      "Epoch: 170, Steps: 1 | Train Loss: 114.4337387 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.1116952896118164\n",
      "Epoch: 171, Steps: 1 | Train Loss: 114.0846939 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.1177682876586914\n",
      "Epoch: 172, Steps: 1 | Train Loss: 113.9449005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.12219786643981934\n",
      "Epoch: 173, Steps: 1 | Train Loss: 114.0925064 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.13050580024719238\n",
      "Epoch: 174, Steps: 1 | Train Loss: 113.8236008 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.12052726745605469\n",
      "Epoch: 175, Steps: 1 | Train Loss: 114.2131348 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.12941813468933105\n",
      "Epoch: 176, Steps: 1 | Train Loss: 113.9133911 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.12013363838195801\n",
      "Epoch: 177, Steps: 1 | Train Loss: 113.9657593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11844873428344727\n",
      "Epoch: 178, Steps: 1 | Train Loss: 114.1968536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10843324661254883\n",
      "Epoch: 179, Steps: 1 | Train Loss: 113.9457550 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11362123489379883\n",
      "Epoch: 180, Steps: 1 | Train Loss: 114.3442383 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11262011528015137\n",
      "Epoch: 181, Steps: 1 | Train Loss: 113.9452057 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10759592056274414\n",
      "Epoch: 182, Steps: 1 | Train Loss: 113.8693848 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11572408676147461\n",
      "Epoch: 183, Steps: 1 | Train Loss: 113.9344711 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.10735321044921875\n",
      "Epoch: 184, Steps: 1 | Train Loss: 114.0106430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11546015739440918\n",
      "Epoch: 185, Steps: 1 | Train Loss: 113.9584961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.10974979400634766\n",
      "Epoch: 186, Steps: 1 | Train Loss: 114.3559113 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10844588279724121\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.2518539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11490082740783691\n",
      "Epoch: 188, Steps: 1 | Train Loss: 113.5871582 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.1074678897857666\n",
      "Epoch: 189, Steps: 1 | Train Loss: 113.7772827 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10801029205322266\n",
      "Epoch: 190, Steps: 1 | Train Loss: 114.1753159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10785484313964844\n",
      "Epoch: 191, Steps: 1 | Train Loss: 113.8296127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.1252892017364502\n",
      "Epoch: 192, Steps: 1 | Train Loss: 114.2775421 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.12260293960571289\n",
      "Epoch: 193, Steps: 1 | Train Loss: 114.3267212 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.12407159805297852\n",
      "Epoch: 194, Steps: 1 | Train Loss: 113.7665176 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.12941217422485352\n",
      "Epoch: 195, Steps: 1 | Train Loss: 113.5805435 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11166071891784668\n",
      "Epoch: 196, Steps: 1 | Train Loss: 114.1955109 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.1081702709197998\n",
      "Epoch: 197, Steps: 1 | Train Loss: 114.1125259 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11815953254699707\n",
      "Epoch: 198, Steps: 1 | Train Loss: 113.8616257 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10933279991149902\n",
      "Epoch: 199, Steps: 1 | Train Loss: 114.1657028 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10959482192993164\n",
      "Epoch: 200, Steps: 1 | Train Loss: 113.9825211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10749363899230957\n",
      "Epoch: 201, Steps: 1 | Train Loss: 114.0290298 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10888075828552246\n",
      "Epoch: 202, Steps: 1 | Train Loss: 114.0122223 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10845732688903809\n",
      "Epoch: 203, Steps: 1 | Train Loss: 113.8039093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.10451436042785645\n",
      "Epoch: 204, Steps: 1 | Train Loss: 113.9099121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.11246705055236816\n",
      "Epoch: 205, Steps: 1 | Train Loss: 113.9697037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10917472839355469\n",
      "Epoch: 206, Steps: 1 | Train Loss: 114.1892929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10485625267028809\n",
      "Epoch: 207, Steps: 1 | Train Loss: 113.8741989 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.11359357833862305\n",
      "Epoch: 208, Steps: 1 | Train Loss: 114.0141830 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11809754371643066\n",
      "Epoch: 209, Steps: 1 | Train Loss: 113.6555557 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.1284775733947754\n",
      "Epoch: 210, Steps: 1 | Train Loss: 113.8311539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.11957669258117676\n",
      "Epoch: 211, Steps: 1 | Train Loss: 113.8435822 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11052298545837402\n",
      "Epoch: 212, Steps: 1 | Train Loss: 113.8919220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11317968368530273\n",
      "Epoch: 213, Steps: 1 | Train Loss: 114.0107574 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10931539535522461\n",
      "Epoch: 214, Steps: 1 | Train Loss: 113.9521637 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10919761657714844\n",
      "Epoch: 215, Steps: 1 | Train Loss: 113.8480225 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10855245590209961\n",
      "Epoch: 216, Steps: 1 | Train Loss: 114.0155640 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10810971260070801\n",
      "Epoch: 217, Steps: 1 | Train Loss: 114.2919235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10852670669555664\n",
      "Epoch: 218, Steps: 1 | Train Loss: 114.0730209 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.1132957935333252\n",
      "Epoch: 219, Steps: 1 | Train Loss: 114.2209625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11654353141784668\n",
      "Epoch: 220, Steps: 1 | Train Loss: 114.1315689 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11704015731811523\n",
      "Epoch: 221, Steps: 1 | Train Loss: 113.7097092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.1081235408782959\n",
      "Epoch: 222, Steps: 1 | Train Loss: 113.7848663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11046051979064941\n",
      "Epoch: 223, Steps: 1 | Train Loss: 114.0691681 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11259579658508301\n",
      "Epoch: 224, Steps: 1 | Train Loss: 113.9669113 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.12098288536071777\n",
      "Epoch: 225, Steps: 1 | Train Loss: 113.6786041 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.1118156909942627\n",
      "Epoch: 226, Steps: 1 | Train Loss: 114.1124496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10737991333007812\n",
      "Epoch: 227, Steps: 1 | Train Loss: 114.2915802 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.107147216796875\n",
      "Epoch: 228, Steps: 1 | Train Loss: 114.3711700 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.10642623901367188\n",
      "Epoch: 229, Steps: 1 | Train Loss: 114.0277710 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.10493850708007812\n",
      "Epoch: 230, Steps: 1 | Train Loss: 114.0875244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.10672163963317871\n",
      "Epoch: 231, Steps: 1 | Train Loss: 114.1110153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10687947273254395\n",
      "Epoch: 232, Steps: 1 | Train Loss: 113.8697510 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.10686612129211426\n",
      "Epoch: 233, Steps: 1 | Train Loss: 114.3112793 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.10707759857177734\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.9977570 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.10750699043273926\n",
      "Epoch: 235, Steps: 1 | Train Loss: 114.0698853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.12054920196533203\n",
      "Epoch: 236, Steps: 1 | Train Loss: 114.2680817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11324930191040039\n",
      "Epoch: 237, Steps: 1 | Train Loss: 113.9704132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10913920402526855\n",
      "Epoch: 238, Steps: 1 | Train Loss: 113.9705811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10956835746765137\n",
      "Epoch: 239, Steps: 1 | Train Loss: 113.8716660 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10754108428955078\n",
      "Epoch: 240, Steps: 1 | Train Loss: 114.0833511 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1076958179473877\n",
      "Epoch: 241, Steps: 1 | Train Loss: 114.2007980 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.1073148250579834\n",
      "Epoch: 242, Steps: 1 | Train Loss: 113.8593521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10717463493347168\n",
      "Epoch: 243, Steps: 1 | Train Loss: 114.2633591 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10810971260070801\n",
      "Epoch: 244, Steps: 1 | Train Loss: 113.8928223 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.10796523094177246\n",
      "Epoch: 245, Steps: 1 | Train Loss: 113.8083496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11080026626586914\n",
      "Epoch: 246, Steps: 1 | Train Loss: 113.8916168 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.12437582015991211\n",
      "Epoch: 247, Steps: 1 | Train Loss: 114.1000519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.1240241527557373\n",
      "Epoch: 248, Steps: 1 | Train Loss: 113.9193726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11548876762390137\n",
      "Epoch: 249, Steps: 1 | Train Loss: 113.8900375 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11204957962036133\n",
      "Epoch: 250, Steps: 1 | Train Loss: 114.0890503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_3>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11382174491882324\n",
      "Epoch: 1, Steps: 1 | Train Loss: 117.7071533 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10818314552307129\n",
      "Epoch: 2, Steps: 1 | Train Loss: 115.4596176 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11023879051208496\n",
      "Epoch: 3, Steps: 1 | Train Loss: 114.2362061 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.1101999282836914\n",
      "Epoch: 4, Steps: 1 | Train Loss: 113.5058823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.1099081039428711\n",
      "Epoch: 5, Steps: 1 | Train Loss: 112.8376694 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11040544509887695\n",
      "Epoch: 6, Steps: 1 | Train Loss: 113.0281525 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11314988136291504\n",
      "Epoch: 7, Steps: 1 | Train Loss: 112.3747482 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10959601402282715\n",
      "Epoch: 8, Steps: 1 | Train Loss: 112.7211227 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11032652854919434\n",
      "Epoch: 9, Steps: 1 | Train Loss: 113.2909317 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.11002349853515625\n",
      "Epoch: 10, Steps: 1 | Train Loss: 113.1920395 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.11019563674926758\n",
      "Epoch: 11, Steps: 1 | Train Loss: 112.6079330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11095738410949707\n",
      "Epoch: 12, Steps: 1 | Train Loss: 113.0650558 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11099386215209961\n",
      "Epoch: 13, Steps: 1 | Train Loss: 112.9777603 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.11044788360595703\n",
      "Epoch: 14, Steps: 1 | Train Loss: 113.0268784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.12412142753601074\n",
      "Epoch: 15, Steps: 1 | Train Loss: 112.6966324 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12549185752868652\n",
      "Epoch: 16, Steps: 1 | Train Loss: 112.6594238 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.11450600624084473\n",
      "Epoch: 17, Steps: 1 | Train Loss: 112.7124023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11191391944885254\n",
      "Epoch: 18, Steps: 1 | Train Loss: 112.6845703 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.11210989952087402\n",
      "Epoch: 19, Steps: 1 | Train Loss: 112.4916000 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.11108636856079102\n",
      "Epoch: 20, Steps: 1 | Train Loss: 112.9073029 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11095404624938965\n",
      "Epoch: 21, Steps: 1 | Train Loss: 112.6619797 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10965657234191895\n",
      "Epoch: 22, Steps: 1 | Train Loss: 113.6081772 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11029434204101562\n",
      "Epoch: 23, Steps: 1 | Train Loss: 112.5538483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11075973510742188\n",
      "Epoch: 24, Steps: 1 | Train Loss: 112.5086288 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.10987544059753418\n",
      "Epoch: 25, Steps: 1 | Train Loss: 113.1659470 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11069393157958984\n",
      "Epoch: 26, Steps: 1 | Train Loss: 112.0575180 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11549234390258789\n",
      "Epoch: 27, Steps: 1 | Train Loss: 112.8395386 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.11155438423156738\n",
      "Epoch: 28, Steps: 1 | Train Loss: 112.3806381 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.11150527000427246\n",
      "Epoch: 29, Steps: 1 | Train Loss: 113.0422897 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.12108445167541504\n",
      "Epoch: 30, Steps: 1 | Train Loss: 112.8323593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11015701293945312\n",
      "Epoch: 31, Steps: 1 | Train Loss: 112.9942627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.11078858375549316\n",
      "Epoch: 32, Steps: 1 | Train Loss: 112.5867920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11046314239501953\n",
      "Epoch: 33, Steps: 1 | Train Loss: 113.3103180 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.12389183044433594\n",
      "Epoch: 34, Steps: 1 | Train Loss: 112.8308334 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.1229562759399414\n",
      "Epoch: 35, Steps: 1 | Train Loss: 113.0744629 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.12374520301818848\n",
      "Epoch: 36, Steps: 1 | Train Loss: 112.5015640 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.11893224716186523\n",
      "Epoch: 37, Steps: 1 | Train Loss: 113.1282120 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11278772354125977\n",
      "Epoch: 38, Steps: 1 | Train Loss: 112.8874741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.11160898208618164\n",
      "Epoch: 39, Steps: 1 | Train Loss: 113.0980453 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.11106109619140625\n",
      "Epoch: 40, Steps: 1 | Train Loss: 112.9833298 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.11071276664733887\n",
      "Epoch: 41, Steps: 1 | Train Loss: 112.5321426 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.1110687255859375\n",
      "Epoch: 42, Steps: 1 | Train Loss: 112.5503693 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.11000752449035645\n",
      "Epoch: 43, Steps: 1 | Train Loss: 113.0201569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10980987548828125\n",
      "Epoch: 44, Steps: 1 | Train Loss: 113.1375732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.1100611686706543\n",
      "Epoch: 45, Steps: 1 | Train Loss: 113.0048218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11086201667785645\n",
      "Epoch: 46, Steps: 1 | Train Loss: 112.6738281 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.1099081039428711\n",
      "Epoch: 47, Steps: 1 | Train Loss: 112.6172867 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.10990238189697266\n",
      "Epoch: 48, Steps: 1 | Train Loss: 112.5042648 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.1240541934967041\n",
      "Epoch: 49, Steps: 1 | Train Loss: 113.0255737 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.12462687492370605\n",
      "Epoch: 50, Steps: 1 | Train Loss: 113.4637451 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.12428903579711914\n",
      "Epoch: 51, Steps: 1 | Train Loss: 112.3829346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11782217025756836\n",
      "Epoch: 52, Steps: 1 | Train Loss: 112.3996201 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.11366081237792969\n",
      "Epoch: 53, Steps: 1 | Train Loss: 112.6573105 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.11176753044128418\n",
      "Epoch: 54, Steps: 1 | Train Loss: 112.6471176 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.11111164093017578\n",
      "Epoch: 55, Steps: 1 | Train Loss: 113.6364136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.10998082160949707\n",
      "Epoch: 56, Steps: 1 | Train Loss: 112.7997055 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.11086606979370117\n",
      "Epoch: 57, Steps: 1 | Train Loss: 112.8779831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10953164100646973\n",
      "Epoch: 58, Steps: 1 | Train Loss: 113.0162354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11095380783081055\n",
      "Epoch: 59, Steps: 1 | Train Loss: 112.8968048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11217474937438965\n",
      "Epoch: 60, Steps: 1 | Train Loss: 112.6621475 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.10967016220092773\n",
      "Epoch: 61, Steps: 1 | Train Loss: 112.9001389 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11093592643737793\n",
      "Epoch: 62, Steps: 1 | Train Loss: 112.9173965 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.12416410446166992\n",
      "Epoch: 63, Steps: 1 | Train Loss: 112.7354126 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.12898898124694824\n",
      "Epoch: 64, Steps: 1 | Train Loss: 112.6778183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.1267096996307373\n",
      "Epoch: 65, Steps: 1 | Train Loss: 112.7974625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.12731027603149414\n",
      "Epoch: 66, Steps: 1 | Train Loss: 112.8371201 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.1277456283569336\n",
      "Epoch: 67, Steps: 1 | Train Loss: 112.5974121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.12787771224975586\n",
      "Epoch: 68, Steps: 1 | Train Loss: 112.7007980 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.13510560989379883\n",
      "Epoch: 69, Steps: 1 | Train Loss: 112.9967041 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.12600064277648926\n",
      "Epoch: 70, Steps: 1 | Train Loss: 112.7927246 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.1259770393371582\n",
      "Epoch: 71, Steps: 1 | Train Loss: 113.4613190 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.1256096363067627\n",
      "Epoch: 72, Steps: 1 | Train Loss: 113.6520538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.12246203422546387\n",
      "Epoch: 73, Steps: 1 | Train Loss: 112.9174576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12317276000976562\n",
      "Epoch: 74, Steps: 1 | Train Loss: 113.2588425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.1230766773223877\n",
      "Epoch: 75, Steps: 1 | Train Loss: 113.2286377 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.1342616081237793\n",
      "Epoch: 76, Steps: 1 | Train Loss: 112.5433121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.1361851692199707\n",
      "Epoch: 77, Steps: 1 | Train Loss: 113.7927780 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.13730216026306152\n",
      "Epoch: 78, Steps: 1 | Train Loss: 112.5947037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.12989020347595215\n",
      "Epoch: 79, Steps: 1 | Train Loss: 113.3064728 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.12354111671447754\n",
      "Epoch: 80, Steps: 1 | Train Loss: 112.9138947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.12447094917297363\n",
      "Epoch: 81, Steps: 1 | Train Loss: 113.5991440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.12456631660461426\n",
      "Epoch: 82, Steps: 1 | Train Loss: 113.0414429 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.12290620803833008\n",
      "Epoch: 83, Steps: 1 | Train Loss: 112.5567245 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.12572121620178223\n",
      "Epoch: 84, Steps: 1 | Train Loss: 112.8041611 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.12363767623901367\n",
      "Epoch: 85, Steps: 1 | Train Loss: 113.1047897 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.12245655059814453\n",
      "Epoch: 86, Steps: 1 | Train Loss: 112.3877182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.12548160552978516\n",
      "Epoch: 87, Steps: 1 | Train Loss: 112.6946564 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.13767170906066895\n",
      "Epoch: 88, Steps: 1 | Train Loss: 112.5038452 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.12807440757751465\n",
      "Epoch: 89, Steps: 1 | Train Loss: 113.0223923 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.13391923904418945\n",
      "Epoch: 90, Steps: 1 | Train Loss: 112.6055832 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.1276228427886963\n",
      "Epoch: 91, Steps: 1 | Train Loss: 112.5547256 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.12408733367919922\n",
      "Epoch: 92, Steps: 1 | Train Loss: 112.9221725 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.12111997604370117\n",
      "Epoch: 93, Steps: 1 | Train Loss: 112.7364273 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.12450957298278809\n",
      "Epoch: 94, Steps: 1 | Train Loss: 112.6943283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.12269783020019531\n",
      "Epoch: 95, Steps: 1 | Train Loss: 112.7499313 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.12097954750061035\n",
      "Epoch: 96, Steps: 1 | Train Loss: 112.5928268 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.12647700309753418\n",
      "Epoch: 97, Steps: 1 | Train Loss: 112.1225357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.12610650062561035\n",
      "Epoch: 98, Steps: 1 | Train Loss: 112.6710358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.11815357208251953\n",
      "Epoch: 99, Steps: 1 | Train Loss: 112.7482681 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.12573003768920898\n",
      "Epoch: 100, Steps: 1 | Train Loss: 112.9691315 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.13178157806396484\n",
      "Epoch: 101, Steps: 1 | Train Loss: 113.1604004 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.13711142539978027\n",
      "Epoch: 102, Steps: 1 | Train Loss: 112.7190704 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.12958645820617676\n",
      "Epoch: 103, Steps: 1 | Train Loss: 112.7490616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.1260373592376709\n",
      "Epoch: 104, Steps: 1 | Train Loss: 112.6075211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10828995704650879\n",
      "Epoch: 105, Steps: 1 | Train Loss: 112.8306656 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10778355598449707\n",
      "Epoch: 106, Steps: 1 | Train Loss: 112.3195419 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10745048522949219\n",
      "Epoch: 107, Steps: 1 | Train Loss: 112.7869797 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10748505592346191\n",
      "Epoch: 108, Steps: 1 | Train Loss: 112.7549362 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.12098884582519531\n",
      "Epoch: 109, Steps: 1 | Train Loss: 112.5471725 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.12665367126464844\n",
      "Epoch: 110, Steps: 1 | Train Loss: 112.9565659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11231732368469238\n",
      "Epoch: 111, Steps: 1 | Train Loss: 112.6279068 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10985016822814941\n",
      "Epoch: 112, Steps: 1 | Train Loss: 112.7769928 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10658788681030273\n",
      "Epoch: 113, Steps: 1 | Train Loss: 113.0204697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.11052155494689941\n",
      "Epoch: 114, Steps: 1 | Train Loss: 113.1793365 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11069631576538086\n",
      "Epoch: 115, Steps: 1 | Train Loss: 112.4239120 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11109423637390137\n",
      "Epoch: 116, Steps: 1 | Train Loss: 113.0533218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.11038923263549805\n",
      "Epoch: 117, Steps: 1 | Train Loss: 113.3000488 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.11040234565734863\n",
      "Epoch: 118, Steps: 1 | Train Loss: 112.6885605 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.1105644702911377\n",
      "Epoch: 119, Steps: 1 | Train Loss: 113.0721207 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.11054635047912598\n",
      "Epoch: 120, Steps: 1 | Train Loss: 113.0594864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11068940162658691\n",
      "Epoch: 121, Steps: 1 | Train Loss: 113.4350815 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11695599555969238\n",
      "Epoch: 122, Steps: 1 | Train Loss: 112.7004929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.12311244010925293\n",
      "Epoch: 123, Steps: 1 | Train Loss: 113.1200104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11720752716064453\n",
      "Epoch: 124, Steps: 1 | Train Loss: 112.7602158 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.11231374740600586\n",
      "Epoch: 125, Steps: 1 | Train Loss: 112.8511353 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.11119246482849121\n",
      "Epoch: 126, Steps: 1 | Train Loss: 112.8006592 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11225748062133789\n",
      "Epoch: 127, Steps: 1 | Train Loss: 112.7280655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.11207437515258789\n",
      "Epoch: 128, Steps: 1 | Train Loss: 112.7942734 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.11377739906311035\n",
      "Epoch: 129, Steps: 1 | Train Loss: 113.2302017 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10805940628051758\n",
      "Epoch: 130, Steps: 1 | Train Loss: 112.7481918 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.1089010238647461\n",
      "Epoch: 131, Steps: 1 | Train Loss: 112.7725220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11315155029296875\n",
      "Epoch: 132, Steps: 1 | Train Loss: 112.9007339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.12303876876831055\n",
      "Epoch: 133, Steps: 1 | Train Loss: 113.6070099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.11444354057312012\n",
      "Epoch: 134, Steps: 1 | Train Loss: 113.1335449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.11061334609985352\n",
      "Epoch: 135, Steps: 1 | Train Loss: 113.2519150 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10959267616271973\n",
      "Epoch: 136, Steps: 1 | Train Loss: 112.9160385 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.11382102966308594\n",
      "Epoch: 137, Steps: 1 | Train Loss: 113.2008591 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11048173904418945\n",
      "Epoch: 138, Steps: 1 | Train Loss: 112.8739853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11063861846923828\n",
      "Epoch: 139, Steps: 1 | Train Loss: 112.6688232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.11063361167907715\n",
      "Epoch: 140, Steps: 1 | Train Loss: 112.8724136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.11013078689575195\n",
      "Epoch: 141, Steps: 1 | Train Loss: 112.9286270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11042332649230957\n",
      "Epoch: 142, Steps: 1 | Train Loss: 112.2473755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11087751388549805\n",
      "Epoch: 143, Steps: 1 | Train Loss: 112.7618790 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11418819427490234\n",
      "Epoch: 144, Steps: 1 | Train Loss: 113.2953033 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.11208701133728027\n",
      "Epoch: 145, Steps: 1 | Train Loss: 112.5088425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.11033248901367188\n",
      "Epoch: 146, Steps: 1 | Train Loss: 112.7771835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.1093142032623291\n",
      "Epoch: 147, Steps: 1 | Train Loss: 113.3876572 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10995888710021973\n",
      "Epoch: 148, Steps: 1 | Train Loss: 112.8364487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11016964912414551\n",
      "Epoch: 149, Steps: 1 | Train Loss: 112.8227310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.11141705513000488\n",
      "Epoch: 150, Steps: 1 | Train Loss: 112.7598038 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.11110544204711914\n",
      "Epoch: 151, Steps: 1 | Train Loss: 112.8149567 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10944080352783203\n",
      "Epoch: 152, Steps: 1 | Train Loss: 112.4282379 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12098813056945801\n",
      "Epoch: 153, Steps: 1 | Train Loss: 112.8430328 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.11328887939453125\n",
      "Epoch: 154, Steps: 1 | Train Loss: 112.9190445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.11060261726379395\n",
      "Epoch: 155, Steps: 1 | Train Loss: 112.2076645 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11083340644836426\n",
      "Epoch: 156, Steps: 1 | Train Loss: 112.9531021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.11100935935974121\n",
      "Epoch: 157, Steps: 1 | Train Loss: 113.1181412 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.11127924919128418\n",
      "Epoch: 158, Steps: 1 | Train Loss: 112.7143097 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.1217339038848877\n",
      "Epoch: 159, Steps: 1 | Train Loss: 113.3395996 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.12414717674255371\n",
      "Epoch: 160, Steps: 1 | Train Loss: 112.8811035 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.1130988597869873\n",
      "Epoch: 161, Steps: 1 | Train Loss: 112.4034195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.1121053695678711\n",
      "Epoch: 162, Steps: 1 | Train Loss: 113.0230255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.11133432388305664\n",
      "Epoch: 163, Steps: 1 | Train Loss: 112.5637207 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.11037588119506836\n",
      "Epoch: 164, Steps: 1 | Train Loss: 112.6184845 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.1103973388671875\n",
      "Epoch: 165, Steps: 1 | Train Loss: 111.8395767 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11043882369995117\n",
      "Epoch: 166, Steps: 1 | Train Loss: 112.6760788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11088705062866211\n",
      "Epoch: 167, Steps: 1 | Train Loss: 112.7952652 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11035346984863281\n",
      "Epoch: 168, Steps: 1 | Train Loss: 112.9587402 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.1113443374633789\n",
      "Epoch: 169, Steps: 1 | Train Loss: 112.9942169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11187553405761719\n",
      "Epoch: 170, Steps: 1 | Train Loss: 112.5386505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11171364784240723\n",
      "Epoch: 171, Steps: 1 | Train Loss: 112.9664688 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11886405944824219\n",
      "Epoch: 172, Steps: 1 | Train Loss: 113.0251007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.1127476692199707\n",
      "Epoch: 173, Steps: 1 | Train Loss: 112.5696411 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11245512962341309\n",
      "Epoch: 174, Steps: 1 | Train Loss: 113.1799316 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.11090493202209473\n",
      "Epoch: 175, Steps: 1 | Train Loss: 112.3327179 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.11075568199157715\n",
      "Epoch: 176, Steps: 1 | Train Loss: 112.6331787 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.1080632209777832\n",
      "Epoch: 177, Steps: 1 | Train Loss: 113.1610107 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10779905319213867\n",
      "Epoch: 178, Steps: 1 | Train Loss: 112.6737289 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10774946212768555\n",
      "Epoch: 179, Steps: 1 | Train Loss: 113.0287704 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10847926139831543\n",
      "Epoch: 180, Steps: 1 | Train Loss: 113.3228531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.1073002815246582\n",
      "Epoch: 181, Steps: 1 | Train Loss: 113.1738815 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10729765892028809\n",
      "Epoch: 182, Steps: 1 | Train Loss: 113.1165314 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10684657096862793\n",
      "Epoch: 183, Steps: 1 | Train Loss: 112.6253433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.1067209243774414\n",
      "Epoch: 184, Steps: 1 | Train Loss: 112.5209503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.12254738807678223\n",
      "Epoch: 185, Steps: 1 | Train Loss: 112.6091080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11533641815185547\n",
      "Epoch: 186, Steps: 1 | Train Loss: 112.5467529 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10930490493774414\n",
      "Epoch: 187, Steps: 1 | Train Loss: 112.6570969 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10837697982788086\n",
      "Epoch: 188, Steps: 1 | Train Loss: 113.1541519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.10755538940429688\n",
      "Epoch: 189, Steps: 1 | Train Loss: 113.4119873 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12416744232177734\n",
      "Epoch: 190, Steps: 1 | Train Loss: 112.7999191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.12461066246032715\n",
      "Epoch: 191, Steps: 1 | Train Loss: 113.2123566 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.12479853630065918\n",
      "Epoch: 192, Steps: 1 | Train Loss: 112.7094727 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.12473416328430176\n",
      "Epoch: 193, Steps: 1 | Train Loss: 113.1825714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.12459492683410645\n",
      "Epoch: 194, Steps: 1 | Train Loss: 112.7585678 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.12613487243652344\n",
      "Epoch: 195, Steps: 1 | Train Loss: 112.8700714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.1249082088470459\n",
      "Epoch: 196, Steps: 1 | Train Loss: 113.2742920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.1270160675048828\n",
      "Epoch: 197, Steps: 1 | Train Loss: 112.7461166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.12531518936157227\n",
      "Epoch: 198, Steps: 1 | Train Loss: 112.7781525 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11671328544616699\n",
      "Epoch: 199, Steps: 1 | Train Loss: 112.3631744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.13664031028747559\n",
      "Epoch: 200, Steps: 1 | Train Loss: 113.2325668 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.1342318058013916\n",
      "Epoch: 201, Steps: 1 | Train Loss: 112.8963776 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.12565159797668457\n",
      "Epoch: 202, Steps: 1 | Train Loss: 112.5427399 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.12232279777526855\n",
      "Epoch: 203, Steps: 1 | Train Loss: 112.7651978 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.12287068367004395\n",
      "Epoch: 204, Steps: 1 | Train Loss: 113.2494507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12420463562011719\n",
      "Epoch: 205, Steps: 1 | Train Loss: 112.7362442 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.12373542785644531\n",
      "Epoch: 206, Steps: 1 | Train Loss: 112.8076096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.12399911880493164\n",
      "Epoch: 207, Steps: 1 | Train Loss: 112.9179916 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.12577605247497559\n",
      "Epoch: 208, Steps: 1 | Train Loss: 112.5566635 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.1369190216064453\n",
      "Epoch: 209, Steps: 1 | Train Loss: 113.1614914 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.13435745239257812\n",
      "Epoch: 210, Steps: 1 | Train Loss: 112.5154800 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.1409907341003418\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.6509399 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11144685745239258\n",
      "Epoch: 212, Steps: 1 | Train Loss: 112.6238022 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.12718820571899414\n",
      "Epoch: 213, Steps: 1 | Train Loss: 113.0402756 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.12370729446411133\n",
      "Epoch: 214, Steps: 1 | Train Loss: 113.2871628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.1244971752166748\n",
      "Epoch: 215, Steps: 1 | Train Loss: 112.7316895 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.12609601020812988\n",
      "Epoch: 216, Steps: 1 | Train Loss: 113.1686554 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.13632750511169434\n",
      "Epoch: 217, Steps: 1 | Train Loss: 113.2630386 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.1354982852935791\n",
      "Epoch: 218, Steps: 1 | Train Loss: 112.9402390 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.12874579429626465\n",
      "Epoch: 219, Steps: 1 | Train Loss: 112.9706573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.12275505065917969\n",
      "Epoch: 220, Steps: 1 | Train Loss: 112.6061783 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.12424755096435547\n",
      "Epoch: 221, Steps: 1 | Train Loss: 113.0255356 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.12525200843811035\n",
      "Epoch: 222, Steps: 1 | Train Loss: 112.6466064 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.12144780158996582\n",
      "Epoch: 223, Steps: 1 | Train Loss: 113.0423126 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.1380162239074707\n",
      "Epoch: 224, Steps: 1 | Train Loss: 112.4664536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.12894654273986816\n",
      "Epoch: 225, Steps: 1 | Train Loss: 112.9105606 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.13405752182006836\n",
      "Epoch: 226, Steps: 1 | Train Loss: 112.7585068 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.1365516185760498\n",
      "Epoch: 227, Steps: 1 | Train Loss: 112.7982101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.1372385025024414\n",
      "Epoch: 228, Steps: 1 | Train Loss: 112.8781738 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.13611960411071777\n",
      "Epoch: 229, Steps: 1 | Train Loss: 112.7328644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.13655829429626465\n",
      "Epoch: 230, Steps: 1 | Train Loss: 112.5854416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.13585591316223145\n",
      "Epoch: 231, Steps: 1 | Train Loss: 112.1343307 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.13200807571411133\n",
      "Epoch: 232, Steps: 1 | Train Loss: 112.8326950 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.12555146217346191\n",
      "Epoch: 233, Steps: 1 | Train Loss: 112.3599167 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.12636399269104004\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.0712433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.12539887428283691\n",
      "Epoch: 235, Steps: 1 | Train Loss: 113.0534210 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.12099003791809082\n",
      "Epoch: 236, Steps: 1 | Train Loss: 113.0062866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10805821418762207\n",
      "Epoch: 237, Steps: 1 | Train Loss: 113.1865234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10789871215820312\n",
      "Epoch: 238, Steps: 1 | Train Loss: 112.5007324 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.11155509948730469\n",
      "Epoch: 239, Steps: 1 | Train Loss: 112.5337677 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10875988006591797\n",
      "Epoch: 240, Steps: 1 | Train Loss: 112.4683380 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1252155303955078\n",
      "Epoch: 241, Steps: 1 | Train Loss: 112.7636032 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.12699294090270996\n",
      "Epoch: 242, Steps: 1 | Train Loss: 113.3884048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.1298065185546875\n",
      "Epoch: 243, Steps: 1 | Train Loss: 112.7469254 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.12833380699157715\n",
      "Epoch: 244, Steps: 1 | Train Loss: 112.9459991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.13768482208251953\n",
      "Epoch: 245, Steps: 1 | Train Loss: 112.8562012 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.13676977157592773\n",
      "Epoch: 246, Steps: 1 | Train Loss: 113.3852310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.13662981986999512\n",
      "Epoch: 247, Steps: 1 | Train Loss: 112.7508545 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.12977910041809082\n",
      "Epoch: 248, Steps: 1 | Train Loss: 112.7289352 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.12736082077026367\n",
      "Epoch: 249, Steps: 1 | Train Loss: 112.6709137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.1266169548034668\n",
      "Epoch: 250, Steps: 1 | Train Loss: 112.9350815 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_4>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.10080432891845703\n",
      "Epoch: 1, Steps: 1 | Train Loss: 119.2606201 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.09942746162414551\n",
      "Epoch: 2, Steps: 1 | Train Loss: 117.5142365 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.12149357795715332\n",
      "Epoch: 3, Steps: 1 | Train Loss: 115.8011017 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10805702209472656\n",
      "Epoch: 4, Steps: 1 | Train Loss: 115.0242233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11038970947265625\n",
      "Epoch: 5, Steps: 1 | Train Loss: 114.9485855 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10990381240844727\n",
      "Epoch: 6, Steps: 1 | Train Loss: 114.7571030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10735964775085449\n",
      "Epoch: 7, Steps: 1 | Train Loss: 114.2124176 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.12040233612060547\n",
      "Epoch: 8, Steps: 1 | Train Loss: 114.4376984 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.12181401252746582\n",
      "Epoch: 9, Steps: 1 | Train Loss: 114.5120010 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.12046432495117188\n",
      "Epoch: 10, Steps: 1 | Train Loss: 113.6844101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.12418556213378906\n",
      "Epoch: 11, Steps: 1 | Train Loss: 114.5300522 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.12236237525939941\n",
      "Epoch: 12, Steps: 1 | Train Loss: 113.8469467 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10590982437133789\n",
      "Epoch: 13, Steps: 1 | Train Loss: 114.4602280 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.1245718002319336\n",
      "Epoch: 14, Steps: 1 | Train Loss: 114.4413452 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.12030839920043945\n",
      "Epoch: 15, Steps: 1 | Train Loss: 113.6963882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.1255931854248047\n",
      "Epoch: 16, Steps: 1 | Train Loss: 114.4289169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.12933754920959473\n",
      "Epoch: 17, Steps: 1 | Train Loss: 114.4074707 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.13221144676208496\n",
      "Epoch: 18, Steps: 1 | Train Loss: 114.2083359 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.1358647346496582\n",
      "Epoch: 19, Steps: 1 | Train Loss: 114.3091354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.13944220542907715\n",
      "Epoch: 20, Steps: 1 | Train Loss: 114.2402725 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.1365034580230713\n",
      "Epoch: 21, Steps: 1 | Train Loss: 114.0974960 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.13715219497680664\n",
      "Epoch: 22, Steps: 1 | Train Loss: 114.1802139 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11860847473144531\n",
      "Epoch: 23, Steps: 1 | Train Loss: 114.1770630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.12357211112976074\n",
      "Epoch: 24, Steps: 1 | Train Loss: 114.1796799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.1217350959777832\n",
      "Epoch: 25, Steps: 1 | Train Loss: 114.0111618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11698198318481445\n",
      "Epoch: 26, Steps: 1 | Train Loss: 114.4183578 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11851763725280762\n",
      "Epoch: 27, Steps: 1 | Train Loss: 113.9192657 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.11582398414611816\n",
      "Epoch: 28, Steps: 1 | Train Loss: 114.0254135 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.12613248825073242\n",
      "Epoch: 29, Steps: 1 | Train Loss: 114.0414047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11795687675476074\n",
      "Epoch: 30, Steps: 1 | Train Loss: 114.0848312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11875677108764648\n",
      "Epoch: 31, Steps: 1 | Train Loss: 114.1669312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.11693072319030762\n",
      "Epoch: 32, Steps: 1 | Train Loss: 114.5641098 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11694836616516113\n",
      "Epoch: 33, Steps: 1 | Train Loss: 114.2477646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.1178445816040039\n",
      "Epoch: 34, Steps: 1 | Train Loss: 114.5904083 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.12065815925598145\n",
      "Epoch: 35, Steps: 1 | Train Loss: 114.6613541 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.12206840515136719\n",
      "Epoch: 36, Steps: 1 | Train Loss: 114.0597153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.11878299713134766\n",
      "Epoch: 37, Steps: 1 | Train Loss: 114.1999435 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.13228344917297363\n",
      "Epoch: 38, Steps: 1 | Train Loss: 115.1009293 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.12287521362304688\n",
      "Epoch: 39, Steps: 1 | Train Loss: 114.5110092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.1210489273071289\n",
      "Epoch: 40, Steps: 1 | Train Loss: 114.8514938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.11787867546081543\n",
      "Epoch: 41, Steps: 1 | Train Loss: 114.2223282 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.11823105812072754\n",
      "Epoch: 42, Steps: 1 | Train Loss: 114.0796432 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.11819005012512207\n",
      "Epoch: 43, Steps: 1 | Train Loss: 114.3064728 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11655545234680176\n",
      "Epoch: 44, Steps: 1 | Train Loss: 114.5544586 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.11991119384765625\n",
      "Epoch: 45, Steps: 1 | Train Loss: 114.5929565 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11963653564453125\n",
      "Epoch: 46, Steps: 1 | Train Loss: 114.7238312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.13131332397460938\n",
      "Epoch: 47, Steps: 1 | Train Loss: 114.2398605 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.12746286392211914\n",
      "Epoch: 48, Steps: 1 | Train Loss: 114.0937500 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.12267732620239258\n",
      "Epoch: 49, Steps: 1 | Train Loss: 114.0727081 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11980724334716797\n",
      "Epoch: 50, Steps: 1 | Train Loss: 113.9225082 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11160516738891602\n",
      "Epoch: 51, Steps: 1 | Train Loss: 114.9239044 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11119890213012695\n",
      "Epoch: 52, Steps: 1 | Train Loss: 114.1175690 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.11071634292602539\n",
      "Epoch: 53, Steps: 1 | Train Loss: 113.9802628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.11192178726196289\n",
      "Epoch: 54, Steps: 1 | Train Loss: 113.8628159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.11270380020141602\n",
      "Epoch: 55, Steps: 1 | Train Loss: 114.0556183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.12448549270629883\n",
      "Epoch: 56, Steps: 1 | Train Loss: 114.5555801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.12579107284545898\n",
      "Epoch: 57, Steps: 1 | Train Loss: 114.8516617 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.1136014461517334\n",
      "Epoch: 58, Steps: 1 | Train Loss: 114.0857315 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11207890510559082\n",
      "Epoch: 59, Steps: 1 | Train Loss: 114.5064468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11010336875915527\n",
      "Epoch: 60, Steps: 1 | Train Loss: 113.8513947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.1106264591217041\n",
      "Epoch: 61, Steps: 1 | Train Loss: 114.3578644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.1108248233795166\n",
      "Epoch: 62, Steps: 1 | Train Loss: 113.9637375 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11103367805480957\n",
      "Epoch: 63, Steps: 1 | Train Loss: 114.3365021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.10935831069946289\n",
      "Epoch: 64, Steps: 1 | Train Loss: 113.7673340 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11099910736083984\n",
      "Epoch: 65, Steps: 1 | Train Loss: 114.5087433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11036324501037598\n",
      "Epoch: 66, Steps: 1 | Train Loss: 114.0186310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11378312110900879\n",
      "Epoch: 67, Steps: 1 | Train Loss: 113.8790512 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.1106882095336914\n",
      "Epoch: 68, Steps: 1 | Train Loss: 114.2453232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.11107683181762695\n",
      "Epoch: 69, Steps: 1 | Train Loss: 114.2539673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.1118173599243164\n",
      "Epoch: 70, Steps: 1 | Train Loss: 114.1522980 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.1109464168548584\n",
      "Epoch: 71, Steps: 1 | Train Loss: 114.6155319 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.11550211906433105\n",
      "Epoch: 72, Steps: 1 | Train Loss: 114.2405319 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.13464045524597168\n",
      "Epoch: 73, Steps: 1 | Train Loss: 114.3306885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.15474724769592285\n",
      "Epoch: 74, Steps: 1 | Train Loss: 114.0027847 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.15278220176696777\n",
      "Epoch: 75, Steps: 1 | Train Loss: 113.8418503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.12412667274475098\n",
      "Epoch: 76, Steps: 1 | Train Loss: 114.0768204 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11204743385314941\n",
      "Epoch: 77, Steps: 1 | Train Loss: 114.3345490 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10974597930908203\n",
      "Epoch: 78, Steps: 1 | Train Loss: 114.1236115 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.11127829551696777\n",
      "Epoch: 79, Steps: 1 | Train Loss: 114.2734604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10997581481933594\n",
      "Epoch: 80, Steps: 1 | Train Loss: 114.0417709 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10982060432434082\n",
      "Epoch: 81, Steps: 1 | Train Loss: 114.4273224 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11076474189758301\n",
      "Epoch: 82, Steps: 1 | Train Loss: 114.2653198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.12436866760253906\n",
      "Epoch: 83, Steps: 1 | Train Loss: 114.0150986 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.12109875679016113\n",
      "Epoch: 84, Steps: 1 | Train Loss: 114.5751114 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.11336517333984375\n",
      "Epoch: 85, Steps: 1 | Train Loss: 114.0695801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11200141906738281\n",
      "Epoch: 86, Steps: 1 | Train Loss: 114.2428207 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.11198949813842773\n",
      "Epoch: 87, Steps: 1 | Train Loss: 114.2897263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11139273643493652\n",
      "Epoch: 88, Steps: 1 | Train Loss: 114.1945801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11147665977478027\n",
      "Epoch: 89, Steps: 1 | Train Loss: 114.0889282 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11207342147827148\n",
      "Epoch: 90, Steps: 1 | Train Loss: 114.2822037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.11124634742736816\n",
      "Epoch: 91, Steps: 1 | Train Loss: 114.4540558 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.11176323890686035\n",
      "Epoch: 92, Steps: 1 | Train Loss: 113.9101181 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.1108999252319336\n",
      "Epoch: 93, Steps: 1 | Train Loss: 114.3095016 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.12466883659362793\n",
      "Epoch: 94, Steps: 1 | Train Loss: 113.7496948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.12370491027832031\n",
      "Epoch: 95, Steps: 1 | Train Loss: 114.1115341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.11681509017944336\n",
      "Epoch: 96, Steps: 1 | Train Loss: 114.2747726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.1134650707244873\n",
      "Epoch: 97, Steps: 1 | Train Loss: 114.5283127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.1105649471282959\n",
      "Epoch: 98, Steps: 1 | Train Loss: 114.3253937 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.11114144325256348\n",
      "Epoch: 99, Steps: 1 | Train Loss: 114.1973267 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11054730415344238\n",
      "Epoch: 100, Steps: 1 | Train Loss: 114.1474228 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10939383506774902\n",
      "Epoch: 101, Steps: 1 | Train Loss: 114.0360031 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.11015129089355469\n",
      "Epoch: 102, Steps: 1 | Train Loss: 114.3415756 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.11142492294311523\n",
      "Epoch: 103, Steps: 1 | Train Loss: 114.3427124 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10955286026000977\n",
      "Epoch: 104, Steps: 1 | Train Loss: 114.0660629 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.11056876182556152\n",
      "Epoch: 105, Steps: 1 | Train Loss: 113.8595734 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.1236879825592041\n",
      "Epoch: 106, Steps: 1 | Train Loss: 113.8680954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.12364864349365234\n",
      "Epoch: 107, Steps: 1 | Train Loss: 114.1004028 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.12145352363586426\n",
      "Epoch: 108, Steps: 1 | Train Loss: 114.0859909 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.1136474609375\n",
      "Epoch: 109, Steps: 1 | Train Loss: 114.3041382 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.11221623420715332\n",
      "Epoch: 110, Steps: 1 | Train Loss: 114.6660156 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11019492149353027\n",
      "Epoch: 111, Steps: 1 | Train Loss: 114.7170639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.11036467552185059\n",
      "Epoch: 112, Steps: 1 | Train Loss: 114.2246094 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.1102755069732666\n",
      "Epoch: 113, Steps: 1 | Train Loss: 114.5772324 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.11077070236206055\n",
      "Epoch: 114, Steps: 1 | Train Loss: 114.2660675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11129021644592285\n",
      "Epoch: 115, Steps: 1 | Train Loss: 115.0214844 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11078691482543945\n",
      "Epoch: 116, Steps: 1 | Train Loss: 114.4598160 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.12464356422424316\n",
      "Epoch: 117, Steps: 1 | Train Loss: 114.5923843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.12461972236633301\n",
      "Epoch: 118, Steps: 1 | Train Loss: 114.2196655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.11508965492248535\n",
      "Epoch: 119, Steps: 1 | Train Loss: 113.9107285 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.11237382888793945\n",
      "Epoch: 120, Steps: 1 | Train Loss: 114.2522583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.1109471321105957\n",
      "Epoch: 121, Steps: 1 | Train Loss: 114.1277618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11184906959533691\n",
      "Epoch: 122, Steps: 1 | Train Loss: 114.4335938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.11040496826171875\n",
      "Epoch: 123, Steps: 1 | Train Loss: 114.2007675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11117291450500488\n",
      "Epoch: 124, Steps: 1 | Train Loss: 114.1512680 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.11013126373291016\n",
      "Epoch: 125, Steps: 1 | Train Loss: 113.7152328 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.1101381778717041\n",
      "Epoch: 126, Steps: 1 | Train Loss: 114.4350204 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11035013198852539\n",
      "Epoch: 127, Steps: 1 | Train Loss: 114.3101730 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.10944199562072754\n",
      "Epoch: 128, Steps: 1 | Train Loss: 114.4687119 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.12384843826293945\n",
      "Epoch: 129, Steps: 1 | Train Loss: 114.3375244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.12339353561401367\n",
      "Epoch: 130, Steps: 1 | Train Loss: 114.1694870 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11719274520874023\n",
      "Epoch: 131, Steps: 1 | Train Loss: 114.3367844 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11265683174133301\n",
      "Epoch: 132, Steps: 1 | Train Loss: 113.9511948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11155247688293457\n",
      "Epoch: 133, Steps: 1 | Train Loss: 114.3979416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.11050677299499512\n",
      "Epoch: 134, Steps: 1 | Train Loss: 114.0188751 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10915756225585938\n",
      "Epoch: 135, Steps: 1 | Train Loss: 114.3679428 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.11100316047668457\n",
      "Epoch: 136, Steps: 1 | Train Loss: 114.4508286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.1055765151977539\n",
      "Epoch: 137, Steps: 1 | Train Loss: 114.4315720 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.10676693916320801\n",
      "Epoch: 138, Steps: 1 | Train Loss: 114.2527466 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.10732793807983398\n",
      "Epoch: 139, Steps: 1 | Train Loss: 114.2368393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10707569122314453\n",
      "Epoch: 140, Steps: 1 | Train Loss: 114.1439972 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.12091755867004395\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.1063461 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11493301391601562\n",
      "Epoch: 142, Steps: 1 | Train Loss: 113.9603424 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.1095590591430664\n",
      "Epoch: 143, Steps: 1 | Train Loss: 114.5829391 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.10871005058288574\n",
      "Epoch: 144, Steps: 1 | Train Loss: 114.0524445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10747790336608887\n",
      "Epoch: 145, Steps: 1 | Train Loss: 114.5581284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.1072690486907959\n",
      "Epoch: 146, Steps: 1 | Train Loss: 114.0017853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10730338096618652\n",
      "Epoch: 147, Steps: 1 | Train Loss: 113.8747940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10839962959289551\n",
      "Epoch: 148, Steps: 1 | Train Loss: 113.8669205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10758423805236816\n",
      "Epoch: 149, Steps: 1 | Train Loss: 113.9787521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10782408714294434\n",
      "Epoch: 150, Steps: 1 | Train Loss: 114.2184219 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10712289810180664\n",
      "Epoch: 151, Steps: 1 | Train Loss: 113.9658966 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10752129554748535\n",
      "Epoch: 152, Steps: 1 | Train Loss: 113.7683868 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10749268531799316\n",
      "Epoch: 153, Steps: 1 | Train Loss: 114.1273727 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10748648643493652\n",
      "Epoch: 154, Steps: 1 | Train Loss: 114.1919327 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10761499404907227\n",
      "Epoch: 155, Steps: 1 | Train Loss: 114.2597198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.1076502799987793\n",
      "Epoch: 156, Steps: 1 | Train Loss: 113.8399429 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10730147361755371\n",
      "Epoch: 157, Steps: 1 | Train Loss: 114.3098297 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10712027549743652\n",
      "Epoch: 158, Steps: 1 | Train Loss: 114.2160416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10699272155761719\n",
      "Epoch: 159, Steps: 1 | Train Loss: 114.7069321 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11066722869873047\n",
      "Epoch: 160, Steps: 1 | Train Loss: 114.2352295 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10865044593811035\n",
      "Epoch: 161, Steps: 1 | Train Loss: 114.2884293 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10788822174072266\n",
      "Epoch: 162, Steps: 1 | Train Loss: 114.1457672 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10732746124267578\n",
      "Epoch: 163, Steps: 1 | Train Loss: 114.2828293 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10709500312805176\n",
      "Epoch: 164, Steps: 1 | Train Loss: 113.8115005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10693192481994629\n",
      "Epoch: 165, Steps: 1 | Train Loss: 114.1488647 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.10713648796081543\n",
      "Epoch: 166, Steps: 1 | Train Loss: 114.7355499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10721492767333984\n",
      "Epoch: 167, Steps: 1 | Train Loss: 114.3014450 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.1070871353149414\n",
      "Epoch: 168, Steps: 1 | Train Loss: 114.0017319 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.10695838928222656\n",
      "Epoch: 169, Steps: 1 | Train Loss: 114.5909653 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.1207127571105957\n",
      "Epoch: 170, Steps: 1 | Train Loss: 113.9589615 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.12090492248535156\n",
      "Epoch: 171, Steps: 1 | Train Loss: 114.0140610 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.12130928039550781\n",
      "Epoch: 172, Steps: 1 | Train Loss: 114.1552658 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11312317848205566\n",
      "Epoch: 173, Steps: 1 | Train Loss: 114.1369781 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10909605026245117\n",
      "Epoch: 174, Steps: 1 | Train Loss: 114.6126938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10832953453063965\n",
      "Epoch: 175, Steps: 1 | Train Loss: 114.1384430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.10790681838989258\n",
      "Epoch: 176, Steps: 1 | Train Loss: 114.0884399 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10711050033569336\n",
      "Epoch: 177, Steps: 1 | Train Loss: 114.2354660 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10726308822631836\n",
      "Epoch: 178, Steps: 1 | Train Loss: 114.4021835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10713601112365723\n",
      "Epoch: 179, Steps: 1 | Train Loss: 114.2345200 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10689735412597656\n",
      "Epoch: 180, Steps: 1 | Train Loss: 113.9601822 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.10739326477050781\n",
      "Epoch: 181, Steps: 1 | Train Loss: 114.2563858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.107177734375\n",
      "Epoch: 182, Steps: 1 | Train Loss: 114.4414673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10751771926879883\n",
      "Epoch: 183, Steps: 1 | Train Loss: 114.2309341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.1068568229675293\n",
      "Epoch: 184, Steps: 1 | Train Loss: 114.3392258 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.10707736015319824\n",
      "Epoch: 185, Steps: 1 | Train Loss: 114.4074631 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.12069511413574219\n",
      "Epoch: 186, Steps: 1 | Train Loss: 114.1882095 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.12114810943603516\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.2957764 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.12064075469970703\n",
      "Epoch: 188, Steps: 1 | Train Loss: 113.9721298 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.1141502857208252\n",
      "Epoch: 189, Steps: 1 | Train Loss: 113.9481964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10963797569274902\n",
      "Epoch: 190, Steps: 1 | Train Loss: 114.2208023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10817790031433105\n",
      "Epoch: 191, Steps: 1 | Train Loss: 114.0194321 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10756659507751465\n",
      "Epoch: 192, Steps: 1 | Train Loss: 114.0413132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.10762977600097656\n",
      "Epoch: 193, Steps: 1 | Train Loss: 114.3766098 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.10744738578796387\n",
      "Epoch: 194, Steps: 1 | Train Loss: 113.6572037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10724782943725586\n",
      "Epoch: 195, Steps: 1 | Train Loss: 114.8036880 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.10686898231506348\n",
      "Epoch: 196, Steps: 1 | Train Loss: 114.1322174 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.10723757743835449\n",
      "Epoch: 197, Steps: 1 | Train Loss: 114.1348648 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.10715961456298828\n",
      "Epoch: 198, Steps: 1 | Train Loss: 114.6186447 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10765194892883301\n",
      "Epoch: 199, Steps: 1 | Train Loss: 114.1617203 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10718154907226562\n",
      "Epoch: 200, Steps: 1 | Train Loss: 113.6667480 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10697507858276367\n",
      "Epoch: 201, Steps: 1 | Train Loss: 114.2066650 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10731744766235352\n",
      "Epoch: 202, Steps: 1 | Train Loss: 114.2256393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.12069892883300781\n",
      "Epoch: 203, Steps: 1 | Train Loss: 115.1384048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.12119030952453613\n",
      "Epoch: 204, Steps: 1 | Train Loss: 114.3043594 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12072396278381348\n",
      "Epoch: 205, Steps: 1 | Train Loss: 114.2728043 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.11149907112121582\n",
      "Epoch: 206, Steps: 1 | Train Loss: 114.4720230 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10929036140441895\n",
      "Epoch: 207, Steps: 1 | Train Loss: 114.1402969 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.10907435417175293\n",
      "Epoch: 208, Steps: 1 | Train Loss: 114.1380386 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10785722732543945\n",
      "Epoch: 209, Steps: 1 | Train Loss: 114.2556992 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.10735702514648438\n",
      "Epoch: 210, Steps: 1 | Train Loss: 113.8042755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.10706424713134766\n",
      "Epoch: 211, Steps: 1 | Train Loss: 114.2845840 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.10730481147766113\n",
      "Epoch: 212, Steps: 1 | Train Loss: 114.2204132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.1069798469543457\n",
      "Epoch: 213, Steps: 1 | Train Loss: 114.2403564 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10683727264404297\n",
      "Epoch: 214, Steps: 1 | Train Loss: 113.8385010 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10752677917480469\n",
      "Epoch: 215, Steps: 1 | Train Loss: 114.3896866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10750222206115723\n",
      "Epoch: 216, Steps: 1 | Train Loss: 114.1709518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10695099830627441\n",
      "Epoch: 217, Steps: 1 | Train Loss: 114.5171127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.12082028388977051\n",
      "Epoch: 218, Steps: 1 | Train Loss: 114.7269211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.12167096138000488\n",
      "Epoch: 219, Steps: 1 | Train Loss: 114.2088242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.1207435131072998\n",
      "Epoch: 220, Steps: 1 | Train Loss: 114.3564682 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11492013931274414\n",
      "Epoch: 221, Steps: 1 | Train Loss: 114.4357300 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10930442810058594\n",
      "Epoch: 222, Steps: 1 | Train Loss: 114.1368942 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10831022262573242\n",
      "Epoch: 223, Steps: 1 | Train Loss: 114.3319855 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.10764551162719727\n",
      "Epoch: 224, Steps: 1 | Train Loss: 114.5693893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.10762286186218262\n",
      "Epoch: 225, Steps: 1 | Train Loss: 114.2646408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.10695385932922363\n",
      "Epoch: 226, Steps: 1 | Train Loss: 114.1080475 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10700130462646484\n",
      "Epoch: 227, Steps: 1 | Train Loss: 113.7256851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.10694313049316406\n",
      "Epoch: 228, Steps: 1 | Train Loss: 114.0157013 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.10689759254455566\n",
      "Epoch: 229, Steps: 1 | Train Loss: 114.2970963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.10692000389099121\n",
      "Epoch: 230, Steps: 1 | Train Loss: 114.4104538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.10699963569641113\n",
      "Epoch: 231, Steps: 1 | Train Loss: 114.2010422 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10680079460144043\n",
      "Epoch: 232, Steps: 1 | Train Loss: 114.1856232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11191678047180176\n",
      "Epoch: 233, Steps: 1 | Train Loss: 114.1415329 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.10879015922546387\n",
      "Epoch: 234, Steps: 1 | Train Loss: 114.2181168 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.10788941383361816\n",
      "Epoch: 235, Steps: 1 | Train Loss: 113.7928696 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10768651962280273\n",
      "Epoch: 236, Steps: 1 | Train Loss: 114.2663345 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10766196250915527\n",
      "Epoch: 237, Steps: 1 | Train Loss: 113.9381332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10745835304260254\n",
      "Epoch: 238, Steps: 1 | Train Loss: 114.1882858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10722827911376953\n",
      "Epoch: 239, Steps: 1 | Train Loss: 114.0753174 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10722923278808594\n",
      "Epoch: 240, Steps: 1 | Train Loss: 114.2386398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.10744643211364746\n",
      "Epoch: 241, Steps: 1 | Train Loss: 114.3994141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.12182426452636719\n",
      "Epoch: 242, Steps: 1 | Train Loss: 114.6259384 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.11200928688049316\n",
      "Epoch: 243, Steps: 1 | Train Loss: 113.9141159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10907196998596191\n",
      "Epoch: 244, Steps: 1 | Train Loss: 113.9404831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.10810637474060059\n",
      "Epoch: 245, Steps: 1 | Train Loss: 114.6156387 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.1074974536895752\n",
      "Epoch: 246, Steps: 1 | Train Loss: 114.2117920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.10701608657836914\n",
      "Epoch: 247, Steps: 1 | Train Loss: 114.1625137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.1070716381072998\n",
      "Epoch: 248, Steps: 1 | Train Loss: 114.0931625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10715222358703613\n",
      "Epoch: 249, Steps: 1 | Train Loss: 113.7479248 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10692381858825684\n",
      "Epoch: 250, Steps: 1 | Train Loss: 113.8101196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_5>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.1090235710144043\n",
      "Epoch: 1, Steps: 1 | Train Loss: 118.1632233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.1117098331451416\n",
      "Epoch: 2, Steps: 1 | Train Loss: 115.5933151 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11048555374145508\n",
      "Epoch: 3, Steps: 1 | Train Loss: 114.9361115 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.11345458030700684\n",
      "Epoch: 4, Steps: 1 | Train Loss: 113.4913559 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11753106117248535\n",
      "Epoch: 5, Steps: 1 | Train Loss: 113.3050079 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11411380767822266\n",
      "Epoch: 6, Steps: 1 | Train Loss: 112.9954758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.12999963760375977\n",
      "Epoch: 7, Steps: 1 | Train Loss: 112.3022995 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.12636995315551758\n",
      "Epoch: 8, Steps: 1 | Train Loss: 112.9451065 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11582040786743164\n",
      "Epoch: 9, Steps: 1 | Train Loss: 113.0828171 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10084843635559082\n",
      "Epoch: 10, Steps: 1 | Train Loss: 112.3635025 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10064220428466797\n",
      "Epoch: 11, Steps: 1 | Train Loss: 113.0312729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10066843032836914\n",
      "Epoch: 12, Steps: 1 | Train Loss: 112.2865982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10030961036682129\n",
      "Epoch: 13, Steps: 1 | Train Loss: 112.7967300 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10064029693603516\n",
      "Epoch: 14, Steps: 1 | Train Loss: 112.5861206 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10060429573059082\n",
      "Epoch: 15, Steps: 1 | Train Loss: 112.6724396 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10035514831542969\n",
      "Epoch: 16, Steps: 1 | Train Loss: 112.9384995 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.11280202865600586\n",
      "Epoch: 17, Steps: 1 | Train Loss: 113.0484390 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10322427749633789\n",
      "Epoch: 18, Steps: 1 | Train Loss: 112.8005600 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10152173042297363\n",
      "Epoch: 19, Steps: 1 | Train Loss: 112.4551315 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.10074257850646973\n",
      "Epoch: 20, Steps: 1 | Train Loss: 112.6374283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.10048484802246094\n",
      "Epoch: 21, Steps: 1 | Train Loss: 112.9262695 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10019445419311523\n",
      "Epoch: 22, Steps: 1 | Train Loss: 112.8130646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.10019993782043457\n",
      "Epoch: 23, Steps: 1 | Train Loss: 113.2424088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.10020613670349121\n",
      "Epoch: 24, Steps: 1 | Train Loss: 112.5222397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.10058856010437012\n",
      "Epoch: 25, Steps: 1 | Train Loss: 112.8828659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.10440802574157715\n",
      "Epoch: 26, Steps: 1 | Train Loss: 112.6205444 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.10286808013916016\n",
      "Epoch: 27, Steps: 1 | Train Loss: 112.9296875 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.10203862190246582\n",
      "Epoch: 28, Steps: 1 | Train Loss: 112.1851196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10036253929138184\n",
      "Epoch: 29, Steps: 1 | Train Loss: 113.0638962 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.10012245178222656\n",
      "Epoch: 30, Steps: 1 | Train Loss: 112.6371613 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.10010290145874023\n",
      "Epoch: 31, Steps: 1 | Train Loss: 112.6802292 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.0999760627746582\n",
      "Epoch: 32, Steps: 1 | Train Loss: 112.5498276 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10061502456665039\n",
      "Epoch: 33, Steps: 1 | Train Loss: 112.8641129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10373377799987793\n",
      "Epoch: 34, Steps: 1 | Train Loss: 113.3491974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10190773010253906\n",
      "Epoch: 35, Steps: 1 | Train Loss: 112.4853287 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10125422477722168\n",
      "Epoch: 36, Steps: 1 | Train Loss: 112.8225098 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10137414932250977\n",
      "Epoch: 37, Steps: 1 | Train Loss: 112.4967804 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10057759284973145\n",
      "Epoch: 38, Steps: 1 | Train Loss: 112.7870407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.1007544994354248\n",
      "Epoch: 39, Steps: 1 | Train Loss: 112.6901398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10054349899291992\n",
      "Epoch: 40, Steps: 1 | Train Loss: 112.4459991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10007643699645996\n",
      "Epoch: 41, Steps: 1 | Train Loss: 112.7216949 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10631966590881348\n",
      "Epoch: 42, Steps: 1 | Train Loss: 113.1846466 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.1023855209350586\n",
      "Epoch: 43, Steps: 1 | Train Loss: 112.7124863 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10114789009094238\n",
      "Epoch: 44, Steps: 1 | Train Loss: 113.1501694 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10063743591308594\n",
      "Epoch: 45, Steps: 1 | Train Loss: 112.8530807 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.10045218467712402\n",
      "Epoch: 46, Steps: 1 | Train Loss: 112.6631012 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.10067534446716309\n",
      "Epoch: 47, Steps: 1 | Train Loss: 112.9058838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.10028266906738281\n",
      "Epoch: 48, Steps: 1 | Train Loss: 112.3360367 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.1003255844116211\n",
      "Epoch: 49, Steps: 1 | Train Loss: 112.7414093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.10094070434570312\n",
      "Epoch: 50, Steps: 1 | Train Loss: 112.8736496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.10396075248718262\n",
      "Epoch: 51, Steps: 1 | Train Loss: 112.3666534 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10178065299987793\n",
      "Epoch: 52, Steps: 1 | Train Loss: 112.5064316 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10191798210144043\n",
      "Epoch: 53, Steps: 1 | Train Loss: 112.9253082 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.1004023551940918\n",
      "Epoch: 54, Steps: 1 | Train Loss: 112.6135864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.1004648208618164\n",
      "Epoch: 55, Steps: 1 | Train Loss: 112.9289780 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.10808444023132324\n",
      "Epoch: 56, Steps: 1 | Train Loss: 112.7170639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10738396644592285\n",
      "Epoch: 57, Steps: 1 | Train Loss: 112.5333252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10733270645141602\n",
      "Epoch: 58, Steps: 1 | Train Loss: 112.5152969 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11938929557800293\n",
      "Epoch: 59, Steps: 1 | Train Loss: 112.5880890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11031150817871094\n",
      "Epoch: 60, Steps: 1 | Train Loss: 112.4745102 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.10886597633361816\n",
      "Epoch: 61, Steps: 1 | Train Loss: 112.7055435 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.10764908790588379\n",
      "Epoch: 62, Steps: 1 | Train Loss: 113.0149307 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11533617973327637\n",
      "Epoch: 63, Steps: 1 | Train Loss: 112.5221100 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.11677360534667969\n",
      "Epoch: 64, Steps: 1 | Train Loss: 112.8477783 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11539912223815918\n",
      "Epoch: 65, Steps: 1 | Train Loss: 112.4688492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11482977867126465\n",
      "Epoch: 66, Steps: 1 | Train Loss: 112.4010544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11597490310668945\n",
      "Epoch: 67, Steps: 1 | Train Loss: 112.5179214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.12552475929260254\n",
      "Epoch: 68, Steps: 1 | Train Loss: 112.6908340 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.11753153800964355\n",
      "Epoch: 69, Steps: 1 | Train Loss: 112.5626984 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11722064018249512\n",
      "Epoch: 70, Steps: 1 | Train Loss: 112.8473511 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.11607980728149414\n",
      "Epoch: 71, Steps: 1 | Train Loss: 112.9866867 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.11618947982788086\n",
      "Epoch: 72, Steps: 1 | Train Loss: 113.0423584 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11562490463256836\n",
      "Epoch: 73, Steps: 1 | Train Loss: 112.8585587 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.11521339416503906\n",
      "Epoch: 74, Steps: 1 | Train Loss: 112.6750031 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11492800712585449\n",
      "Epoch: 75, Steps: 1 | Train Loss: 112.4021149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.1161189079284668\n",
      "Epoch: 76, Steps: 1 | Train Loss: 112.5029449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11755990982055664\n",
      "Epoch: 77, Steps: 1 | Train Loss: 112.7869415 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.11599874496459961\n",
      "Epoch: 78, Steps: 1 | Train Loss: 112.6094360 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.11537957191467285\n",
      "Epoch: 79, Steps: 1 | Train Loss: 112.5067291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.11415910720825195\n",
      "Epoch: 80, Steps: 1 | Train Loss: 112.7017212 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.11718320846557617\n",
      "Epoch: 81, Steps: 1 | Train Loss: 112.4100494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11661314964294434\n",
      "Epoch: 82, Steps: 1 | Train Loss: 112.2586441 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.11659431457519531\n",
      "Epoch: 83, Steps: 1 | Train Loss: 112.8923264 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11635661125183105\n",
      "Epoch: 84, Steps: 1 | Train Loss: 112.4302979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.12770366668701172\n",
      "Epoch: 85, Steps: 1 | Train Loss: 112.7376099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11813545227050781\n",
      "Epoch: 86, Steps: 1 | Train Loss: 113.3465576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.11577987670898438\n",
      "Epoch: 87, Steps: 1 | Train Loss: 112.6539841 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11510705947875977\n",
      "Epoch: 88, Steps: 1 | Train Loss: 113.2196198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.1159822940826416\n",
      "Epoch: 89, Steps: 1 | Train Loss: 112.8997726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11568188667297363\n",
      "Epoch: 90, Steps: 1 | Train Loss: 112.2769089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.11600470542907715\n",
      "Epoch: 91, Steps: 1 | Train Loss: 112.6686020 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.11475419998168945\n",
      "Epoch: 92, Steps: 1 | Train Loss: 112.6140518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.11510276794433594\n",
      "Epoch: 93, Steps: 1 | Train Loss: 112.5457153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10741233825683594\n",
      "Epoch: 94, Steps: 1 | Train Loss: 112.8269653 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.11796212196350098\n",
      "Epoch: 95, Steps: 1 | Train Loss: 112.8042984 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10974717140197754\n",
      "Epoch: 96, Steps: 1 | Train Loss: 113.0356598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10860490798950195\n",
      "Epoch: 97, Steps: 1 | Train Loss: 112.9942398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10782575607299805\n",
      "Epoch: 98, Steps: 1 | Train Loss: 112.5980225 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10762691497802734\n",
      "Epoch: 99, Steps: 1 | Train Loss: 112.8473434 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10739612579345703\n",
      "Epoch: 100, Steps: 1 | Train Loss: 112.9076920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.1072685718536377\n",
      "Epoch: 101, Steps: 1 | Train Loss: 112.2941208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10710906982421875\n",
      "Epoch: 102, Steps: 1 | Train Loss: 112.6205368 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.1071016788482666\n",
      "Epoch: 103, Steps: 1 | Train Loss: 112.8143311 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10737943649291992\n",
      "Epoch: 104, Steps: 1 | Train Loss: 112.6104355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10763669013977051\n",
      "Epoch: 105, Steps: 1 | Train Loss: 112.9799576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.1206810474395752\n",
      "Epoch: 106, Steps: 1 | Train Loss: 112.5160141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.12084221839904785\n",
      "Epoch: 107, Steps: 1 | Train Loss: 112.6774063 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.11118435859680176\n",
      "Epoch: 108, Steps: 1 | Train Loss: 112.7697067 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10910916328430176\n",
      "Epoch: 109, Steps: 1 | Train Loss: 112.7554092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.1094205379486084\n",
      "Epoch: 110, Steps: 1 | Train Loss: 112.5058594 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11044478416442871\n",
      "Epoch: 111, Steps: 1 | Train Loss: 113.0017624 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10756778717041016\n",
      "Epoch: 112, Steps: 1 | Train Loss: 112.2880402 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10735464096069336\n",
      "Epoch: 113, Steps: 1 | Train Loss: 113.0843048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10711264610290527\n",
      "Epoch: 114, Steps: 1 | Train Loss: 112.7654572 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10699295997619629\n",
      "Epoch: 115, Steps: 1 | Train Loss: 113.1210556 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10742497444152832\n",
      "Epoch: 116, Steps: 1 | Train Loss: 112.9457321 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.11612796783447266\n",
      "Epoch: 117, Steps: 1 | Train Loss: 112.5294952 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.12738490104675293\n",
      "Epoch: 118, Steps: 1 | Train Loss: 113.0958023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.1250324249267578\n",
      "Epoch: 119, Steps: 1 | Train Loss: 112.6316910 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1103670597076416\n",
      "Epoch: 120, Steps: 1 | Train Loss: 112.2515793 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.10860347747802734\n",
      "Epoch: 121, Steps: 1 | Train Loss: 112.6863556 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10873985290527344\n",
      "Epoch: 122, Steps: 1 | Train Loss: 112.9556656 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10762310028076172\n",
      "Epoch: 123, Steps: 1 | Train Loss: 112.5763092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.10741043090820312\n",
      "Epoch: 124, Steps: 1 | Train Loss: 112.4296494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.10827922821044922\n",
      "Epoch: 125, Steps: 1 | Train Loss: 112.6505661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.1072242259979248\n",
      "Epoch: 126, Steps: 1 | Train Loss: 112.3387222 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.1318221092224121\n",
      "Epoch: 127, Steps: 1 | Train Loss: 112.9074936 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.1285691261291504\n",
      "Epoch: 128, Steps: 1 | Train Loss: 112.4638290 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.12304115295410156\n",
      "Epoch: 129, Steps: 1 | Train Loss: 112.8157349 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.12006974220275879\n",
      "Epoch: 130, Steps: 1 | Train Loss: 113.1296616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.12448263168334961\n",
      "Epoch: 131, Steps: 1 | Train Loss: 112.2502441 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11926770210266113\n",
      "Epoch: 132, Steps: 1 | Train Loss: 112.3736115 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11517190933227539\n",
      "Epoch: 133, Steps: 1 | Train Loss: 112.9091797 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.1106557846069336\n",
      "Epoch: 134, Steps: 1 | Train Loss: 112.1961670 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.11043429374694824\n",
      "Epoch: 135, Steps: 1 | Train Loss: 112.4757080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10486865043640137\n",
      "Epoch: 136, Steps: 1 | Train Loss: 112.4297409 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.10743069648742676\n",
      "Epoch: 137, Steps: 1 | Train Loss: 111.9128647 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.10753798484802246\n",
      "Epoch: 138, Steps: 1 | Train Loss: 112.5021133 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.10717988014221191\n",
      "Epoch: 139, Steps: 1 | Train Loss: 112.9076538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.12006783485412598\n",
      "Epoch: 140, Steps: 1 | Train Loss: 112.6447678 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.12042641639709473\n",
      "Epoch: 141, Steps: 1 | Train Loss: 112.7036667 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.12169623374938965\n",
      "Epoch: 142, Steps: 1 | Train Loss: 112.6056519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.12204456329345703\n",
      "Epoch: 143, Steps: 1 | Train Loss: 113.2107544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.12183356285095215\n",
      "Epoch: 144, Steps: 1 | Train Loss: 112.6499023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.11823701858520508\n",
      "Epoch: 145, Steps: 1 | Train Loss: 112.9065781 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.11351323127746582\n",
      "Epoch: 146, Steps: 1 | Train Loss: 112.6681137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.11178898811340332\n",
      "Epoch: 147, Steps: 1 | Train Loss: 112.6386948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10995268821716309\n",
      "Epoch: 148, Steps: 1 | Train Loss: 112.9998093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.12318944931030273\n",
      "Epoch: 149, Steps: 1 | Train Loss: 112.7358170 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.11118125915527344\n",
      "Epoch: 150, Steps: 1 | Train Loss: 113.0898438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.11053609848022461\n",
      "Epoch: 151, Steps: 1 | Train Loss: 112.8164215 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10693979263305664\n",
      "Epoch: 152, Steps: 1 | Train Loss: 112.4891357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12066650390625\n",
      "Epoch: 153, Steps: 1 | Train Loss: 112.5676498 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.12058210372924805\n",
      "Epoch: 154, Steps: 1 | Train Loss: 113.2373581 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.12352609634399414\n",
      "Epoch: 155, Steps: 1 | Train Loss: 112.5605469 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11247849464416504\n",
      "Epoch: 156, Steps: 1 | Train Loss: 112.4398193 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.11678743362426758\n",
      "Epoch: 157, Steps: 1 | Train Loss: 112.5486374 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.12476253509521484\n",
      "Epoch: 158, Steps: 1 | Train Loss: 112.5406036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.12321853637695312\n",
      "Epoch: 159, Steps: 1 | Train Loss: 112.3453369 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.12260794639587402\n",
      "Epoch: 160, Steps: 1 | Train Loss: 112.6775131 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.1370258331298828\n",
      "Epoch: 161, Steps: 1 | Train Loss: 113.0871811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.1255660057067871\n",
      "Epoch: 162, Steps: 1 | Train Loss: 112.2732468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.12873506546020508\n",
      "Epoch: 163, Steps: 1 | Train Loss: 112.5615005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.13231515884399414\n",
      "Epoch: 164, Steps: 1 | Train Loss: 113.4539108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.13647866249084473\n",
      "Epoch: 165, Steps: 1 | Train Loss: 112.9970245 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.13596677780151367\n",
      "Epoch: 166, Steps: 1 | Train Loss: 112.6159210 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.13783645629882812\n",
      "Epoch: 167, Steps: 1 | Train Loss: 112.7999878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.15203475952148438\n",
      "Epoch: 168, Steps: 1 | Train Loss: 112.8997345 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.1181640625\n",
      "Epoch: 169, Steps: 1 | Train Loss: 112.8009644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11971497535705566\n",
      "Epoch: 170, Steps: 1 | Train Loss: 112.9874268 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11882305145263672\n",
      "Epoch: 171, Steps: 1 | Train Loss: 112.4876022 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11091494560241699\n",
      "Epoch: 172, Steps: 1 | Train Loss: 112.8345108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.12755870819091797\n",
      "Epoch: 173, Steps: 1 | Train Loss: 112.7842331 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.12094378471374512\n",
      "Epoch: 174, Steps: 1 | Train Loss: 112.7299805 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.1331496238708496\n",
      "Epoch: 175, Steps: 1 | Train Loss: 113.0196533 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.12561416625976562\n",
      "Epoch: 176, Steps: 1 | Train Loss: 112.2232208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.1273045539855957\n",
      "Epoch: 177, Steps: 1 | Train Loss: 112.8959274 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.12754440307617188\n",
      "Epoch: 178, Steps: 1 | Train Loss: 112.5705338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.1375713348388672\n",
      "Epoch: 179, Steps: 1 | Train Loss: 112.6638184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11243057250976562\n",
      "Epoch: 180, Steps: 1 | Train Loss: 112.5331421 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11004376411437988\n",
      "Epoch: 181, Steps: 1 | Train Loss: 112.9359894 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11057353019714355\n",
      "Epoch: 182, Steps: 1 | Train Loss: 112.6851425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11046743392944336\n",
      "Epoch: 183, Steps: 1 | Train Loss: 112.9379654 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11898040771484375\n",
      "Epoch: 184, Steps: 1 | Train Loss: 112.7158203 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.12235474586486816\n",
      "Epoch: 185, Steps: 1 | Train Loss: 113.2754135 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.13194751739501953\n",
      "Epoch: 186, Steps: 1 | Train Loss: 112.0714874 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11890649795532227\n",
      "Epoch: 187, Steps: 1 | Train Loss: 112.3814316 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10937356948852539\n",
      "Epoch: 188, Steps: 1 | Train Loss: 112.6950455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.10709547996520996\n",
      "Epoch: 189, Steps: 1 | Train Loss: 113.3002243 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10722613334655762\n",
      "Epoch: 190, Steps: 1 | Train Loss: 112.8664932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10704946517944336\n",
      "Epoch: 191, Steps: 1 | Train Loss: 112.7036896 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10657930374145508\n",
      "Epoch: 192, Steps: 1 | Train Loss: 112.6139450 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.10686063766479492\n",
      "Epoch: 193, Steps: 1 | Train Loss: 112.3231812 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.10691499710083008\n",
      "Epoch: 194, Steps: 1 | Train Loss: 113.0506363 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10676002502441406\n",
      "Epoch: 195, Steps: 1 | Train Loss: 112.7115707 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.10933232307434082\n",
      "Epoch: 196, Steps: 1 | Train Loss: 113.0533218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.12148642539978027\n",
      "Epoch: 197, Steps: 1 | Train Loss: 112.6458054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.12044429779052734\n",
      "Epoch: 198, Steps: 1 | Train Loss: 112.5086823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11464238166809082\n",
      "Epoch: 199, Steps: 1 | Train Loss: 112.6576080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10932278633117676\n",
      "Epoch: 200, Steps: 1 | Train Loss: 112.4446793 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.1082148551940918\n",
      "Epoch: 201, Steps: 1 | Train Loss: 112.8085709 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10770177841186523\n",
      "Epoch: 202, Steps: 1 | Train Loss: 112.7268295 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10753941535949707\n",
      "Epoch: 203, Steps: 1 | Train Loss: 112.8976593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.10729074478149414\n",
      "Epoch: 204, Steps: 1 | Train Loss: 112.6891022 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.10663604736328125\n",
      "Epoch: 205, Steps: 1 | Train Loss: 112.7860031 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10783243179321289\n",
      "Epoch: 206, Steps: 1 | Train Loss: 112.8685455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11654305458068848\n",
      "Epoch: 207, Steps: 1 | Train Loss: 112.5454330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.11785578727722168\n",
      "Epoch: 208, Steps: 1 | Train Loss: 112.6720505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11703133583068848\n",
      "Epoch: 209, Steps: 1 | Train Loss: 112.4546890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11804723739624023\n",
      "Epoch: 210, Steps: 1 | Train Loss: 112.6316452 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.13026857376098633\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.9768600 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.1267540454864502\n",
      "Epoch: 212, Steps: 1 | Train Loss: 113.0399704 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.12485814094543457\n",
      "Epoch: 213, Steps: 1 | Train Loss: 112.6266098 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10759496688842773\n",
      "Epoch: 214, Steps: 1 | Train Loss: 112.4473877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11560893058776855\n",
      "Epoch: 215, Steps: 1 | Train Loss: 113.2062988 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.1153256893157959\n",
      "Epoch: 216, Steps: 1 | Train Loss: 112.3687134 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10800838470458984\n",
      "Epoch: 217, Steps: 1 | Train Loss: 113.2980499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10853791236877441\n",
      "Epoch: 218, Steps: 1 | Train Loss: 112.9319611 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.11210227012634277\n",
      "Epoch: 219, Steps: 1 | Train Loss: 113.0790329 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11415958404541016\n",
      "Epoch: 220, Steps: 1 | Train Loss: 112.6661758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11483955383300781\n",
      "Epoch: 221, Steps: 1 | Train Loss: 112.7220688 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.11223959922790527\n",
      "Epoch: 222, Steps: 1 | Train Loss: 112.7065811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10928463935852051\n",
      "Epoch: 223, Steps: 1 | Train Loss: 112.4267502 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11522579193115234\n",
      "Epoch: 224, Steps: 1 | Train Loss: 112.8770752 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11453390121459961\n",
      "Epoch: 225, Steps: 1 | Train Loss: 112.6930542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11614727973937988\n",
      "Epoch: 226, Steps: 1 | Train Loss: 112.4728165 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11574554443359375\n",
      "Epoch: 227, Steps: 1 | Train Loss: 112.5100021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.1264634132385254\n",
      "Epoch: 228, Steps: 1 | Train Loss: 113.0404663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11775064468383789\n",
      "Epoch: 229, Steps: 1 | Train Loss: 112.7585678 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11330747604370117\n",
      "Epoch: 230, Steps: 1 | Train Loss: 112.9630661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.11562418937683105\n",
      "Epoch: 231, Steps: 1 | Train Loss: 112.6950226 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10825085639953613\n",
      "Epoch: 232, Steps: 1 | Train Loss: 112.1144791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11281538009643555\n",
      "Epoch: 233, Steps: 1 | Train Loss: 112.4026413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11330080032348633\n",
      "Epoch: 234, Steps: 1 | Train Loss: 112.4616699 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.10873913764953613\n",
      "Epoch: 235, Steps: 1 | Train Loss: 112.4772263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10771942138671875\n",
      "Epoch: 236, Steps: 1 | Train Loss: 112.5531540 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11576175689697266\n",
      "Epoch: 237, Steps: 1 | Train Loss: 112.3125229 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11316823959350586\n",
      "Epoch: 238, Steps: 1 | Train Loss: 112.6506348 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10832333564758301\n",
      "Epoch: 239, Steps: 1 | Train Loss: 112.8463364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10798764228820801\n",
      "Epoch: 240, Steps: 1 | Train Loss: 113.1611328 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1089169979095459\n",
      "Epoch: 241, Steps: 1 | Train Loss: 112.5985336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11718869209289551\n",
      "Epoch: 242, Steps: 1 | Train Loss: 113.0077286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.12140941619873047\n",
      "Epoch: 243, Steps: 1 | Train Loss: 112.7431183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.12531256675720215\n",
      "Epoch: 244, Steps: 1 | Train Loss: 112.6586227 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.11728572845458984\n",
      "Epoch: 245, Steps: 1 | Train Loss: 112.4128799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11420035362243652\n",
      "Epoch: 246, Steps: 1 | Train Loss: 113.0307770 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11330294609069824\n",
      "Epoch: 247, Steps: 1 | Train Loss: 112.5815964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.11108255386352539\n",
      "Epoch: 248, Steps: 1 | Train Loss: 112.3233414 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10456705093383789\n",
      "Epoch: 249, Steps: 1 | Train Loss: 112.7137604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10730481147766113\n",
      "Epoch: 250, Steps: 1 | Train Loss: 113.1031036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_6>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.1018679141998291\n",
      "Epoch: 1, Steps: 1 | Train Loss: 118.9403076 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10096454620361328\n",
      "Epoch: 2, Steps: 1 | Train Loss: 116.7638321 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10132694244384766\n",
      "Epoch: 3, Steps: 1 | Train Loss: 115.7005615 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10084939002990723\n",
      "Epoch: 4, Steps: 1 | Train Loss: 114.4880295 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10202407836914062\n",
      "Epoch: 5, Steps: 1 | Train Loss: 113.7104874 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.1078345775604248\n",
      "Epoch: 6, Steps: 1 | Train Loss: 113.2854767 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10741877555847168\n",
      "Epoch: 7, Steps: 1 | Train Loss: 113.7905884 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10733485221862793\n",
      "Epoch: 8, Steps: 1 | Train Loss: 113.5490036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11319684982299805\n",
      "Epoch: 9, Steps: 1 | Train Loss: 113.8643723 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.1076960563659668\n",
      "Epoch: 10, Steps: 1 | Train Loss: 113.7178574 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.1071014404296875\n",
      "Epoch: 11, Steps: 1 | Train Loss: 113.9659424 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10754227638244629\n",
      "Epoch: 12, Steps: 1 | Train Loss: 113.5220261 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10724735260009766\n",
      "Epoch: 13, Steps: 1 | Train Loss: 113.3495865 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10743904113769531\n",
      "Epoch: 14, Steps: 1 | Train Loss: 113.8157959 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.1190941333770752\n",
      "Epoch: 15, Steps: 1 | Train Loss: 113.4137497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12099814414978027\n",
      "Epoch: 16, Steps: 1 | Train Loss: 113.5581589 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.12072896957397461\n",
      "Epoch: 17, Steps: 1 | Train Loss: 113.5820084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.12204861640930176\n",
      "Epoch: 18, Steps: 1 | Train Loss: 113.5934067 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.12246847152709961\n",
      "Epoch: 19, Steps: 1 | Train Loss: 113.5944366 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.12006855010986328\n",
      "Epoch: 20, Steps: 1 | Train Loss: 113.7708664 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.1143655776977539\n",
      "Epoch: 21, Steps: 1 | Train Loss: 113.7963867 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.11273670196533203\n",
      "Epoch: 22, Steps: 1 | Train Loss: 113.3250046 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11164641380310059\n",
      "Epoch: 23, Steps: 1 | Train Loss: 113.2603149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11249589920043945\n",
      "Epoch: 24, Steps: 1 | Train Loss: 113.9133301 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11062169075012207\n",
      "Epoch: 25, Steps: 1 | Train Loss: 114.1869278 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.1109473705291748\n",
      "Epoch: 26, Steps: 1 | Train Loss: 113.7791519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11071157455444336\n",
      "Epoch: 27, Steps: 1 | Train Loss: 113.6884766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.11033225059509277\n",
      "Epoch: 28, Steps: 1 | Train Loss: 113.4632111 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.11138725280761719\n",
      "Epoch: 29, Steps: 1 | Train Loss: 113.1049118 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11027407646179199\n",
      "Epoch: 30, Steps: 1 | Train Loss: 113.1089096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11069345474243164\n",
      "Epoch: 31, Steps: 1 | Train Loss: 114.0768967 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.1106557846069336\n",
      "Epoch: 32, Steps: 1 | Train Loss: 113.6996460 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11058712005615234\n",
      "Epoch: 33, Steps: 1 | Train Loss: 113.6499557 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.11066317558288574\n",
      "Epoch: 34, Steps: 1 | Train Loss: 113.3764420 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.12456703186035156\n",
      "Epoch: 35, Steps: 1 | Train Loss: 113.3929672 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.12509703636169434\n",
      "Epoch: 36, Steps: 1 | Train Loss: 113.1244888 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.12436532974243164\n",
      "Epoch: 37, Steps: 1 | Train Loss: 113.8155289 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.12340712547302246\n",
      "Epoch: 38, Steps: 1 | Train Loss: 113.6741104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.11801791191101074\n",
      "Epoch: 39, Steps: 1 | Train Loss: 113.6419220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.11356234550476074\n",
      "Epoch: 40, Steps: 1 | Train Loss: 113.9150543 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.1119391918182373\n",
      "Epoch: 41, Steps: 1 | Train Loss: 113.2936172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.11176371574401855\n",
      "Epoch: 42, Steps: 1 | Train Loss: 113.4076920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10968971252441406\n",
      "Epoch: 43, Steps: 1 | Train Loss: 113.6077042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10984611511230469\n",
      "Epoch: 44, Steps: 1 | Train Loss: 113.6652603 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10987043380737305\n",
      "Epoch: 45, Steps: 1 | Train Loss: 113.3955307 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11080169677734375\n",
      "Epoch: 46, Steps: 1 | Train Loss: 113.5398178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11063241958618164\n",
      "Epoch: 47, Steps: 1 | Train Loss: 113.8910675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.11028313636779785\n",
      "Epoch: 48, Steps: 1 | Train Loss: 113.9304733 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.11176586151123047\n",
      "Epoch: 49, Steps: 1 | Train Loss: 113.3258209 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11078381538391113\n",
      "Epoch: 50, Steps: 1 | Train Loss: 112.9987335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.1117258071899414\n",
      "Epoch: 51, Steps: 1 | Train Loss: 113.1856003 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11027050018310547\n",
      "Epoch: 52, Steps: 1 | Train Loss: 113.3016357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.12433195114135742\n",
      "Epoch: 53, Steps: 1 | Train Loss: 113.7672119 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.12560391426086426\n",
      "Epoch: 54, Steps: 1 | Train Loss: 113.1764908 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.12482023239135742\n",
      "Epoch: 55, Steps: 1 | Train Loss: 113.6436081 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.11970925331115723\n",
      "Epoch: 56, Steps: 1 | Train Loss: 113.3943405 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.1139371395111084\n",
      "Epoch: 57, Steps: 1 | Train Loss: 113.5418701 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.11176395416259766\n",
      "Epoch: 58, Steps: 1 | Train Loss: 113.4856186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11268401145935059\n",
      "Epoch: 59, Steps: 1 | Train Loss: 113.7869644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10830879211425781\n",
      "Epoch: 60, Steps: 1 | Train Loss: 113.5702667 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.1079704761505127\n",
      "Epoch: 61, Steps: 1 | Train Loss: 113.9183121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.10706186294555664\n",
      "Epoch: 62, Steps: 1 | Train Loss: 113.6942062 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11241912841796875\n",
      "Epoch: 63, Steps: 1 | Train Loss: 113.1313477 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.10798883438110352\n",
      "Epoch: 64, Steps: 1 | Train Loss: 113.4682846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.10819578170776367\n",
      "Epoch: 65, Steps: 1 | Train Loss: 113.5525513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.1089930534362793\n",
      "Epoch: 66, Steps: 1 | Train Loss: 113.2625351 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.10923409461975098\n",
      "Epoch: 67, Steps: 1 | Train Loss: 113.2854004 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10834908485412598\n",
      "Epoch: 68, Steps: 1 | Train Loss: 113.4303970 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10867929458618164\n",
      "Epoch: 69, Steps: 1 | Train Loss: 113.4573517 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11287522315979004\n",
      "Epoch: 70, Steps: 1 | Train Loss: 114.0297623 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10967445373535156\n",
      "Epoch: 71, Steps: 1 | Train Loss: 113.2620621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10749530792236328\n",
      "Epoch: 72, Steps: 1 | Train Loss: 113.6245651 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.10718512535095215\n",
      "Epoch: 73, Steps: 1 | Train Loss: 113.3597946 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.10866165161132812\n",
      "Epoch: 74, Steps: 1 | Train Loss: 113.5220261 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11336040496826172\n",
      "Epoch: 75, Steps: 1 | Train Loss: 113.4125519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.10678219795227051\n",
      "Epoch: 76, Steps: 1 | Train Loss: 113.6788864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.10746526718139648\n",
      "Epoch: 77, Steps: 1 | Train Loss: 113.2946777 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10752534866333008\n",
      "Epoch: 78, Steps: 1 | Train Loss: 113.5238571 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10734224319458008\n",
      "Epoch: 79, Steps: 1 | Train Loss: 114.0064468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10742306709289551\n",
      "Epoch: 80, Steps: 1 | Train Loss: 113.4535675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.1082010269165039\n",
      "Epoch: 81, Steps: 1 | Train Loss: 113.6552277 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.12081170082092285\n",
      "Epoch: 82, Steps: 1 | Train Loss: 113.8804092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.1208040714263916\n",
      "Epoch: 83, Steps: 1 | Train Loss: 113.0530777 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11206221580505371\n",
      "Epoch: 84, Steps: 1 | Train Loss: 113.1575851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.1089925765991211\n",
      "Epoch: 85, Steps: 1 | Train Loss: 113.1416245 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.1080164909362793\n",
      "Epoch: 86, Steps: 1 | Train Loss: 113.4394150 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10774517059326172\n",
      "Epoch: 87, Steps: 1 | Train Loss: 113.5685959 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.10759997367858887\n",
      "Epoch: 88, Steps: 1 | Train Loss: 113.5062256 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10706353187561035\n",
      "Epoch: 89, Steps: 1 | Train Loss: 113.8482895 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10716366767883301\n",
      "Epoch: 90, Steps: 1 | Train Loss: 113.7557907 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10719895362854004\n",
      "Epoch: 91, Steps: 1 | Train Loss: 113.5023193 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10731053352355957\n",
      "Epoch: 92, Steps: 1 | Train Loss: 113.5373535 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10715794563293457\n",
      "Epoch: 93, Steps: 1 | Train Loss: 113.2854843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10744976997375488\n",
      "Epoch: 94, Steps: 1 | Train Loss: 114.0297623 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10698318481445312\n",
      "Epoch: 95, Steps: 1 | Train Loss: 113.9413834 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.12062954902648926\n",
      "Epoch: 96, Steps: 1 | Train Loss: 113.2835236 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.12015628814697266\n",
      "Epoch: 97, Steps: 1 | Train Loss: 113.5469742 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.12164044380187988\n",
      "Epoch: 98, Steps: 1 | Train Loss: 113.3673096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.11153626441955566\n",
      "Epoch: 99, Steps: 1 | Train Loss: 113.6769028 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10882210731506348\n",
      "Epoch: 100, Steps: 1 | Train Loss: 113.5971222 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.1078488826751709\n",
      "Epoch: 101, Steps: 1 | Train Loss: 114.2092896 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.11553740501403809\n",
      "Epoch: 102, Steps: 1 | Train Loss: 113.3205795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10737061500549316\n",
      "Epoch: 103, Steps: 1 | Train Loss: 113.5400162 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10737156867980957\n",
      "Epoch: 104, Steps: 1 | Train Loss: 113.6879654 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10712218284606934\n",
      "Epoch: 105, Steps: 1 | Train Loss: 113.2966080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10733580589294434\n",
      "Epoch: 106, Steps: 1 | Train Loss: 113.3530884 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10770010948181152\n",
      "Epoch: 107, Steps: 1 | Train Loss: 113.9478073 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.12181997299194336\n",
      "Epoch: 108, Steps: 1 | Train Loss: 113.4882736 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11109447479248047\n",
      "Epoch: 109, Steps: 1 | Train Loss: 113.6281967 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.11026597023010254\n",
      "Epoch: 110, Steps: 1 | Train Loss: 113.6709213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.10815691947937012\n",
      "Epoch: 111, Steps: 1 | Train Loss: 113.5654526 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10789990425109863\n",
      "Epoch: 112, Steps: 1 | Train Loss: 113.7927856 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.1073768138885498\n",
      "Epoch: 113, Steps: 1 | Train Loss: 113.0795441 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10682916641235352\n",
      "Epoch: 114, Steps: 1 | Train Loss: 113.2263947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11676430702209473\n",
      "Epoch: 115, Steps: 1 | Train Loss: 113.9864655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11397528648376465\n",
      "Epoch: 116, Steps: 1 | Train Loss: 113.5127945 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.1071627140045166\n",
      "Epoch: 117, Steps: 1 | Train Loss: 113.6797867 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.1071920394897461\n",
      "Epoch: 118, Steps: 1 | Train Loss: 113.6182632 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.11127901077270508\n",
      "Epoch: 119, Steps: 1 | Train Loss: 113.9944992 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.10850238800048828\n",
      "Epoch: 120, Steps: 1 | Train Loss: 113.5010300 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.1079552173614502\n",
      "Epoch: 121, Steps: 1 | Train Loss: 113.5777588 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10738205909729004\n",
      "Epoch: 122, Steps: 1 | Train Loss: 113.6944122 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10710406303405762\n",
      "Epoch: 123, Steps: 1 | Train Loss: 113.6610107 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.1080026626586914\n",
      "Epoch: 124, Steps: 1 | Train Loss: 113.2857666 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.10728096961975098\n",
      "Epoch: 125, Steps: 1 | Train Loss: 113.6462555 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.10751652717590332\n",
      "Epoch: 126, Steps: 1 | Train Loss: 113.4684372 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.10729241371154785\n",
      "Epoch: 127, Steps: 1 | Train Loss: 113.8383102 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.1089627742767334\n",
      "Epoch: 128, Steps: 1 | Train Loss: 113.0824509 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10931134223937988\n",
      "Epoch: 129, Steps: 1 | Train Loss: 113.5246811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10896444320678711\n",
      "Epoch: 130, Steps: 1 | Train Loss: 113.8706818 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11958575248718262\n",
      "Epoch: 131, Steps: 1 | Train Loss: 113.4428940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.12202572822570801\n",
      "Epoch: 132, Steps: 1 | Train Loss: 113.4711456 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.12208127975463867\n",
      "Epoch: 133, Steps: 1 | Train Loss: 113.5313721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.12226414680480957\n",
      "Epoch: 134, Steps: 1 | Train Loss: 113.0987091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.12193751335144043\n",
      "Epoch: 135, Steps: 1 | Train Loss: 113.4390640 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.12013840675354004\n",
      "Epoch: 136, Steps: 1 | Train Loss: 113.7465820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.13756322860717773\n",
      "Epoch: 137, Steps: 1 | Train Loss: 113.3313217 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.12099242210388184\n",
      "Epoch: 138, Steps: 1 | Train Loss: 113.8412628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11901116371154785\n",
      "Epoch: 139, Steps: 1 | Train Loss: 113.7639084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.11673998832702637\n",
      "Epoch: 140, Steps: 1 | Train Loss: 113.3343277 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.11803054809570312\n",
      "Epoch: 141, Steps: 1 | Train Loss: 113.8593979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11873650550842285\n",
      "Epoch: 142, Steps: 1 | Train Loss: 113.3613052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11725616455078125\n",
      "Epoch: 143, Steps: 1 | Train Loss: 113.0403595 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11637067794799805\n",
      "Epoch: 144, Steps: 1 | Train Loss: 113.0966339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.1285254955291748\n",
      "Epoch: 145, Steps: 1 | Train Loss: 113.3530273 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.11988234519958496\n",
      "Epoch: 146, Steps: 1 | Train Loss: 113.1277084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.1183772087097168\n",
      "Epoch: 147, Steps: 1 | Train Loss: 113.4860382 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11934876441955566\n",
      "Epoch: 148, Steps: 1 | Train Loss: 113.8682404 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11393594741821289\n",
      "Epoch: 149, Steps: 1 | Train Loss: 113.7722168 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10942602157592773\n",
      "Epoch: 150, Steps: 1 | Train Loss: 113.4470749 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.11246442794799805\n",
      "Epoch: 151, Steps: 1 | Train Loss: 113.4522934 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10738039016723633\n",
      "Epoch: 152, Steps: 1 | Train Loss: 112.9189835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.11487627029418945\n",
      "Epoch: 153, Steps: 1 | Train Loss: 113.5027847 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.11707568168640137\n",
      "Epoch: 154, Steps: 1 | Train Loss: 113.2929230 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10814428329467773\n",
      "Epoch: 155, Steps: 1 | Train Loss: 113.6658707 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10757565498352051\n",
      "Epoch: 156, Steps: 1 | Train Loss: 113.2990494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10757851600646973\n",
      "Epoch: 157, Steps: 1 | Train Loss: 113.6410904 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10783600807189941\n",
      "Epoch: 158, Steps: 1 | Train Loss: 113.6290207 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10738253593444824\n",
      "Epoch: 159, Steps: 1 | Train Loss: 113.7952042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.10747790336608887\n",
      "Epoch: 160, Steps: 1 | Train Loss: 113.8502960 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10735011100769043\n",
      "Epoch: 161, Steps: 1 | Train Loss: 113.4316788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.12101888656616211\n",
      "Epoch: 162, Steps: 1 | Train Loss: 113.1524887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.13054299354553223\n",
      "Epoch: 163, Steps: 1 | Train Loss: 113.9336929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.12313556671142578\n",
      "Epoch: 164, Steps: 1 | Train Loss: 113.2690582 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.11282539367675781\n",
      "Epoch: 165, Steps: 1 | Train Loss: 112.9848633 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11191368103027344\n",
      "Epoch: 166, Steps: 1 | Train Loss: 113.4405289 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10971212387084961\n",
      "Epoch: 167, Steps: 1 | Train Loss: 113.8015671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11083436012268066\n",
      "Epoch: 168, Steps: 1 | Train Loss: 113.7973251 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11040019989013672\n",
      "Epoch: 169, Steps: 1 | Train Loss: 113.4679947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.10922932624816895\n",
      "Epoch: 170, Steps: 1 | Train Loss: 113.5383301 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11040186882019043\n",
      "Epoch: 171, Steps: 1 | Train Loss: 113.4298859 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11083865165710449\n",
      "Epoch: 172, Steps: 1 | Train Loss: 113.3274078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10958099365234375\n",
      "Epoch: 173, Steps: 1 | Train Loss: 113.5645752 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11047720909118652\n",
      "Epoch: 174, Steps: 1 | Train Loss: 113.4181671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.12519097328186035\n",
      "Epoch: 175, Steps: 1 | Train Loss: 113.7769775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.12368988990783691\n",
      "Epoch: 176, Steps: 1 | Train Loss: 113.4611588 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.11585116386413574\n",
      "Epoch: 177, Steps: 1 | Train Loss: 113.2086182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11183595657348633\n",
      "Epoch: 178, Steps: 1 | Train Loss: 113.5757599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11141848564147949\n",
      "Epoch: 179, Steps: 1 | Train Loss: 113.3770981 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11120939254760742\n",
      "Epoch: 180, Steps: 1 | Train Loss: 113.6420822 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11037278175354004\n",
      "Epoch: 181, Steps: 1 | Train Loss: 113.7159195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10968446731567383\n",
      "Epoch: 182, Steps: 1 | Train Loss: 113.8479233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11100149154663086\n",
      "Epoch: 183, Steps: 1 | Train Loss: 113.7592087 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11099123954772949\n",
      "Epoch: 184, Steps: 1 | Train Loss: 112.9892349 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.1109616756439209\n",
      "Epoch: 185, Steps: 1 | Train Loss: 113.2224960 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11037874221801758\n",
      "Epoch: 186, Steps: 1 | Train Loss: 114.0538483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11002635955810547\n",
      "Epoch: 187, Steps: 1 | Train Loss: 113.3823166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11083126068115234\n",
      "Epoch: 188, Steps: 1 | Train Loss: 113.2784348 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.12427425384521484\n",
      "Epoch: 189, Steps: 1 | Train Loss: 113.1336441 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12407469749450684\n",
      "Epoch: 190, Steps: 1 | Train Loss: 113.6070328 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.12205028533935547\n",
      "Epoch: 191, Steps: 1 | Train Loss: 113.0460739 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.1255655288696289\n",
      "Epoch: 192, Steps: 1 | Train Loss: 113.2659454 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.108856201171875\n",
      "Epoch: 193, Steps: 1 | Train Loss: 113.6351852 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11449527740478516\n",
      "Epoch: 194, Steps: 1 | Train Loss: 113.4304123 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11036324501037598\n",
      "Epoch: 195, Steps: 1 | Train Loss: 113.4074707 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.1168677806854248\n",
      "Epoch: 196, Steps: 1 | Train Loss: 113.1806259 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11636924743652344\n",
      "Epoch: 197, Steps: 1 | Train Loss: 113.0225830 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11930680274963379\n",
      "Epoch: 198, Steps: 1 | Train Loss: 114.0014877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11808180809020996\n",
      "Epoch: 199, Steps: 1 | Train Loss: 113.3938522 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.1259768009185791\n",
      "Epoch: 200, Steps: 1 | Train Loss: 113.2744141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.12207293510437012\n",
      "Epoch: 201, Steps: 1 | Train Loss: 113.7010498 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10971689224243164\n",
      "Epoch: 202, Steps: 1 | Train Loss: 113.8655548 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11614227294921875\n",
      "Epoch: 203, Steps: 1 | Train Loss: 113.3677292 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.11939740180969238\n",
      "Epoch: 204, Steps: 1 | Train Loss: 113.4869308 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.1127326488494873\n",
      "Epoch: 205, Steps: 1 | Train Loss: 113.4664841 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10754561424255371\n",
      "Epoch: 206, Steps: 1 | Train Loss: 113.4053268 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10716605186462402\n",
      "Epoch: 207, Steps: 1 | Train Loss: 113.5841827 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.10737419128417969\n",
      "Epoch: 208, Steps: 1 | Train Loss: 113.4245834 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.12138605117797852\n",
      "Epoch: 209, Steps: 1 | Train Loss: 113.7227554 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11634182929992676\n",
      "Epoch: 210, Steps: 1 | Train Loss: 113.5277100 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.10989522933959961\n",
      "Epoch: 211, Steps: 1 | Train Loss: 113.2346725 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.10882258415222168\n",
      "Epoch: 212, Steps: 1 | Train Loss: 113.6987457 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.10824918746948242\n",
      "Epoch: 213, Steps: 1 | Train Loss: 113.3758316 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10765910148620605\n",
      "Epoch: 214, Steps: 1 | Train Loss: 113.3049545 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10761523246765137\n",
      "Epoch: 215, Steps: 1 | Train Loss: 113.4334641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10727930068969727\n",
      "Epoch: 216, Steps: 1 | Train Loss: 113.6172104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10733294486999512\n",
      "Epoch: 217, Steps: 1 | Train Loss: 113.4187775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10724711418151855\n",
      "Epoch: 218, Steps: 1 | Train Loss: 113.3667603 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10776519775390625\n",
      "Epoch: 219, Steps: 1 | Train Loss: 113.2312775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10858798027038574\n",
      "Epoch: 220, Steps: 1 | Train Loss: 113.2136993 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.1075289249420166\n",
      "Epoch: 221, Steps: 1 | Train Loss: 113.1933823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10740828514099121\n",
      "Epoch: 222, Steps: 1 | Train Loss: 113.5340576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.12168359756469727\n",
      "Epoch: 223, Steps: 1 | Train Loss: 113.4619293 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.12120461463928223\n",
      "Epoch: 224, Steps: 1 | Train Loss: 113.1764297 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.12949490547180176\n",
      "Epoch: 225, Steps: 1 | Train Loss: 113.3282013 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.13483309745788574\n",
      "Epoch: 226, Steps: 1 | Train Loss: 113.6227798 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10995149612426758\n",
      "Epoch: 227, Steps: 1 | Train Loss: 113.5741730 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.10817217826843262\n",
      "Epoch: 228, Steps: 1 | Train Loss: 113.4524918 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.10725069046020508\n",
      "Epoch: 229, Steps: 1 | Train Loss: 113.4773788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.1074213981628418\n",
      "Epoch: 230, Steps: 1 | Train Loss: 113.4520416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.11870169639587402\n",
      "Epoch: 231, Steps: 1 | Train Loss: 113.3715820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.12056803703308105\n",
      "Epoch: 232, Steps: 1 | Train Loss: 113.4017868 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11202168464660645\n",
      "Epoch: 233, Steps: 1 | Train Loss: 113.1912460 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.10890340805053711\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.4856186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.10775232315063477\n",
      "Epoch: 235, Steps: 1 | Train Loss: 113.5368042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10681390762329102\n",
      "Epoch: 236, Steps: 1 | Train Loss: 113.6648483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10714507102966309\n",
      "Epoch: 237, Steps: 1 | Train Loss: 113.6755371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10750150680541992\n",
      "Epoch: 238, Steps: 1 | Train Loss: 113.6259918 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10733866691589355\n",
      "Epoch: 239, Steps: 1 | Train Loss: 113.5207291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10720396041870117\n",
      "Epoch: 240, Steps: 1 | Train Loss: 113.3425064 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.11757206916809082\n",
      "Epoch: 241, Steps: 1 | Train Loss: 113.7729492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11288094520568848\n",
      "Epoch: 242, Steps: 1 | Train Loss: 113.4175034 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10996460914611816\n",
      "Epoch: 243, Steps: 1 | Train Loss: 113.4583740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10825538635253906\n",
      "Epoch: 244, Steps: 1 | Train Loss: 113.8695297 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.1074519157409668\n",
      "Epoch: 245, Steps: 1 | Train Loss: 113.7810669 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.10747528076171875\n",
      "Epoch: 246, Steps: 1 | Train Loss: 113.6592560 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.10743379592895508\n",
      "Epoch: 247, Steps: 1 | Train Loss: 113.4563141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10752129554748535\n",
      "Epoch: 248, Steps: 1 | Train Loss: 113.7758179 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10767388343811035\n",
      "Epoch: 249, Steps: 1 | Train Loss: 113.6503448 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10787606239318848\n",
      "Epoch: 250, Steps: 1 | Train Loss: 113.2925034 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_7>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.12308263778686523\n",
      "Epoch: 1, Steps: 1 | Train Loss: 117.3653564 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.11970877647399902\n",
      "Epoch: 2, Steps: 1 | Train Loss: 115.6726837 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10675764083862305\n",
      "Epoch: 3, Steps: 1 | Train Loss: 114.1846542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.11437010765075684\n",
      "Epoch: 4, Steps: 1 | Train Loss: 114.3988571 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10829639434814453\n",
      "Epoch: 5, Steps: 1 | Train Loss: 113.4254303 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10807180404663086\n",
      "Epoch: 6, Steps: 1 | Train Loss: 113.1182632 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10734748840332031\n",
      "Epoch: 7, Steps: 1 | Train Loss: 113.3766632 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10732841491699219\n",
      "Epoch: 8, Steps: 1 | Train Loss: 112.7582321 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10678815841674805\n",
      "Epoch: 9, Steps: 1 | Train Loss: 113.3033066 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10753154754638672\n",
      "Epoch: 10, Steps: 1 | Train Loss: 112.6253662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10947084426879883\n",
      "Epoch: 11, Steps: 1 | Train Loss: 113.0647964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10751819610595703\n",
      "Epoch: 12, Steps: 1 | Train Loss: 113.1106339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10717272758483887\n",
      "Epoch: 13, Steps: 1 | Train Loss: 113.1367416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.1071622371673584\n",
      "Epoch: 14, Steps: 1 | Train Loss: 112.6212769 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10758066177368164\n",
      "Epoch: 15, Steps: 1 | Train Loss: 113.0330353 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12140178680419922\n",
      "Epoch: 16, Steps: 1 | Train Loss: 113.1604233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.1207880973815918\n",
      "Epoch: 17, Steps: 1 | Train Loss: 113.1314240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11236977577209473\n",
      "Epoch: 18, Steps: 1 | Train Loss: 113.5295563 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10978913307189941\n",
      "Epoch: 19, Steps: 1 | Train Loss: 113.3984985 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.10812211036682129\n",
      "Epoch: 20, Steps: 1 | Train Loss: 113.3480225 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.10765409469604492\n",
      "Epoch: 21, Steps: 1 | Train Loss: 113.4651642 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10757899284362793\n",
      "Epoch: 22, Steps: 1 | Train Loss: 113.6513443 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.10844159126281738\n",
      "Epoch: 23, Steps: 1 | Train Loss: 112.7740250 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.10764670372009277\n",
      "Epoch: 24, Steps: 1 | Train Loss: 112.5063095 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.10750174522399902\n",
      "Epoch: 25, Steps: 1 | Train Loss: 113.0130615 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.10730910301208496\n",
      "Epoch: 26, Steps: 1 | Train Loss: 112.5113449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.10730862617492676\n",
      "Epoch: 27, Steps: 1 | Train Loss: 113.3482895 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.10738563537597656\n",
      "Epoch: 28, Steps: 1 | Train Loss: 112.9591599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.12122535705566406\n",
      "Epoch: 29, Steps: 1 | Train Loss: 112.9731445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11958456039428711\n",
      "Epoch: 30, Steps: 1 | Train Loss: 113.3937988 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11090445518493652\n",
      "Epoch: 31, Steps: 1 | Train Loss: 112.9640121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.1088871955871582\n",
      "Epoch: 32, Steps: 1 | Train Loss: 113.4113922 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10812830924987793\n",
      "Epoch: 33, Steps: 1 | Train Loss: 112.4327621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10765504837036133\n",
      "Epoch: 34, Steps: 1 | Train Loss: 113.2225571 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10737323760986328\n",
      "Epoch: 35, Steps: 1 | Train Loss: 113.1496124 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10741257667541504\n",
      "Epoch: 36, Steps: 1 | Train Loss: 112.9356689 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10809063911437988\n",
      "Epoch: 37, Steps: 1 | Train Loss: 113.2019043 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10842490196228027\n",
      "Epoch: 38, Steps: 1 | Train Loss: 113.4711685 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10732889175415039\n",
      "Epoch: 39, Steps: 1 | Train Loss: 112.6544113 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10706138610839844\n",
      "Epoch: 40, Steps: 1 | Train Loss: 113.5735703 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.12434744834899902\n",
      "Epoch: 41, Steps: 1 | Train Loss: 113.1860886 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.12262654304504395\n",
      "Epoch: 42, Steps: 1 | Train Loss: 112.8547592 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.14404535293579102\n",
      "Epoch: 43, Steps: 1 | Train Loss: 112.8894577 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11989378929138184\n",
      "Epoch: 44, Steps: 1 | Train Loss: 112.6747818 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.11978006362915039\n",
      "Epoch: 45, Steps: 1 | Train Loss: 113.1749420 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.10879635810852051\n",
      "Epoch: 46, Steps: 1 | Train Loss: 113.1167221 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11641526222229004\n",
      "Epoch: 47, Steps: 1 | Train Loss: 113.1010666 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.1168365478515625\n",
      "Epoch: 48, Steps: 1 | Train Loss: 112.9898682 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.11568617820739746\n",
      "Epoch: 49, Steps: 1 | Train Loss: 113.2708130 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11190271377563477\n",
      "Epoch: 50, Steps: 1 | Train Loss: 113.2903671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.12067770957946777\n",
      "Epoch: 51, Steps: 1 | Train Loss: 113.1042404 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11300182342529297\n",
      "Epoch: 52, Steps: 1 | Train Loss: 112.7203140 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.11944913864135742\n",
      "Epoch: 53, Steps: 1 | Train Loss: 112.9314728 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.13249611854553223\n",
      "Epoch: 54, Steps: 1 | Train Loss: 112.9125137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.12244892120361328\n",
      "Epoch: 55, Steps: 1 | Train Loss: 113.4569473 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1190946102142334\n",
      "Epoch: 56, Steps: 1 | Train Loss: 112.7957764 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.12157297134399414\n",
      "Epoch: 57, Steps: 1 | Train Loss: 113.8594513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.11817789077758789\n",
      "Epoch: 58, Steps: 1 | Train Loss: 112.9553757 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.1048421859741211\n",
      "Epoch: 59, Steps: 1 | Train Loss: 113.6022263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11638545989990234\n",
      "Epoch: 60, Steps: 1 | Train Loss: 112.9043350 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.11673164367675781\n",
      "Epoch: 61, Steps: 1 | Train Loss: 113.4518356 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11496472358703613\n",
      "Epoch: 62, Steps: 1 | Train Loss: 113.3087387 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11219358444213867\n",
      "Epoch: 63, Steps: 1 | Train Loss: 112.7581329 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.10542631149291992\n",
      "Epoch: 64, Steps: 1 | Train Loss: 112.6367569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.1318199634552002\n",
      "Epoch: 65, Steps: 1 | Train Loss: 112.4833527 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.1282329559326172\n",
      "Epoch: 66, Steps: 1 | Train Loss: 113.1494904 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11757588386535645\n",
      "Epoch: 67, Steps: 1 | Train Loss: 113.2167740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.11985397338867188\n",
      "Epoch: 68, Steps: 1 | Train Loss: 112.4696579 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.1163942813873291\n",
      "Epoch: 69, Steps: 1 | Train Loss: 113.3023682 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11018586158752441\n",
      "Epoch: 70, Steps: 1 | Train Loss: 113.0693893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10796785354614258\n",
      "Epoch: 71, Steps: 1 | Train Loss: 113.3267593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.11429619789123535\n",
      "Epoch: 72, Steps: 1 | Train Loss: 113.3102341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11508846282958984\n",
      "Epoch: 73, Steps: 1 | Train Loss: 113.4259567 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.10717082023620605\n",
      "Epoch: 74, Steps: 1 | Train Loss: 113.2662506 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.10734033584594727\n",
      "Epoch: 75, Steps: 1 | Train Loss: 112.9520187 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.12053418159484863\n",
      "Epoch: 76, Steps: 1 | Train Loss: 112.7819824 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11335229873657227\n",
      "Epoch: 77, Steps: 1 | Train Loss: 113.2531052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10923457145690918\n",
      "Epoch: 78, Steps: 1 | Train Loss: 112.7824936 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10812163352966309\n",
      "Epoch: 79, Steps: 1 | Train Loss: 112.9107056 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10740303993225098\n",
      "Epoch: 80, Steps: 1 | Train Loss: 112.6783447 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.1116793155670166\n",
      "Epoch: 81, Steps: 1 | Train Loss: 112.8261490 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.10760784149169922\n",
      "Epoch: 82, Steps: 1 | Train Loss: 112.6024170 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.1073307991027832\n",
      "Epoch: 83, Steps: 1 | Train Loss: 113.5834732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.10768866539001465\n",
      "Epoch: 84, Steps: 1 | Train Loss: 113.3513184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.12099099159240723\n",
      "Epoch: 85, Steps: 1 | Train Loss: 113.3092194 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11485052108764648\n",
      "Epoch: 86, Steps: 1 | Train Loss: 112.7511749 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.1131443977355957\n",
      "Epoch: 87, Steps: 1 | Train Loss: 112.8000412 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11304712295532227\n",
      "Epoch: 88, Steps: 1 | Train Loss: 113.1151505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11504530906677246\n",
      "Epoch: 89, Steps: 1 | Train Loss: 113.1391144 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11108636856079102\n",
      "Epoch: 90, Steps: 1 | Train Loss: 113.5319977 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.1038656234741211\n",
      "Epoch: 91, Steps: 1 | Train Loss: 112.9486313 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10125088691711426\n",
      "Epoch: 92, Steps: 1 | Train Loss: 112.4506149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10097503662109375\n",
      "Epoch: 93, Steps: 1 | Train Loss: 113.8147125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10061216354370117\n",
      "Epoch: 94, Steps: 1 | Train Loss: 112.8557129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10021138191223145\n",
      "Epoch: 95, Steps: 1 | Train Loss: 112.7772751 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10004186630249023\n",
      "Epoch: 96, Steps: 1 | Train Loss: 112.7671890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10102581977844238\n",
      "Epoch: 97, Steps: 1 | Train Loss: 113.5688705 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10036683082580566\n",
      "Epoch: 98, Steps: 1 | Train Loss: 113.5103683 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.11458706855773926\n",
      "Epoch: 99, Steps: 1 | Train Loss: 113.4029083 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10591006278991699\n",
      "Epoch: 100, Steps: 1 | Train Loss: 112.6056747 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10281658172607422\n",
      "Epoch: 101, Steps: 1 | Train Loss: 113.1530762 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10164618492126465\n",
      "Epoch: 102, Steps: 1 | Train Loss: 112.9189682 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10083317756652832\n",
      "Epoch: 103, Steps: 1 | Train Loss: 112.8847885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10022687911987305\n",
      "Epoch: 104, Steps: 1 | Train Loss: 113.0819244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.11513471603393555\n",
      "Epoch: 105, Steps: 1 | Train Loss: 112.7736969 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10730361938476562\n",
      "Epoch: 106, Steps: 1 | Train Loss: 112.7870102 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10732197761535645\n",
      "Epoch: 107, Steps: 1 | Train Loss: 113.2859650 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10726284980773926\n",
      "Epoch: 108, Steps: 1 | Train Loss: 112.9009018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.12074422836303711\n",
      "Epoch: 109, Steps: 1 | Train Loss: 112.8639450 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.1208641529083252\n",
      "Epoch: 110, Steps: 1 | Train Loss: 112.9354858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11829161643981934\n",
      "Epoch: 111, Steps: 1 | Train Loss: 112.4825668 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.11162257194519043\n",
      "Epoch: 112, Steps: 1 | Train Loss: 113.8541641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11257338523864746\n",
      "Epoch: 113, Steps: 1 | Train Loss: 112.8466339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10991525650024414\n",
      "Epoch: 114, Steps: 1 | Train Loss: 113.0249252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10881328582763672\n",
      "Epoch: 115, Steps: 1 | Train Loss: 113.1410675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10788440704345703\n",
      "Epoch: 116, Steps: 1 | Train Loss: 113.6291733 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10917878150939941\n",
      "Epoch: 117, Steps: 1 | Train Loss: 113.3260040 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.12312960624694824\n",
      "Epoch: 118, Steps: 1 | Train Loss: 113.7542953 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.12146234512329102\n",
      "Epoch: 119, Steps: 1 | Train Loss: 112.9598160 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.11820292472839355\n",
      "Epoch: 120, Steps: 1 | Train Loss: 113.6093979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.12390637397766113\n",
      "Epoch: 121, Steps: 1 | Train Loss: 112.6022263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10829973220825195\n",
      "Epoch: 122, Steps: 1 | Train Loss: 112.9544220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10735082626342773\n",
      "Epoch: 123, Steps: 1 | Train Loss: 112.7850723 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.10766887664794922\n",
      "Epoch: 124, Steps: 1 | Train Loss: 112.6865768 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.10773563385009766\n",
      "Epoch: 125, Steps: 1 | Train Loss: 112.8037643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.11153960227966309\n",
      "Epoch: 126, Steps: 1 | Train Loss: 112.4938583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.1088857650756836\n",
      "Epoch: 127, Steps: 1 | Train Loss: 113.5910187 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.10813689231872559\n",
      "Epoch: 128, Steps: 1 | Train Loss: 113.0527954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10808563232421875\n",
      "Epoch: 129, Steps: 1 | Train Loss: 112.7052155 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10758829116821289\n",
      "Epoch: 130, Steps: 1 | Train Loss: 113.1080093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.10728859901428223\n",
      "Epoch: 131, Steps: 1 | Train Loss: 113.4107666 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10755491256713867\n",
      "Epoch: 132, Steps: 1 | Train Loss: 113.1399460 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.10738158226013184\n",
      "Epoch: 133, Steps: 1 | Train Loss: 113.2941437 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10755467414855957\n",
      "Epoch: 134, Steps: 1 | Train Loss: 112.5502243 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10779333114624023\n",
      "Epoch: 135, Steps: 1 | Train Loss: 112.6378937 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10734248161315918\n",
      "Epoch: 136, Steps: 1 | Train Loss: 112.7244415 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.10733556747436523\n",
      "Epoch: 137, Steps: 1 | Train Loss: 113.1991959 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.10782170295715332\n",
      "Epoch: 138, Steps: 1 | Train Loss: 113.5001221 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.12260651588439941\n",
      "Epoch: 139, Steps: 1 | Train Loss: 112.5055695 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.12207984924316406\n",
      "Epoch: 140, Steps: 1 | Train Loss: 112.5103531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.11324262619018555\n",
      "Epoch: 141, Steps: 1 | Train Loss: 113.5671158 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.1095118522644043\n",
      "Epoch: 142, Steps: 1 | Train Loss: 112.8479538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.10881900787353516\n",
      "Epoch: 143, Steps: 1 | Train Loss: 112.9440842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.1080927848815918\n",
      "Epoch: 144, Steps: 1 | Train Loss: 112.9531250 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10766434669494629\n",
      "Epoch: 145, Steps: 1 | Train Loss: 113.3352280 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10756826400756836\n",
      "Epoch: 146, Steps: 1 | Train Loss: 113.5440216 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.1072084903717041\n",
      "Epoch: 147, Steps: 1 | Train Loss: 113.1600723 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10781168937683105\n",
      "Epoch: 148, Steps: 1 | Train Loss: 113.5996475 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11102056503295898\n",
      "Epoch: 149, Steps: 1 | Train Loss: 112.5457993 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10765981674194336\n",
      "Epoch: 150, Steps: 1 | Train Loss: 112.9944382 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10757255554199219\n",
      "Epoch: 151, Steps: 1 | Train Loss: 113.2700958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10768294334411621\n",
      "Epoch: 152, Steps: 1 | Train Loss: 113.0477219 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12068939208984375\n",
      "Epoch: 153, Steps: 1 | Train Loss: 113.2424240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.12114834785461426\n",
      "Epoch: 154, Steps: 1 | Train Loss: 112.7725143 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.11102414131164551\n",
      "Epoch: 155, Steps: 1 | Train Loss: 113.5366745 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10864710807800293\n",
      "Epoch: 156, Steps: 1 | Train Loss: 113.6824875 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10776042938232422\n",
      "Epoch: 157, Steps: 1 | Train Loss: 113.3767090 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.1077580451965332\n",
      "Epoch: 158, Steps: 1 | Train Loss: 113.3317184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10788726806640625\n",
      "Epoch: 159, Steps: 1 | Train Loss: 112.7744141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.10758805274963379\n",
      "Epoch: 160, Steps: 1 | Train Loss: 113.3683853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.11298084259033203\n",
      "Epoch: 161, Steps: 1 | Train Loss: 113.4628906 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.1087961196899414\n",
      "Epoch: 162, Steps: 1 | Train Loss: 113.4483871 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.11600804328918457\n",
      "Epoch: 163, Steps: 1 | Train Loss: 113.0487213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10961723327636719\n",
      "Epoch: 164, Steps: 1 | Train Loss: 113.1429062 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10858273506164551\n",
      "Epoch: 165, Steps: 1 | Train Loss: 113.3614044 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.1076967716217041\n",
      "Epoch: 166, Steps: 1 | Train Loss: 113.1895752 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10763335227966309\n",
      "Epoch: 167, Steps: 1 | Train Loss: 112.5547714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.10750865936279297\n",
      "Epoch: 168, Steps: 1 | Train Loss: 113.6716690 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.1067352294921875\n",
      "Epoch: 169, Steps: 1 | Train Loss: 112.5468521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11567163467407227\n",
      "Epoch: 170, Steps: 1 | Train Loss: 113.1558151 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11637639999389648\n",
      "Epoch: 171, Steps: 1 | Train Loss: 113.1621017 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.100799560546875\n",
      "Epoch: 172, Steps: 1 | Train Loss: 113.1665573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11483883857727051\n",
      "Epoch: 173, Steps: 1 | Train Loss: 112.8232346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10451197624206543\n",
      "Epoch: 174, Steps: 1 | Train Loss: 113.0422592 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.11003899574279785\n",
      "Epoch: 175, Steps: 1 | Train Loss: 112.4584885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.10824966430664062\n",
      "Epoch: 176, Steps: 1 | Train Loss: 112.5706177 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.11459970474243164\n",
      "Epoch: 177, Steps: 1 | Train Loss: 112.6650314 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11267924308776855\n",
      "Epoch: 178, Steps: 1 | Train Loss: 113.2279816 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11055493354797363\n",
      "Epoch: 179, Steps: 1 | Train Loss: 113.1568756 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10920381546020508\n",
      "Epoch: 180, Steps: 1 | Train Loss: 113.0952377 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11036300659179688\n",
      "Epoch: 181, Steps: 1 | Train Loss: 112.8720932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10904264450073242\n",
      "Epoch: 182, Steps: 1 | Train Loss: 112.8755646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10987091064453125\n",
      "Epoch: 183, Steps: 1 | Train Loss: 112.7723007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11043787002563477\n",
      "Epoch: 184, Steps: 1 | Train Loss: 113.2443008 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.12303590774536133\n",
      "Epoch: 185, Steps: 1 | Train Loss: 112.9744644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.12323951721191406\n",
      "Epoch: 186, Steps: 1 | Train Loss: 112.6879654 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11616373062133789\n",
      "Epoch: 187, Steps: 1 | Train Loss: 112.9165573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11164093017578125\n",
      "Epoch: 188, Steps: 1 | Train Loss: 113.2428970 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.11158514022827148\n",
      "Epoch: 189, Steps: 1 | Train Loss: 113.5524673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.11066651344299316\n",
      "Epoch: 190, Steps: 1 | Train Loss: 113.1011887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10747838020324707\n",
      "Epoch: 191, Steps: 1 | Train Loss: 113.3714371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10742592811584473\n",
      "Epoch: 192, Steps: 1 | Train Loss: 112.1923599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.10754561424255371\n",
      "Epoch: 193, Steps: 1 | Train Loss: 112.4930954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.10765194892883301\n",
      "Epoch: 194, Steps: 1 | Train Loss: 113.1081314 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10756802558898926\n",
      "Epoch: 195, Steps: 1 | Train Loss: 112.9031906 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.10795783996582031\n",
      "Epoch: 196, Steps: 1 | Train Loss: 113.8520660 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.12119483947753906\n",
      "Epoch: 197, Steps: 1 | Train Loss: 113.0770264 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.12116193771362305\n",
      "Epoch: 198, Steps: 1 | Train Loss: 113.1563721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11340451240539551\n",
      "Epoch: 199, Steps: 1 | Train Loss: 113.1157227 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10970830917358398\n",
      "Epoch: 200, Steps: 1 | Train Loss: 113.3124008 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10880184173583984\n",
      "Epoch: 201, Steps: 1 | Train Loss: 114.0780792 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10804605484008789\n",
      "Epoch: 202, Steps: 1 | Train Loss: 113.1474991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10770583152770996\n",
      "Epoch: 203, Steps: 1 | Train Loss: 113.6367722 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.10745739936828613\n",
      "Epoch: 204, Steps: 1 | Train Loss: 113.5084229 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.10858607292175293\n",
      "Epoch: 205, Steps: 1 | Train Loss: 112.8551788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10826373100280762\n",
      "Epoch: 206, Steps: 1 | Train Loss: 113.0946808 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10766720771789551\n",
      "Epoch: 207, Steps: 1 | Train Loss: 112.8982925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.12040424346923828\n",
      "Epoch: 208, Steps: 1 | Train Loss: 112.9799576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11255979537963867\n",
      "Epoch: 209, Steps: 1 | Train Loss: 112.8032608 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.1093912124633789\n",
      "Epoch: 210, Steps: 1 | Train Loss: 112.8701401 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.1083977222442627\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.5250015 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.10736727714538574\n",
      "Epoch: 212, Steps: 1 | Train Loss: 112.9956589 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.10744571685791016\n",
      "Epoch: 213, Steps: 1 | Train Loss: 112.6146774 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10730624198913574\n",
      "Epoch: 214, Steps: 1 | Train Loss: 112.8745117 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10852646827697754\n",
      "Epoch: 215, Steps: 1 | Train Loss: 112.6359177 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11050629615783691\n",
      "Epoch: 216, Steps: 1 | Train Loss: 113.4341965 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11015939712524414\n",
      "Epoch: 217, Steps: 1 | Train Loss: 112.7180786 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10765886306762695\n",
      "Epoch: 218, Steps: 1 | Train Loss: 112.7787704 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10707592964172363\n",
      "Epoch: 219, Steps: 1 | Train Loss: 113.1479492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.12073445320129395\n",
      "Epoch: 220, Steps: 1 | Train Loss: 112.7054214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.12116122245788574\n",
      "Epoch: 221, Steps: 1 | Train Loss: 113.1802521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.12114620208740234\n",
      "Epoch: 222, Steps: 1 | Train Loss: 113.3664169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11707830429077148\n",
      "Epoch: 223, Steps: 1 | Train Loss: 113.1312790 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.1110544204711914\n",
      "Epoch: 224, Steps: 1 | Train Loss: 113.1843491 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.1089944839477539\n",
      "Epoch: 225, Steps: 1 | Train Loss: 112.9751816 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.10700583457946777\n",
      "Epoch: 226, Steps: 1 | Train Loss: 113.2041855 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10746955871582031\n",
      "Epoch: 227, Steps: 1 | Train Loss: 112.6602402 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.10771632194519043\n",
      "Epoch: 228, Steps: 1 | Train Loss: 112.3121109 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.1076667308807373\n",
      "Epoch: 229, Steps: 1 | Train Loss: 113.5470505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.10723328590393066\n",
      "Epoch: 230, Steps: 1 | Train Loss: 113.4240494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.10732436180114746\n",
      "Epoch: 231, Steps: 1 | Train Loss: 112.7261734 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10779643058776855\n",
      "Epoch: 232, Steps: 1 | Train Loss: 113.0182877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.12178611755371094\n",
      "Epoch: 233, Steps: 1 | Train Loss: 112.9429474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11683487892150879\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.1004639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11010122299194336\n",
      "Epoch: 235, Steps: 1 | Train Loss: 112.4329453 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10893058776855469\n",
      "Epoch: 236, Steps: 1 | Train Loss: 113.5152817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10799384117126465\n",
      "Epoch: 237, Steps: 1 | Train Loss: 113.3050766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10716009140014648\n",
      "Epoch: 238, Steps: 1 | Train Loss: 113.2203598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10745072364807129\n",
      "Epoch: 239, Steps: 1 | Train Loss: 112.9403839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10764527320861816\n",
      "Epoch: 240, Steps: 1 | Train Loss: 112.8249512 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1070246696472168\n",
      "Epoch: 241, Steps: 1 | Train Loss: 112.9276276 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.10789680480957031\n",
      "Epoch: 242, Steps: 1 | Train Loss: 113.3893051 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10761404037475586\n",
      "Epoch: 243, Steps: 1 | Train Loss: 113.0752716 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10735654830932617\n",
      "Epoch: 244, Steps: 1 | Train Loss: 113.1779327 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.10774421691894531\n",
      "Epoch: 245, Steps: 1 | Train Loss: 113.9161530 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.12065911293029785\n",
      "Epoch: 246, Steps: 1 | Train Loss: 112.6654663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.12080669403076172\n",
      "Epoch: 247, Steps: 1 | Train Loss: 112.8047638 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.11387205123901367\n",
      "Epoch: 248, Steps: 1 | Train Loss: 112.6512604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10947585105895996\n",
      "Epoch: 249, Steps: 1 | Train Loss: 113.2228546 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10829305648803711\n",
      "Epoch: 250, Steps: 1 | Train Loss: 112.7828598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_8>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11917829513549805\n",
      "Epoch: 1, Steps: 1 | Train Loss: 118.2403183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.11561346054077148\n",
      "Epoch: 2, Steps: 1 | Train Loss: 116.2367554 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10669374465942383\n",
      "Epoch: 3, Steps: 1 | Train Loss: 114.5029449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10876297950744629\n",
      "Epoch: 4, Steps: 1 | Train Loss: 114.0610962 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11737966537475586\n",
      "Epoch: 5, Steps: 1 | Train Loss: 113.3072281 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11241459846496582\n",
      "Epoch: 6, Steps: 1 | Train Loss: 113.4083862 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.12136435508728027\n",
      "Epoch: 7, Steps: 1 | Train Loss: 112.9771729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.11592745780944824\n",
      "Epoch: 8, Steps: 1 | Train Loss: 113.4020767 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10658144950866699\n",
      "Epoch: 9, Steps: 1 | Train Loss: 113.0385284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.11318111419677734\n",
      "Epoch: 10, Steps: 1 | Train Loss: 113.3968506 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.1125802993774414\n",
      "Epoch: 11, Steps: 1 | Train Loss: 112.6263962 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10493993759155273\n",
      "Epoch: 12, Steps: 1 | Train Loss: 113.5605469 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.12387490272521973\n",
      "Epoch: 13, Steps: 1 | Train Loss: 112.8815689 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.11117410659790039\n",
      "Epoch: 14, Steps: 1 | Train Loss: 112.7990723 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.11377620697021484\n",
      "Epoch: 15, Steps: 1 | Train Loss: 112.6558380 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.11594724655151367\n",
      "Epoch: 16, Steps: 1 | Train Loss: 112.6524277 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.11747145652770996\n",
      "Epoch: 17, Steps: 1 | Train Loss: 113.3691330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11519694328308105\n",
      "Epoch: 18, Steps: 1 | Train Loss: 112.8765793 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.11394047737121582\n",
      "Epoch: 19, Steps: 1 | Train Loss: 112.9174194 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.1145942211151123\n",
      "Epoch: 20, Steps: 1 | Train Loss: 112.7627945 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11239814758300781\n",
      "Epoch: 21, Steps: 1 | Train Loss: 112.3959503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.11249208450317383\n",
      "Epoch: 22, Steps: 1 | Train Loss: 112.9766464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.12521839141845703\n",
      "Epoch: 23, Steps: 1 | Train Loss: 113.1539078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.1150503158569336\n",
      "Epoch: 24, Steps: 1 | Train Loss: 112.8194504 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11608362197875977\n",
      "Epoch: 25, Steps: 1 | Train Loss: 113.3210449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.10861349105834961\n",
      "Epoch: 26, Steps: 1 | Train Loss: 113.1787872 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11650323867797852\n",
      "Epoch: 27, Steps: 1 | Train Loss: 112.4872971 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.11319351196289062\n",
      "Epoch: 28, Steps: 1 | Train Loss: 112.9911118 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.11487984657287598\n",
      "Epoch: 29, Steps: 1 | Train Loss: 113.1520538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11140084266662598\n",
      "Epoch: 30, Steps: 1 | Train Loss: 112.8379440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.10790252685546875\n",
      "Epoch: 31, Steps: 1 | Train Loss: 112.9249268 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10942554473876953\n",
      "Epoch: 32, Steps: 1 | Train Loss: 113.3297882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11221694946289062\n",
      "Epoch: 33, Steps: 1 | Train Loss: 113.4305573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10935592651367188\n",
      "Epoch: 34, Steps: 1 | Train Loss: 113.0242691 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10813355445861816\n",
      "Epoch: 35, Steps: 1 | Train Loss: 113.1077881 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.1076970100402832\n",
      "Epoch: 36, Steps: 1 | Train Loss: 113.1147461 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10721278190612793\n",
      "Epoch: 37, Steps: 1 | Train Loss: 113.1243515 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.1071932315826416\n",
      "Epoch: 38, Steps: 1 | Train Loss: 113.1184311 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10689043998718262\n",
      "Epoch: 39, Steps: 1 | Train Loss: 113.2672653 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10711312294006348\n",
      "Epoch: 40, Steps: 1 | Train Loss: 112.6572800 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.1073462963104248\n",
      "Epoch: 41, Steps: 1 | Train Loss: 113.0201187 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10702872276306152\n",
      "Epoch: 42, Steps: 1 | Train Loss: 113.1056137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10744762420654297\n",
      "Epoch: 43, Steps: 1 | Train Loss: 113.1103058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10726404190063477\n",
      "Epoch: 44, Steps: 1 | Train Loss: 113.2145996 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10736417770385742\n",
      "Epoch: 45, Steps: 1 | Train Loss: 112.6445694 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.12067389488220215\n",
      "Epoch: 46, Steps: 1 | Train Loss: 113.2731476 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.12205624580383301\n",
      "Epoch: 47, Steps: 1 | Train Loss: 112.9670639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.12080788612365723\n",
      "Epoch: 48, Steps: 1 | Train Loss: 113.0799789 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.11484694480895996\n",
      "Epoch: 49, Steps: 1 | Train Loss: 113.0642853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.10927605628967285\n",
      "Epoch: 50, Steps: 1 | Train Loss: 112.8463745 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.10855984687805176\n",
      "Epoch: 51, Steps: 1 | Train Loss: 113.3619995 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10871267318725586\n",
      "Epoch: 52, Steps: 1 | Train Loss: 112.5190582 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10858416557312012\n",
      "Epoch: 53, Steps: 1 | Train Loss: 112.6785202 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10799884796142578\n",
      "Epoch: 54, Steps: 1 | Train Loss: 113.1363525 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.1072392463684082\n",
      "Epoch: 55, Steps: 1 | Train Loss: 112.8654938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.10734939575195312\n",
      "Epoch: 56, Steps: 1 | Train Loss: 113.0752182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10752034187316895\n",
      "Epoch: 57, Steps: 1 | Train Loss: 113.1247330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10711407661437988\n",
      "Epoch: 58, Steps: 1 | Train Loss: 112.9984360 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.10705828666687012\n",
      "Epoch: 59, Steps: 1 | Train Loss: 113.2454834 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10735607147216797\n",
      "Epoch: 60, Steps: 1 | Train Loss: 113.1924820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.10705447196960449\n",
      "Epoch: 61, Steps: 1 | Train Loss: 112.9235840 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.10719990730285645\n",
      "Epoch: 62, Steps: 1 | Train Loss: 113.2342758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.12052202224731445\n",
      "Epoch: 63, Steps: 1 | Train Loss: 112.8316650 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.12280511856079102\n",
      "Epoch: 64, Steps: 1 | Train Loss: 112.8111115 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.1126394271850586\n",
      "Epoch: 65, Steps: 1 | Train Loss: 113.1229630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11803245544433594\n",
      "Epoch: 66, Steps: 1 | Train Loss: 112.6825485 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.10801243782043457\n",
      "Epoch: 67, Steps: 1 | Train Loss: 113.1753693 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.1119849681854248\n",
      "Epoch: 68, Steps: 1 | Train Loss: 113.1118164 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.11086440086364746\n",
      "Epoch: 69, Steps: 1 | Train Loss: 113.1408005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11119747161865234\n",
      "Epoch: 70, Steps: 1 | Train Loss: 112.3799362 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10870504379272461\n",
      "Epoch: 71, Steps: 1 | Train Loss: 113.0162354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10801315307617188\n",
      "Epoch: 72, Steps: 1 | Train Loss: 112.9990234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11588907241821289\n",
      "Epoch: 73, Steps: 1 | Train Loss: 112.8464966 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.11049866676330566\n",
      "Epoch: 74, Steps: 1 | Train Loss: 113.0086670 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.10814046859741211\n",
      "Epoch: 75, Steps: 1 | Train Loss: 113.4231949 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.10724306106567383\n",
      "Epoch: 76, Steps: 1 | Train Loss: 112.9460983 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.10727262496948242\n",
      "Epoch: 77, Steps: 1 | Train Loss: 113.0416794 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10707712173461914\n",
      "Epoch: 78, Steps: 1 | Train Loss: 113.0602188 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10742998123168945\n",
      "Epoch: 79, Steps: 1 | Train Loss: 113.0778580 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10729026794433594\n",
      "Epoch: 80, Steps: 1 | Train Loss: 112.9868393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10722970962524414\n",
      "Epoch: 81, Steps: 1 | Train Loss: 113.1882858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.1213078498840332\n",
      "Epoch: 82, Steps: 1 | Train Loss: 113.1761246 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.1173708438873291\n",
      "Epoch: 83, Steps: 1 | Train Loss: 112.6894531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11011314392089844\n",
      "Epoch: 84, Steps: 1 | Train Loss: 113.3884048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.10875296592712402\n",
      "Epoch: 85, Steps: 1 | Train Loss: 112.6949234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.10785198211669922\n",
      "Epoch: 86, Steps: 1 | Train Loss: 112.8786163 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10876035690307617\n",
      "Epoch: 87, Steps: 1 | Train Loss: 113.4604874 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.10947680473327637\n",
      "Epoch: 88, Steps: 1 | Train Loss: 113.2828903 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10774469375610352\n",
      "Epoch: 89, Steps: 1 | Train Loss: 113.2033234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10736989974975586\n",
      "Epoch: 90, Steps: 1 | Train Loss: 113.0631866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10727477073669434\n",
      "Epoch: 91, Steps: 1 | Train Loss: 112.9112091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.1075432300567627\n",
      "Epoch: 92, Steps: 1 | Train Loss: 112.9175568 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.1074981689453125\n",
      "Epoch: 93, Steps: 1 | Train Loss: 113.1579361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.11927413940429688\n",
      "Epoch: 94, Steps: 1 | Train Loss: 112.9567490 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.11252951622009277\n",
      "Epoch: 95, Steps: 1 | Train Loss: 112.7057495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10915660858154297\n",
      "Epoch: 96, Steps: 1 | Train Loss: 112.6361237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10805511474609375\n",
      "Epoch: 97, Steps: 1 | Train Loss: 112.7311020 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10751914978027344\n",
      "Epoch: 98, Steps: 1 | Train Loss: 113.1928024 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10804271697998047\n",
      "Epoch: 99, Steps: 1 | Train Loss: 112.9268799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10751771926879883\n",
      "Epoch: 100, Steps: 1 | Train Loss: 113.0807419 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10715770721435547\n",
      "Epoch: 101, Steps: 1 | Train Loss: 113.4057388 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10747671127319336\n",
      "Epoch: 102, Steps: 1 | Train Loss: 113.0590591 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10730385780334473\n",
      "Epoch: 103, Steps: 1 | Train Loss: 113.4255524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10712409019470215\n",
      "Epoch: 104, Steps: 1 | Train Loss: 113.1982651 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10680580139160156\n",
      "Epoch: 105, Steps: 1 | Train Loss: 113.5618286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.12069940567016602\n",
      "Epoch: 106, Steps: 1 | Train Loss: 113.0221939 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.12032365798950195\n",
      "Epoch: 107, Steps: 1 | Train Loss: 113.1597214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.12021923065185547\n",
      "Epoch: 108, Steps: 1 | Train Loss: 113.4741592 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11378145217895508\n",
      "Epoch: 109, Steps: 1 | Train Loss: 113.1835251 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.10912132263183594\n",
      "Epoch: 110, Steps: 1 | Train Loss: 112.9306641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.10868668556213379\n",
      "Epoch: 111, Steps: 1 | Train Loss: 113.1505051 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.1082158088684082\n",
      "Epoch: 112, Steps: 1 | Train Loss: 112.7915039 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10729742050170898\n",
      "Epoch: 113, Steps: 1 | Train Loss: 112.5814590 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10741949081420898\n",
      "Epoch: 114, Steps: 1 | Train Loss: 113.0249023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10767173767089844\n",
      "Epoch: 115, Steps: 1 | Train Loss: 113.1615143 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10743975639343262\n",
      "Epoch: 116, Steps: 1 | Train Loss: 113.0757065 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.1072380542755127\n",
      "Epoch: 117, Steps: 1 | Train Loss: 113.3052521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10868382453918457\n",
      "Epoch: 118, Steps: 1 | Train Loss: 113.4681931 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10791659355163574\n",
      "Epoch: 119, Steps: 1 | Train Loss: 112.7464752 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.10873937606811523\n",
      "Epoch: 120, Steps: 1 | Train Loss: 112.9472427 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.12076163291931152\n",
      "Epoch: 121, Steps: 1 | Train Loss: 113.1347809 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.1100764274597168\n",
      "Epoch: 122, Steps: 1 | Train Loss: 113.0935593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10813307762145996\n",
      "Epoch: 123, Steps: 1 | Train Loss: 113.1740494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.1076056957244873\n",
      "Epoch: 124, Steps: 1 | Train Loss: 113.3779678 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.10728836059570312\n",
      "Epoch: 125, Steps: 1 | Train Loss: 112.6447144 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.1076042652130127\n",
      "Epoch: 126, Steps: 1 | Train Loss: 112.9155045 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.10728597640991211\n",
      "Epoch: 127, Steps: 1 | Train Loss: 112.5003891 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.10727858543395996\n",
      "Epoch: 128, Steps: 1 | Train Loss: 113.3647690 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10742306709289551\n",
      "Epoch: 129, Steps: 1 | Train Loss: 112.9610138 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.12177062034606934\n",
      "Epoch: 130, Steps: 1 | Train Loss: 112.9556808 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11540865898132324\n",
      "Epoch: 131, Steps: 1 | Train Loss: 113.0254440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10944008827209473\n",
      "Epoch: 132, Steps: 1 | Train Loss: 113.0081558 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.10843896865844727\n",
      "Epoch: 133, Steps: 1 | Train Loss: 113.1297150 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10776591300964355\n",
      "Epoch: 134, Steps: 1 | Train Loss: 113.5586472 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10763144493103027\n",
      "Epoch: 135, Steps: 1 | Train Loss: 113.0705338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.107666015625\n",
      "Epoch: 136, Steps: 1 | Train Loss: 113.0482788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.10720157623291016\n",
      "Epoch: 137, Steps: 1 | Train Loss: 113.0334091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.10734438896179199\n",
      "Epoch: 138, Steps: 1 | Train Loss: 113.6414108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.1075141429901123\n",
      "Epoch: 139, Steps: 1 | Train Loss: 113.1144791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10752606391906738\n",
      "Epoch: 140, Steps: 1 | Train Loss: 113.0303726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.12163066864013672\n",
      "Epoch: 141, Steps: 1 | Train Loss: 112.9421616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11814498901367188\n",
      "Epoch: 142, Steps: 1 | Train Loss: 113.2324448 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11011695861816406\n",
      "Epoch: 143, Steps: 1 | Train Loss: 113.0179825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.1089937686920166\n",
      "Epoch: 144, Steps: 1 | Train Loss: 112.9399643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10805559158325195\n",
      "Epoch: 145, Steps: 1 | Train Loss: 112.6394577 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10754799842834473\n",
      "Epoch: 146, Steps: 1 | Train Loss: 113.0851593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.1074223518371582\n",
      "Epoch: 147, Steps: 1 | Train Loss: 113.0690689 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10735964775085449\n",
      "Epoch: 148, Steps: 1 | Train Loss: 113.2796021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.1076350212097168\n",
      "Epoch: 149, Steps: 1 | Train Loss: 112.8578415 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10747003555297852\n",
      "Epoch: 150, Steps: 1 | Train Loss: 113.1899796 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10778570175170898\n",
      "Epoch: 151, Steps: 1 | Train Loss: 112.9224243 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10778355598449707\n",
      "Epoch: 152, Steps: 1 | Train Loss: 113.0587158 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12144613265991211\n",
      "Epoch: 153, Steps: 1 | Train Loss: 112.9767380 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.11050033569335938\n",
      "Epoch: 154, Steps: 1 | Train Loss: 113.3070221 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10804891586303711\n",
      "Epoch: 155, Steps: 1 | Train Loss: 112.9241486 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10820674896240234\n",
      "Epoch: 156, Steps: 1 | Train Loss: 112.7862549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10757780075073242\n",
      "Epoch: 157, Steps: 1 | Train Loss: 112.9253311 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10738754272460938\n",
      "Epoch: 158, Steps: 1 | Train Loss: 113.2795639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10877060890197754\n",
      "Epoch: 159, Steps: 1 | Train Loss: 112.8610153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11895895004272461\n",
      "Epoch: 160, Steps: 1 | Train Loss: 113.2053146 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10980343818664551\n",
      "Epoch: 161, Steps: 1 | Train Loss: 113.4293594 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10914087295532227\n",
      "Epoch: 162, Steps: 1 | Train Loss: 113.6741257 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10807967185974121\n",
      "Epoch: 163, Steps: 1 | Train Loss: 113.5469513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10747981071472168\n",
      "Epoch: 164, Steps: 1 | Train Loss: 112.8069077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10755181312561035\n",
      "Epoch: 165, Steps: 1 | Train Loss: 113.2334137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.10747957229614258\n",
      "Epoch: 166, Steps: 1 | Train Loss: 112.7822037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.1076052188873291\n",
      "Epoch: 167, Steps: 1 | Train Loss: 112.8288193 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.10734963417053223\n",
      "Epoch: 168, Steps: 1 | Train Loss: 113.3139877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.10748553276062012\n",
      "Epoch: 169, Steps: 1 | Train Loss: 112.8741684 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11909604072570801\n",
      "Epoch: 170, Steps: 1 | Train Loss: 113.1518326 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11020159721374512\n",
      "Epoch: 171, Steps: 1 | Train Loss: 112.8429794 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.10873293876647949\n",
      "Epoch: 172, Steps: 1 | Train Loss: 112.9597397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.1074063777923584\n",
      "Epoch: 173, Steps: 1 | Train Loss: 112.7668839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10724902153015137\n",
      "Epoch: 174, Steps: 1 | Train Loss: 113.3297882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10742878913879395\n",
      "Epoch: 175, Steps: 1 | Train Loss: 112.8703766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.10799956321716309\n",
      "Epoch: 176, Steps: 1 | Train Loss: 112.9524689 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10760307312011719\n",
      "Epoch: 177, Steps: 1 | Train Loss: 113.1001205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10747718811035156\n",
      "Epoch: 178, Steps: 1 | Train Loss: 113.2156982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.1217203140258789\n",
      "Epoch: 179, Steps: 1 | Train Loss: 113.1636505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.1153256893157959\n",
      "Epoch: 180, Steps: 1 | Train Loss: 113.5629807 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.10971236228942871\n",
      "Epoch: 181, Steps: 1 | Train Loss: 113.2957153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10825037956237793\n",
      "Epoch: 182, Steps: 1 | Train Loss: 113.2488556 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10785818099975586\n",
      "Epoch: 183, Steps: 1 | Train Loss: 113.2932129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.10742568969726562\n",
      "Epoch: 184, Steps: 1 | Train Loss: 113.3054428 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.10762882232666016\n",
      "Epoch: 185, Steps: 1 | Train Loss: 112.9606552 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.1073904037475586\n",
      "Epoch: 186, Steps: 1 | Train Loss: 113.1921768 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10753226280212402\n",
      "Epoch: 187, Steps: 1 | Train Loss: 113.0709457 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10781526565551758\n",
      "Epoch: 188, Steps: 1 | Train Loss: 112.9317169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.1074366569519043\n",
      "Epoch: 189, Steps: 1 | Train Loss: 112.9253082 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.1074373722076416\n",
      "Epoch: 190, Steps: 1 | Train Loss: 113.0102081 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.13873624801635742\n",
      "Epoch: 191, Steps: 1 | Train Loss: 112.5668106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.14676547050476074\n",
      "Epoch: 192, Steps: 1 | Train Loss: 113.0705566 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.12259483337402344\n",
      "Epoch: 193, Steps: 1 | Train Loss: 113.0880890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11315536499023438\n",
      "Epoch: 194, Steps: 1 | Train Loss: 113.1815414 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11089015007019043\n",
      "Epoch: 195, Steps: 1 | Train Loss: 113.1816788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11618947982788086\n",
      "Epoch: 196, Steps: 1 | Train Loss: 113.3224106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11171746253967285\n",
      "Epoch: 197, Steps: 1 | Train Loss: 113.1970139 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11090588569641113\n",
      "Epoch: 198, Steps: 1 | Train Loss: 113.0287247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11093902587890625\n",
      "Epoch: 199, Steps: 1 | Train Loss: 112.8257675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11087632179260254\n",
      "Epoch: 200, Steps: 1 | Train Loss: 112.4978027 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11163735389709473\n",
      "Epoch: 201, Steps: 1 | Train Loss: 112.9907379 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.1104278564453125\n",
      "Epoch: 202, Steps: 1 | Train Loss: 113.2497559 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.12461614608764648\n",
      "Epoch: 203, Steps: 1 | Train Loss: 112.5909424 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.1240851879119873\n",
      "Epoch: 204, Steps: 1 | Train Loss: 113.1385498 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12311792373657227\n",
      "Epoch: 205, Steps: 1 | Train Loss: 113.2586823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.12128758430480957\n",
      "Epoch: 206, Steps: 1 | Train Loss: 113.1267090 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11343216896057129\n",
      "Epoch: 207, Steps: 1 | Train Loss: 112.8430328 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.1121675968170166\n",
      "Epoch: 208, Steps: 1 | Train Loss: 113.2032471 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11155915260314941\n",
      "Epoch: 209, Steps: 1 | Train Loss: 113.5069504 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11069798469543457\n",
      "Epoch: 210, Steps: 1 | Train Loss: 113.3825912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.11055707931518555\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.9531021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11099910736083984\n",
      "Epoch: 212, Steps: 1 | Train Loss: 112.9980240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11099934577941895\n",
      "Epoch: 213, Steps: 1 | Train Loss: 113.3197021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11045312881469727\n",
      "Epoch: 214, Steps: 1 | Train Loss: 113.1432114 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11097979545593262\n",
      "Epoch: 215, Steps: 1 | Train Loss: 112.8948975 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11076498031616211\n",
      "Epoch: 216, Steps: 1 | Train Loss: 113.1773682 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11028528213500977\n",
      "Epoch: 217, Steps: 1 | Train Loss: 113.0850143 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.12398028373718262\n",
      "Epoch: 218, Steps: 1 | Train Loss: 113.1349869 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.12406563758850098\n",
      "Epoch: 219, Steps: 1 | Train Loss: 113.2611923 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.12429094314575195\n",
      "Epoch: 220, Steps: 1 | Train Loss: 112.8923721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11983823776245117\n",
      "Epoch: 221, Steps: 1 | Train Loss: 113.1436081 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.11232948303222656\n",
      "Epoch: 222, Steps: 1 | Train Loss: 113.0085220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11330437660217285\n",
      "Epoch: 223, Steps: 1 | Train Loss: 113.6357422 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11301946640014648\n",
      "Epoch: 224, Steps: 1 | Train Loss: 112.9516602 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11100435256958008\n",
      "Epoch: 225, Steps: 1 | Train Loss: 113.2393341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11150074005126953\n",
      "Epoch: 226, Steps: 1 | Train Loss: 112.7433853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11088037490844727\n",
      "Epoch: 227, Steps: 1 | Train Loss: 112.9269333 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11066317558288574\n",
      "Epoch: 228, Steps: 1 | Train Loss: 113.1229172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11020517349243164\n",
      "Epoch: 229, Steps: 1 | Train Loss: 112.8112335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11057710647583008\n",
      "Epoch: 230, Steps: 1 | Train Loss: 113.1245880 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.11957550048828125\n",
      "Epoch: 231, Steps: 1 | Train Loss: 112.9870605 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.11542701721191406\n",
      "Epoch: 232, Steps: 1 | Train Loss: 113.0248947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11741161346435547\n",
      "Epoch: 233, Steps: 1 | Train Loss: 113.2266159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.1226053237915039\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.0645676 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11406111717224121\n",
      "Epoch: 235, Steps: 1 | Train Loss: 113.4131393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10943388938903809\n",
      "Epoch: 236, Steps: 1 | Train Loss: 112.9106445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11165881156921387\n",
      "Epoch: 237, Steps: 1 | Train Loss: 113.2412643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11241459846496582\n",
      "Epoch: 238, Steps: 1 | Train Loss: 112.9402008 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.11267876625061035\n",
      "Epoch: 239, Steps: 1 | Train Loss: 112.9154587 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.11191010475158691\n",
      "Epoch: 240, Steps: 1 | Train Loss: 113.1123810 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.11104607582092285\n",
      "Epoch: 241, Steps: 1 | Train Loss: 113.6703644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.10932159423828125\n",
      "Epoch: 242, Steps: 1 | Train Loss: 113.0336838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10871076583862305\n",
      "Epoch: 243, Steps: 1 | Train Loss: 113.0239029 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.11927199363708496\n",
      "Epoch: 244, Steps: 1 | Train Loss: 112.8170166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.11986184120178223\n",
      "Epoch: 245, Steps: 1 | Train Loss: 113.4355240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11774373054504395\n",
      "Epoch: 246, Steps: 1 | Train Loss: 113.1810074 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.1275172233581543\n",
      "Epoch: 247, Steps: 1 | Train Loss: 113.0101929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.11249256134033203\n",
      "Epoch: 248, Steps: 1 | Train Loss: 113.1815948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10949134826660156\n",
      "Epoch: 249, Steps: 1 | Train Loss: 113.1011963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11451077461242676\n",
      "Epoch: 250, Steps: 1 | Train Loss: 112.7793350 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_9>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.12687206268310547\n",
      "Epoch: 1, Steps: 1 | Train Loss: 119.9680405 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.1258831024169922\n",
      "Epoch: 2, Steps: 1 | Train Loss: 118.2627335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11988019943237305\n",
      "Epoch: 3, Steps: 1 | Train Loss: 116.7240524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.11343789100646973\n",
      "Epoch: 4, Steps: 1 | Train Loss: 115.6325226 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11200809478759766\n",
      "Epoch: 5, Steps: 1 | Train Loss: 116.3891830 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11443638801574707\n",
      "Epoch: 6, Steps: 1 | Train Loss: 115.2443619 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11185002326965332\n",
      "Epoch: 7, Steps: 1 | Train Loss: 115.4259262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.11097502708435059\n",
      "Epoch: 8, Steps: 1 | Train Loss: 115.3171158 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11019349098205566\n",
      "Epoch: 9, Steps: 1 | Train Loss: 114.8433838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.11088180541992188\n",
      "Epoch: 10, Steps: 1 | Train Loss: 115.2826920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.11067318916320801\n",
      "Epoch: 11, Steps: 1 | Train Loss: 115.1884384 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11015868186950684\n",
      "Epoch: 12, Steps: 1 | Train Loss: 115.9852676 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11145281791687012\n",
      "Epoch: 13, Steps: 1 | Train Loss: 115.7950211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.11046814918518066\n",
      "Epoch: 14, Steps: 1 | Train Loss: 115.3924179 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.12429666519165039\n",
      "Epoch: 15, Steps: 1 | Train Loss: 115.3090134 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12460660934448242\n",
      "Epoch: 16, Steps: 1 | Train Loss: 115.1141357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.12293243408203125\n",
      "Epoch: 17, Steps: 1 | Train Loss: 115.2632599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11507320404052734\n",
      "Epoch: 18, Steps: 1 | Train Loss: 115.3820343 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10851883888244629\n",
      "Epoch: 19, Steps: 1 | Train Loss: 115.6556625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.1091315746307373\n",
      "Epoch: 20, Steps: 1 | Train Loss: 115.2038574 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.13331913948059082\n",
      "Epoch: 21, Steps: 1 | Train Loss: 115.5661240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.1264512538909912\n",
      "Epoch: 22, Steps: 1 | Train Loss: 115.5619507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.1225748062133789\n",
      "Epoch: 23, Steps: 1 | Train Loss: 115.0862961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.12458395957946777\n",
      "Epoch: 24, Steps: 1 | Train Loss: 114.5576096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.12145233154296875\n",
      "Epoch: 25, Steps: 1 | Train Loss: 115.3407974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12443089485168457\n",
      "Epoch: 26, Steps: 1 | Train Loss: 115.0458298 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.12755870819091797\n",
      "Epoch: 27, Steps: 1 | Train Loss: 115.6820602 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.1250295639038086\n",
      "Epoch: 28, Steps: 1 | Train Loss: 115.8177032 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.12647151947021484\n",
      "Epoch: 29, Steps: 1 | Train Loss: 115.5919724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.12461018562316895\n",
      "Epoch: 30, Steps: 1 | Train Loss: 115.2584991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.12704873085021973\n",
      "Epoch: 31, Steps: 1 | Train Loss: 115.2377548 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.13516879081726074\n",
      "Epoch: 32, Steps: 1 | Train Loss: 114.8934860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.1376049518585205\n",
      "Epoch: 33, Steps: 1 | Train Loss: 115.2038345 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.13431358337402344\n",
      "Epoch: 34, Steps: 1 | Train Loss: 115.2087936 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.13787031173706055\n",
      "Epoch: 35, Steps: 1 | Train Loss: 114.7703781 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.11800289154052734\n",
      "Epoch: 36, Steps: 1 | Train Loss: 115.2454376 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.12581515312194824\n",
      "Epoch: 37, Steps: 1 | Train Loss: 115.6072922 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.12430596351623535\n",
      "Epoch: 38, Steps: 1 | Train Loss: 115.4977341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.12553119659423828\n",
      "Epoch: 39, Steps: 1 | Train Loss: 114.8944016 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.12676382064819336\n",
      "Epoch: 40, Steps: 1 | Train Loss: 115.6389771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.12222051620483398\n",
      "Epoch: 41, Steps: 1 | Train Loss: 114.9700851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.13527798652648926\n",
      "Epoch: 42, Steps: 1 | Train Loss: 114.8821564 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.12254071235656738\n",
      "Epoch: 43, Steps: 1 | Train Loss: 116.1278839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.13584256172180176\n",
      "Epoch: 44, Steps: 1 | Train Loss: 114.9720230 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.12305617332458496\n",
      "Epoch: 45, Steps: 1 | Train Loss: 115.7676544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11617302894592285\n",
      "Epoch: 46, Steps: 1 | Train Loss: 114.9579086 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11591887474060059\n",
      "Epoch: 47, Steps: 1 | Train Loss: 115.0949020 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.11598014831542969\n",
      "Epoch: 48, Steps: 1 | Train Loss: 114.8405304 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.1159672737121582\n",
      "Epoch: 49, Steps: 1 | Train Loss: 114.9386215 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.12018609046936035\n",
      "Epoch: 50, Steps: 1 | Train Loss: 115.0612335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.1205298900604248\n",
      "Epoch: 51, Steps: 1 | Train Loss: 114.6903610 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.1165010929107666\n",
      "Epoch: 52, Steps: 1 | Train Loss: 115.7047958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10924386978149414\n",
      "Epoch: 53, Steps: 1 | Train Loss: 115.2838516 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10778307914733887\n",
      "Epoch: 54, Steps: 1 | Train Loss: 115.5786667 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.10711860656738281\n",
      "Epoch: 55, Steps: 1 | Train Loss: 115.2656784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.10680174827575684\n",
      "Epoch: 56, Steps: 1 | Train Loss: 115.6817856 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10665154457092285\n",
      "Epoch: 57, Steps: 1 | Train Loss: 114.9767075 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.1069188117980957\n",
      "Epoch: 58, Steps: 1 | Train Loss: 115.7307129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.1064140796661377\n",
      "Epoch: 59, Steps: 1 | Train Loss: 115.4994659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10655426979064941\n",
      "Epoch: 60, Steps: 1 | Train Loss: 115.1792526 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.10684061050415039\n",
      "Epoch: 61, Steps: 1 | Train Loss: 115.4010544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.12010741233825684\n",
      "Epoch: 62, Steps: 1 | Train Loss: 115.4661407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11995339393615723\n",
      "Epoch: 63, Steps: 1 | Train Loss: 115.4238052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.13017749786376953\n",
      "Epoch: 64, Steps: 1 | Train Loss: 115.2084579 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.1232767105102539\n",
      "Epoch: 65, Steps: 1 | Train Loss: 115.5603561 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11863136291503906\n",
      "Epoch: 66, Steps: 1 | Train Loss: 115.0540771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11164379119873047\n",
      "Epoch: 67, Steps: 1 | Train Loss: 115.3454361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.11254334449768066\n",
      "Epoch: 68, Steps: 1 | Train Loss: 115.3033829 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.11005830764770508\n",
      "Epoch: 69, Steps: 1 | Train Loss: 115.1431656 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11097574234008789\n",
      "Epoch: 70, Steps: 1 | Train Loss: 115.3773193 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.11126923561096191\n",
      "Epoch: 71, Steps: 1 | Train Loss: 114.6996384 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10963034629821777\n",
      "Epoch: 72, Steps: 1 | Train Loss: 115.4597778 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11123895645141602\n",
      "Epoch: 73, Steps: 1 | Train Loss: 115.4394760 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.11136364936828613\n",
      "Epoch: 74, Steps: 1 | Train Loss: 115.4375000 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.12331485748291016\n",
      "Epoch: 75, Steps: 1 | Train Loss: 115.4771042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11434745788574219\n",
      "Epoch: 76, Steps: 1 | Train Loss: 115.6433945 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11208963394165039\n",
      "Epoch: 77, Steps: 1 | Train Loss: 115.6847763 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.11122679710388184\n",
      "Epoch: 78, Steps: 1 | Train Loss: 115.0665054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.1107640266418457\n",
      "Epoch: 79, Steps: 1 | Train Loss: 115.8956070 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.11065101623535156\n",
      "Epoch: 80, Steps: 1 | Train Loss: 115.1845703 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.11039280891418457\n",
      "Epoch: 81, Steps: 1 | Train Loss: 115.6329575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11002016067504883\n",
      "Epoch: 82, Steps: 1 | Train Loss: 115.1598434 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.10957980155944824\n",
      "Epoch: 83, Steps: 1 | Train Loss: 115.4038315 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11120724678039551\n",
      "Epoch: 84, Steps: 1 | Train Loss: 114.9406967 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.1101844310760498\n",
      "Epoch: 85, Steps: 1 | Train Loss: 115.5680695 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11019372940063477\n",
      "Epoch: 86, Steps: 1 | Train Loss: 115.1486588 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.1241464614868164\n",
      "Epoch: 87, Steps: 1 | Train Loss: 115.4423141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.12972378730773926\n",
      "Epoch: 88, Steps: 1 | Train Loss: 115.4393463 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11675477027893066\n",
      "Epoch: 89, Steps: 1 | Train Loss: 115.2346344 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11157369613647461\n",
      "Epoch: 90, Steps: 1 | Train Loss: 115.5618515 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.11093401908874512\n",
      "Epoch: 91, Steps: 1 | Train Loss: 115.2703857 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10939192771911621\n",
      "Epoch: 92, Steps: 1 | Train Loss: 114.7773590 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.11093497276306152\n",
      "Epoch: 93, Steps: 1 | Train Loss: 115.1167984 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.11092925071716309\n",
      "Epoch: 94, Steps: 1 | Train Loss: 115.9460449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.1103963851928711\n",
      "Epoch: 95, Steps: 1 | Train Loss: 114.9708176 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10996460914611816\n",
      "Epoch: 96, Steps: 1 | Train Loss: 115.4962158 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.11009812355041504\n",
      "Epoch: 97, Steps: 1 | Train Loss: 115.0007935 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10925722122192383\n",
      "Epoch: 98, Steps: 1 | Train Loss: 115.2007828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.12304472923278809\n",
      "Epoch: 99, Steps: 1 | Train Loss: 115.0684128 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.1351931095123291\n",
      "Epoch: 100, Steps: 1 | Train Loss: 115.8703766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.13009190559387207\n",
      "Epoch: 101, Steps: 1 | Train Loss: 115.0239182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.13142871856689453\n",
      "Epoch: 102, Steps: 1 | Train Loss: 114.9389877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.13065814971923828\n",
      "Epoch: 103, Steps: 1 | Train Loss: 115.4061279 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.13078665733337402\n",
      "Epoch: 104, Steps: 1 | Train Loss: 115.1374435 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.13581037521362305\n",
      "Epoch: 105, Steps: 1 | Train Loss: 115.1459732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.13799595832824707\n",
      "Epoch: 106, Steps: 1 | Train Loss: 115.5857086 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.13652682304382324\n",
      "Epoch: 107, Steps: 1 | Train Loss: 115.3905792 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.13624191284179688\n",
      "Epoch: 108, Steps: 1 | Train Loss: 115.3714371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.1374058723449707\n",
      "Epoch: 109, Steps: 1 | Train Loss: 115.6187668 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.14352107048034668\n",
      "Epoch: 110, Steps: 1 | Train Loss: 115.2144318 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.12099075317382812\n",
      "Epoch: 111, Steps: 1 | Train Loss: 115.3269806 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.11821937561035156\n",
      "Epoch: 112, Steps: 1 | Train Loss: 115.5861359 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11911201477050781\n",
      "Epoch: 113, Steps: 1 | Train Loss: 115.6511841 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.11769509315490723\n",
      "Epoch: 114, Steps: 1 | Train Loss: 115.3568878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.12205076217651367\n",
      "Epoch: 115, Steps: 1 | Train Loss: 115.0476074 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.12268996238708496\n",
      "Epoch: 116, Steps: 1 | Train Loss: 115.3284912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.12424373626708984\n",
      "Epoch: 117, Steps: 1 | Train Loss: 115.9554443 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.12398123741149902\n",
      "Epoch: 118, Steps: 1 | Train Loss: 115.2619629 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.11942744255065918\n",
      "Epoch: 119, Steps: 1 | Train Loss: 115.6398849 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.11383318901062012\n",
      "Epoch: 120, Steps: 1 | Train Loss: 114.9157715 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11278009414672852\n",
      "Epoch: 121, Steps: 1 | Train Loss: 115.2839584 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11269378662109375\n",
      "Epoch: 122, Steps: 1 | Train Loss: 116.3977661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.11183285713195801\n",
      "Epoch: 123, Steps: 1 | Train Loss: 115.0724869 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11163711547851562\n",
      "Epoch: 124, Steps: 1 | Train Loss: 115.6940231 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.11216616630554199\n",
      "Epoch: 125, Steps: 1 | Train Loss: 115.5876236 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.11151361465454102\n",
      "Epoch: 126, Steps: 1 | Train Loss: 115.6729279 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11049199104309082\n",
      "Epoch: 127, Steps: 1 | Train Loss: 115.5812149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.11064648628234863\n",
      "Epoch: 128, Steps: 1 | Train Loss: 115.4968414 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.1100611686706543\n",
      "Epoch: 129, Steps: 1 | Train Loss: 115.2151718 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11068606376647949\n",
      "Epoch: 130, Steps: 1 | Train Loss: 115.0948486 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11005735397338867\n",
      "Epoch: 131, Steps: 1 | Train Loss: 115.2167969 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.1106112003326416\n",
      "Epoch: 132, Steps: 1 | Train Loss: 115.3575363 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.1113135814666748\n",
      "Epoch: 133, Steps: 1 | Train Loss: 115.2102814 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.12298130989074707\n",
      "Epoch: 134, Steps: 1 | Train Loss: 115.3582077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.1233057975769043\n",
      "Epoch: 135, Steps: 1 | Train Loss: 115.5992966 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.12332510948181152\n",
      "Epoch: 136, Steps: 1 | Train Loss: 115.7144547 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.1235494613647461\n",
      "Epoch: 137, Steps: 1 | Train Loss: 115.1918335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11927342414855957\n",
      "Epoch: 138, Steps: 1 | Train Loss: 114.6660919 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11249828338623047\n",
      "Epoch: 139, Steps: 1 | Train Loss: 115.6983795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.11217713356018066\n",
      "Epoch: 140, Steps: 1 | Train Loss: 115.5739670 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.111724853515625\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.9409332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11117362976074219\n",
      "Epoch: 142, Steps: 1 | Train Loss: 114.9487839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11144042015075684\n",
      "Epoch: 143, Steps: 1 | Train Loss: 115.4814987 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11051726341247559\n",
      "Epoch: 144, Steps: 1 | Train Loss: 115.6847458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10976243019104004\n",
      "Epoch: 145, Steps: 1 | Train Loss: 115.6470261 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.1100468635559082\n",
      "Epoch: 146, Steps: 1 | Train Loss: 115.2179947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.11044526100158691\n",
      "Epoch: 147, Steps: 1 | Train Loss: 115.4455795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.1155388355255127\n",
      "Epoch: 148, Steps: 1 | Train Loss: 115.6312866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11001443862915039\n",
      "Epoch: 149, Steps: 1 | Train Loss: 115.4298248 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.11062502861022949\n",
      "Epoch: 150, Steps: 1 | Train Loss: 115.2116089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.11066412925720215\n",
      "Epoch: 151, Steps: 1 | Train Loss: 115.3063278 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.11139917373657227\n",
      "Epoch: 152, Steps: 1 | Train Loss: 115.3714828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.11043500900268555\n",
      "Epoch: 153, Steps: 1 | Train Loss: 115.5232925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.1240391731262207\n",
      "Epoch: 154, Steps: 1 | Train Loss: 115.2138214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.12407064437866211\n",
      "Epoch: 155, Steps: 1 | Train Loss: 115.5899429 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.12391018867492676\n",
      "Epoch: 156, Steps: 1 | Train Loss: 115.2039337 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.12286567687988281\n",
      "Epoch: 157, Steps: 1 | Train Loss: 115.5815430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.12055349349975586\n",
      "Epoch: 158, Steps: 1 | Train Loss: 115.3550797 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.11429452896118164\n",
      "Epoch: 159, Steps: 1 | Train Loss: 115.5333023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11214041709899902\n",
      "Epoch: 160, Steps: 1 | Train Loss: 115.5555191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.11117696762084961\n",
      "Epoch: 161, Steps: 1 | Train Loss: 115.2505417 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.1109933853149414\n",
      "Epoch: 162, Steps: 1 | Train Loss: 115.6083755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.11167526245117188\n",
      "Epoch: 163, Steps: 1 | Train Loss: 114.9841080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.1099238395690918\n",
      "Epoch: 164, Steps: 1 | Train Loss: 115.8105011 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.11410093307495117\n",
      "Epoch: 165, Steps: 1 | Train Loss: 115.5866699 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11098241806030273\n",
      "Epoch: 166, Steps: 1 | Train Loss: 115.1051025 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11082911491394043\n",
      "Epoch: 167, Steps: 1 | Train Loss: 115.2480698 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11051654815673828\n",
      "Epoch: 168, Steps: 1 | Train Loss: 115.6493912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11042547225952148\n",
      "Epoch: 169, Steps: 1 | Train Loss: 115.2493439 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11122632026672363\n",
      "Epoch: 170, Steps: 1 | Train Loss: 115.4227829 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.12140345573425293\n",
      "Epoch: 171, Steps: 1 | Train Loss: 115.1364517 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.1239480972290039\n",
      "Epoch: 172, Steps: 1 | Train Loss: 115.5102158 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.12643814086914062\n",
      "Epoch: 173, Steps: 1 | Train Loss: 115.1826706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.1324160099029541\n",
      "Epoch: 174, Steps: 1 | Train Loss: 115.2783432 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.13486719131469727\n",
      "Epoch: 175, Steps: 1 | Train Loss: 115.3615952 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.13646888732910156\n",
      "Epoch: 176, Steps: 1 | Train Loss: 115.8590088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.13574695587158203\n",
      "Epoch: 177, Steps: 1 | Train Loss: 115.4112778 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.13646936416625977\n",
      "Epoch: 178, Steps: 1 | Train Loss: 115.2479477 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11892485618591309\n",
      "Epoch: 179, Steps: 1 | Train Loss: 115.3288193 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.13286256790161133\n",
      "Epoch: 180, Steps: 1 | Train Loss: 116.2314606 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.1268618106842041\n",
      "Epoch: 181, Steps: 1 | Train Loss: 114.9961700 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.12417173385620117\n",
      "Epoch: 182, Steps: 1 | Train Loss: 114.8653488 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.12330746650695801\n",
      "Epoch: 183, Steps: 1 | Train Loss: 115.5258560 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.12415313720703125\n",
      "Epoch: 184, Steps: 1 | Train Loss: 114.9863205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.12605547904968262\n",
      "Epoch: 185, Steps: 1 | Train Loss: 115.2346954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.12386298179626465\n",
      "Epoch: 186, Steps: 1 | Train Loss: 115.3750839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.12590885162353516\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.8733521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.12665796279907227\n",
      "Epoch: 188, Steps: 1 | Train Loss: 115.5157471 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.13605856895446777\n",
      "Epoch: 189, Steps: 1 | Train Loss: 115.3146515 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.13622331619262695\n",
      "Epoch: 190, Steps: 1 | Train Loss: 115.5957413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.13654708862304688\n",
      "Epoch: 191, Steps: 1 | Train Loss: 115.3371124 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.1375269889831543\n",
      "Epoch: 192, Steps: 1 | Train Loss: 115.4711227 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.13781309127807617\n",
      "Epoch: 193, Steps: 1 | Train Loss: 114.7109528 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.13751673698425293\n",
      "Epoch: 194, Steps: 1 | Train Loss: 115.7099838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.13570094108581543\n",
      "Epoch: 195, Steps: 1 | Train Loss: 115.3933258 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11789631843566895\n",
      "Epoch: 196, Steps: 1 | Train Loss: 115.8331833 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.1387319564819336\n",
      "Epoch: 197, Steps: 1 | Train Loss: 115.0550766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.13046050071716309\n",
      "Epoch: 198, Steps: 1 | Train Loss: 115.2259750 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.12493109703063965\n",
      "Epoch: 199, Steps: 1 | Train Loss: 115.4766388 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.1261916160583496\n",
      "Epoch: 200, Steps: 1 | Train Loss: 115.1411896 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.1272885799407959\n",
      "Epoch: 201, Steps: 1 | Train Loss: 115.6519089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.12611627578735352\n",
      "Epoch: 202, Steps: 1 | Train Loss: 115.7499924 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.12608766555786133\n",
      "Epoch: 203, Steps: 1 | Train Loss: 115.7512970 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.12651848793029785\n",
      "Epoch: 204, Steps: 1 | Train Loss: 115.3501816 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12332963943481445\n",
      "Epoch: 205, Steps: 1 | Train Loss: 115.4610825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.13225126266479492\n",
      "Epoch: 206, Steps: 1 | Train Loss: 115.5573044 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.13504409790039062\n",
      "Epoch: 207, Steps: 1 | Train Loss: 115.4605713 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.13814401626586914\n",
      "Epoch: 208, Steps: 1 | Train Loss: 115.3765030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.13518524169921875\n",
      "Epoch: 209, Steps: 1 | Train Loss: 115.2241211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.13363051414489746\n",
      "Epoch: 210, Steps: 1 | Train Loss: 115.4257584 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.12344646453857422\n",
      "Epoch: 211, Steps: 1 | Train Loss: 114.8223648 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11028528213500977\n",
      "Epoch: 212, Steps: 1 | Train Loss: 114.4233932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.1099386215209961\n",
      "Epoch: 213, Steps: 1 | Train Loss: 115.2611465 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10952234268188477\n",
      "Epoch: 214, Steps: 1 | Train Loss: 115.8011246 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.1090085506439209\n",
      "Epoch: 215, Steps: 1 | Train Loss: 115.0609512 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11745262145996094\n",
      "Epoch: 216, Steps: 1 | Train Loss: 115.9495392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11014437675476074\n",
      "Epoch: 217, Steps: 1 | Train Loss: 115.2424240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10961413383483887\n",
      "Epoch: 218, Steps: 1 | Train Loss: 115.1700974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10879158973693848\n",
      "Epoch: 219, Steps: 1 | Train Loss: 114.9924927 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10798931121826172\n",
      "Epoch: 220, Steps: 1 | Train Loss: 114.8874664 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.10811853408813477\n",
      "Epoch: 221, Steps: 1 | Train Loss: 115.6052246 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.1082313060760498\n",
      "Epoch: 222, Steps: 1 | Train Loss: 115.1038742 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10767388343811035\n",
      "Epoch: 223, Steps: 1 | Train Loss: 115.1825180 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.10805130004882812\n",
      "Epoch: 224, Steps: 1 | Train Loss: 115.2742233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.10808658599853516\n",
      "Epoch: 225, Steps: 1 | Train Loss: 115.1123047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.10818791389465332\n",
      "Epoch: 226, Steps: 1 | Train Loss: 115.0773468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.12226414680480957\n",
      "Epoch: 227, Steps: 1 | Train Loss: 114.8340836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.12146687507629395\n",
      "Epoch: 228, Steps: 1 | Train Loss: 115.3328018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.12162256240844727\n",
      "Epoch: 229, Steps: 1 | Train Loss: 115.5552292 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11264753341674805\n",
      "Epoch: 230, Steps: 1 | Train Loss: 115.2256851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.10968828201293945\n",
      "Epoch: 231, Steps: 1 | Train Loss: 115.6516342 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10919594764709473\n",
      "Epoch: 232, Steps: 1 | Train Loss: 115.0728378 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.10919404029846191\n",
      "Epoch: 233, Steps: 1 | Train Loss: 114.9085693 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.10966157913208008\n",
      "Epoch: 234, Steps: 1 | Train Loss: 114.9548340 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.10825991630554199\n",
      "Epoch: 235, Steps: 1 | Train Loss: 115.4145660 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10841631889343262\n",
      "Epoch: 236, Steps: 1 | Train Loss: 114.8979416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10854482650756836\n",
      "Epoch: 237, Steps: 1 | Train Loss: 115.2656479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10859203338623047\n",
      "Epoch: 238, Steps: 1 | Train Loss: 116.0526047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10844659805297852\n",
      "Epoch: 239, Steps: 1 | Train Loss: 115.7807770 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.12294721603393555\n",
      "Epoch: 240, Steps: 1 | Train Loss: 115.0762100 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.11788201332092285\n",
      "Epoch: 241, Steps: 1 | Train Loss: 115.7941208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.1115720272064209\n",
      "Epoch: 242, Steps: 1 | Train Loss: 115.4031982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.11032438278198242\n",
      "Epoch: 243, Steps: 1 | Train Loss: 115.2711182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.1095435619354248\n",
      "Epoch: 244, Steps: 1 | Train Loss: 115.5300446 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.10865116119384766\n",
      "Epoch: 245, Steps: 1 | Train Loss: 115.4514999 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.10860037803649902\n",
      "Epoch: 246, Steps: 1 | Train Loss: 116.0743179 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.10838699340820312\n",
      "Epoch: 247, Steps: 1 | Train Loss: 115.3217697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10745549201965332\n",
      "Epoch: 248, Steps: 1 | Train Loss: 115.6284409 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.1098487377166748\n",
      "Epoch: 249, Steps: 1 | Train Loss: 115.5710220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10784387588500977\n",
      "Epoch: 250, Steps: 1 | Train Loss: 115.5524902 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_10>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.10811686515808105\n",
      "Epoch: 1, Steps: 1 | Train Loss: 118.1369781 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10834479331970215\n",
      "Epoch: 2, Steps: 1 | Train Loss: 116.1272354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10882735252380371\n",
      "Epoch: 3, Steps: 1 | Train Loss: 114.9267807 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10898661613464355\n",
      "Epoch: 4, Steps: 1 | Train Loss: 114.2597656 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10817384719848633\n",
      "Epoch: 5, Steps: 1 | Train Loss: 113.1429443 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.1098625659942627\n",
      "Epoch: 6, Steps: 1 | Train Loss: 113.7616196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10993647575378418\n",
      "Epoch: 7, Steps: 1 | Train Loss: 113.4194107 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10902571678161621\n",
      "Epoch: 8, Steps: 1 | Train Loss: 113.8606720 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11435437202453613\n",
      "Epoch: 9, Steps: 1 | Train Loss: 113.2984619 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.11102533340454102\n",
      "Epoch: 10, Steps: 1 | Train Loss: 113.1903305 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10981202125549316\n",
      "Epoch: 11, Steps: 1 | Train Loss: 113.0863876 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10965251922607422\n",
      "Epoch: 12, Steps: 1 | Train Loss: 113.3054199 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.1091008186340332\n",
      "Epoch: 13, Steps: 1 | Train Loss: 113.3858643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10696649551391602\n",
      "Epoch: 14, Steps: 1 | Train Loss: 113.1835480 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10868120193481445\n",
      "Epoch: 15, Steps: 1 | Train Loss: 113.5830078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10894179344177246\n",
      "Epoch: 16, Steps: 1 | Train Loss: 113.9920654 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.11806130409240723\n",
      "Epoch: 17, Steps: 1 | Train Loss: 113.2078171 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11684203147888184\n",
      "Epoch: 18, Steps: 1 | Train Loss: 112.8874283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.12456893920898438\n",
      "Epoch: 19, Steps: 1 | Train Loss: 113.3310318 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.12287354469299316\n",
      "Epoch: 20, Steps: 1 | Train Loss: 113.8544464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.12334299087524414\n",
      "Epoch: 21, Steps: 1 | Train Loss: 113.6226120 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.1231069564819336\n",
      "Epoch: 22, Steps: 1 | Train Loss: 113.4497604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.1190183162689209\n",
      "Epoch: 23, Steps: 1 | Train Loss: 113.3459015 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.12001419067382812\n",
      "Epoch: 24, Steps: 1 | Train Loss: 113.0596237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11890077590942383\n",
      "Epoch: 25, Steps: 1 | Train Loss: 113.4453125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11513614654541016\n",
      "Epoch: 26, Steps: 1 | Train Loss: 113.8806992 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.13428163528442383\n",
      "Epoch: 27, Steps: 1 | Train Loss: 113.1028442 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.12229442596435547\n",
      "Epoch: 28, Steps: 1 | Train Loss: 113.3255844 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.12029385566711426\n",
      "Epoch: 29, Steps: 1 | Train Loss: 113.3438721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11231851577758789\n",
      "Epoch: 30, Steps: 1 | Train Loss: 113.2757492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11185789108276367\n",
      "Epoch: 31, Steps: 1 | Train Loss: 113.8608780 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.12176179885864258\n",
      "Epoch: 32, Steps: 1 | Train Loss: 113.0479355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11957669258117676\n",
      "Epoch: 33, Steps: 1 | Train Loss: 113.6453629 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.1117398738861084\n",
      "Epoch: 34, Steps: 1 | Train Loss: 113.5018539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.1165914535522461\n",
      "Epoch: 35, Steps: 1 | Train Loss: 113.4181671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.12707161903381348\n",
      "Epoch: 36, Steps: 1 | Train Loss: 113.3917236 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.12295389175415039\n",
      "Epoch: 37, Steps: 1 | Train Loss: 113.4343796 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11634254455566406\n",
      "Epoch: 38, Steps: 1 | Train Loss: 113.5647507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.11792397499084473\n",
      "Epoch: 39, Steps: 1 | Train Loss: 113.6256866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.12136411666870117\n",
      "Epoch: 40, Steps: 1 | Train Loss: 113.4629288 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.11806416511535645\n",
      "Epoch: 41, Steps: 1 | Train Loss: 113.0279541 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.11590790748596191\n",
      "Epoch: 42, Steps: 1 | Train Loss: 113.5987930 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.11809563636779785\n",
      "Epoch: 43, Steps: 1 | Train Loss: 112.8272247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11361169815063477\n",
      "Epoch: 44, Steps: 1 | Train Loss: 113.0375900 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.11276888847351074\n",
      "Epoch: 45, Steps: 1 | Train Loss: 112.7248764 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11687064170837402\n",
      "Epoch: 46, Steps: 1 | Train Loss: 113.4683762 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.13105058670043945\n",
      "Epoch: 47, Steps: 1 | Train Loss: 112.8907242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.12417864799499512\n",
      "Epoch: 48, Steps: 1 | Train Loss: 113.3187408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.11878442764282227\n",
      "Epoch: 49, Steps: 1 | Train Loss: 113.4287262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11834263801574707\n",
      "Epoch: 50, Steps: 1 | Train Loss: 113.3046188 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11348724365234375\n",
      "Epoch: 51, Steps: 1 | Train Loss: 112.3555222 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11952495574951172\n",
      "Epoch: 52, Steps: 1 | Train Loss: 112.9979858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.1195077896118164\n",
      "Epoch: 53, Steps: 1 | Train Loss: 113.4721909 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.12279868125915527\n",
      "Epoch: 54, Steps: 1 | Train Loss: 113.3327866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.12392377853393555\n",
      "Epoch: 55, Steps: 1 | Train Loss: 113.3689804 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.11272192001342773\n",
      "Epoch: 56, Steps: 1 | Train Loss: 113.3669662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.11266207695007324\n",
      "Epoch: 57, Steps: 1 | Train Loss: 113.2063751 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.12559914588928223\n",
      "Epoch: 58, Steps: 1 | Train Loss: 112.9882736 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.1249082088470459\n",
      "Epoch: 59, Steps: 1 | Train Loss: 113.0531235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.12565040588378906\n",
      "Epoch: 60, Steps: 1 | Train Loss: 113.2454224 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.12318038940429688\n",
      "Epoch: 61, Steps: 1 | Train Loss: 113.2180710 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.12042546272277832\n",
      "Epoch: 62, Steps: 1 | Train Loss: 113.4013901 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.1163787841796875\n",
      "Epoch: 63, Steps: 1 | Train Loss: 113.5260239 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.11208081245422363\n",
      "Epoch: 64, Steps: 1 | Train Loss: 113.3363190 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.1217796802520752\n",
      "Epoch: 65, Steps: 1 | Train Loss: 113.4762497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11750364303588867\n",
      "Epoch: 66, Steps: 1 | Train Loss: 113.0349350 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11879658699035645\n",
      "Epoch: 67, Steps: 1 | Train Loss: 113.5064240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10916876792907715\n",
      "Epoch: 68, Steps: 1 | Train Loss: 113.6249313 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.1197195053100586\n",
      "Epoch: 69, Steps: 1 | Train Loss: 113.3528595 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11564254760742188\n",
      "Epoch: 70, Steps: 1 | Train Loss: 113.6235580 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10917282104492188\n",
      "Epoch: 71, Steps: 1 | Train Loss: 113.4445572 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.1164236068725586\n",
      "Epoch: 72, Steps: 1 | Train Loss: 113.4446564 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.12000036239624023\n",
      "Epoch: 73, Steps: 1 | Train Loss: 113.2371597 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12729501724243164\n",
      "Epoch: 74, Steps: 1 | Train Loss: 113.3558121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.12757349014282227\n",
      "Epoch: 75, Steps: 1 | Train Loss: 113.5136948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.12374639511108398\n",
      "Epoch: 76, Steps: 1 | Train Loss: 113.2173691 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11495304107666016\n",
      "Epoch: 77, Steps: 1 | Train Loss: 113.4983444 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.11454987525939941\n",
      "Epoch: 78, Steps: 1 | Train Loss: 113.3654938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.11110877990722656\n",
      "Epoch: 79, Steps: 1 | Train Loss: 113.1528091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10898327827453613\n",
      "Epoch: 80, Steps: 1 | Train Loss: 112.8660660 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10858559608459473\n",
      "Epoch: 81, Steps: 1 | Train Loss: 113.4087143 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.1088414192199707\n",
      "Epoch: 82, Steps: 1 | Train Loss: 113.3151855 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.12210345268249512\n",
      "Epoch: 83, Steps: 1 | Train Loss: 113.3753204 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.1215200424194336\n",
      "Epoch: 84, Steps: 1 | Train Loss: 113.5812607 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.11363387107849121\n",
      "Epoch: 85, Steps: 1 | Train Loss: 113.5026398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11146736145019531\n",
      "Epoch: 86, Steps: 1 | Train Loss: 113.5053329 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10974693298339844\n",
      "Epoch: 87, Steps: 1 | Train Loss: 113.4072876 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.10909557342529297\n",
      "Epoch: 88, Steps: 1 | Train Loss: 112.6988449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11075711250305176\n",
      "Epoch: 89, Steps: 1 | Train Loss: 113.0607300 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10819625854492188\n",
      "Epoch: 90, Steps: 1 | Train Loss: 113.3533554 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10862398147583008\n",
      "Epoch: 91, Steps: 1 | Train Loss: 113.1885986 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10825610160827637\n",
      "Epoch: 92, Steps: 1 | Train Loss: 112.9273911 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10843276977539062\n",
      "Epoch: 93, Steps: 1 | Train Loss: 113.2505112 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10844087600708008\n",
      "Epoch: 94, Steps: 1 | Train Loss: 113.0335083 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10837078094482422\n",
      "Epoch: 95, Steps: 1 | Train Loss: 113.5249786 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.12230563163757324\n",
      "Epoch: 96, Steps: 1 | Train Loss: 113.5118179 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.122039794921875\n",
      "Epoch: 97, Steps: 1 | Train Loss: 113.3158112 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.1230170726776123\n",
      "Epoch: 98, Steps: 1 | Train Loss: 113.2608566 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.12094330787658691\n",
      "Epoch: 99, Steps: 1 | Train Loss: 113.0910187 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.12201547622680664\n",
      "Epoch: 100, Steps: 1 | Train Loss: 113.5649948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.11324954032897949\n",
      "Epoch: 101, Steps: 1 | Train Loss: 113.2287598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10993766784667969\n",
      "Epoch: 102, Steps: 1 | Train Loss: 113.1031036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10990452766418457\n",
      "Epoch: 103, Steps: 1 | Train Loss: 113.3540878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10885858535766602\n",
      "Epoch: 104, Steps: 1 | Train Loss: 113.7173843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.1080930233001709\n",
      "Epoch: 105, Steps: 1 | Train Loss: 113.1671906 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.1083974838256836\n",
      "Epoch: 106, Steps: 1 | Train Loss: 113.7837296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10838818550109863\n",
      "Epoch: 107, Steps: 1 | Train Loss: 113.8032455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10848808288574219\n",
      "Epoch: 108, Steps: 1 | Train Loss: 113.1986313 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10855984687805176\n",
      "Epoch: 109, Steps: 1 | Train Loss: 113.3598404 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.10845065116882324\n",
      "Epoch: 110, Steps: 1 | Train Loss: 113.2664795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.10889697074890137\n",
      "Epoch: 111, Steps: 1 | Train Loss: 112.9136734 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.1223900318145752\n",
      "Epoch: 112, Steps: 1 | Train Loss: 112.7858658 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.12217974662780762\n",
      "Epoch: 113, Steps: 1 | Train Loss: 113.2652359 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.11491584777832031\n",
      "Epoch: 114, Steps: 1 | Train Loss: 113.0353394 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11064291000366211\n",
      "Epoch: 115, Steps: 1 | Train Loss: 112.7923203 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10958194732666016\n",
      "Epoch: 116, Steps: 1 | Train Loss: 112.8790512 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.1090848445892334\n",
      "Epoch: 117, Steps: 1 | Train Loss: 113.0069351 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10848617553710938\n",
      "Epoch: 118, Steps: 1 | Train Loss: 113.6217575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10855770111083984\n",
      "Epoch: 119, Steps: 1 | Train Loss: 113.3040085 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1087799072265625\n",
      "Epoch: 120, Steps: 1 | Train Loss: 113.3694839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.10874772071838379\n",
      "Epoch: 121, Steps: 1 | Train Loss: 113.2208023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10867881774902344\n",
      "Epoch: 122, Steps: 1 | Train Loss: 113.3186188 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10848522186279297\n",
      "Epoch: 123, Steps: 1 | Train Loss: 113.6969986 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.10888123512268066\n",
      "Epoch: 124, Steps: 1 | Train Loss: 113.6920166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.12229633331298828\n",
      "Epoch: 125, Steps: 1 | Train Loss: 113.4409409 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.12197136878967285\n",
      "Epoch: 126, Steps: 1 | Train Loss: 113.6817017 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.1121060848236084\n",
      "Epoch: 127, Steps: 1 | Train Loss: 113.1997910 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.10967469215393066\n",
      "Epoch: 128, Steps: 1 | Train Loss: 113.3903198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10878992080688477\n",
      "Epoch: 129, Steps: 1 | Train Loss: 113.0491104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10852861404418945\n",
      "Epoch: 130, Steps: 1 | Train Loss: 114.1500626 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.1083686351776123\n",
      "Epoch: 131, Steps: 1 | Train Loss: 113.7504044 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10868716239929199\n",
      "Epoch: 132, Steps: 1 | Train Loss: 113.5201187 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.10853171348571777\n",
      "Epoch: 133, Steps: 1 | Train Loss: 113.4118576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10846543312072754\n",
      "Epoch: 134, Steps: 1 | Train Loss: 113.4626083 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.1085667610168457\n",
      "Epoch: 135, Steps: 1 | Train Loss: 113.1829224 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.12281322479248047\n",
      "Epoch: 136, Steps: 1 | Train Loss: 113.2716446 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.11737370491027832\n",
      "Epoch: 137, Steps: 1 | Train Loss: 113.8545456 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11067318916320801\n",
      "Epoch: 138, Steps: 1 | Train Loss: 113.4438095 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.10968589782714844\n",
      "Epoch: 139, Steps: 1 | Train Loss: 114.1341553 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10878276824951172\n",
      "Epoch: 140, Steps: 1 | Train Loss: 113.1976242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.10846853256225586\n",
      "Epoch: 141, Steps: 1 | Train Loss: 113.7002335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.10825586318969727\n",
      "Epoch: 142, Steps: 1 | Train Loss: 113.4757233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.1076972484588623\n",
      "Epoch: 143, Steps: 1 | Train Loss: 113.3795929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.10816168785095215\n",
      "Epoch: 144, Steps: 1 | Train Loss: 113.3672028 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10846662521362305\n",
      "Epoch: 145, Steps: 1 | Train Loss: 112.8189621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10850977897644043\n",
      "Epoch: 146, Steps: 1 | Train Loss: 113.0598526 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.1088402271270752\n",
      "Epoch: 147, Steps: 1 | Train Loss: 113.1706772 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10874772071838379\n",
      "Epoch: 148, Steps: 1 | Train Loss: 113.5243912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.12306904792785645\n",
      "Epoch: 149, Steps: 1 | Train Loss: 113.2912598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.1197214126586914\n",
      "Epoch: 150, Steps: 1 | Train Loss: 112.8953018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.11140608787536621\n",
      "Epoch: 151, Steps: 1 | Train Loss: 113.0927353 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.11009478569030762\n",
      "Epoch: 152, Steps: 1 | Train Loss: 113.1259766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10924816131591797\n",
      "Epoch: 153, Steps: 1 | Train Loss: 113.6812744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10827136039733887\n",
      "Epoch: 154, Steps: 1 | Train Loss: 113.7620850 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10844063758850098\n",
      "Epoch: 155, Steps: 1 | Train Loss: 113.2670059 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10852479934692383\n",
      "Epoch: 156, Steps: 1 | Train Loss: 113.4377594 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10848689079284668\n",
      "Epoch: 157, Steps: 1 | Train Loss: 113.1142502 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10849952697753906\n",
      "Epoch: 158, Steps: 1 | Train Loss: 113.5920410 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10883855819702148\n",
      "Epoch: 159, Steps: 1 | Train Loss: 112.9443130 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11126899719238281\n",
      "Epoch: 160, Steps: 1 | Train Loss: 113.2489624 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10939455032348633\n",
      "Epoch: 161, Steps: 1 | Train Loss: 113.5726242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10874199867248535\n",
      "Epoch: 162, Steps: 1 | Train Loss: 113.2316895 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10819506645202637\n",
      "Epoch: 163, Steps: 1 | Train Loss: 113.2312546 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10888266563415527\n",
      "Epoch: 164, Steps: 1 | Train Loss: 113.0235748 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10875725746154785\n",
      "Epoch: 165, Steps: 1 | Train Loss: 113.3370895 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.10845828056335449\n",
      "Epoch: 166, Steps: 1 | Train Loss: 113.2151108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10855555534362793\n",
      "Epoch: 167, Steps: 1 | Train Loss: 113.3585358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.1084132194519043\n",
      "Epoch: 168, Steps: 1 | Train Loss: 113.2446823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.12181878089904785\n",
      "Epoch: 169, Steps: 1 | Train Loss: 113.4080582 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.12194013595581055\n",
      "Epoch: 170, Steps: 1 | Train Loss: 113.6617432 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11762213706970215\n",
      "Epoch: 171, Steps: 1 | Train Loss: 113.2590561 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11114358901977539\n",
      "Epoch: 172, Steps: 1 | Train Loss: 112.9788742 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10986566543579102\n",
      "Epoch: 173, Steps: 1 | Train Loss: 113.5454330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10873198509216309\n",
      "Epoch: 174, Steps: 1 | Train Loss: 113.5474854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10848307609558105\n",
      "Epoch: 175, Steps: 1 | Train Loss: 113.2367706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.10860681533813477\n",
      "Epoch: 176, Steps: 1 | Train Loss: 113.2577667 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10886216163635254\n",
      "Epoch: 177, Steps: 1 | Train Loss: 113.1983032 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.1086416244506836\n",
      "Epoch: 178, Steps: 1 | Train Loss: 112.9135361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.108642578125\n",
      "Epoch: 179, Steps: 1 | Train Loss: 113.7212906 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.12385845184326172\n",
      "Epoch: 180, Steps: 1 | Train Loss: 113.0762711 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.12119364738464355\n",
      "Epoch: 181, Steps: 1 | Train Loss: 113.3397598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11363577842712402\n",
      "Epoch: 182, Steps: 1 | Train Loss: 113.1274414 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.1091465950012207\n",
      "Epoch: 183, Steps: 1 | Train Loss: 113.4547958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.1083674430847168\n",
      "Epoch: 184, Steps: 1 | Train Loss: 113.7450790 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.10788440704345703\n",
      "Epoch: 185, Steps: 1 | Train Loss: 113.1735764 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.10772848129272461\n",
      "Epoch: 186, Steps: 1 | Train Loss: 113.7347641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10757327079772949\n",
      "Epoch: 187, Steps: 1 | Train Loss: 113.9054184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10788440704345703\n",
      "Epoch: 188, Steps: 1 | Train Loss: 113.9964371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.10725116729736328\n",
      "Epoch: 189, Steps: 1 | Train Loss: 113.7979889 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10736846923828125\n",
      "Epoch: 190, Steps: 1 | Train Loss: 113.6970825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10725641250610352\n",
      "Epoch: 191, Steps: 1 | Train Loss: 113.2702789 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10741114616394043\n",
      "Epoch: 192, Steps: 1 | Train Loss: 113.0938721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.10767793655395508\n",
      "Epoch: 193, Steps: 1 | Train Loss: 113.7818832 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.12141823768615723\n",
      "Epoch: 194, Steps: 1 | Train Loss: 113.5056686 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.12091946601867676\n",
      "Epoch: 195, Steps: 1 | Train Loss: 113.5096970 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.12094616889953613\n",
      "Epoch: 196, Steps: 1 | Train Loss: 113.1287155 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11947154998779297\n",
      "Epoch: 197, Steps: 1 | Train Loss: 113.8726120 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11144518852233887\n",
      "Epoch: 198, Steps: 1 | Train Loss: 112.7619019 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10904908180236816\n",
      "Epoch: 199, Steps: 1 | Train Loss: 113.5951157 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10845565795898438\n",
      "Epoch: 200, Steps: 1 | Train Loss: 113.3722458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10799241065979004\n",
      "Epoch: 201, Steps: 1 | Train Loss: 113.6862564 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10778331756591797\n",
      "Epoch: 202, Steps: 1 | Train Loss: 113.5893784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10750937461853027\n",
      "Epoch: 203, Steps: 1 | Train Loss: 113.3018723 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.10735273361206055\n",
      "Epoch: 204, Steps: 1 | Train Loss: 113.4604263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.10767459869384766\n",
      "Epoch: 205, Steps: 1 | Train Loss: 113.1691513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10737347602844238\n",
      "Epoch: 206, Steps: 1 | Train Loss: 113.1043701 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10739898681640625\n",
      "Epoch: 207, Steps: 1 | Train Loss: 113.0648575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.1074073314666748\n",
      "Epoch: 208, Steps: 1 | Train Loss: 113.5127716 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10723710060119629\n",
      "Epoch: 209, Steps: 1 | Train Loss: 113.5892563 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.10751628875732422\n",
      "Epoch: 210, Steps: 1 | Train Loss: 113.6007080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.12092947959899902\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.9334946 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.12082099914550781\n",
      "Epoch: 212, Steps: 1 | Train Loss: 113.0913620 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.12085461616516113\n",
      "Epoch: 213, Steps: 1 | Train Loss: 113.4167252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11701416969299316\n",
      "Epoch: 214, Steps: 1 | Train Loss: 113.8095474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11026120185852051\n",
      "Epoch: 215, Steps: 1 | Train Loss: 113.2414551 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10869598388671875\n",
      "Epoch: 216, Steps: 1 | Train Loss: 113.7006836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.1115727424621582\n",
      "Epoch: 217, Steps: 1 | Train Loss: 113.5978775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.11141324043273926\n",
      "Epoch: 218, Steps: 1 | Train Loss: 113.4680557 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10869336128234863\n",
      "Epoch: 219, Steps: 1 | Train Loss: 113.3314743 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.1104288101196289\n",
      "Epoch: 220, Steps: 1 | Train Loss: 113.3549347 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.1151421070098877\n",
      "Epoch: 221, Steps: 1 | Train Loss: 113.2037964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.1101226806640625\n",
      "Epoch: 222, Steps: 1 | Train Loss: 113.0608139 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10989522933959961\n",
      "Epoch: 223, Steps: 1 | Train Loss: 113.7526474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.10990500450134277\n",
      "Epoch: 224, Steps: 1 | Train Loss: 113.2421036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11005091667175293\n",
      "Epoch: 225, Steps: 1 | Train Loss: 113.2171860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11175894737243652\n",
      "Epoch: 226, Steps: 1 | Train Loss: 113.5414658 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.12352108955383301\n",
      "Epoch: 227, Steps: 1 | Train Loss: 113.6037827 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11965703964233398\n",
      "Epoch: 228, Steps: 1 | Train Loss: 113.1246338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11273646354675293\n",
      "Epoch: 229, Steps: 1 | Train Loss: 113.2520294 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11172962188720703\n",
      "Epoch: 230, Steps: 1 | Train Loss: 113.1834030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.1110079288482666\n",
      "Epoch: 231, Steps: 1 | Train Loss: 113.1374054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.11097073554992676\n",
      "Epoch: 232, Steps: 1 | Train Loss: 112.9079132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11023116111755371\n",
      "Epoch: 233, Steps: 1 | Train Loss: 112.8290558 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11066460609436035\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.2122574 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11222028732299805\n",
      "Epoch: 235, Steps: 1 | Train Loss: 112.9255905 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.1103363037109375\n",
      "Epoch: 236, Steps: 1 | Train Loss: 113.0816269 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10962557792663574\n",
      "Epoch: 237, Steps: 1 | Train Loss: 113.7092438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11027908325195312\n",
      "Epoch: 238, Steps: 1 | Train Loss: 113.5340195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.11059212684631348\n",
      "Epoch: 239, Steps: 1 | Train Loss: 113.6644669 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.12336516380310059\n",
      "Epoch: 240, Steps: 1 | Train Loss: 113.0606842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1226046085357666\n",
      "Epoch: 241, Steps: 1 | Train Loss: 113.6687164 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.12051534652709961\n",
      "Epoch: 242, Steps: 1 | Train Loss: 113.3627243 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.12290620803833008\n",
      "Epoch: 243, Steps: 1 | Train Loss: 113.3502350 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.11618256568908691\n",
      "Epoch: 244, Steps: 1 | Train Loss: 113.2271957 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.1143496036529541\n",
      "Epoch: 245, Steps: 1 | Train Loss: 112.5894089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11173009872436523\n",
      "Epoch: 246, Steps: 1 | Train Loss: 113.2182388 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.10665130615234375\n",
      "Epoch: 247, Steps: 1 | Train Loss: 113.4150009 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10891294479370117\n",
      "Epoch: 248, Steps: 1 | Train Loss: 113.4952774 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.1102757453918457\n",
      "Epoch: 249, Steps: 1 | Train Loss: 112.9016037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11108207702636719\n",
      "Epoch: 250, Steps: 1 | Train Loss: 113.6259918 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_11>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.12374711036682129\n",
      "Epoch: 1, Steps: 1 | Train Loss: 119.3151779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.12224650382995605\n",
      "Epoch: 2, Steps: 1 | Train Loss: 118.1995392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10740137100219727\n",
      "Epoch: 3, Steps: 1 | Train Loss: 116.5129929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10882377624511719\n",
      "Epoch: 4, Steps: 1 | Train Loss: 115.5343399 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.13475251197814941\n",
      "Epoch: 5, Steps: 1 | Train Loss: 115.4476471 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.12282681465148926\n",
      "Epoch: 6, Steps: 1 | Train Loss: 115.4659576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.1311028003692627\n",
      "Epoch: 7, Steps: 1 | Train Loss: 114.7251205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.13454866409301758\n",
      "Epoch: 8, Steps: 1 | Train Loss: 114.8663559 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.1328902244567871\n",
      "Epoch: 9, Steps: 1 | Train Loss: 114.8864059 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.13330602645874023\n",
      "Epoch: 10, Steps: 1 | Train Loss: 115.2315826 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.13338255882263184\n",
      "Epoch: 11, Steps: 1 | Train Loss: 114.4453354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.1258833408355713\n",
      "Epoch: 12, Steps: 1 | Train Loss: 114.9927597 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.12421989440917969\n",
      "Epoch: 13, Steps: 1 | Train Loss: 115.1374664 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.128281831741333\n",
      "Epoch: 14, Steps: 1 | Train Loss: 114.8412857 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.1259169578552246\n",
      "Epoch: 15, Steps: 1 | Train Loss: 114.9552994 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12437558174133301\n",
      "Epoch: 16, Steps: 1 | Train Loss: 114.9901505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.13287043571472168\n",
      "Epoch: 17, Steps: 1 | Train Loss: 114.7864609 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.13362479209899902\n",
      "Epoch: 18, Steps: 1 | Train Loss: 115.0293350 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.13726568222045898\n",
      "Epoch: 19, Steps: 1 | Train Loss: 114.9808731 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.1372065544128418\n",
      "Epoch: 20, Steps: 1 | Train Loss: 115.1075974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.13544917106628418\n",
      "Epoch: 21, Steps: 1 | Train Loss: 114.5369644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.13404560089111328\n",
      "Epoch: 22, Steps: 1 | Train Loss: 114.7073364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.12713956832885742\n",
      "Epoch: 23, Steps: 1 | Train Loss: 115.1541519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.10617423057556152\n",
      "Epoch: 24, Steps: 1 | Train Loss: 114.9391403 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.12608861923217773\n",
      "Epoch: 25, Steps: 1 | Train Loss: 114.6553497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12420010566711426\n",
      "Epoch: 26, Steps: 1 | Train Loss: 115.1089478 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.126112699508667\n",
      "Epoch: 27, Steps: 1 | Train Loss: 114.8383331 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.1265120506286621\n",
      "Epoch: 28, Steps: 1 | Train Loss: 115.0603256 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.12484097480773926\n",
      "Epoch: 29, Steps: 1 | Train Loss: 114.9108887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.13638925552368164\n",
      "Epoch: 30, Steps: 1 | Train Loss: 115.2999496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.13549232482910156\n",
      "Epoch: 31, Steps: 1 | Train Loss: 115.2467041 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.13640451431274414\n",
      "Epoch: 32, Steps: 1 | Train Loss: 114.7749252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.13505029678344727\n",
      "Epoch: 33, Steps: 1 | Train Loss: 115.5699463 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10774445533752441\n",
      "Epoch: 34, Steps: 1 | Train Loss: 114.8024673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.12355446815490723\n",
      "Epoch: 35, Steps: 1 | Train Loss: 115.1349106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.12422871589660645\n",
      "Epoch: 36, Steps: 1 | Train Loss: 114.9881516 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.12634754180908203\n",
      "Epoch: 37, Steps: 1 | Train Loss: 114.6291504 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.12334346771240234\n",
      "Epoch: 38, Steps: 1 | Train Loss: 114.7010040 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.12619638442993164\n",
      "Epoch: 39, Steps: 1 | Train Loss: 114.5717392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.12895870208740234\n",
      "Epoch: 40, Steps: 1 | Train Loss: 114.8622665 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.1138763427734375\n",
      "Epoch: 41, Steps: 1 | Train Loss: 114.7897263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.1355435848236084\n",
      "Epoch: 42, Steps: 1 | Train Loss: 114.5958176 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.1355440616607666\n",
      "Epoch: 43, Steps: 1 | Train Loss: 114.8061752 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.1371750831604004\n",
      "Epoch: 44, Steps: 1 | Train Loss: 114.5954590 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.12695741653442383\n",
      "Epoch: 45, Steps: 1 | Train Loss: 114.8966064 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.12661099433898926\n",
      "Epoch: 46, Steps: 1 | Train Loss: 115.1102905 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.12404966354370117\n",
      "Epoch: 47, Steps: 1 | Train Loss: 114.6921921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.12562012672424316\n",
      "Epoch: 48, Steps: 1 | Train Loss: 114.6635056 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.12579822540283203\n",
      "Epoch: 49, Steps: 1 | Train Loss: 114.8825912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.12465476989746094\n",
      "Epoch: 50, Steps: 1 | Train Loss: 115.2473068 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.12437248229980469\n",
      "Epoch: 51, Steps: 1 | Train Loss: 114.7504807 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.13143491744995117\n",
      "Epoch: 52, Steps: 1 | Train Loss: 114.6915283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.13025665283203125\n",
      "Epoch: 53, Steps: 1 | Train Loss: 115.0395432 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.1346588134765625\n",
      "Epoch: 54, Steps: 1 | Train Loss: 115.0890274 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.13726043701171875\n",
      "Epoch: 55, Steps: 1 | Train Loss: 114.8782120 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1376941204071045\n",
      "Epoch: 56, Steps: 1 | Train Loss: 115.2950668 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.13108134269714355\n",
      "Epoch: 57, Steps: 1 | Train Loss: 115.4606934 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.12191987037658691\n",
      "Epoch: 58, Steps: 1 | Train Loss: 115.1216202 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.1142575740814209\n",
      "Epoch: 59, Steps: 1 | Train Loss: 115.1270065 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10906291007995605\n",
      "Epoch: 60, Steps: 1 | Train Loss: 114.8515167 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.11171865463256836\n",
      "Epoch: 61, Steps: 1 | Train Loss: 115.0405273 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.10925793647766113\n",
      "Epoch: 62, Steps: 1 | Train Loss: 115.1410675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11858463287353516\n",
      "Epoch: 63, Steps: 1 | Train Loss: 114.5496368 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.13550806045532227\n",
      "Epoch: 64, Steps: 1 | Train Loss: 115.0405502 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.12886786460876465\n",
      "Epoch: 65, Steps: 1 | Train Loss: 115.1441803 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.12277030944824219\n",
      "Epoch: 66, Steps: 1 | Train Loss: 115.2285690 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.12640810012817383\n",
      "Epoch: 67, Steps: 1 | Train Loss: 114.6847916 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.12444686889648438\n",
      "Epoch: 68, Steps: 1 | Train Loss: 115.1647873 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.12022781372070312\n",
      "Epoch: 69, Steps: 1 | Train Loss: 114.7837143 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11895561218261719\n",
      "Epoch: 70, Steps: 1 | Train Loss: 114.8739166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.1196739673614502\n",
      "Epoch: 71, Steps: 1 | Train Loss: 114.7428513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.11515212059020996\n",
      "Epoch: 72, Steps: 1 | Train Loss: 114.7229843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11768507957458496\n",
      "Epoch: 73, Steps: 1 | Train Loss: 114.9746628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.11849308013916016\n",
      "Epoch: 74, Steps: 1 | Train Loss: 114.5591049 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11484074592590332\n",
      "Epoch: 75, Steps: 1 | Train Loss: 114.9048080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11904621124267578\n",
      "Epoch: 76, Steps: 1 | Train Loss: 115.1828384 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11501097679138184\n",
      "Epoch: 77, Steps: 1 | Train Loss: 114.5026169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.11263394355773926\n",
      "Epoch: 78, Steps: 1 | Train Loss: 115.3643570 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.11431336402893066\n",
      "Epoch: 79, Steps: 1 | Train Loss: 114.5321198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.13090825080871582\n",
      "Epoch: 80, Steps: 1 | Train Loss: 115.0135727 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.11885786056518555\n",
      "Epoch: 81, Steps: 1 | Train Loss: 115.0996628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11340618133544922\n",
      "Epoch: 82, Steps: 1 | Train Loss: 115.4551315 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.11684846878051758\n",
      "Epoch: 83, Steps: 1 | Train Loss: 115.3917465 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11789822578430176\n",
      "Epoch: 84, Steps: 1 | Train Loss: 114.7198257 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.1102144718170166\n",
      "Epoch: 85, Steps: 1 | Train Loss: 115.0089264 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11612582206726074\n",
      "Epoch: 86, Steps: 1 | Train Loss: 114.8488693 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.11246418952941895\n",
      "Epoch: 87, Steps: 1 | Train Loss: 114.5273438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11734938621520996\n",
      "Epoch: 88, Steps: 1 | Train Loss: 115.1853561 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.12394475936889648\n",
      "Epoch: 89, Steps: 1 | Train Loss: 114.6378326 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11533212661743164\n",
      "Epoch: 90, Steps: 1 | Train Loss: 115.1051178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.11923861503601074\n",
      "Epoch: 91, Steps: 1 | Train Loss: 114.8017349 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.1107792854309082\n",
      "Epoch: 92, Steps: 1 | Train Loss: 114.9202652 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.12041258811950684\n",
      "Epoch: 93, Steps: 1 | Train Loss: 114.9626999 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.11390900611877441\n",
      "Epoch: 94, Steps: 1 | Train Loss: 114.8314438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10791993141174316\n",
      "Epoch: 95, Steps: 1 | Train Loss: 115.5104370 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.12137579917907715\n",
      "Epoch: 96, Steps: 1 | Train Loss: 114.7451172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.11287975311279297\n",
      "Epoch: 97, Steps: 1 | Train Loss: 114.6634140 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.11763978004455566\n",
      "Epoch: 98, Steps: 1 | Train Loss: 114.9294662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.11444091796875\n",
      "Epoch: 99, Steps: 1 | Train Loss: 115.1078415 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11062741279602051\n",
      "Epoch: 100, Steps: 1 | Train Loss: 114.7471924 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.11327934265136719\n",
      "Epoch: 101, Steps: 1 | Train Loss: 114.8355942 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.11058831214904785\n",
      "Epoch: 102, Steps: 1 | Train Loss: 114.7891769 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.11465287208557129\n",
      "Epoch: 103, Steps: 1 | Train Loss: 115.7293701 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.11625266075134277\n",
      "Epoch: 104, Steps: 1 | Train Loss: 114.8012466 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.11525082588195801\n",
      "Epoch: 105, Steps: 1 | Train Loss: 114.7068863 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.1264946460723877\n",
      "Epoch: 106, Steps: 1 | Train Loss: 115.0219879 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.11832904815673828\n",
      "Epoch: 107, Steps: 1 | Train Loss: 115.3245163 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.11535882949829102\n",
      "Epoch: 108, Steps: 1 | Train Loss: 115.2121582 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11499428749084473\n",
      "Epoch: 109, Steps: 1 | Train Loss: 114.5856476 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.11447572708129883\n",
      "Epoch: 110, Steps: 1 | Train Loss: 115.3991013 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11502933502197266\n",
      "Epoch: 111, Steps: 1 | Train Loss: 115.1564102 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.11681699752807617\n",
      "Epoch: 112, Steps: 1 | Train Loss: 115.0780869 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10941123962402344\n",
      "Epoch: 113, Steps: 1 | Train Loss: 114.5724487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.11739587783813477\n",
      "Epoch: 114, Steps: 1 | Train Loss: 114.8611984 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11774086952209473\n",
      "Epoch: 115, Steps: 1 | Train Loss: 114.9619370 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.13346004486083984\n",
      "Epoch: 116, Steps: 1 | Train Loss: 114.7730637 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.11847615242004395\n",
      "Epoch: 117, Steps: 1 | Train Loss: 115.0904922 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.1188805103302002\n",
      "Epoch: 118, Steps: 1 | Train Loss: 114.8803253 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10906672477722168\n",
      "Epoch: 119, Steps: 1 | Train Loss: 114.7385483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1169888973236084\n",
      "Epoch: 120, Steps: 1 | Train Loss: 115.0742798 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11485624313354492\n",
      "Epoch: 121, Steps: 1 | Train Loss: 115.0801773 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11933588981628418\n",
      "Epoch: 122, Steps: 1 | Train Loss: 115.0065918 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.1161506175994873\n",
      "Epoch: 123, Steps: 1 | Train Loss: 115.6061020 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11765456199645996\n",
      "Epoch: 124, Steps: 1 | Train Loss: 114.7735214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.11183881759643555\n",
      "Epoch: 125, Steps: 1 | Train Loss: 114.6270905 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.1294872760772705\n",
      "Epoch: 126, Steps: 1 | Train Loss: 114.6820450 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.12275934219360352\n",
      "Epoch: 127, Steps: 1 | Train Loss: 114.9208374 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12102317810058594\n",
      "Epoch: 128, Steps: 1 | Train Loss: 115.0613403 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.1180880069732666\n",
      "Epoch: 129, Steps: 1 | Train Loss: 114.9804001 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11132931709289551\n",
      "Epoch: 130, Steps: 1 | Train Loss: 114.5428238 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11470532417297363\n",
      "Epoch: 131, Steps: 1 | Train Loss: 114.1237335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11748385429382324\n",
      "Epoch: 132, Steps: 1 | Train Loss: 115.1246338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11730003356933594\n",
      "Epoch: 133, Steps: 1 | Train Loss: 114.1854630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.11464881896972656\n",
      "Epoch: 134, Steps: 1 | Train Loss: 115.1401901 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.12498641014099121\n",
      "Epoch: 135, Steps: 1 | Train Loss: 114.9491196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.11895394325256348\n",
      "Epoch: 136, Steps: 1 | Train Loss: 114.5819931 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.11220932006835938\n",
      "Epoch: 137, Steps: 1 | Train Loss: 115.0821915 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11788272857666016\n",
      "Epoch: 138, Steps: 1 | Train Loss: 115.1905136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11699748039245605\n",
      "Epoch: 139, Steps: 1 | Train Loss: 114.8744659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.1168816089630127\n",
      "Epoch: 140, Steps: 1 | Train Loss: 114.5553741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.10788822174072266\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.8521500 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11097526550292969\n",
      "Epoch: 142, Steps: 1 | Train Loss: 115.0896225 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11383461952209473\n",
      "Epoch: 143, Steps: 1 | Train Loss: 115.3472900 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11447405815124512\n",
      "Epoch: 144, Steps: 1 | Train Loss: 115.5365982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.12660622596740723\n",
      "Epoch: 145, Steps: 1 | Train Loss: 115.1981430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.11762213706970215\n",
      "Epoch: 146, Steps: 1 | Train Loss: 114.8913956 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10846114158630371\n",
      "Epoch: 147, Steps: 1 | Train Loss: 114.9466476 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11445450782775879\n",
      "Epoch: 148, Steps: 1 | Train Loss: 115.4585724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.1182405948638916\n",
      "Epoch: 149, Steps: 1 | Train Loss: 114.9342194 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.11809921264648438\n",
      "Epoch: 150, Steps: 1 | Train Loss: 115.1711349 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.1105804443359375\n",
      "Epoch: 151, Steps: 1 | Train Loss: 114.6684799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.11013126373291016\n",
      "Epoch: 152, Steps: 1 | Train Loss: 114.8569565 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.11055469512939453\n",
      "Epoch: 153, Steps: 1 | Train Loss: 115.3299789 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.11283230781555176\n",
      "Epoch: 154, Steps: 1 | Train Loss: 114.9297104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.11057209968566895\n",
      "Epoch: 155, Steps: 1 | Train Loss: 114.5213242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11079525947570801\n",
      "Epoch: 156, Steps: 1 | Train Loss: 114.9148788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10998082160949707\n",
      "Epoch: 157, Steps: 1 | Train Loss: 114.9296265 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10950446128845215\n",
      "Epoch: 158, Steps: 1 | Train Loss: 114.6775665 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.11078333854675293\n",
      "Epoch: 159, Steps: 1 | Train Loss: 114.5421371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11110448837280273\n",
      "Epoch: 160, Steps: 1 | Train Loss: 114.5669479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.1107015609741211\n",
      "Epoch: 161, Steps: 1 | Train Loss: 115.3035812 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.11850738525390625\n",
      "Epoch: 162, Steps: 1 | Train Loss: 114.4999161 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.13457131385803223\n",
      "Epoch: 163, Steps: 1 | Train Loss: 114.3869858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.12397956848144531\n",
      "Epoch: 164, Steps: 1 | Train Loss: 115.0290070 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.11122965812683105\n",
      "Epoch: 165, Steps: 1 | Train Loss: 114.6778717 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11086797714233398\n",
      "Epoch: 166, Steps: 1 | Train Loss: 115.1940689 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10978078842163086\n",
      "Epoch: 167, Steps: 1 | Train Loss: 114.7733688 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.10602259635925293\n",
      "Epoch: 168, Steps: 1 | Train Loss: 115.1202621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.1168816089630127\n",
      "Epoch: 169, Steps: 1 | Train Loss: 115.1558990 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.10513496398925781\n",
      "Epoch: 170, Steps: 1 | Train Loss: 115.0431442 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.10933899879455566\n",
      "Epoch: 171, Steps: 1 | Train Loss: 115.4483185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.12125277519226074\n",
      "Epoch: 172, Steps: 1 | Train Loss: 114.5002670 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11507868766784668\n",
      "Epoch: 173, Steps: 1 | Train Loss: 114.8790817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.1118323802947998\n",
      "Epoch: 174, Steps: 1 | Train Loss: 114.8668442 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10840010643005371\n",
      "Epoch: 175, Steps: 1 | Train Loss: 114.9788437 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.1103060245513916\n",
      "Epoch: 176, Steps: 1 | Train Loss: 115.1433868 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.1129903793334961\n",
      "Epoch: 177, Steps: 1 | Train Loss: 115.1971817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11184501647949219\n",
      "Epoch: 178, Steps: 1 | Train Loss: 114.9734726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.1074821949005127\n",
      "Epoch: 179, Steps: 1 | Train Loss: 114.7641373 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10902857780456543\n",
      "Epoch: 180, Steps: 1 | Train Loss: 114.6703262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11365675926208496\n",
      "Epoch: 181, Steps: 1 | Train Loss: 114.5204697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10787653923034668\n",
      "Epoch: 182, Steps: 1 | Train Loss: 114.9597168 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.12099790573120117\n",
      "Epoch: 183, Steps: 1 | Train Loss: 115.0570679 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.1195683479309082\n",
      "Epoch: 184, Steps: 1 | Train Loss: 114.8593521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11112785339355469\n",
      "Epoch: 185, Steps: 1 | Train Loss: 115.2088852 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.10913991928100586\n",
      "Epoch: 186, Steps: 1 | Train Loss: 115.0034943 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10852432250976562\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.6048355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10807657241821289\n",
      "Epoch: 188, Steps: 1 | Train Loss: 114.6606216 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.1078486442565918\n",
      "Epoch: 189, Steps: 1 | Train Loss: 114.9349747 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10777831077575684\n",
      "Epoch: 190, Steps: 1 | Train Loss: 114.7217026 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10769248008728027\n",
      "Epoch: 191, Steps: 1 | Train Loss: 114.6492920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10797953605651855\n",
      "Epoch: 192, Steps: 1 | Train Loss: 114.8937149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.10801196098327637\n",
      "Epoch: 193, Steps: 1 | Train Loss: 114.7451172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.10747385025024414\n",
      "Epoch: 194, Steps: 1 | Train Loss: 115.1666489 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10777592658996582\n",
      "Epoch: 195, Steps: 1 | Train Loss: 114.6058884 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.1221461296081543\n",
      "Epoch: 196, Steps: 1 | Train Loss: 114.8251495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11653828620910645\n",
      "Epoch: 197, Steps: 1 | Train Loss: 114.7041397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11035466194152832\n",
      "Epoch: 198, Steps: 1 | Train Loss: 114.9905396 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10894656181335449\n",
      "Epoch: 199, Steps: 1 | Train Loss: 115.0111618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10848021507263184\n",
      "Epoch: 200, Steps: 1 | Train Loss: 114.7935333 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10822892189025879\n",
      "Epoch: 201, Steps: 1 | Train Loss: 114.4183884 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10756969451904297\n",
      "Epoch: 202, Steps: 1 | Train Loss: 114.5809860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10792970657348633\n",
      "Epoch: 203, Steps: 1 | Train Loss: 114.7872086 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.10760164260864258\n",
      "Epoch: 204, Steps: 1 | Train Loss: 114.6682510 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.10793900489807129\n",
      "Epoch: 205, Steps: 1 | Train Loss: 114.6715088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10810542106628418\n",
      "Epoch: 206, Steps: 1 | Train Loss: 114.7756882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10786247253417969\n",
      "Epoch: 207, Steps: 1 | Train Loss: 114.8353271 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.12136197090148926\n",
      "Epoch: 208, Steps: 1 | Train Loss: 114.6777954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.12123513221740723\n",
      "Epoch: 209, Steps: 1 | Train Loss: 115.3209457 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11678481101989746\n",
      "Epoch: 210, Steps: 1 | Train Loss: 114.7570801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.10984158515930176\n",
      "Epoch: 211, Steps: 1 | Train Loss: 114.9578018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.10813760757446289\n",
      "Epoch: 212, Steps: 1 | Train Loss: 115.0586777 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.10759615898132324\n",
      "Epoch: 213, Steps: 1 | Train Loss: 114.9675446 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10838699340820312\n",
      "Epoch: 214, Steps: 1 | Train Loss: 115.0219955 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10866332054138184\n",
      "Epoch: 215, Steps: 1 | Train Loss: 114.6735840 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10782408714294434\n",
      "Epoch: 216, Steps: 1 | Train Loss: 114.6570358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10769939422607422\n",
      "Epoch: 217, Steps: 1 | Train Loss: 115.4158096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10802865028381348\n",
      "Epoch: 218, Steps: 1 | Train Loss: 115.4512939 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.12136220932006836\n",
      "Epoch: 219, Steps: 1 | Train Loss: 114.9743042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.12137460708618164\n",
      "Epoch: 220, Steps: 1 | Train Loss: 115.1132126 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11240506172180176\n",
      "Epoch: 221, Steps: 1 | Train Loss: 114.9605255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10915565490722656\n",
      "Epoch: 222, Steps: 1 | Train Loss: 114.8937912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10834193229675293\n",
      "Epoch: 223, Steps: 1 | Train Loss: 114.7046127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.10819315910339355\n",
      "Epoch: 224, Steps: 1 | Train Loss: 114.7997360 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.10771465301513672\n",
      "Epoch: 225, Steps: 1 | Train Loss: 115.1158676 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.10777831077575684\n",
      "Epoch: 226, Steps: 1 | Train Loss: 114.7497559 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10788702964782715\n",
      "Epoch: 227, Steps: 1 | Train Loss: 114.5425034 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11619400978088379\n",
      "Epoch: 228, Steps: 1 | Train Loss: 114.7861252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11525917053222656\n",
      "Epoch: 229, Steps: 1 | Train Loss: 114.2371368 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.10505914688110352\n",
      "Epoch: 230, Steps: 1 | Train Loss: 115.0217514 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.10696530342102051\n",
      "Epoch: 231, Steps: 1 | Train Loss: 115.3062973 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10913324356079102\n",
      "Epoch: 232, Steps: 1 | Train Loss: 114.6622849 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11930513381958008\n",
      "Epoch: 233, Steps: 1 | Train Loss: 114.4092789 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.1211090087890625\n",
      "Epoch: 234, Steps: 1 | Train Loss: 115.1123276 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.1109619140625\n",
      "Epoch: 235, Steps: 1 | Train Loss: 114.7026367 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10837125778198242\n",
      "Epoch: 236, Steps: 1 | Train Loss: 114.9659195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10784053802490234\n",
      "Epoch: 237, Steps: 1 | Train Loss: 114.6923218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10756349563598633\n",
      "Epoch: 238, Steps: 1 | Train Loss: 115.1757355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10836982727050781\n",
      "Epoch: 239, Steps: 1 | Train Loss: 114.9860458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10772156715393066\n",
      "Epoch: 240, Steps: 1 | Train Loss: 115.1760864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.12190556526184082\n",
      "Epoch: 241, Steps: 1 | Train Loss: 115.1812134 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11404228210449219\n",
      "Epoch: 242, Steps: 1 | Train Loss: 114.9764633 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10982799530029297\n",
      "Epoch: 243, Steps: 1 | Train Loss: 115.3453369 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10865998268127441\n",
      "Epoch: 244, Steps: 1 | Train Loss: 114.7603073 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.10805296897888184\n",
      "Epoch: 245, Steps: 1 | Train Loss: 115.4782639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.10750126838684082\n",
      "Epoch: 246, Steps: 1 | Train Loss: 115.0251389 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.10741925239562988\n",
      "Epoch: 247, Steps: 1 | Train Loss: 114.6394196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10753464698791504\n",
      "Epoch: 248, Steps: 1 | Train Loss: 114.6479263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10744452476501465\n",
      "Epoch: 249, Steps: 1 | Train Loss: 115.2111130 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10759186744689941\n",
      "Epoch: 250, Steps: 1 | Train Loss: 114.7132339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_12>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.12276506423950195\n",
      "Epoch: 1, Steps: 1 | Train Loss: 116.6642456 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.11644268035888672\n",
      "Epoch: 2, Steps: 1 | Train Loss: 115.0921402 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10840487480163574\n",
      "Epoch: 3, Steps: 1 | Train Loss: 113.4313965 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10858154296875\n",
      "Epoch: 4, Steps: 1 | Train Loss: 112.7060776 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.1080479621887207\n",
      "Epoch: 5, Steps: 1 | Train Loss: 112.4141846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10809993743896484\n",
      "Epoch: 6, Steps: 1 | Train Loss: 112.5994644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.1083834171295166\n",
      "Epoch: 7, Steps: 1 | Train Loss: 112.2001495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10775494575500488\n",
      "Epoch: 8, Steps: 1 | Train Loss: 111.6426315 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10773253440856934\n",
      "Epoch: 9, Steps: 1 | Train Loss: 112.0794830 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10780549049377441\n",
      "Epoch: 10, Steps: 1 | Train Loss: 112.0328369 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10753154754638672\n",
      "Epoch: 11, Steps: 1 | Train Loss: 112.1748581 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.1074821949005127\n",
      "Epoch: 12, Steps: 1 | Train Loss: 112.3952866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10743594169616699\n",
      "Epoch: 13, Steps: 1 | Train Loss: 112.1655273 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.12130284309387207\n",
      "Epoch: 14, Steps: 1 | Train Loss: 111.9107208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.11131787300109863\n",
      "Epoch: 15, Steps: 1 | Train Loss: 111.9651718 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.1091756820678711\n",
      "Epoch: 16, Steps: 1 | Train Loss: 112.8064499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.10831499099731445\n",
      "Epoch: 17, Steps: 1 | Train Loss: 112.3243942 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10779738426208496\n",
      "Epoch: 18, Steps: 1 | Train Loss: 112.3591309 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.1079099178314209\n",
      "Epoch: 19, Steps: 1 | Train Loss: 111.8045425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.10769271850585938\n",
      "Epoch: 20, Steps: 1 | Train Loss: 111.8495255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.10791659355163574\n",
      "Epoch: 21, Steps: 1 | Train Loss: 112.1707077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10771703720092773\n",
      "Epoch: 22, Steps: 1 | Train Loss: 111.6531525 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.1074676513671875\n",
      "Epoch: 23, Steps: 1 | Train Loss: 112.8948364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.10760235786437988\n",
      "Epoch: 24, Steps: 1 | Train Loss: 113.0211945 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.1081845760345459\n",
      "Epoch: 25, Steps: 1 | Train Loss: 111.8442612 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12132549285888672\n",
      "Epoch: 26, Steps: 1 | Train Loss: 111.6208496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.12105059623718262\n",
      "Epoch: 27, Steps: 1 | Train Loss: 111.7989044 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.12227463722229004\n",
      "Epoch: 28, Steps: 1 | Train Loss: 112.1115723 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.12080049514770508\n",
      "Epoch: 29, Steps: 1 | Train Loss: 111.6040268 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11502957344055176\n",
      "Epoch: 30, Steps: 1 | Train Loss: 112.8156357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11113095283508301\n",
      "Epoch: 31, Steps: 1 | Train Loss: 111.9212799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10875821113586426\n",
      "Epoch: 32, Steps: 1 | Train Loss: 112.4660416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10874605178833008\n",
      "Epoch: 33, Steps: 1 | Train Loss: 111.6534424 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.1089327335357666\n",
      "Epoch: 34, Steps: 1 | Train Loss: 112.7249527 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10727477073669434\n",
      "Epoch: 35, Steps: 1 | Train Loss: 111.9102325 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10740041732788086\n",
      "Epoch: 36, Steps: 1 | Train Loss: 112.3362350 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10787463188171387\n",
      "Epoch: 37, Steps: 1 | Train Loss: 111.7041626 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10737299919128418\n",
      "Epoch: 38, Steps: 1 | Train Loss: 112.3368149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10729217529296875\n",
      "Epoch: 39, Steps: 1 | Train Loss: 112.2981186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.1075890064239502\n",
      "Epoch: 40, Steps: 1 | Train Loss: 111.8237305 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.12120795249938965\n",
      "Epoch: 41, Steps: 1 | Train Loss: 111.6211777 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.12088131904602051\n",
      "Epoch: 42, Steps: 1 | Train Loss: 112.1456680 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.1255793571472168\n",
      "Epoch: 43, Steps: 1 | Train Loss: 112.0282135 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11504912376403809\n",
      "Epoch: 44, Steps: 1 | Train Loss: 111.9460068 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.11004447937011719\n",
      "Epoch: 45, Steps: 1 | Train Loss: 111.6918488 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.1091146469116211\n",
      "Epoch: 46, Steps: 1 | Train Loss: 112.1758194 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.1084904670715332\n",
      "Epoch: 47, Steps: 1 | Train Loss: 112.4585495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.10784292221069336\n",
      "Epoch: 48, Steps: 1 | Train Loss: 112.1404419 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.10729289054870605\n",
      "Epoch: 49, Steps: 1 | Train Loss: 111.8263931 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.10687637329101562\n",
      "Epoch: 50, Steps: 1 | Train Loss: 112.2444611 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.1071937084197998\n",
      "Epoch: 51, Steps: 1 | Train Loss: 112.1796188 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10682916641235352\n",
      "Epoch: 52, Steps: 1 | Train Loss: 112.5386734 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10731267929077148\n",
      "Epoch: 53, Steps: 1 | Train Loss: 111.9890976 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10700678825378418\n",
      "Epoch: 54, Steps: 1 | Train Loss: 112.1806030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.1073908805847168\n",
      "Epoch: 55, Steps: 1 | Train Loss: 112.0339127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1072845458984375\n",
      "Epoch: 56, Steps: 1 | Train Loss: 111.7770920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.1075596809387207\n",
      "Epoch: 57, Steps: 1 | Train Loss: 112.4888916 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10737371444702148\n",
      "Epoch: 58, Steps: 1 | Train Loss: 111.9078369 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.12079739570617676\n",
      "Epoch: 59, Steps: 1 | Train Loss: 112.3466568 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.12091708183288574\n",
      "Epoch: 60, Steps: 1 | Train Loss: 112.2612839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.12099504470825195\n",
      "Epoch: 61, Steps: 1 | Train Loss: 112.5391998 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11714506149291992\n",
      "Epoch: 62, Steps: 1 | Train Loss: 111.8627472 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11249375343322754\n",
      "Epoch: 63, Steps: 1 | Train Loss: 111.8777084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.10935759544372559\n",
      "Epoch: 64, Steps: 1 | Train Loss: 112.6112442 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.1084752082824707\n",
      "Epoch: 65, Steps: 1 | Train Loss: 112.5451431 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.10815978050231934\n",
      "Epoch: 66, Steps: 1 | Train Loss: 112.1276627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.10756540298461914\n",
      "Epoch: 67, Steps: 1 | Train Loss: 111.9566422 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10724949836730957\n",
      "Epoch: 68, Steps: 1 | Train Loss: 111.9122925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10757565498352051\n",
      "Epoch: 69, Steps: 1 | Train Loss: 111.9604263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.1071004867553711\n",
      "Epoch: 70, Steps: 1 | Train Loss: 112.4063492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10745716094970703\n",
      "Epoch: 71, Steps: 1 | Train Loss: 111.9232941 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10762739181518555\n",
      "Epoch: 72, Steps: 1 | Train Loss: 111.9048080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.1073763370513916\n",
      "Epoch: 73, Steps: 1 | Train Loss: 112.1745758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.10733151435852051\n",
      "Epoch: 74, Steps: 1 | Train Loss: 112.9825211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.10745954513549805\n",
      "Epoch: 75, Steps: 1 | Train Loss: 112.1236343 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.12108302116394043\n",
      "Epoch: 76, Steps: 1 | Train Loss: 112.0747299 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.1208343505859375\n",
      "Epoch: 77, Steps: 1 | Train Loss: 111.9458847 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.11822700500488281\n",
      "Epoch: 78, Steps: 1 | Train Loss: 111.9427643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.11226773262023926\n",
      "Epoch: 79, Steps: 1 | Train Loss: 112.2458878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10920143127441406\n",
      "Epoch: 80, Steps: 1 | Train Loss: 112.4375229 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10829806327819824\n",
      "Epoch: 81, Steps: 1 | Train Loss: 112.0466690 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.10760211944580078\n",
      "Epoch: 82, Steps: 1 | Train Loss: 112.4888458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.10757780075073242\n",
      "Epoch: 83, Steps: 1 | Train Loss: 112.1279831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.10756039619445801\n",
      "Epoch: 84, Steps: 1 | Train Loss: 111.7437973 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.10793185234069824\n",
      "Epoch: 85, Steps: 1 | Train Loss: 112.1860886 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.10743117332458496\n",
      "Epoch: 86, Steps: 1 | Train Loss: 111.8454208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.12165641784667969\n",
      "Epoch: 87, Steps: 1 | Train Loss: 112.1569443 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11373257637023926\n",
      "Epoch: 88, Steps: 1 | Train Loss: 112.2606735 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10948729515075684\n",
      "Epoch: 89, Steps: 1 | Train Loss: 111.7731705 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10855793952941895\n",
      "Epoch: 90, Steps: 1 | Train Loss: 112.6875534 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10762667655944824\n",
      "Epoch: 91, Steps: 1 | Train Loss: 112.1630630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10736680030822754\n",
      "Epoch: 92, Steps: 1 | Train Loss: 112.9493408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10737800598144531\n",
      "Epoch: 93, Steps: 1 | Train Loss: 112.1861725 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10753774642944336\n",
      "Epoch: 94, Steps: 1 | Train Loss: 111.8432999 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10735821723937988\n",
      "Epoch: 95, Steps: 1 | Train Loss: 112.2063217 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10766959190368652\n",
      "Epoch: 96, Steps: 1 | Train Loss: 111.8001480 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10740423202514648\n",
      "Epoch: 97, Steps: 1 | Train Loss: 112.1255493 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10745501518249512\n",
      "Epoch: 98, Steps: 1 | Train Loss: 112.0156403 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.12153887748718262\n",
      "Epoch: 99, Steps: 1 | Train Loss: 112.2878571 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11436653137207031\n",
      "Epoch: 100, Steps: 1 | Train Loss: 111.9941940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10987091064453125\n",
      "Epoch: 101, Steps: 1 | Train Loss: 111.9843140 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10863661766052246\n",
      "Epoch: 102, Steps: 1 | Train Loss: 112.3292389 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10791230201721191\n",
      "Epoch: 103, Steps: 1 | Train Loss: 112.0769577 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10750079154968262\n",
      "Epoch: 104, Steps: 1 | Train Loss: 112.0241013 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10758352279663086\n",
      "Epoch: 105, Steps: 1 | Train Loss: 112.0907135 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10738468170166016\n",
      "Epoch: 106, Steps: 1 | Train Loss: 111.5971909 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10722899436950684\n",
      "Epoch: 107, Steps: 1 | Train Loss: 111.5457382 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10749697685241699\n",
      "Epoch: 108, Steps: 1 | Train Loss: 112.2376251 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10748958587646484\n",
      "Epoch: 109, Steps: 1 | Train Loss: 111.8124542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.10722756385803223\n",
      "Epoch: 110, Steps: 1 | Train Loss: 111.7428207 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.12056660652160645\n",
      "Epoch: 111, Steps: 1 | Train Loss: 112.1405029 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.12110400199890137\n",
      "Epoch: 112, Steps: 1 | Train Loss: 112.0079346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11495566368103027\n",
      "Epoch: 113, Steps: 1 | Train Loss: 111.9118576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10970234870910645\n",
      "Epoch: 114, Steps: 1 | Train Loss: 112.0893936 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10899996757507324\n",
      "Epoch: 115, Steps: 1 | Train Loss: 112.1999054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10806107521057129\n",
      "Epoch: 116, Steps: 1 | Train Loss: 112.6885910 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10725855827331543\n",
      "Epoch: 117, Steps: 1 | Train Loss: 112.1501465 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10775470733642578\n",
      "Epoch: 118, Steps: 1 | Train Loss: 112.2374420 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10744738578796387\n",
      "Epoch: 119, Steps: 1 | Train Loss: 111.8281860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.10736298561096191\n",
      "Epoch: 120, Steps: 1 | Train Loss: 112.0253220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.10721230506896973\n",
      "Epoch: 121, Steps: 1 | Train Loss: 112.1839600 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10764622688293457\n",
      "Epoch: 122, Steps: 1 | Train Loss: 112.1280518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.1073448657989502\n",
      "Epoch: 123, Steps: 1 | Train Loss: 112.4879913 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.10739421844482422\n",
      "Epoch: 124, Steps: 1 | Train Loss: 112.0629425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.12102198600769043\n",
      "Epoch: 125, Steps: 1 | Train Loss: 111.9253082 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.12089037895202637\n",
      "Epoch: 126, Steps: 1 | Train Loss: 112.4441910 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11497640609741211\n",
      "Epoch: 127, Steps: 1 | Train Loss: 112.0618439 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.11019134521484375\n",
      "Epoch: 128, Steps: 1 | Train Loss: 112.0331802 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.1078939437866211\n",
      "Epoch: 129, Steps: 1 | Train Loss: 111.8733749 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10745668411254883\n",
      "Epoch: 130, Steps: 1 | Train Loss: 111.9310532 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.10731124877929688\n",
      "Epoch: 131, Steps: 1 | Train Loss: 112.4374619 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10754537582397461\n",
      "Epoch: 132, Steps: 1 | Train Loss: 112.1025391 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.10747671127319336\n",
      "Epoch: 133, Steps: 1 | Train Loss: 112.3606186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10722613334655762\n",
      "Epoch: 134, Steps: 1 | Train Loss: 111.8660431 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10742640495300293\n",
      "Epoch: 135, Steps: 1 | Train Loss: 112.5436325 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.12157607078552246\n",
      "Epoch: 136, Steps: 1 | Train Loss: 111.8730469 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.12088489532470703\n",
      "Epoch: 137, Steps: 1 | Train Loss: 111.5816879 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11341094970703125\n",
      "Epoch: 138, Steps: 1 | Train Loss: 112.2037277 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.10998749732971191\n",
      "Epoch: 139, Steps: 1 | Train Loss: 111.8333511 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10850381851196289\n",
      "Epoch: 140, Steps: 1 | Train Loss: 111.7222214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.10918879508972168\n",
      "Epoch: 141, Steps: 1 | Train Loss: 112.1319962 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.1084291934967041\n",
      "Epoch: 142, Steps: 1 | Train Loss: 112.0354843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.10854244232177734\n",
      "Epoch: 143, Steps: 1 | Train Loss: 112.1775513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.10785412788391113\n",
      "Epoch: 144, Steps: 1 | Train Loss: 112.1314697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10759329795837402\n",
      "Epoch: 145, Steps: 1 | Train Loss: 111.8640671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10774469375610352\n",
      "Epoch: 146, Steps: 1 | Train Loss: 111.7198715 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10762643814086914\n",
      "Epoch: 147, Steps: 1 | Train Loss: 111.9460220 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.1076512336730957\n",
      "Epoch: 148, Steps: 1 | Train Loss: 112.3330078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.1212468147277832\n",
      "Epoch: 149, Steps: 1 | Train Loss: 112.1007690 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.12186241149902344\n",
      "Epoch: 150, Steps: 1 | Train Loss: 112.5618210 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.11127161979675293\n",
      "Epoch: 151, Steps: 1 | Train Loss: 111.5089111 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10931587219238281\n",
      "Epoch: 152, Steps: 1 | Train Loss: 111.9390488 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.1081688404083252\n",
      "Epoch: 153, Steps: 1 | Train Loss: 111.9207077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10756444931030273\n",
      "Epoch: 154, Steps: 1 | Train Loss: 111.8943710 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10772252082824707\n",
      "Epoch: 155, Steps: 1 | Train Loss: 112.2164917 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11532878875732422\n",
      "Epoch: 156, Steps: 1 | Train Loss: 111.6574936 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10741806030273438\n",
      "Epoch: 157, Steps: 1 | Train Loss: 112.3505859 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.1073598861694336\n",
      "Epoch: 158, Steps: 1 | Train Loss: 112.1450729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10731816291809082\n",
      "Epoch: 159, Steps: 1 | Train Loss: 111.8508301 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.1074514389038086\n",
      "Epoch: 160, Steps: 1 | Train Loss: 112.2016373 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.1073918342590332\n",
      "Epoch: 161, Steps: 1 | Train Loss: 111.7663345 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.12118387222290039\n",
      "Epoch: 162, Steps: 1 | Train Loss: 111.4942398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.12211418151855469\n",
      "Epoch: 163, Steps: 1 | Train Loss: 112.1893311 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.11750936508178711\n",
      "Epoch: 164, Steps: 1 | Train Loss: 111.8649292 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.11011862754821777\n",
      "Epoch: 165, Steps: 1 | Train Loss: 112.1831055 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.10867071151733398\n",
      "Epoch: 166, Steps: 1 | Train Loss: 112.1458740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10815858840942383\n",
      "Epoch: 167, Steps: 1 | Train Loss: 112.2972183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.10785055160522461\n",
      "Epoch: 168, Steps: 1 | Train Loss: 111.8815460 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.10771012306213379\n",
      "Epoch: 169, Steps: 1 | Train Loss: 112.4060440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.10718393325805664\n",
      "Epoch: 170, Steps: 1 | Train Loss: 112.2157974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.10733222961425781\n",
      "Epoch: 171, Steps: 1 | Train Loss: 111.8544235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.10795307159423828\n",
      "Epoch: 172, Steps: 1 | Train Loss: 111.9212418 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10739636421203613\n",
      "Epoch: 173, Steps: 1 | Train Loss: 112.3958130 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10708332061767578\n",
      "Epoch: 174, Steps: 1 | Train Loss: 112.3757553 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10771369934082031\n",
      "Epoch: 175, Steps: 1 | Train Loss: 112.1859665 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.10729837417602539\n",
      "Epoch: 176, Steps: 1 | Train Loss: 112.3893814 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.11626577377319336\n",
      "Epoch: 177, Steps: 1 | Train Loss: 112.0835495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11225008964538574\n",
      "Epoch: 178, Steps: 1 | Train Loss: 112.3173141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10877871513366699\n",
      "Epoch: 179, Steps: 1 | Train Loss: 111.9217072 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10825872421264648\n",
      "Epoch: 180, Steps: 1 | Train Loss: 112.1079636 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.1078336238861084\n",
      "Epoch: 181, Steps: 1 | Train Loss: 111.6434097 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10814690589904785\n",
      "Epoch: 182, Steps: 1 | Train Loss: 112.0631332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10869216918945312\n",
      "Epoch: 183, Steps: 1 | Train Loss: 112.4199753 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.10741305351257324\n",
      "Epoch: 184, Steps: 1 | Train Loss: 112.2123795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.10741114616394043\n",
      "Epoch: 185, Steps: 1 | Train Loss: 112.0171890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.10738658905029297\n",
      "Epoch: 186, Steps: 1 | Train Loss: 112.3027573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.12076044082641602\n",
      "Epoch: 187, Steps: 1 | Train Loss: 112.3880539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.12062358856201172\n",
      "Epoch: 188, Steps: 1 | Train Loss: 111.7549057 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.11597251892089844\n",
      "Epoch: 189, Steps: 1 | Train Loss: 112.0411530 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10993480682373047\n",
      "Epoch: 190, Steps: 1 | Train Loss: 112.1041412 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10846352577209473\n",
      "Epoch: 191, Steps: 1 | Train Loss: 112.2493820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10821700096130371\n",
      "Epoch: 192, Steps: 1 | Train Loss: 112.2506943 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.10780549049377441\n",
      "Epoch: 193, Steps: 1 | Train Loss: 112.1626968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.10708498954772949\n",
      "Epoch: 194, Steps: 1 | Train Loss: 111.9840088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10727119445800781\n",
      "Epoch: 195, Steps: 1 | Train Loss: 112.7041779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.10711288452148438\n",
      "Epoch: 196, Steps: 1 | Train Loss: 112.1078033 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.1072084903717041\n",
      "Epoch: 197, Steps: 1 | Train Loss: 111.6159821 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.10706377029418945\n",
      "Epoch: 198, Steps: 1 | Train Loss: 111.8720627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10742568969726562\n",
      "Epoch: 199, Steps: 1 | Train Loss: 112.0139313 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10716581344604492\n",
      "Epoch: 200, Steps: 1 | Train Loss: 112.4312744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10725760459899902\n",
      "Epoch: 201, Steps: 1 | Train Loss: 112.2698517 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10706281661987305\n",
      "Epoch: 202, Steps: 1 | Train Loss: 112.9210205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10751771926879883\n",
      "Epoch: 203, Steps: 1 | Train Loss: 112.2264404 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.12126731872558594\n",
      "Epoch: 204, Steps: 1 | Train Loss: 112.1567383 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12078094482421875\n",
      "Epoch: 205, Steps: 1 | Train Loss: 112.2625046 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.12062215805053711\n",
      "Epoch: 206, Steps: 1 | Train Loss: 112.2467041 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11811137199401855\n",
      "Epoch: 207, Steps: 1 | Train Loss: 111.6240234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.11077690124511719\n",
      "Epoch: 208, Steps: 1 | Train Loss: 111.9094467 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10867071151733398\n",
      "Epoch: 209, Steps: 1 | Train Loss: 112.7130127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.1083974838256836\n",
      "Epoch: 210, Steps: 1 | Train Loss: 111.9827499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.10790348052978516\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.3019791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.10737085342407227\n",
      "Epoch: 212, Steps: 1 | Train Loss: 111.9367294 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.10725045204162598\n",
      "Epoch: 213, Steps: 1 | Train Loss: 112.0706100 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10706806182861328\n",
      "Epoch: 214, Steps: 1 | Train Loss: 111.8990250 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.1071932315826416\n",
      "Epoch: 215, Steps: 1 | Train Loss: 112.1938248 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10699772834777832\n",
      "Epoch: 216, Steps: 1 | Train Loss: 112.2252731 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.1072075366973877\n",
      "Epoch: 217, Steps: 1 | Train Loss: 111.9494781 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10710716247558594\n",
      "Epoch: 218, Steps: 1 | Train Loss: 112.2498703 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10700058937072754\n",
      "Epoch: 219, Steps: 1 | Train Loss: 112.0921173 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10736799240112305\n",
      "Epoch: 220, Steps: 1 | Train Loss: 112.4635162 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.10713863372802734\n",
      "Epoch: 221, Steps: 1 | Train Loss: 112.2917252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.1208503246307373\n",
      "Epoch: 222, Steps: 1 | Train Loss: 111.9070435 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.12068462371826172\n",
      "Epoch: 223, Steps: 1 | Train Loss: 112.7367172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.12110424041748047\n",
      "Epoch: 224, Steps: 1 | Train Loss: 111.7347183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11834073066711426\n",
      "Epoch: 225, Steps: 1 | Train Loss: 111.8797379 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11039543151855469\n",
      "Epoch: 226, Steps: 1 | Train Loss: 112.0559845 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10871386528015137\n",
      "Epoch: 227, Steps: 1 | Train Loss: 111.7707748 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.10786271095275879\n",
      "Epoch: 228, Steps: 1 | Train Loss: 111.9943161 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.10750246047973633\n",
      "Epoch: 229, Steps: 1 | Train Loss: 111.9254761 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.10715794563293457\n",
      "Epoch: 230, Steps: 1 | Train Loss: 112.2234421 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.1069951057434082\n",
      "Epoch: 231, Steps: 1 | Train Loss: 112.2283096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10715389251708984\n",
      "Epoch: 232, Steps: 1 | Train Loss: 112.2436905 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.10751152038574219\n",
      "Epoch: 233, Steps: 1 | Train Loss: 112.1701889 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.1072225570678711\n",
      "Epoch: 234, Steps: 1 | Train Loss: 112.0866470 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.10716748237609863\n",
      "Epoch: 235, Steps: 1 | Train Loss: 112.0577011 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10710000991821289\n",
      "Epoch: 236, Steps: 1 | Train Loss: 112.1056290 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.12173223495483398\n",
      "Epoch: 237, Steps: 1 | Train Loss: 112.1974411 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.12055349349975586\n",
      "Epoch: 238, Steps: 1 | Train Loss: 112.0182037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.12074637413024902\n",
      "Epoch: 239, Steps: 1 | Train Loss: 112.4270782 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.1123349666595459\n",
      "Epoch: 240, Steps: 1 | Train Loss: 112.2944870 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1092844009399414\n",
      "Epoch: 241, Steps: 1 | Train Loss: 111.8283310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.10856056213378906\n",
      "Epoch: 242, Steps: 1 | Train Loss: 112.4747696 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10758066177368164\n",
      "Epoch: 243, Steps: 1 | Train Loss: 111.9941940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10737442970275879\n",
      "Epoch: 244, Steps: 1 | Train Loss: 112.1665421 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.1073458194732666\n",
      "Epoch: 245, Steps: 1 | Train Loss: 111.8803940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.12751078605651855\n",
      "Epoch: 246, Steps: 1 | Train Loss: 112.8358688 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.13551998138427734\n",
      "Epoch: 247, Steps: 1 | Train Loss: 111.8001328 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.12400102615356445\n",
      "Epoch: 248, Steps: 1 | Train Loss: 112.0302505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11116313934326172\n",
      "Epoch: 249, Steps: 1 | Train Loss: 111.8992310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11291289329528809\n",
      "Epoch: 250, Steps: 1 | Train Loss: 112.1281967 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_13>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11616802215576172\n",
      "Epoch: 1, Steps: 1 | Train Loss: 118.1364059 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10474753379821777\n",
      "Epoch: 2, Steps: 1 | Train Loss: 115.9070358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.1068718433380127\n",
      "Epoch: 3, Steps: 1 | Train Loss: 114.8868942 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10619831085205078\n",
      "Epoch: 4, Steps: 1 | Train Loss: 113.6647949 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10663437843322754\n",
      "Epoch: 5, Steps: 1 | Train Loss: 112.8365021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10579180717468262\n",
      "Epoch: 6, Steps: 1 | Train Loss: 113.2472687 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10961055755615234\n",
      "Epoch: 7, Steps: 1 | Train Loss: 112.7514648 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10934591293334961\n",
      "Epoch: 8, Steps: 1 | Train Loss: 112.8983154 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.1069803237915039\n",
      "Epoch: 9, Steps: 1 | Train Loss: 113.1292114 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10653305053710938\n",
      "Epoch: 10, Steps: 1 | Train Loss: 113.4980698 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.1074833869934082\n",
      "Epoch: 11, Steps: 1 | Train Loss: 113.0223160 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11462807655334473\n",
      "Epoch: 12, Steps: 1 | Train Loss: 113.0073013 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10742044448852539\n",
      "Epoch: 13, Steps: 1 | Train Loss: 113.1637497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.1152811050415039\n",
      "Epoch: 14, Steps: 1 | Train Loss: 112.9567184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10711359977722168\n",
      "Epoch: 15, Steps: 1 | Train Loss: 113.2406540 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10713481903076172\n",
      "Epoch: 16, Steps: 1 | Train Loss: 113.3855591 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.1061255931854248\n",
      "Epoch: 17, Steps: 1 | Train Loss: 112.7212677 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10591268539428711\n",
      "Epoch: 18, Steps: 1 | Train Loss: 113.6060181 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10673284530639648\n",
      "Epoch: 19, Steps: 1 | Train Loss: 113.4099350 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.11816644668579102\n",
      "Epoch: 20, Steps: 1 | Train Loss: 112.6687393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.12047815322875977\n",
      "Epoch: 21, Steps: 1 | Train Loss: 113.4785919 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.1210641860961914\n",
      "Epoch: 22, Steps: 1 | Train Loss: 113.0237961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.12110424041748047\n",
      "Epoch: 23, Steps: 1 | Train Loss: 112.9101944 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11566734313964844\n",
      "Epoch: 24, Steps: 1 | Train Loss: 112.8129425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11288166046142578\n",
      "Epoch: 25, Steps: 1 | Train Loss: 112.8965073 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11219191551208496\n",
      "Epoch: 26, Steps: 1 | Train Loss: 112.8729019 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11136984825134277\n",
      "Epoch: 27, Steps: 1 | Train Loss: 113.5974960 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.1117241382598877\n",
      "Epoch: 28, Steps: 1 | Train Loss: 113.0488892 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10996770858764648\n",
      "Epoch: 29, Steps: 1 | Train Loss: 113.4162827 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11057662963867188\n",
      "Epoch: 30, Steps: 1 | Train Loss: 113.2034454 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11012506484985352\n",
      "Epoch: 31, Steps: 1 | Train Loss: 112.8173141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.11036157608032227\n",
      "Epoch: 32, Steps: 1 | Train Loss: 112.8350449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11063003540039062\n",
      "Epoch: 33, Steps: 1 | Train Loss: 113.0016479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.12369656562805176\n",
      "Epoch: 34, Steps: 1 | Train Loss: 112.8549347 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.12382102012634277\n",
      "Epoch: 35, Steps: 1 | Train Loss: 113.2462921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.12033200263977051\n",
      "Epoch: 36, Steps: 1 | Train Loss: 113.0373077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.1125948429107666\n",
      "Epoch: 37, Steps: 1 | Train Loss: 113.2408600 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11038541793823242\n",
      "Epoch: 38, Steps: 1 | Train Loss: 113.1762466 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.11070418357849121\n",
      "Epoch: 39, Steps: 1 | Train Loss: 112.9156113 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.11080217361450195\n",
      "Epoch: 40, Steps: 1 | Train Loss: 112.7180405 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.1110377311706543\n",
      "Epoch: 41, Steps: 1 | Train Loss: 113.0260010 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.11128854751586914\n",
      "Epoch: 42, Steps: 1 | Train Loss: 113.0481186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10919380187988281\n",
      "Epoch: 43, Steps: 1 | Train Loss: 112.7611618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11058521270751953\n",
      "Epoch: 44, Steps: 1 | Train Loss: 113.4374542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.11071252822875977\n",
      "Epoch: 45, Steps: 1 | Train Loss: 112.8578873 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.1106879711151123\n",
      "Epoch: 46, Steps: 1 | Train Loss: 113.6256714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.1137230396270752\n",
      "Epoch: 47, Steps: 1 | Train Loss: 113.2157516 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.11537003517150879\n",
      "Epoch: 48, Steps: 1 | Train Loss: 113.1404037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.1105489730834961\n",
      "Epoch: 49, Steps: 1 | Train Loss: 113.3680954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11131691932678223\n",
      "Epoch: 50, Steps: 1 | Train Loss: 112.6904526 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.10685539245605469\n",
      "Epoch: 51, Steps: 1 | Train Loss: 112.9374542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10700845718383789\n",
      "Epoch: 52, Steps: 1 | Train Loss: 113.4794464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10722875595092773\n",
      "Epoch: 53, Steps: 1 | Train Loss: 113.0890884 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10735607147216797\n",
      "Epoch: 54, Steps: 1 | Train Loss: 113.2681885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.10773396492004395\n",
      "Epoch: 55, Steps: 1 | Train Loss: 113.0750961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1072993278503418\n",
      "Epoch: 56, Steps: 1 | Train Loss: 113.1817398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.1075441837310791\n",
      "Epoch: 57, Steps: 1 | Train Loss: 113.0563126 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10772252082824707\n",
      "Epoch: 58, Steps: 1 | Train Loss: 112.9933853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.1071939468383789\n",
      "Epoch: 59, Steps: 1 | Train Loss: 112.8776855 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.12084794044494629\n",
      "Epoch: 60, Steps: 1 | Train Loss: 112.8044968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.1206521987915039\n",
      "Epoch: 61, Steps: 1 | Train Loss: 113.2624435 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11517715454101562\n",
      "Epoch: 62, Steps: 1 | Train Loss: 113.5985641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.10944008827209473\n",
      "Epoch: 63, Steps: 1 | Train Loss: 113.1625519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.10867810249328613\n",
      "Epoch: 64, Steps: 1 | Train Loss: 113.1806870 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.10801482200622559\n",
      "Epoch: 65, Steps: 1 | Train Loss: 112.8193893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.10748839378356934\n",
      "Epoch: 66, Steps: 1 | Train Loss: 112.5346909 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.1075894832611084\n",
      "Epoch: 67, Steps: 1 | Train Loss: 112.9565048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10738253593444824\n",
      "Epoch: 68, Steps: 1 | Train Loss: 113.1788101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.1072835922241211\n",
      "Epoch: 69, Steps: 1 | Train Loss: 112.8724899 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.10739326477050781\n",
      "Epoch: 70, Steps: 1 | Train Loss: 112.7521744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10750436782836914\n",
      "Epoch: 71, Steps: 1 | Train Loss: 113.1262741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10797595977783203\n",
      "Epoch: 72, Steps: 1 | Train Loss: 112.7517090 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.12118363380432129\n",
      "Epoch: 73, Steps: 1 | Train Loss: 112.8125763 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.1211240291595459\n",
      "Epoch: 74, Steps: 1 | Train Loss: 113.1735458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.12111330032348633\n",
      "Epoch: 75, Steps: 1 | Train Loss: 112.8532028 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11834931373596191\n",
      "Epoch: 76, Steps: 1 | Train Loss: 112.9981461 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.1107480525970459\n",
      "Epoch: 77, Steps: 1 | Train Loss: 113.0895767 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10863065719604492\n",
      "Epoch: 78, Steps: 1 | Train Loss: 113.5157013 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10749316215515137\n",
      "Epoch: 79, Steps: 1 | Train Loss: 112.7852325 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10781526565551758\n",
      "Epoch: 80, Steps: 1 | Train Loss: 112.9347305 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10747408866882324\n",
      "Epoch: 81, Steps: 1 | Train Loss: 112.9128799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.1072230339050293\n",
      "Epoch: 82, Steps: 1 | Train Loss: 113.6665573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.11947226524353027\n",
      "Epoch: 83, Steps: 1 | Train Loss: 112.6935196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11447858810424805\n",
      "Epoch: 84, Steps: 1 | Train Loss: 113.1597824 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.11098194122314453\n",
      "Epoch: 85, Steps: 1 | Train Loss: 113.3065414 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.10845422744750977\n",
      "Epoch: 86, Steps: 1 | Train Loss: 112.8205566 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10764598846435547\n",
      "Epoch: 87, Steps: 1 | Train Loss: 113.2620010 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.1075582504272461\n",
      "Epoch: 88, Steps: 1 | Train Loss: 113.5169296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.1073160171508789\n",
      "Epoch: 89, Steps: 1 | Train Loss: 113.2777939 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10770320892333984\n",
      "Epoch: 90, Steps: 1 | Train Loss: 112.9415283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.1072077751159668\n",
      "Epoch: 91, Steps: 1 | Train Loss: 112.9254150 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10753583908081055\n",
      "Epoch: 92, Steps: 1 | Train Loss: 113.4305649 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10716772079467773\n",
      "Epoch: 93, Steps: 1 | Train Loss: 112.9931183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.12073945999145508\n",
      "Epoch: 94, Steps: 1 | Train Loss: 113.0698853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.11081600189208984\n",
      "Epoch: 95, Steps: 1 | Train Loss: 112.8674469 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10899519920349121\n",
      "Epoch: 96, Steps: 1 | Train Loss: 113.0204239 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10820317268371582\n",
      "Epoch: 97, Steps: 1 | Train Loss: 112.6225586 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10745763778686523\n",
      "Epoch: 98, Steps: 1 | Train Loss: 113.2217178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10704922676086426\n",
      "Epoch: 99, Steps: 1 | Train Loss: 113.0873032 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10748958587646484\n",
      "Epoch: 100, Steps: 1 | Train Loss: 113.3348160 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10722565650939941\n",
      "Epoch: 101, Steps: 1 | Train Loss: 112.6816788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10707545280456543\n",
      "Epoch: 102, Steps: 1 | Train Loss: 113.5476074 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10709214210510254\n",
      "Epoch: 103, Steps: 1 | Train Loss: 113.3419113 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10738277435302734\n",
      "Epoch: 104, Steps: 1 | Train Loss: 113.3451767 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.1217641830444336\n",
      "Epoch: 105, Steps: 1 | Train Loss: 112.9642105 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.11256670951843262\n",
      "Epoch: 106, Steps: 1 | Train Loss: 112.8321533 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10921525955200195\n",
      "Epoch: 107, Steps: 1 | Train Loss: 113.0149155 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10834765434265137\n",
      "Epoch: 108, Steps: 1 | Train Loss: 112.8859634 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10780096054077148\n",
      "Epoch: 109, Steps: 1 | Train Loss: 113.1066513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.1077566146850586\n",
      "Epoch: 110, Steps: 1 | Train Loss: 112.9294205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.10731792449951172\n",
      "Epoch: 111, Steps: 1 | Train Loss: 112.7756119 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10762476921081543\n",
      "Epoch: 112, Steps: 1 | Train Loss: 112.8194580 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10723185539245605\n",
      "Epoch: 113, Steps: 1 | Train Loss: 113.0726929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10727858543395996\n",
      "Epoch: 114, Steps: 1 | Train Loss: 112.6338882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10787224769592285\n",
      "Epoch: 115, Steps: 1 | Train Loss: 112.9598312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.1075441837310791\n",
      "Epoch: 116, Steps: 1 | Train Loss: 112.6480255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10815834999084473\n",
      "Epoch: 117, Steps: 1 | Train Loss: 113.2255173 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10783123970031738\n",
      "Epoch: 118, Steps: 1 | Train Loss: 113.3616943 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.12160348892211914\n",
      "Epoch: 119, Steps: 1 | Train Loss: 113.0865860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1206824779510498\n",
      "Epoch: 120, Steps: 1 | Train Loss: 112.8513336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.1190190315246582\n",
      "Epoch: 121, Steps: 1 | Train Loss: 113.4764938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.1108088493347168\n",
      "Epoch: 122, Steps: 1 | Train Loss: 113.1894531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10879325866699219\n",
      "Epoch: 123, Steps: 1 | Train Loss: 112.9542007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.10802221298217773\n",
      "Epoch: 124, Steps: 1 | Train Loss: 112.9708786 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.10787606239318848\n",
      "Epoch: 125, Steps: 1 | Train Loss: 112.9725342 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.10758304595947266\n",
      "Epoch: 126, Steps: 1 | Train Loss: 113.3001480 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.10725903511047363\n",
      "Epoch: 127, Steps: 1 | Train Loss: 113.1753159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.10711145401000977\n",
      "Epoch: 128, Steps: 1 | Train Loss: 113.3282700 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10732889175415039\n",
      "Epoch: 129, Steps: 1 | Train Loss: 113.0888901 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10731172561645508\n",
      "Epoch: 130, Steps: 1 | Train Loss: 113.5944824 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.10695934295654297\n",
      "Epoch: 131, Steps: 1 | Train Loss: 112.8793716 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10715317726135254\n",
      "Epoch: 132, Steps: 1 | Train Loss: 113.2820053 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.10760331153869629\n",
      "Epoch: 133, Steps: 1 | Train Loss: 113.3619919 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10705804824829102\n",
      "Epoch: 134, Steps: 1 | Train Loss: 113.1591110 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.12109041213989258\n",
      "Epoch: 135, Steps: 1 | Train Loss: 112.8298111 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.12061405181884766\n",
      "Epoch: 136, Steps: 1 | Train Loss: 112.9862595 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.11940884590148926\n",
      "Epoch: 137, Steps: 1 | Train Loss: 112.9658813 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11267495155334473\n",
      "Epoch: 138, Steps: 1 | Train Loss: 113.2291183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11032748222351074\n",
      "Epoch: 139, Steps: 1 | Train Loss: 113.2874527 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10885930061340332\n",
      "Epoch: 140, Steps: 1 | Train Loss: 112.7886353 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.10850000381469727\n",
      "Epoch: 141, Steps: 1 | Train Loss: 112.8861618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.10796618461608887\n",
      "Epoch: 142, Steps: 1 | Train Loss: 112.5681305 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.10744810104370117\n",
      "Epoch: 143, Steps: 1 | Train Loss: 113.0123291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.10710859298706055\n",
      "Epoch: 144, Steps: 1 | Train Loss: 112.7423019 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10736823081970215\n",
      "Epoch: 145, Steps: 1 | Train Loss: 112.7865219 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10689258575439453\n",
      "Epoch: 146, Steps: 1 | Train Loss: 112.6097412 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10705208778381348\n",
      "Epoch: 147, Steps: 1 | Train Loss: 112.9119186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10715675354003906\n",
      "Epoch: 148, Steps: 1 | Train Loss: 112.9158249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10748958587646484\n",
      "Epoch: 149, Steps: 1 | Train Loss: 113.0062637 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.1209712028503418\n",
      "Epoch: 150, Steps: 1 | Train Loss: 113.3965378 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.12070417404174805\n",
      "Epoch: 151, Steps: 1 | Train Loss: 113.0329742 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.12153840065002441\n",
      "Epoch: 152, Steps: 1 | Train Loss: 112.5459595 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12125325202941895\n",
      "Epoch: 153, Steps: 1 | Train Loss: 112.7289047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.11804437637329102\n",
      "Epoch: 154, Steps: 1 | Train Loss: 113.0020676 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.11677002906799316\n",
      "Epoch: 155, Steps: 1 | Train Loss: 113.3933868 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11261510848999023\n",
      "Epoch: 156, Steps: 1 | Train Loss: 112.9098129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.1114645004272461\n",
      "Epoch: 157, Steps: 1 | Train Loss: 113.4120255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.11096787452697754\n",
      "Epoch: 158, Steps: 1 | Train Loss: 113.2406387 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.11011648178100586\n",
      "Epoch: 159, Steps: 1 | Train Loss: 112.8395538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.10935163497924805\n",
      "Epoch: 160, Steps: 1 | Train Loss: 112.6194839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10919404029846191\n",
      "Epoch: 161, Steps: 1 | Train Loss: 113.3644943 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.11035680770874023\n",
      "Epoch: 162, Steps: 1 | Train Loss: 112.9358139 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10991287231445312\n",
      "Epoch: 163, Steps: 1 | Train Loss: 113.0805054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10893487930297852\n",
      "Epoch: 164, Steps: 1 | Train Loss: 113.4634323 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.11008882522583008\n",
      "Epoch: 165, Steps: 1 | Train Loss: 112.8114166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.1114051342010498\n",
      "Epoch: 166, Steps: 1 | Train Loss: 113.2678375 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11073660850524902\n",
      "Epoch: 167, Steps: 1 | Train Loss: 112.8224487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.1130516529083252\n",
      "Epoch: 168, Steps: 1 | Train Loss: 112.7678986 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11084794998168945\n",
      "Epoch: 169, Steps: 1 | Train Loss: 113.1613083 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11083769798278809\n",
      "Epoch: 170, Steps: 1 | Train Loss: 113.1813583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11028671264648438\n",
      "Epoch: 171, Steps: 1 | Train Loss: 112.7086182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11032295227050781\n",
      "Epoch: 172, Steps: 1 | Train Loss: 113.2489243 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10997986793518066\n",
      "Epoch: 173, Steps: 1 | Train Loss: 113.1714630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11052894592285156\n",
      "Epoch: 174, Steps: 1 | Train Loss: 112.5566025 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10979413986206055\n",
      "Epoch: 175, Steps: 1 | Train Loss: 113.2038574 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.1241610050201416\n",
      "Epoch: 176, Steps: 1 | Train Loss: 113.0631027 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.11553406715393066\n",
      "Epoch: 177, Steps: 1 | Train Loss: 112.7107086 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11283731460571289\n",
      "Epoch: 178, Steps: 1 | Train Loss: 113.5444717 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11055874824523926\n",
      "Epoch: 179, Steps: 1 | Train Loss: 112.6647110 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11103200912475586\n",
      "Epoch: 180, Steps: 1 | Train Loss: 113.0905914 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11005330085754395\n",
      "Epoch: 181, Steps: 1 | Train Loss: 112.7968979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11049079895019531\n",
      "Epoch: 182, Steps: 1 | Train Loss: 112.8544235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11023902893066406\n",
      "Epoch: 183, Steps: 1 | Train Loss: 112.6505890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11033248901367188\n",
      "Epoch: 184, Steps: 1 | Train Loss: 113.5127106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11023449897766113\n",
      "Epoch: 185, Steps: 1 | Train Loss: 113.0263824 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.1105351448059082\n",
      "Epoch: 186, Steps: 1 | Train Loss: 113.3376236 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.12408804893493652\n",
      "Epoch: 187, Steps: 1 | Train Loss: 113.3318710 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.12406730651855469\n",
      "Epoch: 188, Steps: 1 | Train Loss: 112.8021774 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.11321520805358887\n",
      "Epoch: 189, Steps: 1 | Train Loss: 112.8008194 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.1116483211517334\n",
      "Epoch: 190, Steps: 1 | Train Loss: 112.9255600 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.11152863502502441\n",
      "Epoch: 191, Steps: 1 | Train Loss: 112.7152863 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.11124300956726074\n",
      "Epoch: 192, Steps: 1 | Train Loss: 112.9813004 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.1104726791381836\n",
      "Epoch: 193, Steps: 1 | Train Loss: 113.2136993 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11048483848571777\n",
      "Epoch: 194, Steps: 1 | Train Loss: 112.9357452 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.1104283332824707\n",
      "Epoch: 195, Steps: 1 | Train Loss: 113.0967407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.1098787784576416\n",
      "Epoch: 196, Steps: 1 | Train Loss: 113.1122360 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.1106710433959961\n",
      "Epoch: 197, Steps: 1 | Train Loss: 112.6874771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.12422013282775879\n",
      "Epoch: 198, Steps: 1 | Train Loss: 113.2520981 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.1233680248260498\n",
      "Epoch: 199, Steps: 1 | Train Loss: 113.1009064 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11465287208557129\n",
      "Epoch: 200, Steps: 1 | Train Loss: 113.2441559 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11275553703308105\n",
      "Epoch: 201, Steps: 1 | Train Loss: 113.0506744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.11248660087585449\n",
      "Epoch: 202, Steps: 1 | Train Loss: 113.1374283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11176180839538574\n",
      "Epoch: 203, Steps: 1 | Train Loss: 112.5557861 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.11054849624633789\n",
      "Epoch: 204, Steps: 1 | Train Loss: 112.9589386 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.11055827140808105\n",
      "Epoch: 205, Steps: 1 | Train Loss: 113.2882080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.11073517799377441\n",
      "Epoch: 206, Steps: 1 | Train Loss: 113.4573517 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11036849021911621\n",
      "Epoch: 207, Steps: 1 | Train Loss: 113.2186813 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.11017489433288574\n",
      "Epoch: 208, Steps: 1 | Train Loss: 112.6375961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10999107360839844\n",
      "Epoch: 209, Steps: 1 | Train Loss: 113.3369141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11001396179199219\n",
      "Epoch: 210, Steps: 1 | Train Loss: 113.3027878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.12490987777709961\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.8187256 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11800003051757812\n",
      "Epoch: 212, Steps: 1 | Train Loss: 113.4691696 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11325407028198242\n",
      "Epoch: 213, Steps: 1 | Train Loss: 113.0602036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11234545707702637\n",
      "Epoch: 214, Steps: 1 | Train Loss: 113.2663345 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11141443252563477\n",
      "Epoch: 215, Steps: 1 | Train Loss: 112.6942520 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11056303977966309\n",
      "Epoch: 216, Steps: 1 | Train Loss: 112.7978439 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11091423034667969\n",
      "Epoch: 217, Steps: 1 | Train Loss: 113.4679947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.11039423942565918\n",
      "Epoch: 218, Steps: 1 | Train Loss: 112.8643112 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.11067032814025879\n",
      "Epoch: 219, Steps: 1 | Train Loss: 112.8167496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11011171340942383\n",
      "Epoch: 220, Steps: 1 | Train Loss: 112.6002426 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.12352776527404785\n",
      "Epoch: 221, Steps: 1 | Train Loss: 112.9255524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.12305140495300293\n",
      "Epoch: 222, Steps: 1 | Train Loss: 113.1777878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11431193351745605\n",
      "Epoch: 223, Steps: 1 | Train Loss: 113.1030655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11197471618652344\n",
      "Epoch: 224, Steps: 1 | Train Loss: 113.4556427 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11157846450805664\n",
      "Epoch: 225, Steps: 1 | Train Loss: 112.8920517 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11022758483886719\n",
      "Epoch: 226, Steps: 1 | Train Loss: 113.0854492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11019039154052734\n",
      "Epoch: 227, Steps: 1 | Train Loss: 112.6883316 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11059260368347168\n",
      "Epoch: 228, Steps: 1 | Train Loss: 113.0940170 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11054301261901855\n",
      "Epoch: 229, Steps: 1 | Train Loss: 112.8010635 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.1098337173461914\n",
      "Epoch: 230, Steps: 1 | Train Loss: 113.5415192 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.11095809936523438\n",
      "Epoch: 231, Steps: 1 | Train Loss: 112.9253464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10704421997070312\n",
      "Epoch: 232, Steps: 1 | Train Loss: 113.3181686 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.12060427665710449\n",
      "Epoch: 233, Steps: 1 | Train Loss: 113.0392609 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11364030838012695\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.1769562 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.10978102684020996\n",
      "Epoch: 235, Steps: 1 | Train Loss: 113.1137238 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.1086890697479248\n",
      "Epoch: 236, Steps: 1 | Train Loss: 113.0776138 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10784673690795898\n",
      "Epoch: 237, Steps: 1 | Train Loss: 112.6187286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10797762870788574\n",
      "Epoch: 238, Steps: 1 | Train Loss: 112.8306427 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10709190368652344\n",
      "Epoch: 239, Steps: 1 | Train Loss: 113.6970978 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10752606391906738\n",
      "Epoch: 240, Steps: 1 | Train Loss: 113.0266342 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.10718321800231934\n",
      "Epoch: 241, Steps: 1 | Train Loss: 113.0250778 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.10719943046569824\n",
      "Epoch: 242, Steps: 1 | Train Loss: 112.8002243 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10710000991821289\n",
      "Epoch: 243, Steps: 1 | Train Loss: 113.3151169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10710740089416504\n",
      "Epoch: 244, Steps: 1 | Train Loss: 112.5806198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.12148022651672363\n",
      "Epoch: 245, Steps: 1 | Train Loss: 113.0812759 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11749124526977539\n",
      "Epoch: 246, Steps: 1 | Train Loss: 113.2784958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11004161834716797\n",
      "Epoch: 247, Steps: 1 | Train Loss: 112.6831589 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10858511924743652\n",
      "Epoch: 248, Steps: 1 | Train Loss: 113.1384048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10781598091125488\n",
      "Epoch: 249, Steps: 1 | Train Loss: 112.7635498 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10759353637695312\n",
      "Epoch: 250, Steps: 1 | Train Loss: 112.5162582 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_14>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.10927128791809082\n",
      "Epoch: 1, Steps: 1 | Train Loss: 120.2005157 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.1086430549621582\n",
      "Epoch: 2, Steps: 1 | Train Loss: 118.1209946 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10707569122314453\n",
      "Epoch: 3, Steps: 1 | Train Loss: 116.0349884 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10720324516296387\n",
      "Epoch: 4, Steps: 1 | Train Loss: 115.1787338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10754919052124023\n",
      "Epoch: 5, Steps: 1 | Train Loss: 114.9607544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10736703872680664\n",
      "Epoch: 6, Steps: 1 | Train Loss: 114.7800903 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10719037055969238\n",
      "Epoch: 7, Steps: 1 | Train Loss: 114.9849472 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10729098320007324\n",
      "Epoch: 8, Steps: 1 | Train Loss: 114.9437408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10760188102722168\n",
      "Epoch: 9, Steps: 1 | Train Loss: 114.5655136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10785818099975586\n",
      "Epoch: 10, Steps: 1 | Train Loss: 114.7970505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.12068462371826172\n",
      "Epoch: 11, Steps: 1 | Train Loss: 114.3917999 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.12108612060546875\n",
      "Epoch: 12, Steps: 1 | Train Loss: 114.9790192 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11155462265014648\n",
      "Epoch: 13, Steps: 1 | Train Loss: 114.1162872 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10919380187988281\n",
      "Epoch: 14, Steps: 1 | Train Loss: 114.5857468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10817646980285645\n",
      "Epoch: 15, Steps: 1 | Train Loss: 114.5042496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10800933837890625\n",
      "Epoch: 16, Steps: 1 | Train Loss: 114.2462921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.10747194290161133\n",
      "Epoch: 17, Steps: 1 | Train Loss: 114.4363327 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10733723640441895\n",
      "Epoch: 18, Steps: 1 | Train Loss: 115.1009064 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10732579231262207\n",
      "Epoch: 19, Steps: 1 | Train Loss: 114.4227524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.10733628273010254\n",
      "Epoch: 20, Steps: 1 | Train Loss: 114.3005142 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.10755515098571777\n",
      "Epoch: 21, Steps: 1 | Train Loss: 114.6744614 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10761070251464844\n",
      "Epoch: 22, Steps: 1 | Train Loss: 114.8168106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.1071770191192627\n",
      "Epoch: 23, Steps: 1 | Train Loss: 115.1052475 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.10764527320861816\n",
      "Epoch: 24, Steps: 1 | Train Loss: 114.3229599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.10754728317260742\n",
      "Epoch: 25, Steps: 1 | Train Loss: 114.5581055 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12093162536621094\n",
      "Epoch: 26, Steps: 1 | Train Loss: 114.2898331 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.1209712028503418\n",
      "Epoch: 27, Steps: 1 | Train Loss: 114.7230988 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.12079119682312012\n",
      "Epoch: 28, Steps: 1 | Train Loss: 114.2945099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.11584663391113281\n",
      "Epoch: 29, Steps: 1 | Train Loss: 114.5416870 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.10982966423034668\n",
      "Epoch: 30, Steps: 1 | Train Loss: 114.3149338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.10862970352172852\n",
      "Epoch: 31, Steps: 1 | Train Loss: 114.7289810 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10807013511657715\n",
      "Epoch: 32, Steps: 1 | Train Loss: 114.6039581 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.1077578067779541\n",
      "Epoch: 33, Steps: 1 | Train Loss: 114.6051178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10756206512451172\n",
      "Epoch: 34, Steps: 1 | Train Loss: 114.5413589 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.1072993278503418\n",
      "Epoch: 35, Steps: 1 | Train Loss: 114.5655441 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10740494728088379\n",
      "Epoch: 36, Steps: 1 | Train Loss: 114.6990356 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10740375518798828\n",
      "Epoch: 37, Steps: 1 | Train Loss: 114.7607651 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10712432861328125\n",
      "Epoch: 38, Steps: 1 | Train Loss: 114.4384995 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10742926597595215\n",
      "Epoch: 39, Steps: 1 | Train Loss: 114.4850006 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10747289657592773\n",
      "Epoch: 40, Steps: 1 | Train Loss: 114.2737045 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10722589492797852\n",
      "Epoch: 41, Steps: 1 | Train Loss: 114.5618210 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10724902153015137\n",
      "Epoch: 42, Steps: 1 | Train Loss: 114.3281784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.12097978591918945\n",
      "Epoch: 43, Steps: 1 | Train Loss: 114.6214218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.12051224708557129\n",
      "Epoch: 44, Steps: 1 | Train Loss: 114.2272339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.12018036842346191\n",
      "Epoch: 45, Steps: 1 | Train Loss: 114.1146011 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11924219131469727\n",
      "Epoch: 46, Steps: 1 | Train Loss: 114.8298569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11132073402404785\n",
      "Epoch: 47, Steps: 1 | Train Loss: 114.3994141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.10893559455871582\n",
      "Epoch: 48, Steps: 1 | Train Loss: 114.4390411 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.10821008682250977\n",
      "Epoch: 49, Steps: 1 | Train Loss: 114.3775558 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.10774779319763184\n",
      "Epoch: 50, Steps: 1 | Train Loss: 114.4177780 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.1073300838470459\n",
      "Epoch: 51, Steps: 1 | Train Loss: 114.1820831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.1077570915222168\n",
      "Epoch: 52, Steps: 1 | Train Loss: 114.6941757 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10714554786682129\n",
      "Epoch: 53, Steps: 1 | Train Loss: 114.6081696 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10850644111633301\n",
      "Epoch: 54, Steps: 1 | Train Loss: 114.4458237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.1081695556640625\n",
      "Epoch: 55, Steps: 1 | Train Loss: 114.5647202 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.10770177841186523\n",
      "Epoch: 56, Steps: 1 | Train Loss: 114.4442368 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.12735462188720703\n",
      "Epoch: 57, Steps: 1 | Train Loss: 114.6448364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.12403750419616699\n",
      "Epoch: 58, Steps: 1 | Train Loss: 114.5608444 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.10868382453918457\n",
      "Epoch: 59, Steps: 1 | Train Loss: 114.5663605 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.12279248237609863\n",
      "Epoch: 60, Steps: 1 | Train Loss: 114.3607407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.1262826919555664\n",
      "Epoch: 61, Steps: 1 | Train Loss: 114.7036362 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11052370071411133\n",
      "Epoch: 62, Steps: 1 | Train Loss: 114.6204147 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.1087188720703125\n",
      "Epoch: 63, Steps: 1 | Train Loss: 114.6899185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.1075587272644043\n",
      "Epoch: 64, Steps: 1 | Train Loss: 114.8039780 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.10717153549194336\n",
      "Epoch: 65, Steps: 1 | Train Loss: 114.3921051 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.10731053352355957\n",
      "Epoch: 66, Steps: 1 | Train Loss: 114.7833862 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.10683751106262207\n",
      "Epoch: 67, Steps: 1 | Train Loss: 114.5445099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10715866088867188\n",
      "Epoch: 68, Steps: 1 | Train Loss: 114.5771027 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10705304145812988\n",
      "Epoch: 69, Steps: 1 | Train Loss: 114.2470016 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.10746431350708008\n",
      "Epoch: 70, Steps: 1 | Train Loss: 114.7816086 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10682392120361328\n",
      "Epoch: 71, Steps: 1 | Train Loss: 114.4241257 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.12069845199584961\n",
      "Epoch: 72, Steps: 1 | Train Loss: 114.3645630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.1205451488494873\n",
      "Epoch: 73, Steps: 1 | Train Loss: 114.8399200 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12053418159484863\n",
      "Epoch: 74, Steps: 1 | Train Loss: 114.2823029 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11729693412780762\n",
      "Epoch: 75, Steps: 1 | Train Loss: 114.3521729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.1099863052368164\n",
      "Epoch: 76, Steps: 1 | Train Loss: 114.4017105 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11334586143493652\n",
      "Epoch: 77, Steps: 1 | Train Loss: 115.1241226 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10884928703308105\n",
      "Epoch: 78, Steps: 1 | Train Loss: 114.6619644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10760354995727539\n",
      "Epoch: 79, Steps: 1 | Train Loss: 114.7766495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10727620124816895\n",
      "Epoch: 80, Steps: 1 | Train Loss: 114.3319702 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.11692047119140625\n",
      "Epoch: 81, Steps: 1 | Train Loss: 115.0932617 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.13434147834777832\n",
      "Epoch: 82, Steps: 1 | Train Loss: 114.9730759 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.13744831085205078\n",
      "Epoch: 83, Steps: 1 | Train Loss: 114.4977798 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11593937873840332\n",
      "Epoch: 84, Steps: 1 | Train Loss: 114.6837921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.1103975772857666\n",
      "Epoch: 85, Steps: 1 | Train Loss: 114.8815536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11485648155212402\n",
      "Epoch: 86, Steps: 1 | Train Loss: 115.0026398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10548162460327148\n",
      "Epoch: 87, Steps: 1 | Train Loss: 114.5292740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.10723423957824707\n",
      "Epoch: 88, Steps: 1 | Train Loss: 114.8220367 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10690569877624512\n",
      "Epoch: 89, Steps: 1 | Train Loss: 113.8053360 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10744500160217285\n",
      "Epoch: 90, Steps: 1 | Train Loss: 114.1129684 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.1072239875793457\n",
      "Epoch: 91, Steps: 1 | Train Loss: 114.6642609 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10696268081665039\n",
      "Epoch: 92, Steps: 1 | Train Loss: 114.7488022 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.12086725234985352\n",
      "Epoch: 93, Steps: 1 | Train Loss: 115.0107956 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.11685466766357422\n",
      "Epoch: 94, Steps: 1 | Train Loss: 114.7541733 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.11500763893127441\n",
      "Epoch: 95, Steps: 1 | Train Loss: 114.5315552 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10846590995788574\n",
      "Epoch: 96, Steps: 1 | Train Loss: 114.1187973 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10794520378112793\n",
      "Epoch: 97, Steps: 1 | Train Loss: 114.6128540 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10711312294006348\n",
      "Epoch: 98, Steps: 1 | Train Loss: 114.1673355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10725188255310059\n",
      "Epoch: 99, Steps: 1 | Train Loss: 114.4152374 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10721850395202637\n",
      "Epoch: 100, Steps: 1 | Train Loss: 114.4294586 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10722541809082031\n",
      "Epoch: 101, Steps: 1 | Train Loss: 114.4891129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.1072380542755127\n",
      "Epoch: 102, Steps: 1 | Train Loss: 114.3118286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10682439804077148\n",
      "Epoch: 103, Steps: 1 | Train Loss: 114.1861191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.1113126277923584\n",
      "Epoch: 104, Steps: 1 | Train Loss: 114.5961685 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.12378096580505371\n",
      "Epoch: 105, Steps: 1 | Train Loss: 114.7155533 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.11998677253723145\n",
      "Epoch: 106, Steps: 1 | Train Loss: 113.8044815 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.11312508583068848\n",
      "Epoch: 107, Steps: 1 | Train Loss: 114.1210480 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.11169743537902832\n",
      "Epoch: 108, Steps: 1 | Train Loss: 114.3230743 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11044120788574219\n",
      "Epoch: 109, Steps: 1 | Train Loss: 114.5883331 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.11045718193054199\n",
      "Epoch: 110, Steps: 1 | Train Loss: 114.7645416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11027169227600098\n",
      "Epoch: 111, Steps: 1 | Train Loss: 115.0624008 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.1097407341003418\n",
      "Epoch: 112, Steps: 1 | Train Loss: 114.6067429 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11032247543334961\n",
      "Epoch: 113, Steps: 1 | Train Loss: 114.1955795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.11038780212402344\n",
      "Epoch: 114, Steps: 1 | Train Loss: 114.2689438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.1095423698425293\n",
      "Epoch: 115, Steps: 1 | Train Loss: 114.6450577 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11012458801269531\n",
      "Epoch: 116, Steps: 1 | Train Loss: 114.8494263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.12479782104492188\n",
      "Epoch: 117, Steps: 1 | Train Loss: 114.3406372 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.12393307685852051\n",
      "Epoch: 118, Steps: 1 | Train Loss: 114.5157471 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.11443877220153809\n",
      "Epoch: 119, Steps: 1 | Train Loss: 114.6458893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.11196160316467285\n",
      "Epoch: 120, Steps: 1 | Train Loss: 114.2566299 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11096787452697754\n",
      "Epoch: 121, Steps: 1 | Train Loss: 114.1418228 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11050033569335938\n",
      "Epoch: 122, Steps: 1 | Train Loss: 114.3093414 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.1349797248840332\n",
      "Epoch: 123, Steps: 1 | Train Loss: 114.7616501 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.12169194221496582\n",
      "Epoch: 124, Steps: 1 | Train Loss: 114.4987717 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.12350678443908691\n",
      "Epoch: 125, Steps: 1 | Train Loss: 114.6349869 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.12306475639343262\n",
      "Epoch: 126, Steps: 1 | Train Loss: 115.1434860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.12480664253234863\n",
      "Epoch: 127, Steps: 1 | Train Loss: 114.4583740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12267589569091797\n",
      "Epoch: 128, Steps: 1 | Train Loss: 114.8998795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.12341904640197754\n",
      "Epoch: 129, Steps: 1 | Train Loss: 114.8781357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.12688517570495605\n",
      "Epoch: 130, Steps: 1 | Train Loss: 114.6556015 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.1262497901916504\n",
      "Epoch: 131, Steps: 1 | Train Loss: 114.5673828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.12769389152526855\n",
      "Epoch: 132, Steps: 1 | Train Loss: 114.9193344 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.12591028213500977\n",
      "Epoch: 133, Steps: 1 | Train Loss: 114.6419907 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.12736821174621582\n",
      "Epoch: 134, Steps: 1 | Train Loss: 114.2523804 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.1305077075958252\n",
      "Epoch: 135, Steps: 1 | Train Loss: 114.6012726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.12490272521972656\n",
      "Epoch: 136, Steps: 1 | Train Loss: 114.2958374 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.12613248825073242\n",
      "Epoch: 137, Steps: 1 | Train Loss: 114.6142807 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.12485241889953613\n",
      "Epoch: 138, Steps: 1 | Train Loss: 114.8127060 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.1255943775177002\n",
      "Epoch: 139, Steps: 1 | Train Loss: 114.5438843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.12395644187927246\n",
      "Epoch: 140, Steps: 1 | Train Loss: 114.9361801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.125838041305542\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.3574982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11381077766418457\n",
      "Epoch: 142, Steps: 1 | Train Loss: 114.4332657 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.13885188102722168\n",
      "Epoch: 143, Steps: 1 | Train Loss: 115.1819992 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.15277624130249023\n",
      "Epoch: 144, Steps: 1 | Train Loss: 114.7213364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.14557743072509766\n",
      "Epoch: 145, Steps: 1 | Train Loss: 114.3385391 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.1255931854248047\n",
      "Epoch: 146, Steps: 1 | Train Loss: 114.6764755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.1242666244506836\n",
      "Epoch: 147, Steps: 1 | Train Loss: 114.3602829 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.12469291687011719\n",
      "Epoch: 148, Steps: 1 | Train Loss: 114.9760056 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.12228775024414062\n",
      "Epoch: 149, Steps: 1 | Train Loss: 114.6080933 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.12424325942993164\n",
      "Epoch: 150, Steps: 1 | Train Loss: 114.7440186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.12400698661804199\n",
      "Epoch: 151, Steps: 1 | Train Loss: 114.3250732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.12228250503540039\n",
      "Epoch: 152, Steps: 1 | Train Loss: 114.5378418 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.11652064323425293\n",
      "Epoch: 153, Steps: 1 | Train Loss: 114.5894547 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.1311197280883789\n",
      "Epoch: 154, Steps: 1 | Train Loss: 114.2687531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.13037967681884766\n",
      "Epoch: 155, Steps: 1 | Train Loss: 114.7743073 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11387205123901367\n",
      "Epoch: 156, Steps: 1 | Train Loss: 114.3887253 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.11353898048400879\n",
      "Epoch: 157, Steps: 1 | Train Loss: 114.6261520 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.1136331558227539\n",
      "Epoch: 158, Steps: 1 | Train Loss: 114.5278549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.1040494441986084\n",
      "Epoch: 159, Steps: 1 | Train Loss: 114.6258926 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11500310897827148\n",
      "Epoch: 160, Steps: 1 | Train Loss: 113.9611740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.1074211597442627\n",
      "Epoch: 161, Steps: 1 | Train Loss: 114.9808731 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.11180448532104492\n",
      "Epoch: 162, Steps: 1 | Train Loss: 114.7790451 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.1071474552154541\n",
      "Epoch: 163, Steps: 1 | Train Loss: 115.1703110 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10708737373352051\n",
      "Epoch: 164, Steps: 1 | Train Loss: 114.4877701 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10750436782836914\n",
      "Epoch: 165, Steps: 1 | Train Loss: 114.5229874 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.1070713996887207\n",
      "Epoch: 166, Steps: 1 | Train Loss: 114.3284683 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10719680786132812\n",
      "Epoch: 167, Steps: 1 | Train Loss: 114.6747055 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.12074518203735352\n",
      "Epoch: 168, Steps: 1 | Train Loss: 114.2654037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.1207735538482666\n",
      "Epoch: 169, Steps: 1 | Train Loss: 114.0131149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11327695846557617\n",
      "Epoch: 170, Steps: 1 | Train Loss: 115.2603302 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.10945582389831543\n",
      "Epoch: 171, Steps: 1 | Train Loss: 114.2081909 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.10830879211425781\n",
      "Epoch: 172, Steps: 1 | Train Loss: 114.2988815 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10692620277404785\n",
      "Epoch: 173, Steps: 1 | Train Loss: 114.7537003 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10727858543395996\n",
      "Epoch: 174, Steps: 1 | Train Loss: 114.7346191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10829877853393555\n",
      "Epoch: 175, Steps: 1 | Train Loss: 114.6658936 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.10736799240112305\n",
      "Epoch: 176, Steps: 1 | Train Loss: 114.2803955 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.11880826950073242\n",
      "Epoch: 177, Steps: 1 | Train Loss: 114.3279037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.1219325065612793\n",
      "Epoch: 178, Steps: 1 | Train Loss: 114.2017593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11106491088867188\n",
      "Epoch: 179, Steps: 1 | Train Loss: 114.5045700 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10909247398376465\n",
      "Epoch: 180, Steps: 1 | Train Loss: 114.2118912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.10832595825195312\n",
      "Epoch: 181, Steps: 1 | Train Loss: 114.6978302 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10745978355407715\n",
      "Epoch: 182, Steps: 1 | Train Loss: 114.7251968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10705709457397461\n",
      "Epoch: 183, Steps: 1 | Train Loss: 114.2983170 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.10731744766235352\n",
      "Epoch: 184, Steps: 1 | Train Loss: 114.3833008 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.10713911056518555\n",
      "Epoch: 185, Steps: 1 | Train Loss: 114.5421906 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.10773038864135742\n",
      "Epoch: 186, Steps: 1 | Train Loss: 114.3203964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.1145317554473877\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.3103027 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11059927940368652\n",
      "Epoch: 188, Steps: 1 | Train Loss: 114.7940445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.12417769432067871\n",
      "Epoch: 189, Steps: 1 | Train Loss: 115.1677399 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12466239929199219\n",
      "Epoch: 190, Steps: 1 | Train Loss: 114.6405258 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.12365102767944336\n",
      "Epoch: 191, Steps: 1 | Train Loss: 114.8470459 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.1217660903930664\n",
      "Epoch: 192, Steps: 1 | Train Loss: 115.1625519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.12081408500671387\n",
      "Epoch: 193, Steps: 1 | Train Loss: 113.9678497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11340785026550293\n",
      "Epoch: 194, Steps: 1 | Train Loss: 114.9845352 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11188650131225586\n",
      "Epoch: 195, Steps: 1 | Train Loss: 114.1327515 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.1113884449005127\n",
      "Epoch: 196, Steps: 1 | Train Loss: 114.3447266 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11101269721984863\n",
      "Epoch: 197, Steps: 1 | Train Loss: 114.4859390 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11086726188659668\n",
      "Epoch: 198, Steps: 1 | Train Loss: 114.1055450 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10982632637023926\n",
      "Epoch: 199, Steps: 1 | Train Loss: 114.8491592 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11005830764770508\n",
      "Epoch: 200, Steps: 1 | Train Loss: 114.6943130 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11016583442687988\n",
      "Epoch: 201, Steps: 1 | Train Loss: 114.4464035 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10963845252990723\n",
      "Epoch: 202, Steps: 1 | Train Loss: 114.8342819 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11051511764526367\n",
      "Epoch: 203, Steps: 1 | Train Loss: 114.6762085 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.1101081371307373\n",
      "Epoch: 204, Steps: 1 | Train Loss: 114.5782013 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.110748291015625\n",
      "Epoch: 205, Steps: 1 | Train Loss: 114.3469009 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.11114025115966797\n",
      "Epoch: 206, Steps: 1 | Train Loss: 115.4469223 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.1092066764831543\n",
      "Epoch: 207, Steps: 1 | Train Loss: 114.4393539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.12410712242126465\n",
      "Epoch: 208, Steps: 1 | Train Loss: 114.2839584 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.1233823299407959\n",
      "Epoch: 209, Steps: 1 | Train Loss: 114.3321304 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.12414669990539551\n",
      "Epoch: 210, Steps: 1 | Train Loss: 114.2558594 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.12412905693054199\n",
      "Epoch: 211, Steps: 1 | Train Loss: 114.1908569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.12290573120117188\n",
      "Epoch: 212, Steps: 1 | Train Loss: 114.4355850 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.1235494613647461\n",
      "Epoch: 213, Steps: 1 | Train Loss: 114.4290771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11210036277770996\n",
      "Epoch: 214, Steps: 1 | Train Loss: 114.8546371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10819721221923828\n",
      "Epoch: 215, Steps: 1 | Train Loss: 114.3717270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.1082613468170166\n",
      "Epoch: 216, Steps: 1 | Train Loss: 114.0708160 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10778427124023438\n",
      "Epoch: 217, Steps: 1 | Train Loss: 114.5146255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10736370086669922\n",
      "Epoch: 218, Steps: 1 | Train Loss: 114.0574265 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10736227035522461\n",
      "Epoch: 219, Steps: 1 | Train Loss: 114.2391968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10709476470947266\n",
      "Epoch: 220, Steps: 1 | Train Loss: 114.0674438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.10719966888427734\n",
      "Epoch: 221, Steps: 1 | Train Loss: 114.5118637 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10699582099914551\n",
      "Epoch: 222, Steps: 1 | Train Loss: 114.5467072 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10718035697937012\n",
      "Epoch: 223, Steps: 1 | Train Loss: 114.2962036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.10740303993225098\n",
      "Epoch: 224, Steps: 1 | Train Loss: 114.6682663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.10749697685241699\n",
      "Epoch: 225, Steps: 1 | Train Loss: 114.7481003 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.10740423202514648\n",
      "Epoch: 226, Steps: 1 | Train Loss: 114.3932266 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10754537582397461\n",
      "Epoch: 227, Steps: 1 | Train Loss: 114.7171402 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.10724234580993652\n",
      "Epoch: 228, Steps: 1 | Train Loss: 114.4309311 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.1207120418548584\n",
      "Epoch: 229, Steps: 1 | Train Loss: 114.5814438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.13300395011901855\n",
      "Epoch: 230, Steps: 1 | Train Loss: 115.2272720 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.13064169883728027\n",
      "Epoch: 231, Steps: 1 | Train Loss: 114.2981186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.12308907508850098\n",
      "Epoch: 232, Steps: 1 | Train Loss: 114.4692383 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.1225895881652832\n",
      "Epoch: 233, Steps: 1 | Train Loss: 114.6206207 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.12544655799865723\n",
      "Epoch: 234, Steps: 1 | Train Loss: 114.6783218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.12392950057983398\n",
      "Epoch: 235, Steps: 1 | Train Loss: 114.3349609 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.1362442970275879\n",
      "Epoch: 236, Steps: 1 | Train Loss: 114.4619141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.13646364212036133\n",
      "Epoch: 237, Steps: 1 | Train Loss: 114.5572891 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.13669586181640625\n",
      "Epoch: 238, Steps: 1 | Train Loss: 114.3541870 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.13583803176879883\n",
      "Epoch: 239, Steps: 1 | Train Loss: 114.4603043 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.1375870704650879\n",
      "Epoch: 240, Steps: 1 | Train Loss: 114.7315445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1353754997253418\n",
      "Epoch: 241, Steps: 1 | Train Loss: 114.5823898 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.13473010063171387\n",
      "Epoch: 242, Steps: 1 | Train Loss: 114.5291061 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.13102293014526367\n",
      "Epoch: 243, Steps: 1 | Train Loss: 114.3631592 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.12352848052978516\n",
      "Epoch: 244, Steps: 1 | Train Loss: 114.5875778 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.12551069259643555\n",
      "Epoch: 245, Steps: 1 | Train Loss: 114.4824753 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.12422060966491699\n",
      "Epoch: 246, Steps: 1 | Train Loss: 114.5710449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11840295791625977\n",
      "Epoch: 247, Steps: 1 | Train Loss: 114.4305344 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.11754989624023438\n",
      "Epoch: 248, Steps: 1 | Train Loss: 113.9397583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11936092376708984\n",
      "Epoch: 249, Steps: 1 | Train Loss: 114.3826218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.1047663688659668\n",
      "Epoch: 250, Steps: 1 | Train Loss: 114.2331161 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_15>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.1130516529083252\n",
      "Epoch: 1, Steps: 1 | Train Loss: 117.9552612 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10707974433898926\n",
      "Epoch: 2, Steps: 1 | Train Loss: 115.8633804 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10686182975769043\n",
      "Epoch: 3, Steps: 1 | Train Loss: 113.9982910 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10680913925170898\n",
      "Epoch: 4, Steps: 1 | Train Loss: 113.1407242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10738968849182129\n",
      "Epoch: 5, Steps: 1 | Train Loss: 113.1745377 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10716390609741211\n",
      "Epoch: 6, Steps: 1 | Train Loss: 112.5731964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10698771476745605\n",
      "Epoch: 7, Steps: 1 | Train Loss: 112.2411270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10653066635131836\n",
      "Epoch: 8, Steps: 1 | Train Loss: 112.3687286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10716676712036133\n",
      "Epoch: 9, Steps: 1 | Train Loss: 112.1197052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10670042037963867\n",
      "Epoch: 10, Steps: 1 | Train Loss: 112.4893341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10695052146911621\n",
      "Epoch: 11, Steps: 1 | Train Loss: 112.1320724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10716104507446289\n",
      "Epoch: 12, Steps: 1 | Train Loss: 112.1358185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.1070868968963623\n",
      "Epoch: 13, Steps: 1 | Train Loss: 111.9574203 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10705232620239258\n",
      "Epoch: 14, Steps: 1 | Train Loss: 112.2417984 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.12040948867797852\n",
      "Epoch: 15, Steps: 1 | Train Loss: 112.5495377 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12091588973999023\n",
      "Epoch: 16, Steps: 1 | Train Loss: 112.2191162 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.12034893035888672\n",
      "Epoch: 17, Steps: 1 | Train Loss: 112.0713501 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.12219738960266113\n",
      "Epoch: 18, Steps: 1 | Train Loss: 112.3073654 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.12141275405883789\n",
      "Epoch: 19, Steps: 1 | Train Loss: 112.7026367 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.12138843536376953\n",
      "Epoch: 20, Steps: 1 | Train Loss: 112.6133575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.14338421821594238\n",
      "Epoch: 21, Steps: 1 | Train Loss: 112.5811920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.15214872360229492\n",
      "Epoch: 22, Steps: 1 | Train Loss: 112.8830566 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11042475700378418\n",
      "Epoch: 23, Steps: 1 | Train Loss: 112.4456177 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.1295154094696045\n",
      "Epoch: 24, Steps: 1 | Train Loss: 112.0337143 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.12315249443054199\n",
      "Epoch: 25, Steps: 1 | Train Loss: 112.2866135 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12630844116210938\n",
      "Epoch: 26, Steps: 1 | Train Loss: 112.0400391 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.1251215934753418\n",
      "Epoch: 27, Steps: 1 | Train Loss: 112.1492233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.12592720985412598\n",
      "Epoch: 28, Steps: 1 | Train Loss: 111.8888092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.12462425231933594\n",
      "Epoch: 29, Steps: 1 | Train Loss: 112.4208374 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.1293933391571045\n",
      "Epoch: 30, Steps: 1 | Train Loss: 112.8137436 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.1277623176574707\n",
      "Epoch: 31, Steps: 1 | Train Loss: 112.2943878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.12575268745422363\n",
      "Epoch: 32, Steps: 1 | Train Loss: 112.3464355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.11108732223510742\n",
      "Epoch: 33, Steps: 1 | Train Loss: 112.7209702 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.11040234565734863\n",
      "Epoch: 34, Steps: 1 | Train Loss: 112.3722076 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10954928398132324\n",
      "Epoch: 35, Steps: 1 | Train Loss: 112.0378799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10971832275390625\n",
      "Epoch: 36, Steps: 1 | Train Loss: 112.1168365 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10970759391784668\n",
      "Epoch: 37, Steps: 1 | Train Loss: 112.5473251 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11052632331848145\n",
      "Epoch: 38, Steps: 1 | Train Loss: 112.0256729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10999441146850586\n",
      "Epoch: 39, Steps: 1 | Train Loss: 112.6657867 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.11026144027709961\n",
      "Epoch: 40, Steps: 1 | Train Loss: 111.8825455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.11078333854675293\n",
      "Epoch: 41, Steps: 1 | Train Loss: 112.4204636 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.12017202377319336\n",
      "Epoch: 42, Steps: 1 | Train Loss: 112.1137924 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.12394356727600098\n",
      "Epoch: 43, Steps: 1 | Train Loss: 112.5173111 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.12262439727783203\n",
      "Epoch: 44, Steps: 1 | Train Loss: 111.9541779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.12310957908630371\n",
      "Epoch: 45, Steps: 1 | Train Loss: 112.2449341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.12163448333740234\n",
      "Epoch: 46, Steps: 1 | Train Loss: 112.7923813 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11834216117858887\n",
      "Epoch: 47, Steps: 1 | Train Loss: 112.0844498 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.1127462387084961\n",
      "Epoch: 48, Steps: 1 | Train Loss: 112.2958298 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.12130045890808105\n",
      "Epoch: 49, Steps: 1 | Train Loss: 111.8746872 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.1119999885559082\n",
      "Epoch: 50, Steps: 1 | Train Loss: 111.6048813 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.10959887504577637\n",
      "Epoch: 51, Steps: 1 | Train Loss: 112.1528931 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11149358749389648\n",
      "Epoch: 52, Steps: 1 | Train Loss: 111.8188019 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.11040782928466797\n",
      "Epoch: 53, Steps: 1 | Train Loss: 112.3968277 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.1104433536529541\n",
      "Epoch: 54, Steps: 1 | Train Loss: 111.7418137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.11005115509033203\n",
      "Epoch: 55, Steps: 1 | Train Loss: 112.5459518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.11038446426391602\n",
      "Epoch: 56, Steps: 1 | Train Loss: 112.3564224 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.11052060127258301\n",
      "Epoch: 57, Steps: 1 | Train Loss: 112.2305527 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.12285614013671875\n",
      "Epoch: 58, Steps: 1 | Train Loss: 112.5581284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.12275886535644531\n",
      "Epoch: 59, Steps: 1 | Train Loss: 112.1606369 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.12268590927124023\n",
      "Epoch: 60, Steps: 1 | Train Loss: 112.8055344 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.12486791610717773\n",
      "Epoch: 61, Steps: 1 | Train Loss: 112.2909164 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11742734909057617\n",
      "Epoch: 62, Steps: 1 | Train Loss: 111.9403458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11257672309875488\n",
      "Epoch: 63, Steps: 1 | Train Loss: 112.2228546 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.11115837097167969\n",
      "Epoch: 64, Steps: 1 | Train Loss: 112.6651001 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11053347587585449\n",
      "Epoch: 65, Steps: 1 | Train Loss: 113.1199875 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.10979390144348145\n",
      "Epoch: 66, Steps: 1 | Train Loss: 112.2963791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11063885688781738\n",
      "Epoch: 67, Steps: 1 | Train Loss: 111.9591904 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.1104583740234375\n",
      "Epoch: 68, Steps: 1 | Train Loss: 112.5467529 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10921669006347656\n",
      "Epoch: 69, Steps: 1 | Train Loss: 112.5188370 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11039328575134277\n",
      "Epoch: 70, Steps: 1 | Train Loss: 111.7171021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.11013317108154297\n",
      "Epoch: 71, Steps: 1 | Train Loss: 112.1916428 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.12475728988647461\n",
      "Epoch: 72, Steps: 1 | Train Loss: 112.2572784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.12403273582458496\n",
      "Epoch: 73, Steps: 1 | Train Loss: 112.4837418 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12239241600036621\n",
      "Epoch: 74, Steps: 1 | Train Loss: 112.5629883 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11468148231506348\n",
      "Epoch: 75, Steps: 1 | Train Loss: 112.0555954 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11147117614746094\n",
      "Epoch: 76, Steps: 1 | Train Loss: 111.9968262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11025452613830566\n",
      "Epoch: 77, Steps: 1 | Train Loss: 112.9096451 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.11066675186157227\n",
      "Epoch: 78, Steps: 1 | Train Loss: 112.4408493 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10999131202697754\n",
      "Epoch: 79, Steps: 1 | Train Loss: 111.8572388 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10866451263427734\n",
      "Epoch: 80, Steps: 1 | Train Loss: 112.2708054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.11033487319946289\n",
      "Epoch: 81, Steps: 1 | Train Loss: 112.9003448 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11037445068359375\n",
      "Epoch: 82, Steps: 1 | Train Loss: 112.1173019 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.11008048057556152\n",
      "Epoch: 83, Steps: 1 | Train Loss: 112.7998886 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.10924029350280762\n",
      "Epoch: 84, Steps: 1 | Train Loss: 112.8849411 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.12253737449645996\n",
      "Epoch: 85, Steps: 1 | Train Loss: 112.7404404 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11405038833618164\n",
      "Epoch: 86, Steps: 1 | Train Loss: 112.4315948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.11277055740356445\n",
      "Epoch: 87, Steps: 1 | Train Loss: 112.3185043 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11165857315063477\n",
      "Epoch: 88, Steps: 1 | Train Loss: 112.7782516 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11055827140808105\n",
      "Epoch: 89, Steps: 1 | Train Loss: 112.7049484 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11075425148010254\n",
      "Epoch: 90, Steps: 1 | Train Loss: 112.4522705 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10972476005554199\n",
      "Epoch: 91, Steps: 1 | Train Loss: 112.0094604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.11242270469665527\n",
      "Epoch: 92, Steps: 1 | Train Loss: 113.1005173 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.11334085464477539\n",
      "Epoch: 93, Steps: 1 | Train Loss: 112.0058746 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10728144645690918\n",
      "Epoch: 94, Steps: 1 | Train Loss: 112.8807144 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.12092304229736328\n",
      "Epoch: 95, Steps: 1 | Train Loss: 111.9254913 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.12089657783508301\n",
      "Epoch: 96, Steps: 1 | Train Loss: 112.7666779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.11446094512939453\n",
      "Epoch: 97, Steps: 1 | Train Loss: 112.0949097 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10930299758911133\n",
      "Epoch: 98, Steps: 1 | Train Loss: 112.5089645 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.1082615852355957\n",
      "Epoch: 99, Steps: 1 | Train Loss: 112.6195831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10770201683044434\n",
      "Epoch: 100, Steps: 1 | Train Loss: 111.9890366 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.1078345775604248\n",
      "Epoch: 101, Steps: 1 | Train Loss: 112.5655746 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10759162902832031\n",
      "Epoch: 102, Steps: 1 | Train Loss: 112.3356323 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10708117485046387\n",
      "Epoch: 103, Steps: 1 | Train Loss: 112.2089386 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10722661018371582\n",
      "Epoch: 104, Steps: 1 | Train Loss: 112.2669067 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10707974433898926\n",
      "Epoch: 105, Steps: 1 | Train Loss: 112.2211838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10757231712341309\n",
      "Epoch: 106, Steps: 1 | Train Loss: 112.0273972 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10741901397705078\n",
      "Epoch: 107, Steps: 1 | Train Loss: 112.6397705 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.12099719047546387\n",
      "Epoch: 108, Steps: 1 | Train Loss: 113.0724106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11931347846984863\n",
      "Epoch: 109, Steps: 1 | Train Loss: 112.0335007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.11451053619384766\n",
      "Epoch: 110, Steps: 1 | Train Loss: 112.1079636 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11391997337341309\n",
      "Epoch: 111, Steps: 1 | Train Loss: 112.5498810 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10952544212341309\n",
      "Epoch: 112, Steps: 1 | Train Loss: 112.0721436 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.12826108932495117\n",
      "Epoch: 113, Steps: 1 | Train Loss: 112.2524185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10871148109436035\n",
      "Epoch: 114, Steps: 1 | Train Loss: 112.6624527 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10770225524902344\n",
      "Epoch: 115, Steps: 1 | Train Loss: 112.2478256 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10783529281616211\n",
      "Epoch: 116, Steps: 1 | Train Loss: 112.6170425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10843205451965332\n",
      "Epoch: 117, Steps: 1 | Train Loss: 112.4018555 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10805559158325195\n",
      "Epoch: 118, Steps: 1 | Train Loss: 112.2977524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.1083829402923584\n",
      "Epoch: 119, Steps: 1 | Train Loss: 112.3929214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1078486442565918\n",
      "Epoch: 120, Steps: 1 | Train Loss: 113.0443649 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.10793495178222656\n",
      "Epoch: 121, Steps: 1 | Train Loss: 112.1455078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10834360122680664\n",
      "Epoch: 122, Steps: 1 | Train Loss: 111.9661407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.12171149253845215\n",
      "Epoch: 123, Steps: 1 | Train Loss: 112.7472916 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.1237187385559082\n",
      "Epoch: 124, Steps: 1 | Train Loss: 112.3579330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.12240219116210938\n",
      "Epoch: 125, Steps: 1 | Train Loss: 112.4476852 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.11831784248352051\n",
      "Epoch: 126, Steps: 1 | Train Loss: 112.5992584 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11339616775512695\n",
      "Epoch: 127, Steps: 1 | Train Loss: 112.2201157 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.1122901439666748\n",
      "Epoch: 128, Steps: 1 | Train Loss: 112.3868408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.1113729476928711\n",
      "Epoch: 129, Steps: 1 | Train Loss: 112.7644577 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11143732070922852\n",
      "Epoch: 130, Steps: 1 | Train Loss: 112.2669067 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11159205436706543\n",
      "Epoch: 131, Steps: 1 | Train Loss: 112.9257355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11058545112609863\n",
      "Epoch: 132, Steps: 1 | Train Loss: 112.5338364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.1111593246459961\n",
      "Epoch: 133, Steps: 1 | Train Loss: 112.4234772 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10736274719238281\n",
      "Epoch: 134, Steps: 1 | Train Loss: 112.5353775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.11035680770874023\n",
      "Epoch: 135, Steps: 1 | Train Loss: 112.4399567 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.11005973815917969\n",
      "Epoch: 136, Steps: 1 | Train Loss: 111.9098129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.11041712760925293\n",
      "Epoch: 137, Steps: 1 | Train Loss: 112.4334259 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11068320274353027\n",
      "Epoch: 138, Steps: 1 | Train Loss: 112.3340454 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11098217964172363\n",
      "Epoch: 139, Steps: 1 | Train Loss: 112.2700958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10943293571472168\n",
      "Epoch: 140, Steps: 1 | Train Loss: 112.1147232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.12483549118041992\n",
      "Epoch: 141, Steps: 1 | Train Loss: 112.3806686 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.12460947036743164\n",
      "Epoch: 142, Steps: 1 | Train Loss: 112.2742844 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.12389802932739258\n",
      "Epoch: 143, Steps: 1 | Train Loss: 112.4759521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.12119460105895996\n",
      "Epoch: 144, Steps: 1 | Train Loss: 112.0348663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.11832284927368164\n",
      "Epoch: 145, Steps: 1 | Train Loss: 112.3783798 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.11755824089050293\n",
      "Epoch: 146, Steps: 1 | Train Loss: 112.2376709 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.11340904235839844\n",
      "Epoch: 147, Steps: 1 | Train Loss: 112.4919205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11332082748413086\n",
      "Epoch: 148, Steps: 1 | Train Loss: 112.0984116 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10764479637145996\n",
      "Epoch: 149, Steps: 1 | Train Loss: 112.8700180 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10790729522705078\n",
      "Epoch: 150, Steps: 1 | Train Loss: 112.8670578 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.1078941822052002\n",
      "Epoch: 151, Steps: 1 | Train Loss: 112.5569077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.11359786987304688\n",
      "Epoch: 152, Steps: 1 | Train Loss: 112.3937149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10767793655395508\n",
      "Epoch: 153, Steps: 1 | Train Loss: 112.2475739 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10704255104064941\n",
      "Epoch: 154, Steps: 1 | Train Loss: 113.1836700 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10719132423400879\n",
      "Epoch: 155, Steps: 1 | Train Loss: 112.5747910 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10783004760742188\n",
      "Epoch: 156, Steps: 1 | Train Loss: 112.1166763 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10712027549743652\n",
      "Epoch: 157, Steps: 1 | Train Loss: 112.7765121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.1072392463684082\n",
      "Epoch: 158, Steps: 1 | Train Loss: 112.0577621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.12723731994628906\n",
      "Epoch: 159, Steps: 1 | Train Loss: 112.1296616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11534428596496582\n",
      "Epoch: 160, Steps: 1 | Train Loss: 112.7881699 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.13136744499206543\n",
      "Epoch: 161, Steps: 1 | Train Loss: 112.4157944 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.12320804595947266\n",
      "Epoch: 162, Steps: 1 | Train Loss: 112.4736557 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.1226956844329834\n",
      "Epoch: 163, Steps: 1 | Train Loss: 112.3131104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.12420415878295898\n",
      "Epoch: 164, Steps: 1 | Train Loss: 112.1739883 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.14493083953857422\n",
      "Epoch: 165, Steps: 1 | Train Loss: 112.3043365 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.1294558048248291\n",
      "Epoch: 166, Steps: 1 | Train Loss: 112.2150192 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.1303095817565918\n",
      "Epoch: 167, Steps: 1 | Train Loss: 112.2164536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11978721618652344\n",
      "Epoch: 168, Steps: 1 | Train Loss: 113.0053253 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.1098167896270752\n",
      "Epoch: 169, Steps: 1 | Train Loss: 112.0488281 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11689114570617676\n",
      "Epoch: 170, Steps: 1 | Train Loss: 112.0522690 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11193513870239258\n",
      "Epoch: 171, Steps: 1 | Train Loss: 112.0583267 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11241555213928223\n",
      "Epoch: 172, Steps: 1 | Train Loss: 112.7435532 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10916638374328613\n",
      "Epoch: 173, Steps: 1 | Train Loss: 112.1807175 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10892248153686523\n",
      "Epoch: 174, Steps: 1 | Train Loss: 112.3783951 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.11147284507751465\n",
      "Epoch: 175, Steps: 1 | Train Loss: 112.0679932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.10652518272399902\n",
      "Epoch: 176, Steps: 1 | Train Loss: 111.9786530 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10744142532348633\n",
      "Epoch: 177, Steps: 1 | Train Loss: 112.3213882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10792016983032227\n",
      "Epoch: 178, Steps: 1 | Train Loss: 112.0854645 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.12154626846313477\n",
      "Epoch: 179, Steps: 1 | Train Loss: 112.1403809 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11751532554626465\n",
      "Epoch: 180, Steps: 1 | Train Loss: 112.0750351 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.10821413993835449\n",
      "Epoch: 181, Steps: 1 | Train Loss: 112.2058334 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11142969131469727\n",
      "Epoch: 182, Steps: 1 | Train Loss: 112.7831192 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10860800743103027\n",
      "Epoch: 183, Steps: 1 | Train Loss: 112.3101349 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.1080629825592041\n",
      "Epoch: 184, Steps: 1 | Train Loss: 112.0376205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11325860023498535\n",
      "Epoch: 185, Steps: 1 | Train Loss: 112.0365524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11113476753234863\n",
      "Epoch: 186, Steps: 1 | Train Loss: 112.2483292 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11260581016540527\n",
      "Epoch: 187, Steps: 1 | Train Loss: 112.6667633 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.1090235710144043\n",
      "Epoch: 188, Steps: 1 | Train Loss: 112.4341660 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.12051844596862793\n",
      "Epoch: 189, Steps: 1 | Train Loss: 111.8302383 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12258052825927734\n",
      "Epoch: 190, Steps: 1 | Train Loss: 112.1027451 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10701799392700195\n",
      "Epoch: 191, Steps: 1 | Train Loss: 112.3647995 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.11894392967224121\n",
      "Epoch: 192, Steps: 1 | Train Loss: 112.5299072 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.11758780479431152\n",
      "Epoch: 193, Steps: 1 | Train Loss: 112.2325211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11751198768615723\n",
      "Epoch: 194, Steps: 1 | Train Loss: 112.4086914 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10748124122619629\n",
      "Epoch: 195, Steps: 1 | Train Loss: 112.0613022 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.10772943496704102\n",
      "Epoch: 196, Steps: 1 | Train Loss: 111.9671555 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.10751795768737793\n",
      "Epoch: 197, Steps: 1 | Train Loss: 112.1401901 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.10726809501647949\n",
      "Epoch: 198, Steps: 1 | Train Loss: 112.5668106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.12098503112792969\n",
      "Epoch: 199, Steps: 1 | Train Loss: 112.4444504 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.12087869644165039\n",
      "Epoch: 200, Steps: 1 | Train Loss: 112.3080063 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.12104296684265137\n",
      "Epoch: 201, Steps: 1 | Train Loss: 112.3678131 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.12111687660217285\n",
      "Epoch: 202, Steps: 1 | Train Loss: 111.9807892 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.12393808364868164\n",
      "Epoch: 203, Steps: 1 | Train Loss: 111.8145142 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.11691546440124512\n",
      "Epoch: 204, Steps: 1 | Train Loss: 112.5561752 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.1173696517944336\n",
      "Epoch: 205, Steps: 1 | Train Loss: 112.3331833 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.13724613189697266\n",
      "Epoch: 206, Steps: 1 | Train Loss: 112.0941772 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11893486976623535\n",
      "Epoch: 207, Steps: 1 | Train Loss: 112.4080353 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.118377685546875\n",
      "Epoch: 208, Steps: 1 | Train Loss: 112.2370834 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11784887313842773\n",
      "Epoch: 209, Steps: 1 | Train Loss: 112.5417252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11764287948608398\n",
      "Epoch: 210, Steps: 1 | Train Loss: 112.8190918 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.11791372299194336\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.3796539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11793208122253418\n",
      "Epoch: 212, Steps: 1 | Train Loss: 112.4661636 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11779403686523438\n",
      "Epoch: 213, Steps: 1 | Train Loss: 111.8901138 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.1295945644378662\n",
      "Epoch: 214, Steps: 1 | Train Loss: 112.2524643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.12290835380554199\n",
      "Epoch: 215, Steps: 1 | Train Loss: 112.4178391 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.12050461769104004\n",
      "Epoch: 216, Steps: 1 | Train Loss: 112.7941208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11853194236755371\n",
      "Epoch: 217, Steps: 1 | Train Loss: 112.8272247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.1176002025604248\n",
      "Epoch: 218, Steps: 1 | Train Loss: 112.2309494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.11144757270812988\n",
      "Epoch: 219, Steps: 1 | Train Loss: 112.1194000 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11325931549072266\n",
      "Epoch: 220, Steps: 1 | Train Loss: 112.1638184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.10872101783752441\n",
      "Epoch: 221, Steps: 1 | Train Loss: 112.2813644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10882139205932617\n",
      "Epoch: 222, Steps: 1 | Train Loss: 112.1788559 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11713004112243652\n",
      "Epoch: 223, Steps: 1 | Train Loss: 113.0143585 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11673808097839355\n",
      "Epoch: 224, Steps: 1 | Train Loss: 112.4548721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11255216598510742\n",
      "Epoch: 225, Steps: 1 | Train Loss: 112.6055679 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11501359939575195\n",
      "Epoch: 226, Steps: 1 | Train Loss: 112.1702805 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11715912818908691\n",
      "Epoch: 227, Steps: 1 | Train Loss: 112.8037262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11794710159301758\n",
      "Epoch: 228, Steps: 1 | Train Loss: 111.9768600 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11784887313842773\n",
      "Epoch: 229, Steps: 1 | Train Loss: 112.6090622 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11949372291564941\n",
      "Epoch: 230, Steps: 1 | Train Loss: 112.2409897 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.13302111625671387\n",
      "Epoch: 231, Steps: 1 | Train Loss: 111.7338181 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.11986899375915527\n",
      "Epoch: 232, Steps: 1 | Train Loss: 112.5988541 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11850595474243164\n",
      "Epoch: 233, Steps: 1 | Train Loss: 111.9750519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11517596244812012\n",
      "Epoch: 234, Steps: 1 | Train Loss: 112.2074127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11231112480163574\n",
      "Epoch: 235, Steps: 1 | Train Loss: 112.6291733 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11308956146240234\n",
      "Epoch: 236, Steps: 1 | Train Loss: 112.4064941 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11707091331481934\n",
      "Epoch: 237, Steps: 1 | Train Loss: 112.2501144 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.12202835083007812\n",
      "Epoch: 238, Steps: 1 | Train Loss: 113.0603256 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.11509370803833008\n",
      "Epoch: 239, Steps: 1 | Train Loss: 111.9000244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.12038326263427734\n",
      "Epoch: 240, Steps: 1 | Train Loss: 113.0122833 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.11874556541442871\n",
      "Epoch: 241, Steps: 1 | Train Loss: 112.3020172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11233830451965332\n",
      "Epoch: 242, Steps: 1 | Train Loss: 113.1322632 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10859155654907227\n",
      "Epoch: 243, Steps: 1 | Train Loss: 111.8248291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.11314105987548828\n",
      "Epoch: 244, Steps: 1 | Train Loss: 112.1813507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.11101222038269043\n",
      "Epoch: 245, Steps: 1 | Train Loss: 112.4610367 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.12047100067138672\n",
      "Epoch: 246, Steps: 1 | Train Loss: 112.2942123 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11922788619995117\n",
      "Epoch: 247, Steps: 1 | Train Loss: 112.1754990 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.12771964073181152\n",
      "Epoch: 248, Steps: 1 | Train Loss: 112.6903839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.12383103370666504\n",
      "Epoch: 249, Steps: 1 | Train Loss: 112.5011520 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.12283730506896973\n",
      "Epoch: 250, Steps: 1 | Train Loss: 112.5553513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_16>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.1221609115600586\n",
      "Epoch: 1, Steps: 1 | Train Loss: 119.6257248 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10573220252990723\n",
      "Epoch: 2, Steps: 1 | Train Loss: 117.6600266 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.1074221134185791\n",
      "Epoch: 3, Steps: 1 | Train Loss: 116.0200958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10757875442504883\n",
      "Epoch: 4, Steps: 1 | Train Loss: 115.3380661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10741853713989258\n",
      "Epoch: 5, Steps: 1 | Train Loss: 114.7637711 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10733819007873535\n",
      "Epoch: 6, Steps: 1 | Train Loss: 115.0849838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11064648628234863\n",
      "Epoch: 7, Steps: 1 | Train Loss: 114.6833496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10867118835449219\n",
      "Epoch: 8, Steps: 1 | Train Loss: 114.3963242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10786032676696777\n",
      "Epoch: 9, Steps: 1 | Train Loss: 114.4670639 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10747218132019043\n",
      "Epoch: 10, Steps: 1 | Train Loss: 115.0182877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10918927192687988\n",
      "Epoch: 11, Steps: 1 | Train Loss: 114.7095871 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10805439949035645\n",
      "Epoch: 12, Steps: 1 | Train Loss: 114.3233795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10739374160766602\n",
      "Epoch: 13, Steps: 1 | Train Loss: 114.8772125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.11887073516845703\n",
      "Epoch: 14, Steps: 1 | Train Loss: 114.0698242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.11521506309509277\n",
      "Epoch: 15, Steps: 1 | Train Loss: 114.6092758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.13146686553955078\n",
      "Epoch: 16, Steps: 1 | Train Loss: 114.5555801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.1288142204284668\n",
      "Epoch: 17, Steps: 1 | Train Loss: 114.6606979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.12369847297668457\n",
      "Epoch: 18, Steps: 1 | Train Loss: 114.5661392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.12045049667358398\n",
      "Epoch: 19, Steps: 1 | Train Loss: 114.3060074 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.11482048034667969\n",
      "Epoch: 20, Steps: 1 | Train Loss: 114.2260284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11465954780578613\n",
      "Epoch: 21, Steps: 1 | Train Loss: 114.5508423 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.11690497398376465\n",
      "Epoch: 22, Steps: 1 | Train Loss: 115.0723877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11586165428161621\n",
      "Epoch: 23, Steps: 1 | Train Loss: 114.8015671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11451888084411621\n",
      "Epoch: 24, Steps: 1 | Train Loss: 114.0026169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11454510688781738\n",
      "Epoch: 25, Steps: 1 | Train Loss: 114.2527084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11737275123596191\n",
      "Epoch: 26, Steps: 1 | Train Loss: 114.6079102 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.1269218921661377\n",
      "Epoch: 27, Steps: 1 | Train Loss: 114.7276535 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.12099933624267578\n",
      "Epoch: 28, Steps: 1 | Train Loss: 114.2528915 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.11825895309448242\n",
      "Epoch: 29, Steps: 1 | Train Loss: 114.5960007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11777687072753906\n",
      "Epoch: 30, Steps: 1 | Train Loss: 113.9449005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11458778381347656\n",
      "Epoch: 31, Steps: 1 | Train Loss: 114.7698975 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10544753074645996\n",
      "Epoch: 32, Steps: 1 | Train Loss: 114.7429428 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10660839080810547\n",
      "Epoch: 33, Steps: 1 | Train Loss: 114.4707794 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.1048734188079834\n",
      "Epoch: 34, Steps: 1 | Train Loss: 114.0076523 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10769152641296387\n",
      "Epoch: 35, Steps: 1 | Train Loss: 114.9197006 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.11197423934936523\n",
      "Epoch: 36, Steps: 1 | Train Loss: 114.2572784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.11496949195861816\n",
      "Epoch: 37, Steps: 1 | Train Loss: 114.9172592 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11691951751708984\n",
      "Epoch: 38, Steps: 1 | Train Loss: 114.6440048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.11585545539855957\n",
      "Epoch: 39, Steps: 1 | Train Loss: 114.3888474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.12769699096679688\n",
      "Epoch: 40, Steps: 1 | Train Loss: 114.7050781 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.12750244140625\n",
      "Epoch: 41, Steps: 1 | Train Loss: 114.7601547 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.12104010581970215\n",
      "Epoch: 42, Steps: 1 | Train Loss: 114.6474991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.11254048347473145\n",
      "Epoch: 43, Steps: 1 | Train Loss: 114.6174698 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11365723609924316\n",
      "Epoch: 44, Steps: 1 | Train Loss: 114.4918976 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.11619019508361816\n",
      "Epoch: 45, Steps: 1 | Train Loss: 114.3280182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11507487297058105\n",
      "Epoch: 46, Steps: 1 | Train Loss: 114.7395401 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11559486389160156\n",
      "Epoch: 47, Steps: 1 | Train Loss: 114.3907394 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.11683988571166992\n",
      "Epoch: 48, Steps: 1 | Train Loss: 114.6176987 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.10521268844604492\n",
      "Epoch: 49, Steps: 1 | Train Loss: 114.8992920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.10757184028625488\n",
      "Epoch: 50, Steps: 1 | Train Loss: 114.8084030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.10773015022277832\n",
      "Epoch: 51, Steps: 1 | Train Loss: 114.5346832 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10721468925476074\n",
      "Epoch: 52, Steps: 1 | Train Loss: 114.3381348 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10741496086120605\n",
      "Epoch: 53, Steps: 1 | Train Loss: 114.8292007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10752129554748535\n",
      "Epoch: 54, Steps: 1 | Train Loss: 114.3175201 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.10770368576049805\n",
      "Epoch: 55, Steps: 1 | Train Loss: 114.9911652 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.12077593803405762\n",
      "Epoch: 56, Steps: 1 | Train Loss: 114.4060440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.12063312530517578\n",
      "Epoch: 57, Steps: 1 | Train Loss: 113.9511490 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.12096786499023438\n",
      "Epoch: 58, Steps: 1 | Train Loss: 114.2936783 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.12135934829711914\n",
      "Epoch: 59, Steps: 1 | Train Loss: 114.8387833 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11667084693908691\n",
      "Epoch: 60, Steps: 1 | Train Loss: 114.4120712 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.11464262008666992\n",
      "Epoch: 61, Steps: 1 | Train Loss: 114.5013580 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11086702346801758\n",
      "Epoch: 62, Steps: 1 | Train Loss: 114.1940689 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.10960507392883301\n",
      "Epoch: 63, Steps: 1 | Train Loss: 114.7108688 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.10867571830749512\n",
      "Epoch: 64, Steps: 1 | Train Loss: 114.4623566 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.10794186592102051\n",
      "Epoch: 65, Steps: 1 | Train Loss: 114.5247421 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.10741615295410156\n",
      "Epoch: 66, Steps: 1 | Train Loss: 114.4977341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.10718083381652832\n",
      "Epoch: 67, Steps: 1 | Train Loss: 114.7189713 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10770630836486816\n",
      "Epoch: 68, Steps: 1 | Train Loss: 114.3873825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10794496536254883\n",
      "Epoch: 69, Steps: 1 | Train Loss: 114.2568359 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.10773563385009766\n",
      "Epoch: 70, Steps: 1 | Train Loss: 114.4983139 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10721826553344727\n",
      "Epoch: 71, Steps: 1 | Train Loss: 114.4567032 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10766172409057617\n",
      "Epoch: 72, Steps: 1 | Train Loss: 114.5613785 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.10741996765136719\n",
      "Epoch: 73, Steps: 1 | Train Loss: 113.9020538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12050271034240723\n",
      "Epoch: 74, Steps: 1 | Train Loss: 114.7270050 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.12085819244384766\n",
      "Epoch: 75, Steps: 1 | Train Loss: 114.5645523 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11980938911437988\n",
      "Epoch: 76, Steps: 1 | Train Loss: 114.2534409 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11389899253845215\n",
      "Epoch: 77, Steps: 1 | Train Loss: 114.3630142 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.11349678039550781\n",
      "Epoch: 78, Steps: 1 | Train Loss: 114.8241119 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10799598693847656\n",
      "Epoch: 79, Steps: 1 | Train Loss: 114.7864380 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10821151733398438\n",
      "Epoch: 80, Steps: 1 | Train Loss: 114.4171906 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.1077425479888916\n",
      "Epoch: 81, Steps: 1 | Train Loss: 114.4178238 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.1077728271484375\n",
      "Epoch: 82, Steps: 1 | Train Loss: 113.9584885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.1080467700958252\n",
      "Epoch: 83, Steps: 1 | Train Loss: 114.4841537 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.12122583389282227\n",
      "Epoch: 84, Steps: 1 | Train Loss: 114.5717392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.12085437774658203\n",
      "Epoch: 85, Steps: 1 | Train Loss: 114.8640747 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11949753761291504\n",
      "Epoch: 86, Steps: 1 | Train Loss: 114.2701950 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.11266183853149414\n",
      "Epoch: 87, Steps: 1 | Train Loss: 114.4586716 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.10934162139892578\n",
      "Epoch: 88, Steps: 1 | Train Loss: 114.1949005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10831379890441895\n",
      "Epoch: 89, Steps: 1 | Train Loss: 114.4084091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10942625999450684\n",
      "Epoch: 90, Steps: 1 | Train Loss: 114.8768463 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10824084281921387\n",
      "Epoch: 91, Steps: 1 | Train Loss: 114.2436295 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.1081247329711914\n",
      "Epoch: 92, Steps: 1 | Train Loss: 114.9000397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10849189758300781\n",
      "Epoch: 93, Steps: 1 | Train Loss: 114.3036499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10784316062927246\n",
      "Epoch: 94, Steps: 1 | Train Loss: 115.0410385 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10788512229919434\n",
      "Epoch: 95, Steps: 1 | Train Loss: 114.7457047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.11170744895935059\n",
      "Epoch: 96, Steps: 1 | Train Loss: 114.3012238 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10929012298583984\n",
      "Epoch: 97, Steps: 1 | Train Loss: 114.0831070 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10771059989929199\n",
      "Epoch: 98, Steps: 1 | Train Loss: 114.4878464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10759091377258301\n",
      "Epoch: 99, Steps: 1 | Train Loss: 114.0340729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10730886459350586\n",
      "Epoch: 100, Steps: 1 | Train Loss: 114.3078842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10751914978027344\n",
      "Epoch: 101, Steps: 1 | Train Loss: 114.0216827 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10731983184814453\n",
      "Epoch: 102, Steps: 1 | Train Loss: 114.4839096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10753679275512695\n",
      "Epoch: 103, Steps: 1 | Train Loss: 114.5754623 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.12041163444519043\n",
      "Epoch: 104, Steps: 1 | Train Loss: 114.5153809 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.11127138137817383\n",
      "Epoch: 105, Steps: 1 | Train Loss: 114.3269653 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10970091819763184\n",
      "Epoch: 106, Steps: 1 | Train Loss: 114.3496094 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10794878005981445\n",
      "Epoch: 107, Steps: 1 | Train Loss: 114.5272217 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10778570175170898\n",
      "Epoch: 108, Steps: 1 | Train Loss: 114.8539047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10744309425354004\n",
      "Epoch: 109, Steps: 1 | Train Loss: 114.5388794 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.10849189758300781\n",
      "Epoch: 110, Steps: 1 | Train Loss: 114.6278687 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.10756444931030273\n",
      "Epoch: 111, Steps: 1 | Train Loss: 114.3143845 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10781383514404297\n",
      "Epoch: 112, Steps: 1 | Train Loss: 113.9013672 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10767936706542969\n",
      "Epoch: 113, Steps: 1 | Train Loss: 114.4195099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.12017393112182617\n",
      "Epoch: 114, Steps: 1 | Train Loss: 114.3665695 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.1108388900756836\n",
      "Epoch: 115, Steps: 1 | Train Loss: 114.5890884 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11029267311096191\n",
      "Epoch: 116, Steps: 1 | Train Loss: 114.2874298 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10834693908691406\n",
      "Epoch: 117, Steps: 1 | Train Loss: 113.9723663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10759329795837402\n",
      "Epoch: 118, Steps: 1 | Train Loss: 114.6296616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10717344284057617\n",
      "Epoch: 119, Steps: 1 | Train Loss: 113.8765411 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.10779404640197754\n",
      "Epoch: 120, Steps: 1 | Train Loss: 114.3549805 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.10801386833190918\n",
      "Epoch: 121, Steps: 1 | Train Loss: 114.7316437 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.1110842227935791\n",
      "Epoch: 122, Steps: 1 | Train Loss: 114.5993423 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.11105132102966309\n",
      "Epoch: 123, Steps: 1 | Train Loss: 114.5144196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11105895042419434\n",
      "Epoch: 124, Steps: 1 | Train Loss: 114.6578522 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.1086430549621582\n",
      "Epoch: 125, Steps: 1 | Train Loss: 114.9528809 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.10814809799194336\n",
      "Epoch: 126, Steps: 1 | Train Loss: 114.4229965 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.10735416412353516\n",
      "Epoch: 127, Steps: 1 | Train Loss: 114.2713852 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.10729169845581055\n",
      "Epoch: 128, Steps: 1 | Train Loss: 114.1492462 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10724425315856934\n",
      "Epoch: 129, Steps: 1 | Train Loss: 114.2605743 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10740065574645996\n",
      "Epoch: 130, Steps: 1 | Train Loss: 114.8191147 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.10949492454528809\n",
      "Epoch: 131, Steps: 1 | Train Loss: 114.8230743 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.1074683666229248\n",
      "Epoch: 132, Steps: 1 | Train Loss: 114.7853012 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.1210029125213623\n",
      "Epoch: 133, Steps: 1 | Train Loss: 114.6641846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.11795425415039062\n",
      "Epoch: 134, Steps: 1 | Train Loss: 114.3830185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.11545085906982422\n",
      "Epoch: 135, Steps: 1 | Train Loss: 113.9522858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.11155819892883301\n",
      "Epoch: 136, Steps: 1 | Train Loss: 113.9012604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.11161565780639648\n",
      "Epoch: 137, Steps: 1 | Train Loss: 114.3311310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11054635047912598\n",
      "Epoch: 138, Steps: 1 | Train Loss: 115.1983643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11071109771728516\n",
      "Epoch: 139, Steps: 1 | Train Loss: 114.3226089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10984683036804199\n",
      "Epoch: 140, Steps: 1 | Train Loss: 114.3566284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.11245298385620117\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.7771988 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.1132347583770752\n",
      "Epoch: 142, Steps: 1 | Train Loss: 114.5608902 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.13813042640686035\n",
      "Epoch: 143, Steps: 1 | Train Loss: 114.6722031 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11191582679748535\n",
      "Epoch: 144, Steps: 1 | Train Loss: 115.1461182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.11082983016967773\n",
      "Epoch: 145, Steps: 1 | Train Loss: 114.3413849 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.11065363883972168\n",
      "Epoch: 146, Steps: 1 | Train Loss: 114.3451080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.1109004020690918\n",
      "Epoch: 147, Steps: 1 | Train Loss: 114.3593292 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11072063446044922\n",
      "Epoch: 148, Steps: 1 | Train Loss: 115.2235107 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10935616493225098\n",
      "Epoch: 149, Steps: 1 | Train Loss: 113.9838257 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.11081624031066895\n",
      "Epoch: 150, Steps: 1 | Train Loss: 114.6941452 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.11033296585083008\n",
      "Epoch: 151, Steps: 1 | Train Loss: 114.3625412 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.11120200157165527\n",
      "Epoch: 152, Steps: 1 | Train Loss: 114.6751251 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.11047911643981934\n",
      "Epoch: 153, Steps: 1 | Train Loss: 114.5750122 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.12335920333862305\n",
      "Epoch: 154, Steps: 1 | Train Loss: 115.0542374 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.12354302406311035\n",
      "Epoch: 155, Steps: 1 | Train Loss: 114.4132614 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.12150073051452637\n",
      "Epoch: 156, Steps: 1 | Train Loss: 114.2365112 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.11374497413635254\n",
      "Epoch: 157, Steps: 1 | Train Loss: 114.5655441 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.11227607727050781\n",
      "Epoch: 158, Steps: 1 | Train Loss: 114.8657455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.11455035209655762\n",
      "Epoch: 159, Steps: 1 | Train Loss: 114.6133652 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11182522773742676\n",
      "Epoch: 160, Steps: 1 | Train Loss: 114.5012817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.11184167861938477\n",
      "Epoch: 161, Steps: 1 | Train Loss: 114.3110580 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.11041045188903809\n",
      "Epoch: 162, Steps: 1 | Train Loss: 114.5440445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10991263389587402\n",
      "Epoch: 163, Steps: 1 | Train Loss: 114.4737320 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.11018991470336914\n",
      "Epoch: 164, Steps: 1 | Train Loss: 114.1104736 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.11076617240905762\n",
      "Epoch: 165, Steps: 1 | Train Loss: 115.1489182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11080074310302734\n",
      "Epoch: 166, Steps: 1 | Train Loss: 114.5670700 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10921573638916016\n",
      "Epoch: 167, Steps: 1 | Train Loss: 114.2632675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.1077120304107666\n",
      "Epoch: 168, Steps: 1 | Train Loss: 114.3870621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11132144927978516\n",
      "Epoch: 169, Steps: 1 | Train Loss: 114.7240524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.10837578773498535\n",
      "Epoch: 170, Steps: 1 | Train Loss: 114.4565201 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.1108403205871582\n",
      "Epoch: 171, Steps: 1 | Train Loss: 114.5363770 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.10915851593017578\n",
      "Epoch: 172, Steps: 1 | Train Loss: 114.7876740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11012053489685059\n",
      "Epoch: 173, Steps: 1 | Train Loss: 114.9842529 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11438345909118652\n",
      "Epoch: 174, Steps: 1 | Train Loss: 114.9097290 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.12111854553222656\n",
      "Epoch: 175, Steps: 1 | Train Loss: 114.3341904 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.12306952476501465\n",
      "Epoch: 176, Steps: 1 | Train Loss: 114.6881104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.11981010437011719\n",
      "Epoch: 177, Steps: 1 | Train Loss: 114.2760391 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11271929740905762\n",
      "Epoch: 178, Steps: 1 | Train Loss: 114.4186630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11205029487609863\n",
      "Epoch: 179, Steps: 1 | Train Loss: 114.4643936 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11216330528259277\n",
      "Epoch: 180, Steps: 1 | Train Loss: 114.5238876 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11230731010437012\n",
      "Epoch: 181, Steps: 1 | Train Loss: 114.0468903 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11085867881774902\n",
      "Epoch: 182, Steps: 1 | Train Loss: 114.7044296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11037564277648926\n",
      "Epoch: 183, Steps: 1 | Train Loss: 114.3058624 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11135005950927734\n",
      "Epoch: 184, Steps: 1 | Train Loss: 114.7415543 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.10928821563720703\n",
      "Epoch: 185, Steps: 1 | Train Loss: 114.7519531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11061286926269531\n",
      "Epoch: 186, Steps: 1 | Train Loss: 114.5416489 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11007499694824219\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.8854141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10947012901306152\n",
      "Epoch: 188, Steps: 1 | Train Loss: 114.3586197 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.11062026023864746\n",
      "Epoch: 189, Steps: 1 | Train Loss: 114.7423706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12301087379455566\n",
      "Epoch: 190, Steps: 1 | Train Loss: 114.0764923 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.11329865455627441\n",
      "Epoch: 191, Steps: 1 | Train Loss: 114.2748413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.1111140251159668\n",
      "Epoch: 192, Steps: 1 | Train Loss: 114.5229492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.11200070381164551\n",
      "Epoch: 193, Steps: 1 | Train Loss: 114.6004868 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11018896102905273\n",
      "Epoch: 194, Steps: 1 | Train Loss: 114.2747421 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11107397079467773\n",
      "Epoch: 195, Steps: 1 | Train Loss: 114.6810074 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.10963320732116699\n",
      "Epoch: 196, Steps: 1 | Train Loss: 114.8778305 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11075925827026367\n",
      "Epoch: 197, Steps: 1 | Train Loss: 113.6839371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.1101524829864502\n",
      "Epoch: 198, Steps: 1 | Train Loss: 114.2215576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11111593246459961\n",
      "Epoch: 199, Steps: 1 | Train Loss: 115.2259903 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11059212684631348\n",
      "Epoch: 200, Steps: 1 | Train Loss: 114.2143555 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11188173294067383\n",
      "Epoch: 201, Steps: 1 | Train Loss: 114.8008575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.12294507026672363\n",
      "Epoch: 202, Steps: 1 | Train Loss: 114.5878296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.12316370010375977\n",
      "Epoch: 203, Steps: 1 | Train Loss: 114.3759918 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.1221470832824707\n",
      "Epoch: 204, Steps: 1 | Train Loss: 114.1335983 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12302422523498535\n",
      "Epoch: 205, Steps: 1 | Train Loss: 114.4054794 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.12245798110961914\n",
      "Epoch: 206, Steps: 1 | Train Loss: 114.4583740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11487936973571777\n",
      "Epoch: 207, Steps: 1 | Train Loss: 113.9887619 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.11217832565307617\n",
      "Epoch: 208, Steps: 1 | Train Loss: 114.3436890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11133623123168945\n",
      "Epoch: 209, Steps: 1 | Train Loss: 114.1860886 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11076736450195312\n",
      "Epoch: 210, Steps: 1 | Train Loss: 114.5997543 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.1099240779876709\n",
      "Epoch: 211, Steps: 1 | Train Loss: 114.2401657 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.10975956916809082\n",
      "Epoch: 212, Steps: 1 | Train Loss: 114.3109894 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11017656326293945\n",
      "Epoch: 213, Steps: 1 | Train Loss: 114.8403091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10954642295837402\n",
      "Epoch: 214, Steps: 1 | Train Loss: 114.6199722 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11150479316711426\n",
      "Epoch: 215, Steps: 1 | Train Loss: 114.4482422 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11081242561340332\n",
      "Epoch: 216, Steps: 1 | Train Loss: 114.8295212 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11037564277648926\n",
      "Epoch: 217, Steps: 1 | Train Loss: 114.6683578 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10967278480529785\n",
      "Epoch: 218, Steps: 1 | Train Loss: 114.7250137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.11066198348999023\n",
      "Epoch: 219, Steps: 1 | Train Loss: 114.6404343 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11123299598693848\n",
      "Epoch: 220, Steps: 1 | Train Loss: 114.5440216 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.12346696853637695\n",
      "Epoch: 221, Steps: 1 | Train Loss: 114.8679581 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.12439560890197754\n",
      "Epoch: 222, Steps: 1 | Train Loss: 114.1187515 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.12305951118469238\n",
      "Epoch: 223, Steps: 1 | Train Loss: 114.5954514 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.13302206993103027\n",
      "Epoch: 224, Steps: 1 | Train Loss: 114.4625015 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.12089657783508301\n",
      "Epoch: 225, Steps: 1 | Train Loss: 114.6060333 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11226868629455566\n",
      "Epoch: 226, Steps: 1 | Train Loss: 114.2082291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.1126260757446289\n",
      "Epoch: 227, Steps: 1 | Train Loss: 114.2399673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11047697067260742\n",
      "Epoch: 228, Steps: 1 | Train Loss: 114.0305328 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11119294166564941\n",
      "Epoch: 229, Steps: 1 | Train Loss: 114.8397980 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11067414283752441\n",
      "Epoch: 230, Steps: 1 | Train Loss: 114.6295166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.11529397964477539\n",
      "Epoch: 231, Steps: 1 | Train Loss: 114.8587799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.11139559745788574\n",
      "Epoch: 232, Steps: 1 | Train Loss: 114.3954697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11036276817321777\n",
      "Epoch: 233, Steps: 1 | Train Loss: 114.3846664 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.1116476058959961\n",
      "Epoch: 234, Steps: 1 | Train Loss: 114.7349625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.10993170738220215\n",
      "Epoch: 235, Steps: 1 | Train Loss: 114.6845474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11029982566833496\n",
      "Epoch: 236, Steps: 1 | Train Loss: 113.9110336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.12337589263916016\n",
      "Epoch: 237, Steps: 1 | Train Loss: 113.8918228 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.12375545501708984\n",
      "Epoch: 238, Steps: 1 | Train Loss: 114.2564926 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.12353634834289551\n",
      "Epoch: 239, Steps: 1 | Train Loss: 114.5807724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.12401437759399414\n",
      "Epoch: 240, Steps: 1 | Train Loss: 114.5318604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1237030029296875\n",
      "Epoch: 241, Steps: 1 | Train Loss: 114.4334030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.12111878395080566\n",
      "Epoch: 242, Steps: 1 | Train Loss: 114.1698990 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.11310529708862305\n",
      "Epoch: 243, Steps: 1 | Train Loss: 114.1091919 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.1127324104309082\n",
      "Epoch: 244, Steps: 1 | Train Loss: 114.5959091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.1119685173034668\n",
      "Epoch: 245, Steps: 1 | Train Loss: 114.3907700 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.1112515926361084\n",
      "Epoch: 246, Steps: 1 | Train Loss: 114.0003433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.1098318099975586\n",
      "Epoch: 247, Steps: 1 | Train Loss: 114.7988052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10957121849060059\n",
      "Epoch: 248, Steps: 1 | Train Loss: 114.6520538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11048150062561035\n",
      "Epoch: 249, Steps: 1 | Train Loss: 114.5339127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11003899574279785\n",
      "Epoch: 250, Steps: 1 | Train Loss: 114.3928375 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_17>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11226439476013184\n",
      "Epoch: 1, Steps: 1 | Train Loss: 119.4889145 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.11049509048461914\n",
      "Epoch: 2, Steps: 1 | Train Loss: 117.9682617 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11048650741577148\n",
      "Epoch: 3, Steps: 1 | Train Loss: 116.1882553 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10987162590026855\n",
      "Epoch: 4, Steps: 1 | Train Loss: 115.1959381 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11055183410644531\n",
      "Epoch: 5, Steps: 1 | Train Loss: 114.8438110 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10947942733764648\n",
      "Epoch: 6, Steps: 1 | Train Loss: 114.6335831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.1115272045135498\n",
      "Epoch: 7, Steps: 1 | Train Loss: 115.1552887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.12026500701904297\n",
      "Epoch: 8, Steps: 1 | Train Loss: 114.2056885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.12154936790466309\n",
      "Epoch: 9, Steps: 1 | Train Loss: 115.0906982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.12322616577148438\n",
      "Epoch: 10, Steps: 1 | Train Loss: 114.6367111 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.12275171279907227\n",
      "Epoch: 11, Steps: 1 | Train Loss: 114.6762924 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11947035789489746\n",
      "Epoch: 12, Steps: 1 | Train Loss: 114.5484619 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11149168014526367\n",
      "Epoch: 13, Steps: 1 | Train Loss: 114.4782944 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10965514183044434\n",
      "Epoch: 14, Steps: 1 | Train Loss: 114.6707306 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.11063694953918457\n",
      "Epoch: 15, Steps: 1 | Train Loss: 114.8756943 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10968327522277832\n",
      "Epoch: 16, Steps: 1 | Train Loss: 114.4855499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.11102509498596191\n",
      "Epoch: 17, Steps: 1 | Train Loss: 114.7943497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10977554321289062\n",
      "Epoch: 18, Steps: 1 | Train Loss: 114.8014679 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10986685752868652\n",
      "Epoch: 19, Steps: 1 | Train Loss: 114.9622803 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.11088871955871582\n",
      "Epoch: 20, Steps: 1 | Train Loss: 115.4495163 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.12288212776184082\n",
      "Epoch: 21, Steps: 1 | Train Loss: 114.7535400 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.1227116584777832\n",
      "Epoch: 22, Steps: 1 | Train Loss: 114.5513306 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11620402336120605\n",
      "Epoch: 23, Steps: 1 | Train Loss: 114.9704819 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11266350746154785\n",
      "Epoch: 24, Steps: 1 | Train Loss: 114.9655304 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.10848236083984375\n",
      "Epoch: 25, Steps: 1 | Train Loss: 115.1408920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.10818290710449219\n",
      "Epoch: 26, Steps: 1 | Train Loss: 114.5851364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.10705852508544922\n",
      "Epoch: 27, Steps: 1 | Train Loss: 115.1234894 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.10765719413757324\n",
      "Epoch: 28, Steps: 1 | Train Loss: 114.8353271 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10736393928527832\n",
      "Epoch: 29, Steps: 1 | Train Loss: 114.9365997 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.10718870162963867\n",
      "Epoch: 30, Steps: 1 | Train Loss: 114.5556183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.1071624755859375\n",
      "Epoch: 31, Steps: 1 | Train Loss: 114.7179947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10767817497253418\n",
      "Epoch: 32, Steps: 1 | Train Loss: 115.4913864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10750102996826172\n",
      "Epoch: 33, Steps: 1 | Train Loss: 114.8096848 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.1211097240447998\n",
      "Epoch: 34, Steps: 1 | Train Loss: 114.8652878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.12080812454223633\n",
      "Epoch: 35, Steps: 1 | Train Loss: 114.3366852 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.1119534969329834\n",
      "Epoch: 36, Steps: 1 | Train Loss: 114.8709717 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10917329788208008\n",
      "Epoch: 37, Steps: 1 | Train Loss: 114.8383331 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.1078958511352539\n",
      "Epoch: 38, Steps: 1 | Train Loss: 114.5980225 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10747289657592773\n",
      "Epoch: 39, Steps: 1 | Train Loss: 114.6928329 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10728764533996582\n",
      "Epoch: 40, Steps: 1 | Train Loss: 115.0293961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10731697082519531\n",
      "Epoch: 41, Steps: 1 | Train Loss: 114.8642197 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10689568519592285\n",
      "Epoch: 42, Steps: 1 | Train Loss: 114.8443832 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10740303993225098\n",
      "Epoch: 43, Steps: 1 | Train Loss: 115.0143051 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10732007026672363\n",
      "Epoch: 44, Steps: 1 | Train Loss: 114.7133789 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10726571083068848\n",
      "Epoch: 45, Steps: 1 | Train Loss: 114.4863052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.10781121253967285\n",
      "Epoch: 46, Steps: 1 | Train Loss: 115.1018219 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.12091517448425293\n",
      "Epoch: 47, Steps: 1 | Train Loss: 114.2513351 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.1212606430053711\n",
      "Epoch: 48, Steps: 1 | Train Loss: 114.8356934 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.12350010871887207\n",
      "Epoch: 49, Steps: 1 | Train Loss: 114.6111069 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11230325698852539\n",
      "Epoch: 50, Steps: 1 | Train Loss: 114.9350586 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11028456687927246\n",
      "Epoch: 51, Steps: 1 | Train Loss: 114.1348038 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10946106910705566\n",
      "Epoch: 52, Steps: 1 | Train Loss: 114.9622574 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10798287391662598\n",
      "Epoch: 53, Steps: 1 | Train Loss: 115.5918579 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10744643211364746\n",
      "Epoch: 54, Steps: 1 | Train Loss: 114.8523483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.10735893249511719\n",
      "Epoch: 55, Steps: 1 | Train Loss: 114.9543686 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1076347827911377\n",
      "Epoch: 56, Steps: 1 | Train Loss: 114.3826447 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.1069481372833252\n",
      "Epoch: 57, Steps: 1 | Train Loss: 114.8345261 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.1073312759399414\n",
      "Epoch: 58, Steps: 1 | Train Loss: 115.4379196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.10722041130065918\n",
      "Epoch: 59, Steps: 1 | Train Loss: 114.4697647 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11952018737792969\n",
      "Epoch: 60, Steps: 1 | Train Loss: 114.5407715 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.1106410026550293\n",
      "Epoch: 61, Steps: 1 | Train Loss: 114.6710739 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.10919070243835449\n",
      "Epoch: 62, Steps: 1 | Train Loss: 114.3172150 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.10806965827941895\n",
      "Epoch: 63, Steps: 1 | Train Loss: 114.4062271 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.10750269889831543\n",
      "Epoch: 64, Steps: 1 | Train Loss: 114.9118423 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.10721254348754883\n",
      "Epoch: 65, Steps: 1 | Train Loss: 114.8518066 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.10755109786987305\n",
      "Epoch: 66, Steps: 1 | Train Loss: 114.8695068 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.10739302635192871\n",
      "Epoch: 67, Steps: 1 | Train Loss: 115.0961914 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10765743255615234\n",
      "Epoch: 68, Steps: 1 | Train Loss: 115.2109146 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.1075441837310791\n",
      "Epoch: 69, Steps: 1 | Train Loss: 114.9644318 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.10801959037780762\n",
      "Epoch: 70, Steps: 1 | Train Loss: 114.3648071 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.12183856964111328\n",
      "Epoch: 71, Steps: 1 | Train Loss: 114.7981491 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.12002301216125488\n",
      "Epoch: 72, Steps: 1 | Train Loss: 114.7795563 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11097502708435059\n",
      "Epoch: 73, Steps: 1 | Train Loss: 114.4794235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.1087944507598877\n",
      "Epoch: 74, Steps: 1 | Train Loss: 114.7415543 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11697220802307129\n",
      "Epoch: 75, Steps: 1 | Train Loss: 114.7663345 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11469483375549316\n",
      "Epoch: 76, Steps: 1 | Train Loss: 114.7160416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11633181571960449\n",
      "Epoch: 77, Steps: 1 | Train Loss: 115.0147705 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10902690887451172\n",
      "Epoch: 78, Steps: 1 | Train Loss: 114.9662704 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10934591293334961\n",
      "Epoch: 79, Steps: 1 | Train Loss: 114.9737473 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.11295843124389648\n",
      "Epoch: 80, Steps: 1 | Train Loss: 114.5192184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10854649543762207\n",
      "Epoch: 81, Steps: 1 | Train Loss: 114.3965836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.10744357109069824\n",
      "Epoch: 82, Steps: 1 | Train Loss: 114.9411621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.10747909545898438\n",
      "Epoch: 83, Steps: 1 | Train Loss: 114.7659302 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.12104582786560059\n",
      "Epoch: 84, Steps: 1 | Train Loss: 115.1100082 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.11898183822631836\n",
      "Epoch: 85, Steps: 1 | Train Loss: 114.5406494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.1106569766998291\n",
      "Epoch: 86, Steps: 1 | Train Loss: 114.1034164 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10886335372924805\n",
      "Epoch: 87, Steps: 1 | Train Loss: 114.3297501 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.10812211036682129\n",
      "Epoch: 88, Steps: 1 | Train Loss: 114.8722458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10470461845397949\n",
      "Epoch: 89, Steps: 1 | Train Loss: 114.7580566 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10868072509765625\n",
      "Epoch: 90, Steps: 1 | Train Loss: 115.0576401 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.11320066452026367\n",
      "Epoch: 91, Steps: 1 | Train Loss: 114.9677048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.1090238094329834\n",
      "Epoch: 92, Steps: 1 | Train Loss: 114.8592300 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10446953773498535\n",
      "Epoch: 93, Steps: 1 | Train Loss: 115.0660400 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10620737075805664\n",
      "Epoch: 94, Steps: 1 | Train Loss: 114.9842758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.1063835620880127\n",
      "Epoch: 95, Steps: 1 | Train Loss: 114.9293213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10788202285766602\n",
      "Epoch: 96, Steps: 1 | Train Loss: 114.8211212 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.12285566329956055\n",
      "Epoch: 97, Steps: 1 | Train Loss: 114.8128662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.12259840965270996\n",
      "Epoch: 98, Steps: 1 | Train Loss: 114.3427353 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10874128341674805\n",
      "Epoch: 99, Steps: 1 | Train Loss: 114.8291245 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11029887199401855\n",
      "Epoch: 100, Steps: 1 | Train Loss: 114.3427505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10535049438476562\n",
      "Epoch: 101, Steps: 1 | Train Loss: 114.8559799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.1081233024597168\n",
      "Epoch: 102, Steps: 1 | Train Loss: 114.8194351 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10492253303527832\n",
      "Epoch: 103, Steps: 1 | Train Loss: 114.9624023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.11256527900695801\n",
      "Epoch: 104, Steps: 1 | Train Loss: 115.1056061 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10443305969238281\n",
      "Epoch: 105, Steps: 1 | Train Loss: 114.9936523 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10535883903503418\n",
      "Epoch: 106, Steps: 1 | Train Loss: 114.5872116 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10742759704589844\n",
      "Epoch: 107, Steps: 1 | Train Loss: 115.0127792 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10728764533996582\n",
      "Epoch: 108, Steps: 1 | Train Loss: 114.9569473 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10713052749633789\n",
      "Epoch: 109, Steps: 1 | Train Loss: 114.8554459 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.1112217903137207\n",
      "Epoch: 110, Steps: 1 | Train Loss: 114.8990402 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.1089165210723877\n",
      "Epoch: 111, Steps: 1 | Train Loss: 114.5749435 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10799717903137207\n",
      "Epoch: 112, Steps: 1 | Train Loss: 115.3110580 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10724377632141113\n",
      "Epoch: 113, Steps: 1 | Train Loss: 114.5075226 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10743308067321777\n",
      "Epoch: 114, Steps: 1 | Train Loss: 114.9008942 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.1075441837310791\n",
      "Epoch: 115, Steps: 1 | Train Loss: 115.0021744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10797786712646484\n",
      "Epoch: 116, Steps: 1 | Train Loss: 114.8112183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10757255554199219\n",
      "Epoch: 117, Steps: 1 | Train Loss: 114.4219971 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10791707038879395\n",
      "Epoch: 118, Steps: 1 | Train Loss: 114.8506393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.1234734058380127\n",
      "Epoch: 119, Steps: 1 | Train Loss: 114.8395157 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.12511658668518066\n",
      "Epoch: 120, Steps: 1 | Train Loss: 114.7347946 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.12388014793395996\n",
      "Epoch: 121, Steps: 1 | Train Loss: 114.3611832 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11984753608703613\n",
      "Epoch: 122, Steps: 1 | Train Loss: 115.2735825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.11842966079711914\n",
      "Epoch: 123, Steps: 1 | Train Loss: 114.8851852 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11282467842102051\n",
      "Epoch: 124, Steps: 1 | Train Loss: 114.9661179 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.11755704879760742\n",
      "Epoch: 125, Steps: 1 | Train Loss: 115.3736801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.11053633689880371\n",
      "Epoch: 126, Steps: 1 | Train Loss: 115.0070190 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11108827590942383\n",
      "Epoch: 127, Steps: 1 | Train Loss: 115.3766403 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.11101078987121582\n",
      "Epoch: 128, Steps: 1 | Train Loss: 114.5698013 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.11010622978210449\n",
      "Epoch: 129, Steps: 1 | Train Loss: 115.2756348 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11110949516296387\n",
      "Epoch: 130, Steps: 1 | Train Loss: 115.0746613 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11098957061767578\n",
      "Epoch: 131, Steps: 1 | Train Loss: 114.8669968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11060094833374023\n",
      "Epoch: 132, Steps: 1 | Train Loss: 115.1559219 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11166548728942871\n",
      "Epoch: 133, Steps: 1 | Train Loss: 114.7346191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.12431144714355469\n",
      "Epoch: 134, Steps: 1 | Train Loss: 114.7795181 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.12321114540100098\n",
      "Epoch: 135, Steps: 1 | Train Loss: 114.6720505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.12441754341125488\n",
      "Epoch: 136, Steps: 1 | Train Loss: 114.7317657 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.1240396499633789\n",
      "Epoch: 137, Steps: 1 | Train Loss: 115.4692154 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.12147951126098633\n",
      "Epoch: 138, Steps: 1 | Train Loss: 115.2965622 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11391901969909668\n",
      "Epoch: 139, Steps: 1 | Train Loss: 115.0512009 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.11228084564208984\n",
      "Epoch: 140, Steps: 1 | Train Loss: 114.6984406 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.11185121536254883\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.9369431 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11086463928222656\n",
      "Epoch: 142, Steps: 1 | Train Loss: 114.3150406 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11044502258300781\n",
      "Epoch: 143, Steps: 1 | Train Loss: 114.4891357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11071610450744629\n",
      "Epoch: 144, Steps: 1 | Train Loss: 114.4557877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.11046624183654785\n",
      "Epoch: 145, Steps: 1 | Train Loss: 114.9232712 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.1128232479095459\n",
      "Epoch: 146, Steps: 1 | Train Loss: 114.7987289 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10785531997680664\n",
      "Epoch: 147, Steps: 1 | Train Loss: 114.8477554 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10861515998840332\n",
      "Epoch: 148, Steps: 1 | Train Loss: 114.3152695 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10828995704650879\n",
      "Epoch: 149, Steps: 1 | Train Loss: 115.1810226 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10773968696594238\n",
      "Epoch: 150, Steps: 1 | Train Loss: 114.9212189 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.1076209545135498\n",
      "Epoch: 151, Steps: 1 | Train Loss: 115.5355988 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10744976997375488\n",
      "Epoch: 152, Steps: 1 | Train Loss: 114.6632843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12067270278930664\n",
      "Epoch: 153, Steps: 1 | Train Loss: 115.1674576 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.12101340293884277\n",
      "Epoch: 154, Steps: 1 | Train Loss: 115.0130844 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.11329388618469238\n",
      "Epoch: 155, Steps: 1 | Train Loss: 114.6371384 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10920524597167969\n",
      "Epoch: 156, Steps: 1 | Train Loss: 114.7612534 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10839247703552246\n",
      "Epoch: 157, Steps: 1 | Train Loss: 115.1284332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.1081392765045166\n",
      "Epoch: 158, Steps: 1 | Train Loss: 115.0269775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10758209228515625\n",
      "Epoch: 159, Steps: 1 | Train Loss: 115.0710831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.10748887062072754\n",
      "Epoch: 160, Steps: 1 | Train Loss: 114.8957748 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10722970962524414\n",
      "Epoch: 161, Steps: 1 | Train Loss: 115.1379623 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10736227035522461\n",
      "Epoch: 162, Steps: 1 | Train Loss: 115.2389450 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10711288452148438\n",
      "Epoch: 163, Steps: 1 | Train Loss: 114.3074722 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10728025436401367\n",
      "Epoch: 164, Steps: 1 | Train Loss: 114.7447739 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10747694969177246\n",
      "Epoch: 165, Steps: 1 | Train Loss: 114.8004684 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.10736632347106934\n",
      "Epoch: 166, Steps: 1 | Train Loss: 115.1981125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.10735797882080078\n",
      "Epoch: 167, Steps: 1 | Train Loss: 114.7618637 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.12108087539672852\n",
      "Epoch: 168, Steps: 1 | Train Loss: 114.6787872 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.12074780464172363\n",
      "Epoch: 169, Steps: 1 | Train Loss: 115.3144531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.1208806037902832\n",
      "Epoch: 170, Steps: 1 | Train Loss: 114.6758041 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11534976959228516\n",
      "Epoch: 171, Steps: 1 | Train Loss: 115.2854233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11542582511901855\n",
      "Epoch: 172, Steps: 1 | Train Loss: 114.8552246 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10955095291137695\n",
      "Epoch: 173, Steps: 1 | Train Loss: 115.0475082 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.1081228256225586\n",
      "Epoch: 174, Steps: 1 | Train Loss: 114.8465500 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10752248764038086\n",
      "Epoch: 175, Steps: 1 | Train Loss: 114.2824020 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.1074671745300293\n",
      "Epoch: 176, Steps: 1 | Train Loss: 115.0720978 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10744524002075195\n",
      "Epoch: 177, Steps: 1 | Train Loss: 114.6936035 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10718297958374023\n",
      "Epoch: 178, Steps: 1 | Train Loss: 114.6348190 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10682940483093262\n",
      "Epoch: 179, Steps: 1 | Train Loss: 114.7651978 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10728955268859863\n",
      "Epoch: 180, Steps: 1 | Train Loss: 114.7985611 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11937117576599121\n",
      "Epoch: 181, Steps: 1 | Train Loss: 114.7343979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11208391189575195\n",
      "Epoch: 182, Steps: 1 | Train Loss: 114.6204453 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10894608497619629\n",
      "Epoch: 183, Steps: 1 | Train Loss: 114.6349411 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.10838556289672852\n",
      "Epoch: 184, Steps: 1 | Train Loss: 114.6313019 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.10751485824584961\n",
      "Epoch: 185, Steps: 1 | Train Loss: 114.6579514 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.10875892639160156\n",
      "Epoch: 186, Steps: 1 | Train Loss: 114.5449982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10781621932983398\n",
      "Epoch: 187, Steps: 1 | Train Loss: 114.4993439 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10748720169067383\n",
      "Epoch: 188, Steps: 1 | Train Loss: 115.0699997 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.10785794258117676\n",
      "Epoch: 189, Steps: 1 | Train Loss: 114.3088379 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10793495178222656\n",
      "Epoch: 190, Steps: 1 | Train Loss: 114.1877213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10709929466247559\n",
      "Epoch: 191, Steps: 1 | Train Loss: 114.5629425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.12096428871154785\n",
      "Epoch: 192, Steps: 1 | Train Loss: 115.2603531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.12166905403137207\n",
      "Epoch: 193, Steps: 1 | Train Loss: 115.0401382 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11118292808532715\n",
      "Epoch: 194, Steps: 1 | Train Loss: 114.9697189 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10881328582763672\n",
      "Epoch: 195, Steps: 1 | Train Loss: 114.5041733 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.1081852912902832\n",
      "Epoch: 196, Steps: 1 | Train Loss: 114.5918732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.10764455795288086\n",
      "Epoch: 197, Steps: 1 | Train Loss: 114.9775925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.10736513137817383\n",
      "Epoch: 198, Steps: 1 | Train Loss: 114.8962631 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10735964775085449\n",
      "Epoch: 199, Steps: 1 | Train Loss: 114.7514801 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10731387138366699\n",
      "Epoch: 200, Steps: 1 | Train Loss: 114.8169098 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10769271850585938\n",
      "Epoch: 201, Steps: 1 | Train Loss: 114.9475479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.10749936103820801\n",
      "Epoch: 202, Steps: 1 | Train Loss: 115.1102066 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10721874237060547\n",
      "Epoch: 203, Steps: 1 | Train Loss: 114.2082291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.10730957984924316\n",
      "Epoch: 204, Steps: 1 | Train Loss: 115.0184250 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12098884582519531\n",
      "Epoch: 205, Steps: 1 | Train Loss: 114.9433136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.12071061134338379\n",
      "Epoch: 206, Steps: 1 | Train Loss: 114.7234039 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11532282829284668\n",
      "Epoch: 207, Steps: 1 | Train Loss: 114.8124008 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.10947799682617188\n",
      "Epoch: 208, Steps: 1 | Train Loss: 115.2315903 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10823225975036621\n",
      "Epoch: 209, Steps: 1 | Train Loss: 115.1396027 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.10718297958374023\n",
      "Epoch: 210, Steps: 1 | Train Loss: 115.0118408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.10763192176818848\n",
      "Epoch: 211, Steps: 1 | Train Loss: 114.8097305 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.1075584888458252\n",
      "Epoch: 212, Steps: 1 | Train Loss: 114.8825912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.1080782413482666\n",
      "Epoch: 213, Steps: 1 | Train Loss: 114.1962433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.1071615219116211\n",
      "Epoch: 214, Steps: 1 | Train Loss: 115.2934494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10853219032287598\n",
      "Epoch: 215, Steps: 1 | Train Loss: 114.4468307 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10029864311218262\n",
      "Epoch: 216, Steps: 1 | Train Loss: 114.6215134 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10422658920288086\n",
      "Epoch: 217, Steps: 1 | Train Loss: 114.7015991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10199093818664551\n",
      "Epoch: 218, Steps: 1 | Train Loss: 114.5848999 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10060477256774902\n",
      "Epoch: 219, Steps: 1 | Train Loss: 114.7297363 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10079765319824219\n",
      "Epoch: 220, Steps: 1 | Train Loss: 115.0483398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.10025930404663086\n",
      "Epoch: 221, Steps: 1 | Train Loss: 115.2620621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10015583038330078\n",
      "Epoch: 222, Steps: 1 | Train Loss: 114.5667038 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10013628005981445\n",
      "Epoch: 223, Steps: 1 | Train Loss: 114.9656525 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.10063982009887695\n",
      "Epoch: 224, Steps: 1 | Train Loss: 114.8171158 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.10106182098388672\n",
      "Epoch: 225, Steps: 1 | Train Loss: 114.9785919 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.10337328910827637\n",
      "Epoch: 226, Steps: 1 | Train Loss: 115.1338882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10134744644165039\n",
      "Epoch: 227, Steps: 1 | Train Loss: 114.6197891 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.10056400299072266\n",
      "Epoch: 228, Steps: 1 | Train Loss: 114.7648544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.10052227973937988\n",
      "Epoch: 229, Steps: 1 | Train Loss: 115.5063629 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.1004629135131836\n",
      "Epoch: 230, Steps: 1 | Train Loss: 115.0513306 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.10028696060180664\n",
      "Epoch: 231, Steps: 1 | Train Loss: 115.0854111 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.10033631324768066\n",
      "Epoch: 232, Steps: 1 | Train Loss: 114.9779053 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.10003304481506348\n",
      "Epoch: 233, Steps: 1 | Train Loss: 114.9541397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.10716009140014648\n",
      "Epoch: 234, Steps: 1 | Train Loss: 114.9734421 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.1020212173461914\n",
      "Epoch: 235, Steps: 1 | Train Loss: 114.8074112 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.1008763313293457\n",
      "Epoch: 236, Steps: 1 | Train Loss: 114.7064590 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.1004331111907959\n",
      "Epoch: 237, Steps: 1 | Train Loss: 114.5445786 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10039162635803223\n",
      "Epoch: 238, Steps: 1 | Train Loss: 114.8717041 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10107111930847168\n",
      "Epoch: 239, Steps: 1 | Train Loss: 114.3905029 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.10038566589355469\n",
      "Epoch: 240, Steps: 1 | Train Loss: 114.7406769 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.10023689270019531\n",
      "Epoch: 241, Steps: 1 | Train Loss: 115.0524445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.10031795501708984\n",
      "Epoch: 242, Steps: 1 | Train Loss: 114.6180573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.114410400390625\n",
      "Epoch: 243, Steps: 1 | Train Loss: 114.9415054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10402512550354004\n",
      "Epoch: 244, Steps: 1 | Train Loss: 114.9341965 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.10159969329833984\n",
      "Epoch: 245, Steps: 1 | Train Loss: 114.8707199 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.10034370422363281\n",
      "Epoch: 246, Steps: 1 | Train Loss: 114.8927536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.09995722770690918\n",
      "Epoch: 247, Steps: 1 | Train Loss: 114.9326706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10008120536804199\n",
      "Epoch: 248, Steps: 1 | Train Loss: 115.1833267 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.10005426406860352\n",
      "Epoch: 249, Steps: 1 | Train Loss: 115.1400146 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10024714469909668\n",
      "Epoch: 250, Steps: 1 | Train Loss: 114.7958298 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_18>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.1018671989440918\n",
      "Epoch: 1, Steps: 1 | Train Loss: 120.2999420 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10025167465209961\n",
      "Epoch: 2, Steps: 1 | Train Loss: 118.0338669 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10041332244873047\n",
      "Epoch: 3, Steps: 1 | Train Loss: 117.0648346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10040044784545898\n",
      "Epoch: 4, Steps: 1 | Train Loss: 116.5222397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10020971298217773\n",
      "Epoch: 5, Steps: 1 | Train Loss: 116.2821808 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10053181648254395\n",
      "Epoch: 6, Steps: 1 | Train Loss: 115.7283478 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10029196739196777\n",
      "Epoch: 7, Steps: 1 | Train Loss: 115.5261765 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10044741630554199\n",
      "Epoch: 8, Steps: 1 | Train Loss: 115.3038101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10042238235473633\n",
      "Epoch: 9, Steps: 1 | Train Loss: 115.0666275 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10008478164672852\n",
      "Epoch: 10, Steps: 1 | Train Loss: 115.2241974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10010838508605957\n",
      "Epoch: 11, Steps: 1 | Train Loss: 115.3750763 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10035586357116699\n",
      "Epoch: 12, Steps: 1 | Train Loss: 115.6732788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11312389373779297\n",
      "Epoch: 13, Steps: 1 | Train Loss: 115.1793823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10287928581237793\n",
      "Epoch: 14, Steps: 1 | Train Loss: 114.9962387 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.11156368255615234\n",
      "Epoch: 15, Steps: 1 | Train Loss: 115.0695724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10785722732543945\n",
      "Epoch: 16, Steps: 1 | Train Loss: 115.9971695 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.10761547088623047\n",
      "Epoch: 17, Steps: 1 | Train Loss: 115.7564011 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10726308822631836\n",
      "Epoch: 18, Steps: 1 | Train Loss: 115.5746994 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10752320289611816\n",
      "Epoch: 19, Steps: 1 | Train Loss: 115.0686646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.10739684104919434\n",
      "Epoch: 20, Steps: 1 | Train Loss: 115.3730621 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.10736513137817383\n",
      "Epoch: 21, Steps: 1 | Train Loss: 115.1492462 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10726451873779297\n",
      "Epoch: 22, Steps: 1 | Train Loss: 115.6490707 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.10754203796386719\n",
      "Epoch: 23, Steps: 1 | Train Loss: 115.5464249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.1210780143737793\n",
      "Epoch: 24, Steps: 1 | Train Loss: 114.9162598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11099410057067871\n",
      "Epoch: 25, Steps: 1 | Train Loss: 115.4545288 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.10816049575805664\n",
      "Epoch: 26, Steps: 1 | Train Loss: 115.9406052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.10905718803405762\n",
      "Epoch: 27, Steps: 1 | Train Loss: 115.1112442 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.10733890533447266\n",
      "Epoch: 28, Steps: 1 | Train Loss: 115.3529816 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10728764533996582\n",
      "Epoch: 29, Steps: 1 | Train Loss: 115.5667496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11103391647338867\n",
      "Epoch: 30, Steps: 1 | Train Loss: 115.6406021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.10824203491210938\n",
      "Epoch: 31, Steps: 1 | Train Loss: 114.9764175 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.1079106330871582\n",
      "Epoch: 32, Steps: 1 | Train Loss: 115.5738525 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.12122797966003418\n",
      "Epoch: 33, Steps: 1 | Train Loss: 115.1389313 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.12111258506774902\n",
      "Epoch: 34, Steps: 1 | Train Loss: 115.4553757 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.11610150337219238\n",
      "Epoch: 35, Steps: 1 | Train Loss: 115.7927780 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10957479476928711\n",
      "Epoch: 36, Steps: 1 | Train Loss: 114.9926758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10969901084899902\n",
      "Epoch: 37, Steps: 1 | Train Loss: 115.6698227 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10819387435913086\n",
      "Epoch: 38, Steps: 1 | Train Loss: 115.5433350 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.11218142509460449\n",
      "Epoch: 39, Steps: 1 | Train Loss: 115.3686447 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.11195492744445801\n",
      "Epoch: 40, Steps: 1 | Train Loss: 115.4729996 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10964345932006836\n",
      "Epoch: 41, Steps: 1 | Train Loss: 115.2075119 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.11119484901428223\n",
      "Epoch: 42, Steps: 1 | Train Loss: 116.0855865 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.11076807975769043\n",
      "Epoch: 43, Steps: 1 | Train Loss: 114.8666153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.1129767894744873\n",
      "Epoch: 44, Steps: 1 | Train Loss: 115.3320465 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10923576354980469\n",
      "Epoch: 45, Steps: 1 | Train Loss: 115.6614380 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.10713529586791992\n",
      "Epoch: 46, Steps: 1 | Train Loss: 115.4969711 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.12389254570007324\n",
      "Epoch: 47, Steps: 1 | Train Loss: 115.0462799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.1214742660522461\n",
      "Epoch: 48, Steps: 1 | Train Loss: 115.0918503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.12078571319580078\n",
      "Epoch: 49, Steps: 1 | Train Loss: 115.5176544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.12094783782958984\n",
      "Epoch: 50, Steps: 1 | Train Loss: 115.5655289 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11746001243591309\n",
      "Epoch: 51, Steps: 1 | Train Loss: 115.5414200 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11707711219787598\n",
      "Epoch: 52, Steps: 1 | Train Loss: 115.5474777 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10995006561279297\n",
      "Epoch: 53, Steps: 1 | Train Loss: 115.4655762 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.1086721420288086\n",
      "Epoch: 54, Steps: 1 | Train Loss: 115.6442261 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.1285102367401123\n",
      "Epoch: 55, Steps: 1 | Train Loss: 115.2473145 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1378767490386963\n",
      "Epoch: 56, Steps: 1 | Train Loss: 115.3430557 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10750699043273926\n",
      "Epoch: 57, Steps: 1 | Train Loss: 115.2713623 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10674214363098145\n",
      "Epoch: 58, Steps: 1 | Train Loss: 115.7093506 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.10676121711730957\n",
      "Epoch: 59, Steps: 1 | Train Loss: 115.5509033 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.1071784496307373\n",
      "Epoch: 60, Steps: 1 | Train Loss: 115.5294571 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.10771608352661133\n",
      "Epoch: 61, Steps: 1 | Train Loss: 115.3215332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11130452156066895\n",
      "Epoch: 62, Steps: 1 | Train Loss: 115.4396973 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.12167549133300781\n",
      "Epoch: 63, Steps: 1 | Train Loss: 115.7286758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.1206517219543457\n",
      "Epoch: 64, Steps: 1 | Train Loss: 115.3018951 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.12067174911499023\n",
      "Epoch: 65, Steps: 1 | Train Loss: 115.5010300 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.1152350902557373\n",
      "Epoch: 66, Steps: 1 | Train Loss: 115.1799927 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11404848098754883\n",
      "Epoch: 67, Steps: 1 | Train Loss: 115.9349899 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.1098778247833252\n",
      "Epoch: 68, Steps: 1 | Train Loss: 114.9101791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10845446586608887\n",
      "Epoch: 69, Steps: 1 | Train Loss: 115.2738266 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.1075739860534668\n",
      "Epoch: 70, Steps: 1 | Train Loss: 116.1548843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10758090019226074\n",
      "Epoch: 71, Steps: 1 | Train Loss: 115.9009018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10753440856933594\n",
      "Epoch: 72, Steps: 1 | Train Loss: 115.8766861 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.10695910453796387\n",
      "Epoch: 73, Steps: 1 | Train Loss: 115.1965866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.1071786880493164\n",
      "Epoch: 74, Steps: 1 | Train Loss: 115.4212646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.10720252990722656\n",
      "Epoch: 75, Steps: 1 | Train Loss: 115.1524887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.10723257064819336\n",
      "Epoch: 76, Steps: 1 | Train Loss: 115.1031494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.1073000431060791\n",
      "Epoch: 77, Steps: 1 | Train Loss: 115.4832382 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10738539695739746\n",
      "Epoch: 78, Steps: 1 | Train Loss: 115.9251480 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10764932632446289\n",
      "Epoch: 79, Steps: 1 | Train Loss: 115.3094635 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10775923728942871\n",
      "Epoch: 80, Steps: 1 | Train Loss: 115.4712753 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10732007026672363\n",
      "Epoch: 81, Steps: 1 | Train Loss: 115.5045700 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.12087607383728027\n",
      "Epoch: 82, Steps: 1 | Train Loss: 115.6677170 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.12062382698059082\n",
      "Epoch: 83, Steps: 1 | Train Loss: 115.6125031 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.12085962295532227\n",
      "Epoch: 84, Steps: 1 | Train Loss: 115.9386215 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.1094214916229248\n",
      "Epoch: 85, Steps: 1 | Train Loss: 115.2515869 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.1075441837310791\n",
      "Epoch: 86, Steps: 1 | Train Loss: 115.2059326 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10740184783935547\n",
      "Epoch: 87, Steps: 1 | Train Loss: 115.5014420 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.1071617603302002\n",
      "Epoch: 88, Steps: 1 | Train Loss: 116.0897598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10731124877929688\n",
      "Epoch: 89, Steps: 1 | Train Loss: 115.6480713 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10755109786987305\n",
      "Epoch: 90, Steps: 1 | Train Loss: 115.0990219 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.12074923515319824\n",
      "Epoch: 91, Steps: 1 | Train Loss: 115.7708511 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.12103486061096191\n",
      "Epoch: 92, Steps: 1 | Train Loss: 115.2309341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.12071752548217773\n",
      "Epoch: 93, Steps: 1 | Train Loss: 115.7157135 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.12067127227783203\n",
      "Epoch: 94, Steps: 1 | Train Loss: 115.2950211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.1255781650543213\n",
      "Epoch: 95, Steps: 1 | Train Loss: 115.2813492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.11362648010253906\n",
      "Epoch: 96, Steps: 1 | Train Loss: 115.3480225 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.1094970703125\n",
      "Epoch: 97, Steps: 1 | Train Loss: 115.4867096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10647916793823242\n",
      "Epoch: 98, Steps: 1 | Train Loss: 115.3257828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10869002342224121\n",
      "Epoch: 99, Steps: 1 | Train Loss: 115.3517838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11282014846801758\n",
      "Epoch: 100, Steps: 1 | Train Loss: 115.1303253 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10811996459960938\n",
      "Epoch: 101, Steps: 1 | Train Loss: 115.1828156 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10772895812988281\n",
      "Epoch: 102, Steps: 1 | Train Loss: 115.4482422 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10911917686462402\n",
      "Epoch: 103, Steps: 1 | Train Loss: 115.8255539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.12285923957824707\n",
      "Epoch: 104, Steps: 1 | Train Loss: 115.2663956 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.13060808181762695\n",
      "Epoch: 105, Steps: 1 | Train Loss: 116.3630600 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.11866307258605957\n",
      "Epoch: 106, Steps: 1 | Train Loss: 115.8162842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10623431205749512\n",
      "Epoch: 107, Steps: 1 | Train Loss: 115.1728058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10598635673522949\n",
      "Epoch: 108, Steps: 1 | Train Loss: 115.3391647 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10783839225769043\n",
      "Epoch: 109, Steps: 1 | Train Loss: 115.0054474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.10727548599243164\n",
      "Epoch: 110, Steps: 1 | Train Loss: 115.7372360 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11469793319702148\n",
      "Epoch: 111, Steps: 1 | Train Loss: 115.9197006 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.11198782920837402\n",
      "Epoch: 112, Steps: 1 | Train Loss: 115.4862823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11307835578918457\n",
      "Epoch: 113, Steps: 1 | Train Loss: 115.7842789 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10579633712768555\n",
      "Epoch: 114, Steps: 1 | Train Loss: 115.4572296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11542487144470215\n",
      "Epoch: 115, Steps: 1 | Train Loss: 115.8082809 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11242222785949707\n",
      "Epoch: 116, Steps: 1 | Train Loss: 115.5016403 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.12730073928833008\n",
      "Epoch: 117, Steps: 1 | Train Loss: 115.5141144 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.11339449882507324\n",
      "Epoch: 118, Steps: 1 | Train Loss: 115.5001602 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.11319160461425781\n",
      "Epoch: 119, Steps: 1 | Train Loss: 115.2971420 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.10842370986938477\n",
      "Epoch: 120, Steps: 1 | Train Loss: 115.7076340 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.10886359214782715\n",
      "Epoch: 121, Steps: 1 | Train Loss: 115.9303131 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10545468330383301\n",
      "Epoch: 122, Steps: 1 | Train Loss: 115.7548752 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.11544919013977051\n",
      "Epoch: 123, Steps: 1 | Train Loss: 115.8338623 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.1165781021118164\n",
      "Epoch: 124, Steps: 1 | Train Loss: 115.8411865 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.11456513404846191\n",
      "Epoch: 125, Steps: 1 | Train Loss: 115.7827911 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.11356329917907715\n",
      "Epoch: 126, Steps: 1 | Train Loss: 115.7020798 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.1263115406036377\n",
      "Epoch: 127, Steps: 1 | Train Loss: 115.1237793 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12514019012451172\n",
      "Epoch: 128, Steps: 1 | Train Loss: 115.4918213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.11816525459289551\n",
      "Epoch: 129, Steps: 1 | Train Loss: 115.8928223 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11349606513977051\n",
      "Epoch: 130, Steps: 1 | Train Loss: 115.8506851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11658787727355957\n",
      "Epoch: 131, Steps: 1 | Train Loss: 115.6608200 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11376261711120605\n",
      "Epoch: 132, Steps: 1 | Train Loss: 114.8417740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11412906646728516\n",
      "Epoch: 133, Steps: 1 | Train Loss: 115.1693726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.11211371421813965\n",
      "Epoch: 134, Steps: 1 | Train Loss: 115.6203232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10741806030273438\n",
      "Epoch: 135, Steps: 1 | Train Loss: 115.8733902 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10741400718688965\n",
      "Epoch: 136, Steps: 1 | Train Loss: 115.5554428 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.10747218132019043\n",
      "Epoch: 137, Steps: 1 | Train Loss: 115.4219742 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.12155318260192871\n",
      "Epoch: 138, Steps: 1 | Train Loss: 115.8088608 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11783838272094727\n",
      "Epoch: 139, Steps: 1 | Train Loss: 115.4231491 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.11014056205749512\n",
      "Epoch: 140, Steps: 1 | Train Loss: 115.4069824 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.10840201377868652\n",
      "Epoch: 141, Steps: 1 | Train Loss: 114.9846420 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.10777473449707031\n",
      "Epoch: 142, Steps: 1 | Train Loss: 115.7390518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.10761189460754395\n",
      "Epoch: 143, Steps: 1 | Train Loss: 115.1312485 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.10730791091918945\n",
      "Epoch: 144, Steps: 1 | Train Loss: 115.3646851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10740971565246582\n",
      "Epoch: 145, Steps: 1 | Train Loss: 115.8620605 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10748577117919922\n",
      "Epoch: 146, Steps: 1 | Train Loss: 115.2453384 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10763335227966309\n",
      "Epoch: 147, Steps: 1 | Train Loss: 115.2406540 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11984825134277344\n",
      "Epoch: 148, Steps: 1 | Train Loss: 115.0974045 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11031842231750488\n",
      "Epoch: 149, Steps: 1 | Train Loss: 115.6930542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10891890525817871\n",
      "Epoch: 150, Steps: 1 | Train Loss: 115.5111618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10816574096679688\n",
      "Epoch: 151, Steps: 1 | Train Loss: 115.3240967 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.1076502799987793\n",
      "Epoch: 152, Steps: 1 | Train Loss: 115.4086075 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10791897773742676\n",
      "Epoch: 153, Steps: 1 | Train Loss: 115.2733078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10956621170043945\n",
      "Epoch: 154, Steps: 1 | Train Loss: 115.2595978 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10959434509277344\n",
      "Epoch: 155, Steps: 1 | Train Loss: 115.7174835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11890959739685059\n",
      "Epoch: 156, Steps: 1 | Train Loss: 115.5674438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.12382054328918457\n",
      "Epoch: 157, Steps: 1 | Train Loss: 115.9575806 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.11673402786254883\n",
      "Epoch: 158, Steps: 1 | Train Loss: 115.4906998 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.12845921516418457\n",
      "Epoch: 159, Steps: 1 | Train Loss: 116.1033096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11381745338439941\n",
      "Epoch: 160, Steps: 1 | Train Loss: 115.5670395 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.11306452751159668\n",
      "Epoch: 161, Steps: 1 | Train Loss: 115.4376755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.11083245277404785\n",
      "Epoch: 162, Steps: 1 | Train Loss: 115.6640854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.11007833480834961\n",
      "Epoch: 163, Steps: 1 | Train Loss: 115.8392563 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.1103215217590332\n",
      "Epoch: 164, Steps: 1 | Train Loss: 115.2360611 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10729408264160156\n",
      "Epoch: 165, Steps: 1 | Train Loss: 115.4074631 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11009812355041504\n",
      "Epoch: 166, Steps: 1 | Train Loss: 115.5413361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11082720756530762\n",
      "Epoch: 167, Steps: 1 | Train Loss: 115.8851547 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.1096043586730957\n",
      "Epoch: 168, Steps: 1 | Train Loss: 115.7427902 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11023116111755371\n",
      "Epoch: 169, Steps: 1 | Train Loss: 115.6857071 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.12332653999328613\n",
      "Epoch: 170, Steps: 1 | Train Loss: 116.1662369 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.1158132553100586\n",
      "Epoch: 171, Steps: 1 | Train Loss: 115.6105270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11198186874389648\n",
      "Epoch: 172, Steps: 1 | Train Loss: 115.2743530 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11126708984375\n",
      "Epoch: 173, Steps: 1 | Train Loss: 115.5196686 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11129021644592285\n",
      "Epoch: 174, Steps: 1 | Train Loss: 116.0314941 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.1100759506225586\n",
      "Epoch: 175, Steps: 1 | Train Loss: 115.1699753 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.11019229888916016\n",
      "Epoch: 176, Steps: 1 | Train Loss: 115.2834702 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10978484153747559\n",
      "Epoch: 177, Steps: 1 | Train Loss: 115.3493881 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10993075370788574\n",
      "Epoch: 178, Steps: 1 | Train Loss: 115.7498398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10976219177246094\n",
      "Epoch: 179, Steps: 1 | Train Loss: 115.3997574 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11186504364013672\n",
      "Epoch: 180, Steps: 1 | Train Loss: 115.3379135 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11935973167419434\n",
      "Epoch: 181, Steps: 1 | Train Loss: 115.2846451 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11538982391357422\n",
      "Epoch: 182, Steps: 1 | Train Loss: 115.6460953 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10851907730102539\n",
      "Epoch: 183, Steps: 1 | Train Loss: 115.8769684 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11916279792785645\n",
      "Epoch: 184, Steps: 1 | Train Loss: 114.8764191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.1104278564453125\n",
      "Epoch: 185, Steps: 1 | Train Loss: 115.8481216 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11015915870666504\n",
      "Epoch: 186, Steps: 1 | Train Loss: 115.0875854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.1098780632019043\n",
      "Epoch: 187, Steps: 1 | Train Loss: 115.8130264 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10975074768066406\n",
      "Epoch: 188, Steps: 1 | Train Loss: 115.5065842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.10958123207092285\n",
      "Epoch: 189, Steps: 1 | Train Loss: 115.6191940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.1112055778503418\n",
      "Epoch: 190, Steps: 1 | Train Loss: 114.8144302 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.11020469665527344\n",
      "Epoch: 191, Steps: 1 | Train Loss: 115.1797104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.12404894828796387\n",
      "Epoch: 192, Steps: 1 | Train Loss: 115.5836105 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.11423206329345703\n",
      "Epoch: 193, Steps: 1 | Train Loss: 115.4107590 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.1120150089263916\n",
      "Epoch: 194, Steps: 1 | Train Loss: 115.7069321 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11141467094421387\n",
      "Epoch: 195, Steps: 1 | Train Loss: 115.4050674 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11025547981262207\n",
      "Epoch: 196, Steps: 1 | Train Loss: 115.4400406 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11000967025756836\n",
      "Epoch: 197, Steps: 1 | Train Loss: 115.5213394 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.10958361625671387\n",
      "Epoch: 198, Steps: 1 | Train Loss: 116.0522003 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11012864112854004\n",
      "Epoch: 199, Steps: 1 | Train Loss: 115.7722702 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11009526252746582\n",
      "Epoch: 200, Steps: 1 | Train Loss: 115.5547028 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10800385475158691\n",
      "Epoch: 201, Steps: 1 | Train Loss: 115.2082520 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.12173676490783691\n",
      "Epoch: 202, Steps: 1 | Train Loss: 115.4306641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11158895492553711\n",
      "Epoch: 203, Steps: 1 | Train Loss: 115.7470245 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.10925173759460449\n",
      "Epoch: 204, Steps: 1 | Train Loss: 115.6658707 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.10817503929138184\n",
      "Epoch: 205, Steps: 1 | Train Loss: 115.0625534 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10732269287109375\n",
      "Epoch: 206, Steps: 1 | Train Loss: 115.3935089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10704350471496582\n",
      "Epoch: 207, Steps: 1 | Train Loss: 115.5171432 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.10727787017822266\n",
      "Epoch: 208, Steps: 1 | Train Loss: 115.7525406 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11071205139160156\n",
      "Epoch: 209, Steps: 1 | Train Loss: 115.9122543 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.10659217834472656\n",
      "Epoch: 210, Steps: 1 | Train Loss: 115.9797592 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.10732388496398926\n",
      "Epoch: 211, Steps: 1 | Train Loss: 115.3082504 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.10707664489746094\n",
      "Epoch: 212, Steps: 1 | Train Loss: 115.5382004 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.1072242259979248\n",
      "Epoch: 213, Steps: 1 | Train Loss: 115.4832306 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11978316307067871\n",
      "Epoch: 214, Steps: 1 | Train Loss: 115.8508835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.12077808380126953\n",
      "Epoch: 215, Steps: 1 | Train Loss: 115.4931793 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11818766593933105\n",
      "Epoch: 216, Steps: 1 | Train Loss: 115.3158493 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.1105806827545166\n",
      "Epoch: 217, Steps: 1 | Train Loss: 115.0330124 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10886907577514648\n",
      "Epoch: 218, Steps: 1 | Train Loss: 115.3544235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.1081702709197998\n",
      "Epoch: 219, Steps: 1 | Train Loss: 115.6872101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10790061950683594\n",
      "Epoch: 220, Steps: 1 | Train Loss: 115.1274643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.10743498802185059\n",
      "Epoch: 221, Steps: 1 | Train Loss: 115.7688217 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10731673240661621\n",
      "Epoch: 222, Steps: 1 | Train Loss: 115.3934860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10734820365905762\n",
      "Epoch: 223, Steps: 1 | Train Loss: 115.3922272 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.10726618766784668\n",
      "Epoch: 224, Steps: 1 | Train Loss: 115.1587906 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.10716462135314941\n",
      "Epoch: 225, Steps: 1 | Train Loss: 115.3778839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.10707592964172363\n",
      "Epoch: 226, Steps: 1 | Train Loss: 115.4775620 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10717153549194336\n",
      "Epoch: 227, Steps: 1 | Train Loss: 115.5413132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.10756850242614746\n",
      "Epoch: 228, Steps: 1 | Train Loss: 115.6949234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.12060427665710449\n",
      "Epoch: 229, Steps: 1 | Train Loss: 115.7024078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.1203465461730957\n",
      "Epoch: 230, Steps: 1 | Train Loss: 115.6831894 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.12029767036437988\n",
      "Epoch: 231, Steps: 1 | Train Loss: 115.5499496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.11331677436828613\n",
      "Epoch: 232, Steps: 1 | Train Loss: 115.5460434 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.1179661750793457\n",
      "Epoch: 233, Steps: 1 | Train Loss: 115.3913574 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.13848304748535156\n",
      "Epoch: 234, Steps: 1 | Train Loss: 115.1041031 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.1178138256072998\n",
      "Epoch: 235, Steps: 1 | Train Loss: 115.2264175 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.13194060325622559\n",
      "Epoch: 236, Steps: 1 | Train Loss: 115.6346359 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.12210869789123535\n",
      "Epoch: 237, Steps: 1 | Train Loss: 115.0312881 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.1304633617401123\n",
      "Epoch: 238, Steps: 1 | Train Loss: 115.1816406 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.14662480354309082\n",
      "Epoch: 239, Steps: 1 | Train Loss: 115.4248886 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.14218664169311523\n",
      "Epoch: 240, Steps: 1 | Train Loss: 115.4227524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.12877702713012695\n",
      "Epoch: 241, Steps: 1 | Train Loss: 115.5830307 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.12385058403015137\n",
      "Epoch: 242, Steps: 1 | Train Loss: 115.3832779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.14571881294250488\n",
      "Epoch: 243, Steps: 1 | Train Loss: 115.6340332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.13821053504943848\n",
      "Epoch: 244, Steps: 1 | Train Loss: 115.4195557 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.13295459747314453\n",
      "Epoch: 245, Steps: 1 | Train Loss: 115.6709366 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.12315630912780762\n",
      "Epoch: 246, Steps: 1 | Train Loss: 115.6500626 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.12720012664794922\n",
      "Epoch: 247, Steps: 1 | Train Loss: 115.8809357 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.1281747817993164\n",
      "Epoch: 248, Steps: 1 | Train Loss: 114.9535522 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.12064051628112793\n",
      "Epoch: 249, Steps: 1 | Train Loss: 115.5985336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.12689852714538574\n",
      "Epoch: 250, Steps: 1 | Train Loss: 115.3734665 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_19>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.12589192390441895\n",
      "Epoch: 1, Steps: 1 | Train Loss: 118.3275528 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.12053966522216797\n",
      "Epoch: 2, Steps: 1 | Train Loss: 117.1294937 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11001276969909668\n",
      "Epoch: 3, Steps: 1 | Train Loss: 114.1597443 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10904407501220703\n",
      "Epoch: 4, Steps: 1 | Train Loss: 113.5875244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10943818092346191\n",
      "Epoch: 5, Steps: 1 | Train Loss: 113.0781479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.10970616340637207\n",
      "Epoch: 6, Steps: 1 | Train Loss: 113.6524658 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11891484260559082\n",
      "Epoch: 7, Steps: 1 | Train Loss: 112.5924835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.1101694107055664\n",
      "Epoch: 8, Steps: 1 | Train Loss: 113.0769043 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10881209373474121\n",
      "Epoch: 9, Steps: 1 | Train Loss: 112.6571579 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10782456398010254\n",
      "Epoch: 10, Steps: 1 | Train Loss: 113.1630173 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.1074988842010498\n",
      "Epoch: 11, Steps: 1 | Train Loss: 112.2525864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10705304145812988\n",
      "Epoch: 12, Steps: 1 | Train Loss: 112.5516129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10718870162963867\n",
      "Epoch: 13, Steps: 1 | Train Loss: 112.8635788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10682082176208496\n",
      "Epoch: 14, Steps: 1 | Train Loss: 112.6280289 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10718512535095215\n",
      "Epoch: 15, Steps: 1 | Train Loss: 112.9946518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10703468322753906\n",
      "Epoch: 16, Steps: 1 | Train Loss: 112.6456299 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.10699105262756348\n",
      "Epoch: 17, Steps: 1 | Train Loss: 113.2051544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11089849472045898\n",
      "Epoch: 18, Steps: 1 | Train Loss: 112.6127930 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10845279693603516\n",
      "Epoch: 19, Steps: 1 | Train Loss: 113.2020645 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.10739994049072266\n",
      "Epoch: 20, Steps: 1 | Train Loss: 112.9726791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.1074521541595459\n",
      "Epoch: 21, Steps: 1 | Train Loss: 113.2705612 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10741472244262695\n",
      "Epoch: 22, Steps: 1 | Train Loss: 112.9434204 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.10732507705688477\n",
      "Epoch: 23, Steps: 1 | Train Loss: 112.8794479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.10743308067321777\n",
      "Epoch: 24, Steps: 1 | Train Loss: 112.8386002 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.1211087703704834\n",
      "Epoch: 25, Steps: 1 | Train Loss: 113.2302246 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12116551399230957\n",
      "Epoch: 26, Steps: 1 | Train Loss: 113.3757095 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11075401306152344\n",
      "Epoch: 27, Steps: 1 | Train Loss: 112.9736938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.1087348461151123\n",
      "Epoch: 28, Steps: 1 | Train Loss: 113.4222336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10757756233215332\n",
      "Epoch: 29, Steps: 1 | Train Loss: 113.1532593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.10736250877380371\n",
      "Epoch: 30, Steps: 1 | Train Loss: 112.8401108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.10800290107727051\n",
      "Epoch: 31, Steps: 1 | Train Loss: 113.0163803 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10757780075073242\n",
      "Epoch: 32, Steps: 1 | Train Loss: 113.1779404 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10853290557861328\n",
      "Epoch: 33, Steps: 1 | Train Loss: 112.8079224 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10772061347961426\n",
      "Epoch: 34, Steps: 1 | Train Loss: 113.2674332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10717129707336426\n",
      "Epoch: 35, Steps: 1 | Train Loss: 113.2078629 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.11653256416320801\n",
      "Epoch: 36, Steps: 1 | Train Loss: 113.1617355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10985445976257324\n",
      "Epoch: 37, Steps: 1 | Train Loss: 112.7524338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10866355895996094\n",
      "Epoch: 38, Steps: 1 | Train Loss: 112.6772079 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10756063461303711\n",
      "Epoch: 39, Steps: 1 | Train Loss: 112.9639893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10756897926330566\n",
      "Epoch: 40, Steps: 1 | Train Loss: 112.6857529 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10761260986328125\n",
      "Epoch: 41, Steps: 1 | Train Loss: 112.9021530 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10802984237670898\n",
      "Epoch: 42, Steps: 1 | Train Loss: 112.8326416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10747361183166504\n",
      "Epoch: 43, Steps: 1 | Train Loss: 112.4189072 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10766053199768066\n",
      "Epoch: 44, Steps: 1 | Train Loss: 112.0556641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.11887478828430176\n",
      "Epoch: 45, Steps: 1 | Train Loss: 112.7260361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11035871505737305\n",
      "Epoch: 46, Steps: 1 | Train Loss: 113.4547501 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.1085662841796875\n",
      "Epoch: 47, Steps: 1 | Train Loss: 112.4172974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.10764312744140625\n",
      "Epoch: 48, Steps: 1 | Train Loss: 113.6665039 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.10789990425109863\n",
      "Epoch: 49, Steps: 1 | Train Loss: 112.9314117 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.10782027244567871\n",
      "Epoch: 50, Steps: 1 | Train Loss: 112.7201691 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.10816502571105957\n",
      "Epoch: 51, Steps: 1 | Train Loss: 112.6789322 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10792398452758789\n",
      "Epoch: 52, Steps: 1 | Train Loss: 112.9073029 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10781645774841309\n",
      "Epoch: 53, Steps: 1 | Train Loss: 113.2554932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.1078946590423584\n",
      "Epoch: 54, Steps: 1 | Train Loss: 113.2233887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.11768579483032227\n",
      "Epoch: 55, Steps: 1 | Train Loss: 113.4500961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.11026954650878906\n",
      "Epoch: 56, Steps: 1 | Train Loss: 113.0249176 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10882043838500977\n",
      "Epoch: 57, Steps: 1 | Train Loss: 112.6248093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10789752006530762\n",
      "Epoch: 58, Steps: 1 | Train Loss: 112.2853775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.10772395133972168\n",
      "Epoch: 59, Steps: 1 | Train Loss: 112.4961929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.1078031063079834\n",
      "Epoch: 60, Steps: 1 | Train Loss: 113.3385773 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.10785889625549316\n",
      "Epoch: 61, Steps: 1 | Train Loss: 113.1656036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.10796618461608887\n",
      "Epoch: 62, Steps: 1 | Train Loss: 112.2812119 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.12870121002197266\n",
      "Epoch: 63, Steps: 1 | Train Loss: 112.1642456 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.11225128173828125\n",
      "Epoch: 64, Steps: 1 | Train Loss: 113.0173950 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11360788345336914\n",
      "Epoch: 65, Steps: 1 | Train Loss: 113.1091461 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11065530776977539\n",
      "Epoch: 66, Steps: 1 | Train Loss: 112.5237579 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.12545275688171387\n",
      "Epoch: 67, Steps: 1 | Train Loss: 113.0225830 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.11091113090515137\n",
      "Epoch: 68, Steps: 1 | Train Loss: 112.3601303 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.11089944839477539\n",
      "Epoch: 69, Steps: 1 | Train Loss: 113.2442856 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11171317100524902\n",
      "Epoch: 70, Steps: 1 | Train Loss: 112.5085831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.11080265045166016\n",
      "Epoch: 71, Steps: 1 | Train Loss: 113.2821274 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10742020606994629\n",
      "Epoch: 72, Steps: 1 | Train Loss: 112.8472214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.1208951473236084\n",
      "Epoch: 73, Steps: 1 | Train Loss: 112.7231598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12107610702514648\n",
      "Epoch: 74, Steps: 1 | Train Loss: 113.0318756 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11325359344482422\n",
      "Epoch: 75, Steps: 1 | Train Loss: 112.7507095 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.10940265655517578\n",
      "Epoch: 76, Steps: 1 | Train Loss: 113.3038101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.1083214282989502\n",
      "Epoch: 77, Steps: 1 | Train Loss: 112.9850464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10781097412109375\n",
      "Epoch: 78, Steps: 1 | Train Loss: 112.5085983 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.1072540283203125\n",
      "Epoch: 79, Steps: 1 | Train Loss: 113.0516968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10678553581237793\n",
      "Epoch: 80, Steps: 1 | Train Loss: 113.0777817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10932517051696777\n",
      "Epoch: 81, Steps: 1 | Train Loss: 112.8777084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11021304130554199\n",
      "Epoch: 82, Steps: 1 | Train Loss: 112.9568634 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.11127543449401855\n",
      "Epoch: 83, Steps: 1 | Train Loss: 112.6869125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11070394515991211\n",
      "Epoch: 84, Steps: 1 | Train Loss: 113.1786118 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.12447309494018555\n",
      "Epoch: 85, Steps: 1 | Train Loss: 112.9748077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.12491488456726074\n",
      "Epoch: 86, Steps: 1 | Train Loss: 112.7625961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.11486411094665527\n",
      "Epoch: 87, Steps: 1 | Train Loss: 113.1467667 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.1160273551940918\n",
      "Epoch: 88, Steps: 1 | Train Loss: 112.9817429 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11695146560668945\n",
      "Epoch: 89, Steps: 1 | Train Loss: 112.2444687 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11510157585144043\n",
      "Epoch: 90, Steps: 1 | Train Loss: 113.0267334 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10891199111938477\n",
      "Epoch: 91, Steps: 1 | Train Loss: 112.7161407 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10812997817993164\n",
      "Epoch: 92, Steps: 1 | Train Loss: 113.0274277 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10834503173828125\n",
      "Epoch: 93, Steps: 1 | Train Loss: 113.0773468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10828351974487305\n",
      "Epoch: 94, Steps: 1 | Train Loss: 112.7305908 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10830521583557129\n",
      "Epoch: 95, Steps: 1 | Train Loss: 112.6957550 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10863041877746582\n",
      "Epoch: 96, Steps: 1 | Train Loss: 112.9094620 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10804605484008789\n",
      "Epoch: 97, Steps: 1 | Train Loss: 112.1127853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.11993098258972168\n",
      "Epoch: 98, Steps: 1 | Train Loss: 112.9557419 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.11060810089111328\n",
      "Epoch: 99, Steps: 1 | Train Loss: 113.2607422 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10988211631774902\n",
      "Epoch: 100, Steps: 1 | Train Loss: 112.9401855 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10919880867004395\n",
      "Epoch: 101, Steps: 1 | Train Loss: 112.9419174 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10840249061584473\n",
      "Epoch: 102, Steps: 1 | Train Loss: 112.9528732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.11020183563232422\n",
      "Epoch: 103, Steps: 1 | Train Loss: 112.8673096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10918211936950684\n",
      "Epoch: 104, Steps: 1 | Train Loss: 112.9843369 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10866236686706543\n",
      "Epoch: 105, Steps: 1 | Train Loss: 112.7639389 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10753059387207031\n",
      "Epoch: 106, Steps: 1 | Train Loss: 113.2536392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10767412185668945\n",
      "Epoch: 107, Steps: 1 | Train Loss: 112.8210602 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.12153220176696777\n",
      "Epoch: 108, Steps: 1 | Train Loss: 112.6933517 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11051082611083984\n",
      "Epoch: 109, Steps: 1 | Train Loss: 112.8643341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.10872316360473633\n",
      "Epoch: 110, Steps: 1 | Train Loss: 112.7164230 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.10738348960876465\n",
      "Epoch: 111, Steps: 1 | Train Loss: 113.0147247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10731959342956543\n",
      "Epoch: 112, Steps: 1 | Train Loss: 112.9351120 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10745072364807129\n",
      "Epoch: 113, Steps: 1 | Train Loss: 112.5831223 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10740423202514648\n",
      "Epoch: 114, Steps: 1 | Train Loss: 113.3785172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10717535018920898\n",
      "Epoch: 115, Steps: 1 | Train Loss: 112.5038071 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10714912414550781\n",
      "Epoch: 116, Steps: 1 | Train Loss: 113.1192017 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.11528921127319336\n",
      "Epoch: 117, Steps: 1 | Train Loss: 112.4737091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10933637619018555\n",
      "Epoch: 118, Steps: 1 | Train Loss: 112.9868927 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10867571830749512\n",
      "Epoch: 119, Steps: 1 | Train Loss: 112.5721588 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1081533432006836\n",
      "Epoch: 120, Steps: 1 | Train Loss: 112.8587646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.10779976844787598\n",
      "Epoch: 121, Steps: 1 | Train Loss: 112.8582764 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10849857330322266\n",
      "Epoch: 122, Steps: 1 | Train Loss: 113.0815582 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10729289054870605\n",
      "Epoch: 123, Steps: 1 | Train Loss: 112.7574081 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.10735917091369629\n",
      "Epoch: 124, Steps: 1 | Train Loss: 113.4989014 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.10761547088623047\n",
      "Epoch: 125, Steps: 1 | Train Loss: 112.5681915 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.10708332061767578\n",
      "Epoch: 126, Steps: 1 | Train Loss: 113.1334991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.12098383903503418\n",
      "Epoch: 127, Steps: 1 | Train Loss: 112.1300659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12082934379577637\n",
      "Epoch: 128, Steps: 1 | Train Loss: 112.7857666 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.12098979949951172\n",
      "Epoch: 129, Steps: 1 | Train Loss: 113.5231934 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11624765396118164\n",
      "Epoch: 130, Steps: 1 | Train Loss: 113.2527084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.10973024368286133\n",
      "Epoch: 131, Steps: 1 | Train Loss: 112.5979385 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10831832885742188\n",
      "Epoch: 132, Steps: 1 | Train Loss: 112.7681885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.10750937461853027\n",
      "Epoch: 133, Steps: 1 | Train Loss: 112.8241196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10757017135620117\n",
      "Epoch: 134, Steps: 1 | Train Loss: 112.3075562 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10720658302307129\n",
      "Epoch: 135, Steps: 1 | Train Loss: 112.9401627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10728669166564941\n",
      "Epoch: 136, Steps: 1 | Train Loss: 113.4436035 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.10720562934875488\n",
      "Epoch: 137, Steps: 1 | Train Loss: 112.9554825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.1078481674194336\n",
      "Epoch: 138, Steps: 1 | Train Loss: 112.7723618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.10731649398803711\n",
      "Epoch: 139, Steps: 1 | Train Loss: 112.8523483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10736465454101562\n",
      "Epoch: 140, Steps: 1 | Train Loss: 112.9329987 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.10759449005126953\n",
      "Epoch: 141, Steps: 1 | Train Loss: 113.0152588 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.12122750282287598\n",
      "Epoch: 142, Steps: 1 | Train Loss: 112.6622314 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.12117457389831543\n",
      "Epoch: 143, Steps: 1 | Train Loss: 112.8092270 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11444735527038574\n",
      "Epoch: 144, Steps: 1 | Train Loss: 112.9809341 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10924339294433594\n",
      "Epoch: 145, Steps: 1 | Train Loss: 113.1214218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10818171501159668\n",
      "Epoch: 146, Steps: 1 | Train Loss: 112.7744370 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10775232315063477\n",
      "Epoch: 147, Steps: 1 | Train Loss: 112.8090363 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10718369483947754\n",
      "Epoch: 148, Steps: 1 | Train Loss: 112.9994431 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10669755935668945\n",
      "Epoch: 149, Steps: 1 | Train Loss: 112.7275543 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10714840888977051\n",
      "Epoch: 150, Steps: 1 | Train Loss: 113.0108643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10725784301757812\n",
      "Epoch: 151, Steps: 1 | Train Loss: 112.7396240 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.1072695255279541\n",
      "Epoch: 152, Steps: 1 | Train Loss: 112.8092575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.1072998046875\n",
      "Epoch: 153, Steps: 1 | Train Loss: 112.6940842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10719180107116699\n",
      "Epoch: 154, Steps: 1 | Train Loss: 112.6434097 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.11020970344543457\n",
      "Epoch: 155, Steps: 1 | Train Loss: 112.6215210 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10901403427124023\n",
      "Epoch: 156, Steps: 1 | Train Loss: 112.3909454 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.1080014705657959\n",
      "Epoch: 157, Steps: 1 | Train Loss: 112.8094864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10740971565246582\n",
      "Epoch: 158, Steps: 1 | Train Loss: 112.5144882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10740232467651367\n",
      "Epoch: 159, Steps: 1 | Train Loss: 112.9513321 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.10695862770080566\n",
      "Epoch: 160, Steps: 1 | Train Loss: 113.1165924 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10715413093566895\n",
      "Epoch: 161, Steps: 1 | Train Loss: 113.0916061 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10733294486999512\n",
      "Epoch: 162, Steps: 1 | Train Loss: 112.8529282 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10731315612792969\n",
      "Epoch: 163, Steps: 1 | Train Loss: 112.7573242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10734820365905762\n",
      "Epoch: 164, Steps: 1 | Train Loss: 112.9388199 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.10724878311157227\n",
      "Epoch: 165, Steps: 1 | Train Loss: 112.7442169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.10730338096618652\n",
      "Epoch: 166, Steps: 1 | Train Loss: 113.1577148 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.12049722671508789\n",
      "Epoch: 167, Steps: 1 | Train Loss: 113.0037842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.12077522277832031\n",
      "Epoch: 168, Steps: 1 | Train Loss: 112.9351807 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.12061691284179688\n",
      "Epoch: 169, Steps: 1 | Train Loss: 112.9651642 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11362171173095703\n",
      "Epoch: 170, Steps: 1 | Train Loss: 112.7231216 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.10933732986450195\n",
      "Epoch: 171, Steps: 1 | Train Loss: 113.0301437 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.10821890830993652\n",
      "Epoch: 172, Steps: 1 | Train Loss: 113.0305939 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.1073598861694336\n",
      "Epoch: 173, Steps: 1 | Train Loss: 112.4584503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10712456703186035\n",
      "Epoch: 174, Steps: 1 | Train Loss: 112.1130371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10740971565246582\n",
      "Epoch: 175, Steps: 1 | Train Loss: 112.7570724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.1070866584777832\n",
      "Epoch: 176, Steps: 1 | Train Loss: 113.0113449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10721468925476074\n",
      "Epoch: 177, Steps: 1 | Train Loss: 112.9871750 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10749053955078125\n",
      "Epoch: 178, Steps: 1 | Train Loss: 113.1981201 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10705351829528809\n",
      "Epoch: 179, Steps: 1 | Train Loss: 112.9057159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10870671272277832\n",
      "Epoch: 180, Steps: 1 | Train Loss: 113.3648071 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.10769772529602051\n",
      "Epoch: 181, Steps: 1 | Train Loss: 112.9668732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10690999031066895\n",
      "Epoch: 182, Steps: 1 | Train Loss: 112.8799820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.12127804756164551\n",
      "Epoch: 183, Steps: 1 | Train Loss: 113.5030746 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11114072799682617\n",
      "Epoch: 184, Steps: 1 | Train Loss: 113.4556046 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.10838484764099121\n",
      "Epoch: 185, Steps: 1 | Train Loss: 112.6636505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.10796284675598145\n",
      "Epoch: 186, Steps: 1 | Train Loss: 112.8014450 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10704708099365234\n",
      "Epoch: 187, Steps: 1 | Train Loss: 112.4175797 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10706400871276855\n",
      "Epoch: 188, Steps: 1 | Train Loss: 113.0391998 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.1070704460144043\n",
      "Epoch: 189, Steps: 1 | Train Loss: 112.2998657 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10715413093566895\n",
      "Epoch: 190, Steps: 1 | Train Loss: 113.3305283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.11864018440246582\n",
      "Epoch: 191, Steps: 1 | Train Loss: 112.8393478 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10976862907409668\n",
      "Epoch: 192, Steps: 1 | Train Loss: 112.8030548 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.11795163154602051\n",
      "Epoch: 193, Steps: 1 | Train Loss: 112.9980087 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.1118924617767334\n",
      "Epoch: 194, Steps: 1 | Train Loss: 112.4714127 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10731148719787598\n",
      "Epoch: 195, Steps: 1 | Train Loss: 113.0085678 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11800122261047363\n",
      "Epoch: 196, Steps: 1 | Train Loss: 112.5366211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11700701713562012\n",
      "Epoch: 197, Steps: 1 | Train Loss: 112.6347427 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.10892868041992188\n",
      "Epoch: 198, Steps: 1 | Train Loss: 113.3449020 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11677241325378418\n",
      "Epoch: 199, Steps: 1 | Train Loss: 112.6219254 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11454415321350098\n",
      "Epoch: 200, Steps: 1 | Train Loss: 112.6280899 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.12628173828125\n",
      "Epoch: 201, Steps: 1 | Train Loss: 112.0020523 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.12120723724365234\n",
      "Epoch: 202, Steps: 1 | Train Loss: 112.7780533 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.10914468765258789\n",
      "Epoch: 203, Steps: 1 | Train Loss: 112.9007950 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.1074380874633789\n",
      "Epoch: 204, Steps: 1 | Train Loss: 112.5938263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.10686016082763672\n",
      "Epoch: 205, Steps: 1 | Train Loss: 112.7533188 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10728144645690918\n",
      "Epoch: 206, Steps: 1 | Train Loss: 113.1715927 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10764861106872559\n",
      "Epoch: 207, Steps: 1 | Train Loss: 112.7293701 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.1073915958404541\n",
      "Epoch: 208, Steps: 1 | Train Loss: 112.6291046 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10732388496398926\n",
      "Epoch: 209, Steps: 1 | Train Loss: 113.1869659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.10765266418457031\n",
      "Epoch: 210, Steps: 1 | Train Loss: 112.8244781 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.12096452713012695\n",
      "Epoch: 211, Steps: 1 | Train Loss: 112.8766632 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.12103486061096191\n",
      "Epoch: 212, Steps: 1 | Train Loss: 112.9718018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11072683334350586\n",
      "Epoch: 213, Steps: 1 | Train Loss: 112.6836166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10863900184631348\n",
      "Epoch: 214, Steps: 1 | Train Loss: 112.7122116 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10843253135681152\n",
      "Epoch: 215, Steps: 1 | Train Loss: 113.2754440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10744881629943848\n",
      "Epoch: 216, Steps: 1 | Train Loss: 112.8610764 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10713362693786621\n",
      "Epoch: 217, Steps: 1 | Train Loss: 112.9588852 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10726666450500488\n",
      "Epoch: 218, Steps: 1 | Train Loss: 112.9273224 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.11109161376953125\n",
      "Epoch: 219, Steps: 1 | Train Loss: 112.4477310 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10967326164245605\n",
      "Epoch: 220, Steps: 1 | Train Loss: 113.4473877 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.132737398147583\n",
      "Epoch: 221, Steps: 1 | Train Loss: 112.7998810 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.11945796012878418\n",
      "Epoch: 222, Steps: 1 | Train Loss: 112.8066940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.12113118171691895\n",
      "Epoch: 223, Steps: 1 | Train Loss: 112.8147507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11943721771240234\n",
      "Epoch: 224, Steps: 1 | Train Loss: 113.1841354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11894083023071289\n",
      "Epoch: 225, Steps: 1 | Train Loss: 113.0123291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11810517311096191\n",
      "Epoch: 226, Steps: 1 | Train Loss: 112.8201904 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11911582946777344\n",
      "Epoch: 227, Steps: 1 | Train Loss: 112.7632370 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11734342575073242\n",
      "Epoch: 228, Steps: 1 | Train Loss: 112.6253662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11826848983764648\n",
      "Epoch: 229, Steps: 1 | Train Loss: 112.5452194 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11782574653625488\n",
      "Epoch: 230, Steps: 1 | Train Loss: 113.5282593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.11954569816589355\n",
      "Epoch: 231, Steps: 1 | Train Loss: 112.6889191 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.12486052513122559\n",
      "Epoch: 232, Steps: 1 | Train Loss: 112.2672043 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11857867240905762\n",
      "Epoch: 233, Steps: 1 | Train Loss: 112.7339706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11797714233398438\n",
      "Epoch: 234, Steps: 1 | Train Loss: 113.0666275 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.1183168888092041\n",
      "Epoch: 235, Steps: 1 | Train Loss: 112.3474960 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.1186060905456543\n",
      "Epoch: 236, Steps: 1 | Train Loss: 113.4111099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.12020206451416016\n",
      "Epoch: 237, Steps: 1 | Train Loss: 112.9455948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11488771438598633\n",
      "Epoch: 238, Steps: 1 | Train Loss: 112.7129898 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.1340320110321045\n",
      "Epoch: 239, Steps: 1 | Train Loss: 112.7681427 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.12321329116821289\n",
      "Epoch: 240, Steps: 1 | Train Loss: 113.0354996 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.14151573181152344\n",
      "Epoch: 241, Steps: 1 | Train Loss: 112.8725128 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.15280604362487793\n",
      "Epoch: 242, Steps: 1 | Train Loss: 112.5659790 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.15073633193969727\n",
      "Epoch: 243, Steps: 1 | Train Loss: 113.3688583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.1394493579864502\n",
      "Epoch: 244, Steps: 1 | Train Loss: 112.4627914 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.11248183250427246\n",
      "Epoch: 245, Steps: 1 | Train Loss: 112.9484177 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11845278739929199\n",
      "Epoch: 246, Steps: 1 | Train Loss: 112.7990265 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.1309349536895752\n",
      "Epoch: 247, Steps: 1 | Train Loss: 112.8985748 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.11620140075683594\n",
      "Epoch: 248, Steps: 1 | Train Loss: 113.2819138 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11719965934753418\n",
      "Epoch: 249, Steps: 1 | Train Loss: 112.9101334 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11513662338256836\n",
      "Epoch: 250, Steps: 1 | Train Loss: 112.6990509 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "1\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11916208267211914\n",
      "Epoch: 1, Steps: 1 | Train Loss: 812.8526611 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.12213540077209473\n",
      "Epoch: 2, Steps: 1 | Train Loss: 805.4929199 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.1244664192199707\n",
      "Epoch: 3, Steps: 1 | Train Loss: 800.8446655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.12696099281311035\n",
      "Epoch: 4, Steps: 1 | Train Loss: 799.8683472 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.12478256225585938\n",
      "Epoch: 5, Steps: 1 | Train Loss: 800.3635864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.1388845443725586\n",
      "Epoch: 6, Steps: 1 | Train Loss: 796.1523438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.13462352752685547\n",
      "Epoch: 7, Steps: 1 | Train Loss: 796.5800171 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.12457680702209473\n",
      "Epoch: 8, Steps: 1 | Train Loss: 797.2120972 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.12406373023986816\n",
      "Epoch: 9, Steps: 1 | Train Loss: 796.0120850 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.12214207649230957\n",
      "Epoch: 10, Steps: 1 | Train Loss: 796.9812012 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.1246337890625\n",
      "Epoch: 11, Steps: 1 | Train Loss: 796.4481812 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.12273025512695312\n",
      "Epoch: 12, Steps: 1 | Train Loss: 797.9327393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.12588810920715332\n",
      "Epoch: 13, Steps: 1 | Train Loss: 795.8789673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.12119626998901367\n",
      "Epoch: 14, Steps: 1 | Train Loss: 795.5294800 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.13247132301330566\n",
      "Epoch: 15, Steps: 1 | Train Loss: 796.1022339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12641549110412598\n",
      "Epoch: 16, Steps: 1 | Train Loss: 796.6498413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.12916016578674316\n",
      "Epoch: 17, Steps: 1 | Train Loss: 796.6898804 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.12340569496154785\n",
      "Epoch: 18, Steps: 1 | Train Loss: 796.7525024 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.12390732765197754\n",
      "Epoch: 19, Steps: 1 | Train Loss: 795.5365601 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.12523317337036133\n",
      "Epoch: 20, Steps: 1 | Train Loss: 795.1571655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11939287185668945\n",
      "Epoch: 21, Steps: 1 | Train Loss: 795.9309692 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.126511812210083\n",
      "Epoch: 22, Steps: 1 | Train Loss: 797.3040161 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.12129855155944824\n",
      "Epoch: 23, Steps: 1 | Train Loss: 796.6663818 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11734843254089355\n",
      "Epoch: 24, Steps: 1 | Train Loss: 796.1032104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.1186525821685791\n",
      "Epoch: 25, Steps: 1 | Train Loss: 795.4571533 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12409543991088867\n",
      "Epoch: 26, Steps: 1 | Train Loss: 796.3805542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11595439910888672\n",
      "Epoch: 27, Steps: 1 | Train Loss: 797.9067993 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.12693405151367188\n",
      "Epoch: 28, Steps: 1 | Train Loss: 796.1503906 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.1162569522857666\n",
      "Epoch: 29, Steps: 1 | Train Loss: 797.5358276 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11363029479980469\n",
      "Epoch: 30, Steps: 1 | Train Loss: 795.9848633 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.11554622650146484\n",
      "Epoch: 31, Steps: 1 | Train Loss: 797.7662964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.11517739295959473\n",
      "Epoch: 32, Steps: 1 | Train Loss: 796.4523926 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.12394547462463379\n",
      "Epoch: 33, Steps: 1 | Train Loss: 796.1440430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.12798213958740234\n",
      "Epoch: 34, Steps: 1 | Train Loss: 796.5127563 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.12152862548828125\n",
      "Epoch: 35, Steps: 1 | Train Loss: 795.9874878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.1148533821105957\n",
      "Epoch: 36, Steps: 1 | Train Loss: 796.4962769 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.11571121215820312\n",
      "Epoch: 37, Steps: 1 | Train Loss: 795.2523193 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11426568031311035\n",
      "Epoch: 38, Steps: 1 | Train Loss: 797.4721680 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.1126708984375\n",
      "Epoch: 39, Steps: 1 | Train Loss: 795.5227661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.12272024154663086\n",
      "Epoch: 40, Steps: 1 | Train Loss: 795.4072266 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.12619709968566895\n",
      "Epoch: 41, Steps: 1 | Train Loss: 797.1690063 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.12825989723205566\n",
      "Epoch: 42, Steps: 1 | Train Loss: 796.7897949 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.12734532356262207\n",
      "Epoch: 43, Steps: 1 | Train Loss: 797.0382080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.1189277172088623\n",
      "Epoch: 44, Steps: 1 | Train Loss: 796.7836914 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.11613631248474121\n",
      "Epoch: 45, Steps: 1 | Train Loss: 796.3591919 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.11500692367553711\n",
      "Epoch: 46, Steps: 1 | Train Loss: 796.3958740 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11515545845031738\n",
      "Epoch: 47, Steps: 1 | Train Loss: 797.4176025 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.1286311149597168\n",
      "Epoch: 48, Steps: 1 | Train Loss: 794.9823608 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.12566637992858887\n",
      "Epoch: 49, Steps: 1 | Train Loss: 795.4531860 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.13000106811523438\n",
      "Epoch: 50, Steps: 1 | Train Loss: 796.1719360 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.12256979942321777\n",
      "Epoch: 51, Steps: 1 | Train Loss: 795.8443604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.12880945205688477\n",
      "Epoch: 52, Steps: 1 | Train Loss: 796.2385864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.12875890731811523\n",
      "Epoch: 53, Steps: 1 | Train Loss: 796.9293213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.11934924125671387\n",
      "Epoch: 54, Steps: 1 | Train Loss: 796.6044922 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.11670041084289551\n",
      "Epoch: 55, Steps: 1 | Train Loss: 796.1707153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.11612486839294434\n",
      "Epoch: 56, Steps: 1 | Train Loss: 796.7614136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.11465883255004883\n",
      "Epoch: 57, Steps: 1 | Train Loss: 797.0820312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10804915428161621\n",
      "Epoch: 58, Steps: 1 | Train Loss: 796.7710571 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11455965042114258\n",
      "Epoch: 59, Steps: 1 | Train Loss: 795.4172974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11658716201782227\n",
      "Epoch: 60, Steps: 1 | Train Loss: 795.6575317 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.12558674812316895\n",
      "Epoch: 61, Steps: 1 | Train Loss: 797.9314575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11764883995056152\n",
      "Epoch: 62, Steps: 1 | Train Loss: 796.5739136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11478185653686523\n",
      "Epoch: 63, Steps: 1 | Train Loss: 796.0910034 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.1147153377532959\n",
      "Epoch: 64, Steps: 1 | Train Loss: 796.6072388 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.1156461238861084\n",
      "Epoch: 65, Steps: 1 | Train Loss: 796.9230347 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11485433578491211\n",
      "Epoch: 66, Steps: 1 | Train Loss: 797.3219604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11525988578796387\n",
      "Epoch: 67, Steps: 1 | Train Loss: 797.5975952 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.12758159637451172\n",
      "Epoch: 68, Steps: 1 | Train Loss: 796.2081909 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.12403535842895508\n",
      "Epoch: 69, Steps: 1 | Train Loss: 795.2001953 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11526179313659668\n",
      "Epoch: 70, Steps: 1 | Train Loss: 795.8125000 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.11616992950439453\n",
      "Epoch: 71, Steps: 1 | Train Loss: 795.0891724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.1122121810913086\n",
      "Epoch: 72, Steps: 1 | Train Loss: 796.3171387 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11571764945983887\n",
      "Epoch: 73, Steps: 1 | Train Loss: 796.6735229 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.11545419692993164\n",
      "Epoch: 74, Steps: 1 | Train Loss: 795.4557495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11437821388244629\n",
      "Epoch: 75, Steps: 1 | Train Loss: 796.2808838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11365103721618652\n",
      "Epoch: 76, Steps: 1 | Train Loss: 796.9968872 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.12637853622436523\n",
      "Epoch: 77, Steps: 1 | Train Loss: 796.3610229 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.1270155906677246\n",
      "Epoch: 78, Steps: 1 | Train Loss: 797.5289307 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.12508463859558105\n",
      "Epoch: 79, Steps: 1 | Train Loss: 796.2847290 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.11704659461975098\n",
      "Epoch: 80, Steps: 1 | Train Loss: 796.0116577 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.11483240127563477\n",
      "Epoch: 81, Steps: 1 | Train Loss: 795.7759399 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11553263664245605\n",
      "Epoch: 82, Steps: 1 | Train Loss: 795.7420654 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.11308574676513672\n",
      "Epoch: 83, Steps: 1 | Train Loss: 794.8419800 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11562514305114746\n",
      "Epoch: 84, Steps: 1 | Train Loss: 797.4073486 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.12335348129272461\n",
      "Epoch: 85, Steps: 1 | Train Loss: 796.5390015 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.12916827201843262\n",
      "Epoch: 86, Steps: 1 | Train Loss: 795.6912842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.12706756591796875\n",
      "Epoch: 87, Steps: 1 | Train Loss: 796.7001953 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11579489707946777\n",
      "Epoch: 88, Steps: 1 | Train Loss: 795.5205078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11422991752624512\n",
      "Epoch: 89, Steps: 1 | Train Loss: 799.0673828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11374282836914062\n",
      "Epoch: 90, Steps: 1 | Train Loss: 797.7640381 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.1189565658569336\n",
      "Epoch: 91, Steps: 1 | Train Loss: 797.4822998 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.1152503490447998\n",
      "Epoch: 92, Steps: 1 | Train Loss: 796.2984619 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.11627078056335449\n",
      "Epoch: 93, Steps: 1 | Train Loss: 795.9877319 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.11503267288208008\n",
      "Epoch: 94, Steps: 1 | Train Loss: 795.5261230 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.1133427619934082\n",
      "Epoch: 95, Steps: 1 | Train Loss: 796.3911133 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.11469388008117676\n",
      "Epoch: 96, Steps: 1 | Train Loss: 797.3306885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.11383414268493652\n",
      "Epoch: 97, Steps: 1 | Train Loss: 795.1929932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.11519432067871094\n",
      "Epoch: 98, Steps: 1 | Train Loss: 795.2347412 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.11475348472595215\n",
      "Epoch: 99, Steps: 1 | Train Loss: 797.3577271 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.12816405296325684\n",
      "Epoch: 100, Steps: 1 | Train Loss: 796.7262573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.11728119850158691\n",
      "Epoch: 101, Steps: 1 | Train Loss: 796.4873657 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.11301517486572266\n",
      "Epoch: 102, Steps: 1 | Train Loss: 796.9088745 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.11481857299804688\n",
      "Epoch: 103, Steps: 1 | Train Loss: 796.4162598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.11571383476257324\n",
      "Epoch: 104, Steps: 1 | Train Loss: 795.1886597 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.1150050163269043\n",
      "Epoch: 105, Steps: 1 | Train Loss: 797.2551880 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.1148521900177002\n",
      "Epoch: 106, Steps: 1 | Train Loss: 797.1033325 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.11460733413696289\n",
      "Epoch: 107, Steps: 1 | Train Loss: 795.8795166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.12127971649169922\n",
      "Epoch: 108, Steps: 1 | Train Loss: 797.0608521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11705350875854492\n",
      "Epoch: 109, Steps: 1 | Train Loss: 797.9121094 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.11551618576049805\n",
      "Epoch: 110, Steps: 1 | Train Loss: 796.6515503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11483216285705566\n",
      "Epoch: 111, Steps: 1 | Train Loss: 797.0910034 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.11540722846984863\n",
      "Epoch: 112, Steps: 1 | Train Loss: 797.0515137 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11635565757751465\n",
      "Epoch: 113, Steps: 1 | Train Loss: 796.2870483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10980939865112305\n",
      "Epoch: 114, Steps: 1 | Train Loss: 796.5372925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.12374138832092285\n",
      "Epoch: 115, Steps: 1 | Train Loss: 796.5008545 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.1154625415802002\n",
      "Epoch: 116, Steps: 1 | Train Loss: 794.7562866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.11565065383911133\n",
      "Epoch: 117, Steps: 1 | Train Loss: 797.2214966 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.11595582962036133\n",
      "Epoch: 118, Steps: 1 | Train Loss: 795.9830933 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.12357902526855469\n",
      "Epoch: 119, Steps: 1 | Train Loss: 796.3619995 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1227407455444336\n",
      "Epoch: 120, Steps: 1 | Train Loss: 796.7044067 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.12470030784606934\n",
      "Epoch: 121, Steps: 1 | Train Loss: 795.9846802 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.13764405250549316\n",
      "Epoch: 122, Steps: 1 | Train Loss: 795.8887939 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.13271474838256836\n",
      "Epoch: 123, Steps: 1 | Train Loss: 797.2681885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.12547707557678223\n",
      "Epoch: 124, Steps: 1 | Train Loss: 796.9625854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.1219930648803711\n",
      "Epoch: 125, Steps: 1 | Train Loss: 796.1262817 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.13698768615722656\n",
      "Epoch: 126, Steps: 1 | Train Loss: 796.7874146 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.12325525283813477\n",
      "Epoch: 127, Steps: 1 | Train Loss: 797.7058105 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12618803977966309\n",
      "Epoch: 128, Steps: 1 | Train Loss: 796.3529663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.1280362606048584\n",
      "Epoch: 129, Steps: 1 | Train Loss: 797.5232544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.13029789924621582\n",
      "Epoch: 130, Steps: 1 | Train Loss: 796.7098999 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.1360304355621338\n",
      "Epoch: 131, Steps: 1 | Train Loss: 796.4349365 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.13588953018188477\n",
      "Epoch: 132, Steps: 1 | Train Loss: 797.3481445 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.13651180267333984\n",
      "Epoch: 133, Steps: 1 | Train Loss: 796.4012451 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.13091230392456055\n",
      "Epoch: 134, Steps: 1 | Train Loss: 796.5994873 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.12486815452575684\n",
      "Epoch: 135, Steps: 1 | Train Loss: 797.5175171 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.12346076965332031\n",
      "Epoch: 136, Steps: 1 | Train Loss: 796.2354736 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.1372060775756836\n",
      "Epoch: 137, Steps: 1 | Train Loss: 796.6323242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.12371540069580078\n",
      "Epoch: 138, Steps: 1 | Train Loss: 796.8825073 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.12489461898803711\n",
      "Epoch: 139, Steps: 1 | Train Loss: 796.7846069 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.12746024131774902\n",
      "Epoch: 140, Steps: 1 | Train Loss: 795.9791260 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.13341808319091797\n",
      "Epoch: 141, Steps: 1 | Train Loss: 796.7051392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.13698339462280273\n",
      "Epoch: 142, Steps: 1 | Train Loss: 798.0197754 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.12767505645751953\n",
      "Epoch: 143, Steps: 1 | Train Loss: 796.5956421 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.13562655448913574\n",
      "Epoch: 144, Steps: 1 | Train Loss: 795.1345825 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.1361539363861084\n",
      "Epoch: 145, Steps: 1 | Train Loss: 797.6010742 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.12959623336791992\n",
      "Epoch: 146, Steps: 1 | Train Loss: 796.8362427 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.12491393089294434\n",
      "Epoch: 147, Steps: 1 | Train Loss: 797.5594482 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.12287163734436035\n",
      "Epoch: 148, Steps: 1 | Train Loss: 796.6354370 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.12540721893310547\n",
      "Epoch: 149, Steps: 1 | Train Loss: 796.7157593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.12195777893066406\n",
      "Epoch: 150, Steps: 1 | Train Loss: 797.7504272 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.1236720085144043\n",
      "Epoch: 151, Steps: 1 | Train Loss: 795.6557007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.12758469581604004\n",
      "Epoch: 152, Steps: 1 | Train Loss: 793.9508667 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12488985061645508\n",
      "Epoch: 153, Steps: 1 | Train Loss: 796.0932617 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.12990617752075195\n",
      "Epoch: 154, Steps: 1 | Train Loss: 795.3789673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.1480545997619629\n",
      "Epoch: 155, Steps: 1 | Train Loss: 796.1171265 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.12043619155883789\n",
      "Epoch: 156, Steps: 1 | Train Loss: 796.4960327 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.11601519584655762\n",
      "Epoch: 157, Steps: 1 | Train Loss: 796.1303101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.11598920822143555\n",
      "Epoch: 158, Steps: 1 | Train Loss: 796.4663086 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.11636734008789062\n",
      "Epoch: 159, Steps: 1 | Train Loss: 795.8876343 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11654901504516602\n",
      "Epoch: 160, Steps: 1 | Train Loss: 796.3389282 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.11598396301269531\n",
      "Epoch: 161, Steps: 1 | Train Loss: 795.5699463 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.11678028106689453\n",
      "Epoch: 162, Steps: 1 | Train Loss: 796.3804932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.12732672691345215\n",
      "Epoch: 163, Steps: 1 | Train Loss: 797.3402100 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.12853193283081055\n",
      "Epoch: 164, Steps: 1 | Train Loss: 798.0364380 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.1193392276763916\n",
      "Epoch: 165, Steps: 1 | Train Loss: 797.4648438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11792492866516113\n",
      "Epoch: 166, Steps: 1 | Train Loss: 797.9562988 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.1163933277130127\n",
      "Epoch: 167, Steps: 1 | Train Loss: 795.9951782 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11547708511352539\n",
      "Epoch: 168, Steps: 1 | Train Loss: 795.8522339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11604785919189453\n",
      "Epoch: 169, Steps: 1 | Train Loss: 796.0759888 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11634516716003418\n",
      "Epoch: 170, Steps: 1 | Train Loss: 796.8734741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11765480041503906\n",
      "Epoch: 171, Steps: 1 | Train Loss: 796.9818726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11741924285888672\n",
      "Epoch: 172, Steps: 1 | Train Loss: 796.2015991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11612081527709961\n",
      "Epoch: 173, Steps: 1 | Train Loss: 795.9991455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11532711982727051\n",
      "Epoch: 174, Steps: 1 | Train Loss: 796.2886963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.1162564754486084\n",
      "Epoch: 175, Steps: 1 | Train Loss: 797.4308472 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.12560105323791504\n",
      "Epoch: 176, Steps: 1 | Train Loss: 796.5541992 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.13004851341247559\n",
      "Epoch: 177, Steps: 1 | Train Loss: 795.6065674 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.12786507606506348\n",
      "Epoch: 178, Steps: 1 | Train Loss: 796.2418823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11738705635070801\n",
      "Epoch: 179, Steps: 1 | Train Loss: 795.9428101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11733818054199219\n",
      "Epoch: 180, Steps: 1 | Train Loss: 795.1279907 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11767911911010742\n",
      "Epoch: 181, Steps: 1 | Train Loss: 795.2243042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11721277236938477\n",
      "Epoch: 182, Steps: 1 | Train Loss: 796.8390503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11523580551147461\n",
      "Epoch: 183, Steps: 1 | Train Loss: 795.8248901 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11624670028686523\n",
      "Epoch: 184, Steps: 1 | Train Loss: 795.6817627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11719322204589844\n",
      "Epoch: 185, Steps: 1 | Train Loss: 795.9287109 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.1179656982421875\n",
      "Epoch: 186, Steps: 1 | Train Loss: 796.5291138 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11439275741577148\n",
      "Epoch: 187, Steps: 1 | Train Loss: 797.0776367 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11442971229553223\n",
      "Epoch: 188, Steps: 1 | Train Loss: 796.9039917 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.11629009246826172\n",
      "Epoch: 189, Steps: 1 | Train Loss: 796.2658081 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12413811683654785\n",
      "Epoch: 190, Steps: 1 | Train Loss: 797.7132568 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.127777099609375\n",
      "Epoch: 191, Steps: 1 | Train Loss: 796.5851440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.12274479866027832\n",
      "Epoch: 192, Steps: 1 | Train Loss: 798.0828247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.11943888664245605\n",
      "Epoch: 193, Steps: 1 | Train Loss: 796.9757690 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11754274368286133\n",
      "Epoch: 194, Steps: 1 | Train Loss: 796.5690308 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11768388748168945\n",
      "Epoch: 195, Steps: 1 | Train Loss: 795.7601318 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11679649353027344\n",
      "Epoch: 196, Steps: 1 | Train Loss: 796.9207764 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11628150939941406\n",
      "Epoch: 197, Steps: 1 | Train Loss: 795.0192261 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11668539047241211\n",
      "Epoch: 198, Steps: 1 | Train Loss: 797.3087769 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11536407470703125\n",
      "Epoch: 199, Steps: 1 | Train Loss: 797.6583862 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11626839637756348\n",
      "Epoch: 200, Steps: 1 | Train Loss: 796.6817627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11553311347961426\n",
      "Epoch: 201, Steps: 1 | Train Loss: 797.5277710 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.11643433570861816\n",
      "Epoch: 202, Steps: 1 | Train Loss: 796.3141479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11533308029174805\n",
      "Epoch: 203, Steps: 1 | Train Loss: 796.9638672 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.1300826072692871\n",
      "Epoch: 204, Steps: 1 | Train Loss: 796.7337036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.12795257568359375\n",
      "Epoch: 205, Steps: 1 | Train Loss: 795.7821655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.11675477027893066\n",
      "Epoch: 206, Steps: 1 | Train Loss: 795.8231812 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11750245094299316\n",
      "Epoch: 207, Steps: 1 | Train Loss: 795.7470703 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.11573433876037598\n",
      "Epoch: 208, Steps: 1 | Train Loss: 796.2980347 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11635565757751465\n",
      "Epoch: 209, Steps: 1 | Train Loss: 795.9449463 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11742997169494629\n",
      "Epoch: 210, Steps: 1 | Train Loss: 797.1386719 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.11719560623168945\n",
      "Epoch: 211, Steps: 1 | Train Loss: 797.5380859 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11685609817504883\n",
      "Epoch: 212, Steps: 1 | Train Loss: 796.3823853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11540007591247559\n",
      "Epoch: 213, Steps: 1 | Train Loss: 796.2143555 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11741280555725098\n",
      "Epoch: 214, Steps: 1 | Train Loss: 797.1701050 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11553502082824707\n",
      "Epoch: 215, Steps: 1 | Train Loss: 795.9379272 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11673784255981445\n",
      "Epoch: 216, Steps: 1 | Train Loss: 796.5405273 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.1270296573638916\n",
      "Epoch: 217, Steps: 1 | Train Loss: 795.5290527 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.12898802757263184\n",
      "Epoch: 218, Steps: 1 | Train Loss: 797.6536865 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.12400937080383301\n",
      "Epoch: 219, Steps: 1 | Train Loss: 796.0747681 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11666011810302734\n",
      "Epoch: 220, Steps: 1 | Train Loss: 797.0613403 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11773276329040527\n",
      "Epoch: 221, Steps: 1 | Train Loss: 797.3782959 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.11769866943359375\n",
      "Epoch: 222, Steps: 1 | Train Loss: 796.4674683 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11602497100830078\n",
      "Epoch: 223, Steps: 1 | Train Loss: 796.5813599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11731886863708496\n",
      "Epoch: 224, Steps: 1 | Train Loss: 796.6898804 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.12708330154418945\n",
      "Epoch: 225, Steps: 1 | Train Loss: 796.1283569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11489200592041016\n",
      "Epoch: 226, Steps: 1 | Train Loss: 795.2500000 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11743927001953125\n",
      "Epoch: 227, Steps: 1 | Train Loss: 796.7072144 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11765789985656738\n",
      "Epoch: 228, Steps: 1 | Train Loss: 796.8839722 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11564135551452637\n",
      "Epoch: 229, Steps: 1 | Train Loss: 796.7463989 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11712050437927246\n",
      "Epoch: 230, Steps: 1 | Train Loss: 796.8036499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.1265857219696045\n",
      "Epoch: 231, Steps: 1 | Train Loss: 796.6331787 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.13038969039916992\n",
      "Epoch: 232, Steps: 1 | Train Loss: 796.3703003 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.12312650680541992\n",
      "Epoch: 233, Steps: 1 | Train Loss: 796.9243774 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11823081970214844\n",
      "Epoch: 234, Steps: 1 | Train Loss: 796.6561890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11639261245727539\n",
      "Epoch: 235, Steps: 1 | Train Loss: 797.2843018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11687254905700684\n",
      "Epoch: 236, Steps: 1 | Train Loss: 796.7297974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11586618423461914\n",
      "Epoch: 237, Steps: 1 | Train Loss: 796.4558716 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11761045455932617\n",
      "Epoch: 238, Steps: 1 | Train Loss: 795.1137695 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.11611247062683105\n",
      "Epoch: 239, Steps: 1 | Train Loss: 795.7487183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.11729955673217773\n",
      "Epoch: 240, Steps: 1 | Train Loss: 797.5298462 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.12019658088684082\n",
      "Epoch: 241, Steps: 1 | Train Loss: 794.9785767 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11802005767822266\n",
      "Epoch: 242, Steps: 1 | Train Loss: 797.6414185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.11576366424560547\n",
      "Epoch: 243, Steps: 1 | Train Loss: 796.3068237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.11742162704467773\n",
      "Epoch: 244, Steps: 1 | Train Loss: 795.9671021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.11410355567932129\n",
      "Epoch: 245, Steps: 1 | Train Loss: 797.1084595 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11972212791442871\n",
      "Epoch: 246, Steps: 1 | Train Loss: 796.4630737 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11800670623779297\n",
      "Epoch: 247, Steps: 1 | Train Loss: 797.2288208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.12995362281799316\n",
      "Epoch: 248, Steps: 1 | Train Loss: 797.5640259 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.1207895278930664\n",
      "Epoch: 249, Steps: 1 | Train Loss: 796.1921997 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11765313148498535\n",
      "Epoch: 250, Steps: 1 | Train Loss: 797.3309937 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "21\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_1>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.12418889999389648\n",
      "Epoch: 1, Steps: 1 | Train Loss: 816.1073608 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.11060428619384766\n",
      "Epoch: 2, Steps: 1 | Train Loss: 809.5849609 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10968255996704102\n",
      "Epoch: 3, Steps: 1 | Train Loss: 804.6395874 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.11123919486999512\n",
      "Epoch: 4, Steps: 1 | Train Loss: 801.1173706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11160707473754883\n",
      "Epoch: 5, Steps: 1 | Train Loss: 801.2642822 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11404204368591309\n",
      "Epoch: 6, Steps: 1 | Train Loss: 800.0785522 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11245059967041016\n",
      "Epoch: 7, Steps: 1 | Train Loss: 798.8477173 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.1111445426940918\n",
      "Epoch: 8, Steps: 1 | Train Loss: 800.3728638 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11121964454650879\n",
      "Epoch: 9, Steps: 1 | Train Loss: 799.5083618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.111602783203125\n",
      "Epoch: 10, Steps: 1 | Train Loss: 798.3521729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10998821258544922\n",
      "Epoch: 11, Steps: 1 | Train Loss: 799.5527344 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11059808731079102\n",
      "Epoch: 12, Steps: 1 | Train Loss: 798.1730347 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11137962341308594\n",
      "Epoch: 13, Steps: 1 | Train Loss: 800.6497192 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.1100759506225586\n",
      "Epoch: 14, Steps: 1 | Train Loss: 798.1359863 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.12605071067810059\n",
      "Epoch: 15, Steps: 1 | Train Loss: 799.1017456 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.1210932731628418\n",
      "Epoch: 16, Steps: 1 | Train Loss: 799.7846069 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.11119723320007324\n",
      "Epoch: 17, Steps: 1 | Train Loss: 801.0854492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10976791381835938\n",
      "Epoch: 18, Steps: 1 | Train Loss: 800.4159546 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.1084592342376709\n",
      "Epoch: 19, Steps: 1 | Train Loss: 798.5983276 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.10800385475158691\n",
      "Epoch: 20, Steps: 1 | Train Loss: 800.0253296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11582732200622559\n",
      "Epoch: 21, Steps: 1 | Train Loss: 798.8505249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.11647844314575195\n",
      "Epoch: 22, Steps: 1 | Train Loss: 799.2703857 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11412644386291504\n",
      "Epoch: 23, Steps: 1 | Train Loss: 800.8022461 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.12794017791748047\n",
      "Epoch: 24, Steps: 1 | Train Loss: 801.0628662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.1167604923248291\n",
      "Epoch: 25, Steps: 1 | Train Loss: 800.0700684 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11239457130432129\n",
      "Epoch: 26, Steps: 1 | Train Loss: 799.4306641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.10897207260131836\n",
      "Epoch: 27, Steps: 1 | Train Loss: 799.0136108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.10800957679748535\n",
      "Epoch: 28, Steps: 1 | Train Loss: 800.9960938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10774755477905273\n",
      "Epoch: 29, Steps: 1 | Train Loss: 800.2335815 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.10756969451904297\n",
      "Epoch: 30, Steps: 1 | Train Loss: 798.1622314 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.10790562629699707\n",
      "Epoch: 31, Steps: 1 | Train Loss: 796.9490356 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10793900489807129\n",
      "Epoch: 32, Steps: 1 | Train Loss: 797.5917358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.1138761043548584\n",
      "Epoch: 33, Steps: 1 | Train Loss: 797.7755737 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.1225583553314209\n",
      "Epoch: 34, Steps: 1 | Train Loss: 800.8021851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.11070060729980469\n",
      "Epoch: 35, Steps: 1 | Train Loss: 800.6114502 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10851836204528809\n",
      "Epoch: 36, Steps: 1 | Train Loss: 797.9859619 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10806441307067871\n",
      "Epoch: 37, Steps: 1 | Train Loss: 797.2699585 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10718297958374023\n",
      "Epoch: 38, Steps: 1 | Train Loss: 800.6641846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10705161094665527\n",
      "Epoch: 39, Steps: 1 | Train Loss: 799.3679810 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.1073148250579834\n",
      "Epoch: 40, Steps: 1 | Train Loss: 798.8914185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10700845718383789\n",
      "Epoch: 41, Steps: 1 | Train Loss: 799.3038330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10807394981384277\n",
      "Epoch: 42, Steps: 1 | Train Loss: 799.2409058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10908102989196777\n",
      "Epoch: 43, Steps: 1 | Train Loss: 800.0883789 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10770797729492188\n",
      "Epoch: 44, Steps: 1 | Train Loss: 798.2814331 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10793900489807129\n",
      "Epoch: 45, Steps: 1 | Train Loss: 799.3776245 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.12093067169189453\n",
      "Epoch: 46, Steps: 1 | Train Loss: 798.1737061 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.12036633491516113\n",
      "Epoch: 47, Steps: 1 | Train Loss: 800.0707397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.1207115650177002\n",
      "Epoch: 48, Steps: 1 | Train Loss: 798.6318359 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.11931657791137695\n",
      "Epoch: 49, Steps: 1 | Train Loss: 799.3476562 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11033415794372559\n",
      "Epoch: 50, Steps: 1 | Train Loss: 799.7734375 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11228084564208984\n",
      "Epoch: 51, Steps: 1 | Train Loss: 796.9783325 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.1114499568939209\n",
      "Epoch: 52, Steps: 1 | Train Loss: 800.1229858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.11080598831176758\n",
      "Epoch: 53, Steps: 1 | Train Loss: 800.1528931 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10960221290588379\n",
      "Epoch: 54, Steps: 1 | Train Loss: 798.7907104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.11641120910644531\n",
      "Epoch: 55, Steps: 1 | Train Loss: 799.9574585 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.11124515533447266\n",
      "Epoch: 56, Steps: 1 | Train Loss: 800.2989502 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10660219192504883\n",
      "Epoch: 57, Steps: 1 | Train Loss: 797.6929932 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10618758201599121\n",
      "Epoch: 58, Steps: 1 | Train Loss: 800.5170288 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11009049415588379\n",
      "Epoch: 59, Steps: 1 | Train Loss: 797.4738159 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11057376861572266\n",
      "Epoch: 60, Steps: 1 | Train Loss: 798.6666260 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.12277579307556152\n",
      "Epoch: 61, Steps: 1 | Train Loss: 800.9340820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.12242984771728516\n",
      "Epoch: 62, Steps: 1 | Train Loss: 799.2645874 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.12301301956176758\n",
      "Epoch: 63, Steps: 1 | Train Loss: 799.5057983 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.12114953994750977\n",
      "Epoch: 64, Steps: 1 | Train Loss: 799.2315674 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11034226417541504\n",
      "Epoch: 65, Steps: 1 | Train Loss: 798.0995483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11045145988464355\n",
      "Epoch: 66, Steps: 1 | Train Loss: 798.5071411 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11078619956970215\n",
      "Epoch: 67, Steps: 1 | Train Loss: 798.4527588 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10989737510681152\n",
      "Epoch: 68, Steps: 1 | Train Loss: 798.0144653 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.1108396053314209\n",
      "Epoch: 69, Steps: 1 | Train Loss: 799.7164917 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.10956048965454102\n",
      "Epoch: 70, Steps: 1 | Train Loss: 800.8057251 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10957169532775879\n",
      "Epoch: 71, Steps: 1 | Train Loss: 800.4858398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.1102132797241211\n",
      "Epoch: 72, Steps: 1 | Train Loss: 798.8829346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.10937023162841797\n",
      "Epoch: 73, Steps: 1 | Train Loss: 800.5150146 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12304162979125977\n",
      "Epoch: 74, Steps: 1 | Train Loss: 799.6876221 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.12443685531616211\n",
      "Epoch: 75, Steps: 1 | Train Loss: 796.8395996 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.13055205345153809\n",
      "Epoch: 76, Steps: 1 | Train Loss: 799.9943237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11559557914733887\n",
      "Epoch: 77, Steps: 1 | Train Loss: 800.4365234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.1116783618927002\n",
      "Epoch: 78, Steps: 1 | Train Loss: 800.8629761 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10989022254943848\n",
      "Epoch: 79, Steps: 1 | Train Loss: 799.0648804 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.11043620109558105\n",
      "Epoch: 80, Steps: 1 | Train Loss: 798.6235962 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10929656028747559\n",
      "Epoch: 81, Steps: 1 | Train Loss: 799.4475708 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.1113436222076416\n",
      "Epoch: 82, Steps: 1 | Train Loss: 800.1248169 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.10957670211791992\n",
      "Epoch: 83, Steps: 1 | Train Loss: 798.0976562 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.10747361183166504\n",
      "Epoch: 84, Steps: 1 | Train Loss: 800.3712769 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.10933828353881836\n",
      "Epoch: 85, Steps: 1 | Train Loss: 798.3250122 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.10764002799987793\n",
      "Epoch: 86, Steps: 1 | Train Loss: 800.2728882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.10663342475891113\n",
      "Epoch: 87, Steps: 1 | Train Loss: 797.9384155 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.12086224555969238\n",
      "Epoch: 88, Steps: 1 | Train Loss: 798.5304565 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.12063264846801758\n",
      "Epoch: 89, Steps: 1 | Train Loss: 800.1368408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11924576759338379\n",
      "Epoch: 90, Steps: 1 | Train Loss: 798.3406982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.11213111877441406\n",
      "Epoch: 91, Steps: 1 | Train Loss: 798.6322021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10913586616516113\n",
      "Epoch: 92, Steps: 1 | Train Loss: 799.0539551 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10840535163879395\n",
      "Epoch: 93, Steps: 1 | Train Loss: 800.0446777 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10761690139770508\n",
      "Epoch: 94, Steps: 1 | Train Loss: 798.8688354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10700154304504395\n",
      "Epoch: 95, Steps: 1 | Train Loss: 798.8986206 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10716748237609863\n",
      "Epoch: 96, Steps: 1 | Train Loss: 801.0937500 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10704326629638672\n",
      "Epoch: 97, Steps: 1 | Train Loss: 799.8331299 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10688185691833496\n",
      "Epoch: 98, Steps: 1 | Train Loss: 799.1484375 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.1069631576538086\n",
      "Epoch: 99, Steps: 1 | Train Loss: 798.4662476 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.10739898681640625\n",
      "Epoch: 100, Steps: 1 | Train Loss: 799.4163208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10725259780883789\n",
      "Epoch: 101, Steps: 1 | Train Loss: 800.4851685 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.12101268768310547\n",
      "Epoch: 102, Steps: 1 | Train Loss: 800.5238647 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.12351655960083008\n",
      "Epoch: 103, Steps: 1 | Train Loss: 797.9063110 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.1210179328918457\n",
      "Epoch: 104, Steps: 1 | Train Loss: 799.0747681 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.12825584411621094\n",
      "Epoch: 105, Steps: 1 | Train Loss: 798.5912476 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.11739492416381836\n",
      "Epoch: 106, Steps: 1 | Train Loss: 798.7649536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.11598920822143555\n",
      "Epoch: 107, Steps: 1 | Train Loss: 800.0962524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.1191873550415039\n",
      "Epoch: 108, Steps: 1 | Train Loss: 800.8516846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11470627784729004\n",
      "Epoch: 109, Steps: 1 | Train Loss: 800.9457397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.1183016300201416\n",
      "Epoch: 110, Steps: 1 | Train Loss: 798.1016846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.1211557388305664\n",
      "Epoch: 111, Steps: 1 | Train Loss: 799.9594116 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.12338399887084961\n",
      "Epoch: 112, Steps: 1 | Train Loss: 797.4439697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10848832130432129\n",
      "Epoch: 113, Steps: 1 | Train Loss: 797.3634033 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10802745819091797\n",
      "Epoch: 114, Steps: 1 | Train Loss: 798.6295776 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10756087303161621\n",
      "Epoch: 115, Steps: 1 | Train Loss: 798.1352539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.1072998046875\n",
      "Epoch: 116, Steps: 1 | Train Loss: 798.4468994 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10716843605041504\n",
      "Epoch: 117, Steps: 1 | Train Loss: 799.1499023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.10683822631835938\n",
      "Epoch: 118, Steps: 1 | Train Loss: 798.2896118 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10578083992004395\n",
      "Epoch: 119, Steps: 1 | Train Loss: 798.2741089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.10854005813598633\n",
      "Epoch: 120, Steps: 1 | Train Loss: 797.9442749 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.13082051277160645\n",
      "Epoch: 121, Steps: 1 | Train Loss: 799.9323120 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.13314270973205566\n",
      "Epoch: 122, Steps: 1 | Train Loss: 799.0078125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.12133955955505371\n",
      "Epoch: 123, Steps: 1 | Train Loss: 800.0866089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.12017679214477539\n",
      "Epoch: 124, Steps: 1 | Train Loss: 799.1451416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.12009334564208984\n",
      "Epoch: 125, Steps: 1 | Train Loss: 799.9995117 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.12209725379943848\n",
      "Epoch: 126, Steps: 1 | Train Loss: 798.3805542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.12116003036499023\n",
      "Epoch: 127, Steps: 1 | Train Loss: 799.8811646 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12125849723815918\n",
      "Epoch: 128, Steps: 1 | Train Loss: 796.6612549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.12153267860412598\n",
      "Epoch: 129, Steps: 1 | Train Loss: 799.7031250 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.12128305435180664\n",
      "Epoch: 130, Steps: 1 | Train Loss: 796.7398071 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.12139105796813965\n",
      "Epoch: 131, Steps: 1 | Train Loss: 798.4710083 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.12521147727966309\n",
      "Epoch: 132, Steps: 1 | Train Loss: 799.8930054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.12624907493591309\n",
      "Epoch: 133, Steps: 1 | Train Loss: 798.3721924 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.1102297306060791\n",
      "Epoch: 134, Steps: 1 | Train Loss: 798.3121338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.11150765419006348\n",
      "Epoch: 135, Steps: 1 | Train Loss: 798.8700562 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10804414749145508\n",
      "Epoch: 136, Steps: 1 | Train Loss: 799.8352661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.11052322387695312\n",
      "Epoch: 137, Steps: 1 | Train Loss: 799.1283569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11020660400390625\n",
      "Epoch: 138, Steps: 1 | Train Loss: 799.6494141 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.10958647727966309\n",
      "Epoch: 139, Steps: 1 | Train Loss: 797.5804443 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.1102902889251709\n",
      "Epoch: 140, Steps: 1 | Train Loss: 799.8996582 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.11042952537536621\n",
      "Epoch: 141, Steps: 1 | Train Loss: 799.5143433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.10985779762268066\n",
      "Epoch: 142, Steps: 1 | Train Loss: 799.6459351 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.10726785659790039\n",
      "Epoch: 143, Steps: 1 | Train Loss: 799.1151123 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.1118154525756836\n",
      "Epoch: 144, Steps: 1 | Train Loss: 799.1260986 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10948443412780762\n",
      "Epoch: 145, Steps: 1 | Train Loss: 798.9150391 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10942220687866211\n",
      "Epoch: 146, Steps: 1 | Train Loss: 798.3737183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.15611720085144043\n",
      "Epoch: 147, Steps: 1 | Train Loss: 800.4791260 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11638855934143066\n",
      "Epoch: 148, Steps: 1 | Train Loss: 800.0405273 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11154580116271973\n",
      "Epoch: 149, Steps: 1 | Train Loss: 799.3536377 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10809516906738281\n",
      "Epoch: 150, Steps: 1 | Train Loss: 798.3823242 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10775923728942871\n",
      "Epoch: 151, Steps: 1 | Train Loss: 797.8803101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.1074228286743164\n",
      "Epoch: 152, Steps: 1 | Train Loss: 797.8959961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10689091682434082\n",
      "Epoch: 153, Steps: 1 | Train Loss: 798.7648315 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10732245445251465\n",
      "Epoch: 154, Steps: 1 | Train Loss: 799.6221313 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10748815536499023\n",
      "Epoch: 155, Steps: 1 | Train Loss: 798.4683838 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10700201988220215\n",
      "Epoch: 156, Steps: 1 | Train Loss: 799.7995605 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10709047317504883\n",
      "Epoch: 157, Steps: 1 | Train Loss: 799.5930786 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.1071770191192627\n",
      "Epoch: 158, Steps: 1 | Train Loss: 799.3200073 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.1285567283630371\n",
      "Epoch: 159, Steps: 1 | Train Loss: 798.4357910 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.13379311561584473\n",
      "Epoch: 160, Steps: 1 | Train Loss: 798.5551147 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.128129243850708\n",
      "Epoch: 161, Steps: 1 | Train Loss: 800.9820557 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.12593746185302734\n",
      "Epoch: 162, Steps: 1 | Train Loss: 798.7742920 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.12432551383972168\n",
      "Epoch: 163, Steps: 1 | Train Loss: 798.8776245 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.12575674057006836\n",
      "Epoch: 164, Steps: 1 | Train Loss: 799.0958252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.1260383129119873\n",
      "Epoch: 165, Steps: 1 | Train Loss: 798.7042847 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.13869881629943848\n",
      "Epoch: 166, Steps: 1 | Train Loss: 800.5050049 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.14740324020385742\n",
      "Epoch: 167, Steps: 1 | Train Loss: 798.9786377 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.12013936042785645\n",
      "Epoch: 168, Steps: 1 | Train Loss: 799.3582764 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.12067317962646484\n",
      "Epoch: 169, Steps: 1 | Train Loss: 798.0481567 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.12026286125183105\n",
      "Epoch: 170, Steps: 1 | Train Loss: 799.9206543 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11633443832397461\n",
      "Epoch: 171, Steps: 1 | Train Loss: 798.4086304 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11220359802246094\n",
      "Epoch: 172, Steps: 1 | Train Loss: 798.0444336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11158490180969238\n",
      "Epoch: 173, Steps: 1 | Train Loss: 799.3226929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10953879356384277\n",
      "Epoch: 174, Steps: 1 | Train Loss: 799.1144409 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10959982872009277\n",
      "Epoch: 175, Steps: 1 | Train Loss: 801.0453491 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.11020493507385254\n",
      "Epoch: 176, Steps: 1 | Train Loss: 799.8472900 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.10939383506774902\n",
      "Epoch: 177, Steps: 1 | Train Loss: 799.3970947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11060690879821777\n",
      "Epoch: 178, Steps: 1 | Train Loss: 799.0692749 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10882806777954102\n",
      "Epoch: 179, Steps: 1 | Train Loss: 799.4573975 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11015105247497559\n",
      "Epoch: 180, Steps: 1 | Train Loss: 800.6621704 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11070466041564941\n",
      "Epoch: 181, Steps: 1 | Train Loss: 797.1881714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.12354803085327148\n",
      "Epoch: 182, Steps: 1 | Train Loss: 797.7830811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11927556991577148\n",
      "Epoch: 183, Steps: 1 | Train Loss: 800.4159546 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11249876022338867\n",
      "Epoch: 184, Steps: 1 | Train Loss: 798.0099487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11052441596984863\n",
      "Epoch: 185, Steps: 1 | Train Loss: 799.1455078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11094355583190918\n",
      "Epoch: 186, Steps: 1 | Train Loss: 798.5640869 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.1099095344543457\n",
      "Epoch: 187, Steps: 1 | Train Loss: 800.2430420 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11014413833618164\n",
      "Epoch: 188, Steps: 1 | Train Loss: 798.6279907 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.11016297340393066\n",
      "Epoch: 189, Steps: 1 | Train Loss: 798.7116089 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12235403060913086\n",
      "Epoch: 190, Steps: 1 | Train Loss: 801.0380859 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.12285113334655762\n",
      "Epoch: 191, Steps: 1 | Train Loss: 798.4677124 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.1128838062286377\n",
      "Epoch: 192, Steps: 1 | Train Loss: 798.1796875 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.11211109161376953\n",
      "Epoch: 193, Steps: 1 | Train Loss: 798.6251831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11057472229003906\n",
      "Epoch: 194, Steps: 1 | Train Loss: 799.5789185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10985064506530762\n",
      "Epoch: 195, Steps: 1 | Train Loss: 799.5338745 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11028242111206055\n",
      "Epoch: 196, Steps: 1 | Train Loss: 799.9984741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.10991048812866211\n",
      "Epoch: 197, Steps: 1 | Train Loss: 798.7304077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11043167114257812\n",
      "Epoch: 198, Steps: 1 | Train Loss: 798.2123413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10902738571166992\n",
      "Epoch: 199, Steps: 1 | Train Loss: 798.3565063 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.10998344421386719\n",
      "Epoch: 200, Steps: 1 | Train Loss: 799.0883179 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.12246036529541016\n",
      "Epoch: 201, Steps: 1 | Train Loss: 799.4084473 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.1228184700012207\n",
      "Epoch: 202, Steps: 1 | Train Loss: 801.3786011 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.1179344654083252\n",
      "Epoch: 203, Steps: 1 | Train Loss: 800.9166260 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.1135246753692627\n",
      "Epoch: 204, Steps: 1 | Train Loss: 799.0563354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.11055588722229004\n",
      "Epoch: 205, Steps: 1 | Train Loss: 798.5547485 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.11115789413452148\n",
      "Epoch: 206, Steps: 1 | Train Loss: 799.0280151 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10968160629272461\n",
      "Epoch: 207, Steps: 1 | Train Loss: 798.5155029 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.11019396781921387\n",
      "Epoch: 208, Steps: 1 | Train Loss: 798.6018066 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10589051246643066\n",
      "Epoch: 209, Steps: 1 | Train Loss: 798.0603027 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11101531982421875\n",
      "Epoch: 210, Steps: 1 | Train Loss: 799.1100464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.11606550216674805\n",
      "Epoch: 211, Steps: 1 | Train Loss: 799.8889771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.1114349365234375\n",
      "Epoch: 212, Steps: 1 | Train Loss: 799.9708862 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.12430882453918457\n",
      "Epoch: 213, Steps: 1 | Train Loss: 799.1940308 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11861920356750488\n",
      "Epoch: 214, Steps: 1 | Train Loss: 798.6746826 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11234188079833984\n",
      "Epoch: 215, Steps: 1 | Train Loss: 799.6050415 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11145877838134766\n",
      "Epoch: 216, Steps: 1 | Train Loss: 798.6770020 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11189436912536621\n",
      "Epoch: 217, Steps: 1 | Train Loss: 798.1078491 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.1115121841430664\n",
      "Epoch: 218, Steps: 1 | Train Loss: 798.5362549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.11100411415100098\n",
      "Epoch: 219, Steps: 1 | Train Loss: 801.2363281 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.11517810821533203\n",
      "Epoch: 220, Steps: 1 | Train Loss: 799.5349121 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11093854904174805\n",
      "Epoch: 221, Steps: 1 | Train Loss: 799.4083252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.1099390983581543\n",
      "Epoch: 222, Steps: 1 | Train Loss: 798.6713867 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11008667945861816\n",
      "Epoch: 223, Steps: 1 | Train Loss: 798.8336182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.12245869636535645\n",
      "Epoch: 224, Steps: 1 | Train Loss: 797.6201172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.12052440643310547\n",
      "Epoch: 225, Steps: 1 | Train Loss: 798.9883423 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11247730255126953\n",
      "Epoch: 226, Steps: 1 | Train Loss: 799.1674194 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.11154031753540039\n",
      "Epoch: 227, Steps: 1 | Train Loss: 798.9923706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11085391044616699\n",
      "Epoch: 228, Steps: 1 | Train Loss: 798.7453613 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.11038947105407715\n",
      "Epoch: 229, Steps: 1 | Train Loss: 798.3134155 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11086583137512207\n",
      "Epoch: 230, Steps: 1 | Train Loss: 800.5773315 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.11034798622131348\n",
      "Epoch: 231, Steps: 1 | Train Loss: 800.2265625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.11042523384094238\n",
      "Epoch: 232, Steps: 1 | Train Loss: 798.6580811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.10961008071899414\n",
      "Epoch: 233, Steps: 1 | Train Loss: 799.5020752 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11062169075012207\n",
      "Epoch: 234, Steps: 1 | Train Loss: 800.5394897 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11066579818725586\n",
      "Epoch: 235, Steps: 1 | Train Loss: 801.9628296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.10964393615722656\n",
      "Epoch: 236, Steps: 1 | Train Loss: 799.3666382 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.10940098762512207\n",
      "Epoch: 237, Steps: 1 | Train Loss: 800.5880737 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10988783836364746\n",
      "Epoch: 238, Steps: 1 | Train Loss: 797.4183960 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10968422889709473\n",
      "Epoch: 239, Steps: 1 | Train Loss: 799.3588257 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.11027765274047852\n",
      "Epoch: 240, Steps: 1 | Train Loss: 798.5664673 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.1219644546508789\n",
      "Epoch: 241, Steps: 1 | Train Loss: 799.2171631 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11419057846069336\n",
      "Epoch: 242, Steps: 1 | Train Loss: 799.8295288 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.11147475242614746\n",
      "Epoch: 243, Steps: 1 | Train Loss: 799.0492554 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.11150407791137695\n",
      "Epoch: 244, Steps: 1 | Train Loss: 799.4107666 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.11084914207458496\n",
      "Epoch: 245, Steps: 1 | Train Loss: 798.2219849 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11033034324645996\n",
      "Epoch: 246, Steps: 1 | Train Loss: 799.7958374 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11052489280700684\n",
      "Epoch: 247, Steps: 1 | Train Loss: 798.0382080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10921573638916016\n",
      "Epoch: 248, Steps: 1 | Train Loss: 798.0935669 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11033082008361816\n",
      "Epoch: 249, Steps: 1 | Train Loss: 799.0136108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10936975479125977\n",
      "Epoch: 250, Steps: 1 | Train Loss: 798.9518433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "21\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_2>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.1258094310760498\n",
      "Epoch: 1, Steps: 1 | Train Loss: 815.7452393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.12224006652832031\n",
      "Epoch: 2, Steps: 1 | Train Loss: 809.7043457 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11705565452575684\n",
      "Epoch: 3, Steps: 1 | Train Loss: 805.6130981 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.116790771484375\n",
      "Epoch: 4, Steps: 1 | Train Loss: 805.1864624 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11645317077636719\n",
      "Epoch: 5, Steps: 1 | Train Loss: 803.8919067 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11824417114257812\n",
      "Epoch: 6, Steps: 1 | Train Loss: 802.0980835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.12231183052062988\n",
      "Epoch: 7, Steps: 1 | Train Loss: 801.5556641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.12369990348815918\n",
      "Epoch: 8, Steps: 1 | Train Loss: 802.3146362 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11982178688049316\n",
      "Epoch: 9, Steps: 1 | Train Loss: 802.3753662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.11434364318847656\n",
      "Epoch: 10, Steps: 1 | Train Loss: 802.4199829 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.11189889907836914\n",
      "Epoch: 11, Steps: 1 | Train Loss: 802.9073486 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11019086837768555\n",
      "Epoch: 12, Steps: 1 | Train Loss: 801.8284912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.1094667911529541\n",
      "Epoch: 13, Steps: 1 | Train Loss: 802.0992432 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.1099092960357666\n",
      "Epoch: 14, Steps: 1 | Train Loss: 801.8976440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.11076521873474121\n",
      "Epoch: 15, Steps: 1 | Train Loss: 802.3088989 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10959076881408691\n",
      "Epoch: 16, Steps: 1 | Train Loss: 803.0897217 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.10980772972106934\n",
      "Epoch: 17, Steps: 1 | Train Loss: 801.3954468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.10970449447631836\n",
      "Epoch: 18, Steps: 1 | Train Loss: 801.4959106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.11094331741333008\n",
      "Epoch: 19, Steps: 1 | Train Loss: 802.2412720 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.12351751327514648\n",
      "Epoch: 20, Steps: 1 | Train Loss: 801.0941162 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.12245059013366699\n",
      "Epoch: 21, Steps: 1 | Train Loss: 800.5415649 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.11854195594787598\n",
      "Epoch: 22, Steps: 1 | Train Loss: 801.5877075 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11214971542358398\n",
      "Epoch: 23, Steps: 1 | Train Loss: 801.5650635 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11152958869934082\n",
      "Epoch: 24, Steps: 1 | Train Loss: 802.8668213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11181378364562988\n",
      "Epoch: 25, Steps: 1 | Train Loss: 802.5802002 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11006760597229004\n",
      "Epoch: 26, Steps: 1 | Train Loss: 800.7484741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11022591590881348\n",
      "Epoch: 27, Steps: 1 | Train Loss: 802.4648438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.10927891731262207\n",
      "Epoch: 28, Steps: 1 | Train Loss: 802.9518433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.11061429977416992\n",
      "Epoch: 29, Steps: 1 | Train Loss: 802.3900757 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.1099402904510498\n",
      "Epoch: 30, Steps: 1 | Train Loss: 802.7993164 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.1102607250213623\n",
      "Epoch: 31, Steps: 1 | Train Loss: 801.9913330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.11068153381347656\n",
      "Epoch: 32, Steps: 1 | Train Loss: 800.6280518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.12140488624572754\n",
      "Epoch: 33, Steps: 1 | Train Loss: 801.6268921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.11353731155395508\n",
      "Epoch: 34, Steps: 1 | Train Loss: 801.2144775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.11110901832580566\n",
      "Epoch: 35, Steps: 1 | Train Loss: 801.8111572 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.11113643646240234\n",
      "Epoch: 36, Steps: 1 | Train Loss: 801.7550659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10949850082397461\n",
      "Epoch: 37, Steps: 1 | Train Loss: 801.9270020 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10982632637023926\n",
      "Epoch: 38, Steps: 1 | Train Loss: 800.8100586 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10962629318237305\n",
      "Epoch: 39, Steps: 1 | Train Loss: 801.0858765 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.11047649383544922\n",
      "Epoch: 40, Steps: 1 | Train Loss: 802.1680908 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10983467102050781\n",
      "Epoch: 41, Steps: 1 | Train Loss: 802.2528687 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.1095583438873291\n",
      "Epoch: 42, Steps: 1 | Train Loss: 802.4620972 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.1098623275756836\n",
      "Epoch: 43, Steps: 1 | Train Loss: 802.4061890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.12241792678833008\n",
      "Epoch: 44, Steps: 1 | Train Loss: 802.9661865 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.12214040756225586\n",
      "Epoch: 45, Steps: 1 | Train Loss: 802.4439697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.12262630462646484\n",
      "Epoch: 46, Steps: 1 | Train Loss: 801.6919556 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11850762367248535\n",
      "Epoch: 47, Steps: 1 | Train Loss: 801.1414795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.11364936828613281\n",
      "Epoch: 48, Steps: 1 | Train Loss: 801.7875977 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.1108102798461914\n",
      "Epoch: 49, Steps: 1 | Train Loss: 800.1776733 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.11252975463867188\n",
      "Epoch: 50, Steps: 1 | Train Loss: 801.2057495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11160612106323242\n",
      "Epoch: 51, Steps: 1 | Train Loss: 802.8715820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10970020294189453\n",
      "Epoch: 52, Steps: 1 | Train Loss: 802.3294067 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10983967781066895\n",
      "Epoch: 53, Steps: 1 | Train Loss: 802.8817139 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.1108405590057373\n",
      "Epoch: 54, Steps: 1 | Train Loss: 801.4047852 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.12266921997070312\n",
      "Epoch: 55, Steps: 1 | Train Loss: 799.1227417 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.10656952857971191\n",
      "Epoch: 56, Steps: 1 | Train Loss: 801.5713501 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10613131523132324\n",
      "Epoch: 57, Steps: 1 | Train Loss: 800.9770508 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.11127662658691406\n",
      "Epoch: 58, Steps: 1 | Train Loss: 801.7313843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.1226191520690918\n",
      "Epoch: 59, Steps: 1 | Train Loss: 800.7244873 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.1238396167755127\n",
      "Epoch: 60, Steps: 1 | Train Loss: 802.3085938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.12310123443603516\n",
      "Epoch: 61, Steps: 1 | Train Loss: 802.1348267 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.12330961227416992\n",
      "Epoch: 62, Steps: 1 | Train Loss: 801.3375854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.12009692192077637\n",
      "Epoch: 63, Steps: 1 | Train Loss: 801.9290161 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.11192846298217773\n",
      "Epoch: 64, Steps: 1 | Train Loss: 801.6222534 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11121344566345215\n",
      "Epoch: 65, Steps: 1 | Train Loss: 801.3937988 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11073589324951172\n",
      "Epoch: 66, Steps: 1 | Train Loss: 801.4421997 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11103606224060059\n",
      "Epoch: 67, Steps: 1 | Train Loss: 802.2318726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.1104593276977539\n",
      "Epoch: 68, Steps: 1 | Train Loss: 801.3414917 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.11083817481994629\n",
      "Epoch: 69, Steps: 1 | Train Loss: 800.6005249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11060571670532227\n",
      "Epoch: 70, Steps: 1 | Train Loss: 801.9275513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.11061263084411621\n",
      "Epoch: 71, Steps: 1 | Train Loss: 802.6866455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.11058783531188965\n",
      "Epoch: 72, Steps: 1 | Train Loss: 801.4087524 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11123514175415039\n",
      "Epoch: 73, Steps: 1 | Train Loss: 801.2507324 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.11100578308105469\n",
      "Epoch: 74, Steps: 1 | Train Loss: 801.5721436 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11055731773376465\n",
      "Epoch: 75, Steps: 1 | Train Loss: 801.6667480 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11049461364746094\n",
      "Epoch: 76, Steps: 1 | Train Loss: 802.4076538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.11063122749328613\n",
      "Epoch: 77, Steps: 1 | Train Loss: 802.5658569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.12295055389404297\n",
      "Epoch: 78, Steps: 1 | Train Loss: 801.9638062 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.12248682975769043\n",
      "Epoch: 79, Steps: 1 | Train Loss: 802.8829346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.12244367599487305\n",
      "Epoch: 80, Steps: 1 | Train Loss: 801.6743164 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.11110949516296387\n",
      "Epoch: 81, Steps: 1 | Train Loss: 801.9935913 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.11134219169616699\n",
      "Epoch: 82, Steps: 1 | Train Loss: 801.6553955 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.11002278327941895\n",
      "Epoch: 83, Steps: 1 | Train Loss: 802.1173096 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.10930705070495605\n",
      "Epoch: 84, Steps: 1 | Train Loss: 803.8040161 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.1094827651977539\n",
      "Epoch: 85, Steps: 1 | Train Loss: 801.1331177 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.10968160629272461\n",
      "Epoch: 86, Steps: 1 | Train Loss: 802.7639771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.12282419204711914\n",
      "Epoch: 87, Steps: 1 | Train Loss: 802.6275024 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.12300515174865723\n",
      "Epoch: 88, Steps: 1 | Train Loss: 802.8946533 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.12254071235656738\n",
      "Epoch: 89, Steps: 1 | Train Loss: 802.9288940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.1189565658569336\n",
      "Epoch: 90, Steps: 1 | Train Loss: 802.4420776 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.11341118812561035\n",
      "Epoch: 91, Steps: 1 | Train Loss: 800.9135742 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.11171150207519531\n",
      "Epoch: 92, Steps: 1 | Train Loss: 800.9185791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.11114954948425293\n",
      "Epoch: 93, Steps: 1 | Train Loss: 800.6716309 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10994696617126465\n",
      "Epoch: 94, Steps: 1 | Train Loss: 801.7650757 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.1111900806427002\n",
      "Epoch: 95, Steps: 1 | Train Loss: 800.2982178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10999631881713867\n",
      "Epoch: 96, Steps: 1 | Train Loss: 800.6209717 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.1095726490020752\n",
      "Epoch: 97, Steps: 1 | Train Loss: 801.2553711 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.1103665828704834\n",
      "Epoch: 98, Steps: 1 | Train Loss: 802.1859741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10955023765563965\n",
      "Epoch: 99, Steps: 1 | Train Loss: 802.3192749 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11120724678039551\n",
      "Epoch: 100, Steps: 1 | Train Loss: 801.9149780 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10964226722717285\n",
      "Epoch: 101, Steps: 1 | Train Loss: 801.9886475 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.11075258255004883\n",
      "Epoch: 102, Steps: 1 | Train Loss: 802.4528198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.11358070373535156\n",
      "Epoch: 103, Steps: 1 | Train Loss: 800.9452515 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.11147737503051758\n",
      "Epoch: 104, Steps: 1 | Train Loss: 801.9387207 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.1123056411743164\n",
      "Epoch: 105, Steps: 1 | Train Loss: 802.5138550 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.1106119155883789\n",
      "Epoch: 106, Steps: 1 | Train Loss: 801.6557617 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10996270179748535\n",
      "Epoch: 107, Steps: 1 | Train Loss: 801.5673218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.11017012596130371\n",
      "Epoch: 108, Steps: 1 | Train Loss: 802.2282104 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.11076092720031738\n",
      "Epoch: 109, Steps: 1 | Train Loss: 802.0292969 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.10957980155944824\n",
      "Epoch: 110, Steps: 1 | Train Loss: 802.4185791 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.1228940486907959\n",
      "Epoch: 111, Steps: 1 | Train Loss: 801.0379028 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.12343859672546387\n",
      "Epoch: 112, Steps: 1 | Train Loss: 803.2449951 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11690258979797363\n",
      "Epoch: 113, Steps: 1 | Train Loss: 802.0089722 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.11208319664001465\n",
      "Epoch: 114, Steps: 1 | Train Loss: 802.0582275 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11184239387512207\n",
      "Epoch: 115, Steps: 1 | Train Loss: 801.6770630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11080312728881836\n",
      "Epoch: 116, Steps: 1 | Train Loss: 801.4576416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.11122298240661621\n",
      "Epoch: 117, Steps: 1 | Train Loss: 800.9988403 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.1100006103515625\n",
      "Epoch: 118, Steps: 1 | Train Loss: 802.1435547 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10953855514526367\n",
      "Epoch: 119, Steps: 1 | Train Loss: 801.1995239 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.11038851737976074\n",
      "Epoch: 120, Steps: 1 | Train Loss: 802.3021851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11034750938415527\n",
      "Epoch: 121, Steps: 1 | Train Loss: 803.5917358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.10992312431335449\n",
      "Epoch: 122, Steps: 1 | Train Loss: 802.7437744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10985589027404785\n",
      "Epoch: 123, Steps: 1 | Train Loss: 801.6713867 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.1099846363067627\n",
      "Epoch: 124, Steps: 1 | Train Loss: 801.2225952 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.12299537658691406\n",
      "Epoch: 125, Steps: 1 | Train Loss: 801.7539062 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.11986470222473145\n",
      "Epoch: 126, Steps: 1 | Train Loss: 801.9207153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.1132354736328125\n",
      "Epoch: 127, Steps: 1 | Train Loss: 801.3038330 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.11206793785095215\n",
      "Epoch: 128, Steps: 1 | Train Loss: 802.3041382 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.11050558090209961\n",
      "Epoch: 129, Steps: 1 | Train Loss: 801.9516602 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11131477355957031\n",
      "Epoch: 130, Steps: 1 | Train Loss: 802.4631958 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.10919666290283203\n",
      "Epoch: 131, Steps: 1 | Train Loss: 801.0309448 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.1100473403930664\n",
      "Epoch: 132, Steps: 1 | Train Loss: 801.6336060 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11030077934265137\n",
      "Epoch: 133, Steps: 1 | Train Loss: 803.4611206 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.11052155494689941\n",
      "Epoch: 134, Steps: 1 | Train Loss: 801.2719116 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10949420928955078\n",
      "Epoch: 135, Steps: 1 | Train Loss: 801.9726562 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.11011743545532227\n",
      "Epoch: 136, Steps: 1 | Train Loss: 801.5426636 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.1101083755493164\n",
      "Epoch: 137, Steps: 1 | Train Loss: 801.9580078 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.12275576591491699\n",
      "Epoch: 138, Steps: 1 | Train Loss: 801.1409912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.1227412223815918\n",
      "Epoch: 139, Steps: 1 | Train Loss: 801.9039307 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.11754202842712402\n",
      "Epoch: 140, Steps: 1 | Train Loss: 802.2218018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.1130056381225586\n",
      "Epoch: 141, Steps: 1 | Train Loss: 802.1373901 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.11034822463989258\n",
      "Epoch: 142, Steps: 1 | Train Loss: 801.7545776 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11095142364501953\n",
      "Epoch: 143, Steps: 1 | Train Loss: 801.9656372 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.10994887351989746\n",
      "Epoch: 144, Steps: 1 | Train Loss: 800.3428345 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10874056816101074\n",
      "Epoch: 145, Steps: 1 | Train Loss: 802.5239258 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.1105341911315918\n",
      "Epoch: 146, Steps: 1 | Train Loss: 801.5314941 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.11008524894714355\n",
      "Epoch: 147, Steps: 1 | Train Loss: 801.6539917 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11001205444335938\n",
      "Epoch: 148, Steps: 1 | Train Loss: 802.0946045 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10944509506225586\n",
      "Epoch: 149, Steps: 1 | Train Loss: 802.3886108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.11032390594482422\n",
      "Epoch: 150, Steps: 1 | Train Loss: 802.5559692 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.1102302074432373\n",
      "Epoch: 151, Steps: 1 | Train Loss: 801.6726685 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.12424802780151367\n",
      "Epoch: 152, Steps: 1 | Train Loss: 803.0857544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12130928039550781\n",
      "Epoch: 153, Steps: 1 | Train Loss: 802.2167358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.11348342895507812\n",
      "Epoch: 154, Steps: 1 | Train Loss: 801.2611694 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.11107325553894043\n",
      "Epoch: 155, Steps: 1 | Train Loss: 802.6069946 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.11050891876220703\n",
      "Epoch: 156, Steps: 1 | Train Loss: 801.3224487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.11046552658081055\n",
      "Epoch: 157, Steps: 1 | Train Loss: 801.7629395 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10998654365539551\n",
      "Epoch: 158, Steps: 1 | Train Loss: 801.9011230 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.11022257804870605\n",
      "Epoch: 159, Steps: 1 | Train Loss: 801.1105347 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11060357093811035\n",
      "Epoch: 160, Steps: 1 | Train Loss: 802.0050659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.1110684871673584\n",
      "Epoch: 161, Steps: 1 | Train Loss: 801.1223145 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.1094820499420166\n",
      "Epoch: 162, Steps: 1 | Train Loss: 802.4289551 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.11065292358398438\n",
      "Epoch: 163, Steps: 1 | Train Loss: 800.7664795 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.11041808128356934\n",
      "Epoch: 164, Steps: 1 | Train Loss: 802.5362549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.12242770195007324\n",
      "Epoch: 165, Steps: 1 | Train Loss: 801.5321045 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.12281250953674316\n",
      "Epoch: 166, Steps: 1 | Train Loss: 800.6244507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11769866943359375\n",
      "Epoch: 167, Steps: 1 | Train Loss: 801.5958862 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11265730857849121\n",
      "Epoch: 168, Steps: 1 | Train Loss: 801.7566528 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11194634437561035\n",
      "Epoch: 169, Steps: 1 | Train Loss: 802.6898804 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.10982704162597656\n",
      "Epoch: 170, Steps: 1 | Train Loss: 801.2533569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11031150817871094\n",
      "Epoch: 171, Steps: 1 | Train Loss: 802.0579834 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.10967636108398438\n",
      "Epoch: 172, Steps: 1 | Train Loss: 801.8886108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10953974723815918\n",
      "Epoch: 173, Steps: 1 | Train Loss: 802.8175049 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11047506332397461\n",
      "Epoch: 174, Steps: 1 | Train Loss: 801.2265625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.11041998863220215\n",
      "Epoch: 175, Steps: 1 | Train Loss: 802.0818481 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.11119532585144043\n",
      "Epoch: 176, Steps: 1 | Train Loss: 802.7309570 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.12383294105529785\n",
      "Epoch: 177, Steps: 1 | Train Loss: 801.6679688 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.12208127975463867\n",
      "Epoch: 178, Steps: 1 | Train Loss: 801.3872070 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.11426806449890137\n",
      "Epoch: 179, Steps: 1 | Train Loss: 801.3181763 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11149096488952637\n",
      "Epoch: 180, Steps: 1 | Train Loss: 802.1329346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11034321784973145\n",
      "Epoch: 181, Steps: 1 | Train Loss: 801.1320190 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11131501197814941\n",
      "Epoch: 182, Steps: 1 | Train Loss: 802.2760620 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11073827743530273\n",
      "Epoch: 183, Steps: 1 | Train Loss: 801.7079468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.1101071834564209\n",
      "Epoch: 184, Steps: 1 | Train Loss: 801.8442993 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11219549179077148\n",
      "Epoch: 185, Steps: 1 | Train Loss: 802.1954956 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11116838455200195\n",
      "Epoch: 186, Steps: 1 | Train Loss: 802.2961426 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11011099815368652\n",
      "Epoch: 187, Steps: 1 | Train Loss: 802.0280151 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11124205589294434\n",
      "Epoch: 188, Steps: 1 | Train Loss: 802.8855591 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.12368917465209961\n",
      "Epoch: 189, Steps: 1 | Train Loss: 802.5004883 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.11309504508972168\n",
      "Epoch: 190, Steps: 1 | Train Loss: 801.9545898 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.11211776733398438\n",
      "Epoch: 191, Steps: 1 | Train Loss: 801.7550049 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.11116743087768555\n",
      "Epoch: 192, Steps: 1 | Train Loss: 801.0278320 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.1108551025390625\n",
      "Epoch: 193, Steps: 1 | Train Loss: 802.7259521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11069726943969727\n",
      "Epoch: 194, Steps: 1 | Train Loss: 802.4570923 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11010551452636719\n",
      "Epoch: 195, Steps: 1 | Train Loss: 802.3708496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11027050018310547\n",
      "Epoch: 196, Steps: 1 | Train Loss: 801.7539062 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.1102297306060791\n",
      "Epoch: 197, Steps: 1 | Train Loss: 801.4683228 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11017751693725586\n",
      "Epoch: 198, Steps: 1 | Train Loss: 802.4348145 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11103129386901855\n",
      "Epoch: 199, Steps: 1 | Train Loss: 802.5369263 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.1232445240020752\n",
      "Epoch: 200, Steps: 1 | Train Loss: 802.3778687 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.12331128120422363\n",
      "Epoch: 201, Steps: 1 | Train Loss: 802.0507202 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.1146242618560791\n",
      "Epoch: 202, Steps: 1 | Train Loss: 801.1682129 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.1123189926147461\n",
      "Epoch: 203, Steps: 1 | Train Loss: 802.5444336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.11023521423339844\n",
      "Epoch: 204, Steps: 1 | Train Loss: 802.1249390 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.1109614372253418\n",
      "Epoch: 205, Steps: 1 | Train Loss: 801.5144653 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.110015869140625\n",
      "Epoch: 206, Steps: 1 | Train Loss: 802.1600952 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11109590530395508\n",
      "Epoch: 207, Steps: 1 | Train Loss: 801.4597778 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.10958051681518555\n",
      "Epoch: 208, Steps: 1 | Train Loss: 800.3066406 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.1108856201171875\n",
      "Epoch: 209, Steps: 1 | Train Loss: 801.3165894 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11054801940917969\n",
      "Epoch: 210, Steps: 1 | Train Loss: 801.5203857 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.1101987361907959\n",
      "Epoch: 211, Steps: 1 | Train Loss: 800.2536011 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.110504150390625\n",
      "Epoch: 212, Steps: 1 | Train Loss: 800.9166870 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.1234593391418457\n",
      "Epoch: 213, Steps: 1 | Train Loss: 802.8236084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.1234900951385498\n",
      "Epoch: 214, Steps: 1 | Train Loss: 800.6997070 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11650633811950684\n",
      "Epoch: 215, Steps: 1 | Train Loss: 801.2880249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.1129922866821289\n",
      "Epoch: 216, Steps: 1 | Train Loss: 802.3393555 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11197066307067871\n",
      "Epoch: 217, Steps: 1 | Train Loss: 802.2745972 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.11119389533996582\n",
      "Epoch: 218, Steps: 1 | Train Loss: 803.1099854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.11023926734924316\n",
      "Epoch: 219, Steps: 1 | Train Loss: 800.6090698 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.1102745532989502\n",
      "Epoch: 220, Steps: 1 | Train Loss: 801.5219116 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11062741279602051\n",
      "Epoch: 221, Steps: 1 | Train Loss: 801.3917236 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.11045241355895996\n",
      "Epoch: 222, Steps: 1 | Train Loss: 801.2824097 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11032891273498535\n",
      "Epoch: 223, Steps: 1 | Train Loss: 801.9921265 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11112594604492188\n",
      "Epoch: 224, Steps: 1 | Train Loss: 802.7697144 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11020469665527344\n",
      "Epoch: 225, Steps: 1 | Train Loss: 801.0005493 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11012864112854004\n",
      "Epoch: 226, Steps: 1 | Train Loss: 802.3168945 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.1102609634399414\n",
      "Epoch: 227, Steps: 1 | Train Loss: 802.4894409 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11013484001159668\n",
      "Epoch: 228, Steps: 1 | Train Loss: 802.0647583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.1231374740600586\n",
      "Epoch: 229, Steps: 1 | Train Loss: 803.1578979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.12380814552307129\n",
      "Epoch: 230, Steps: 1 | Train Loss: 800.0613403 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.12333297729492188\n",
      "Epoch: 231, Steps: 1 | Train Loss: 803.6781616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.12373232841491699\n",
      "Epoch: 232, Steps: 1 | Train Loss: 801.6940918 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.1264934539794922\n",
      "Epoch: 233, Steps: 1 | Train Loss: 801.9430542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.12012195587158203\n",
      "Epoch: 234, Steps: 1 | Train Loss: 801.1662598 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11338639259338379\n",
      "Epoch: 235, Steps: 1 | Train Loss: 802.7640381 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11204028129577637\n",
      "Epoch: 236, Steps: 1 | Train Loss: 800.8217773 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11040639877319336\n",
      "Epoch: 237, Steps: 1 | Train Loss: 801.1893921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.10987281799316406\n",
      "Epoch: 238, Steps: 1 | Train Loss: 802.3360596 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.10980391502380371\n",
      "Epoch: 239, Steps: 1 | Train Loss: 802.9118042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.11040830612182617\n",
      "Epoch: 240, Steps: 1 | Train Loss: 801.2055054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.11052298545837402\n",
      "Epoch: 241, Steps: 1 | Train Loss: 801.6669312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11064624786376953\n",
      "Epoch: 242, Steps: 1 | Train Loss: 801.8043213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.10928797721862793\n",
      "Epoch: 243, Steps: 1 | Train Loss: 800.5346680 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.11413407325744629\n",
      "Epoch: 244, Steps: 1 | Train Loss: 802.0699463 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.11208033561706543\n",
      "Epoch: 245, Steps: 1 | Train Loss: 801.1565552 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.11163544654846191\n",
      "Epoch: 246, Steps: 1 | Train Loss: 801.9390259 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11124205589294434\n",
      "Epoch: 247, Steps: 1 | Train Loss: 802.9551392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.11154389381408691\n",
      "Epoch: 248, Steps: 1 | Train Loss: 802.1782227 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11077213287353516\n",
      "Epoch: 249, Steps: 1 | Train Loss: 801.7691650 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11153149604797363\n",
      "Epoch: 250, Steps: 1 | Train Loss: 802.2465210 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "21\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_3>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11344265937805176\n",
      "Epoch: 1, Steps: 1 | Train Loss: 815.8112183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10975265502929688\n",
      "Epoch: 2, Steps: 1 | Train Loss: 808.6526489 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.10994410514831543\n",
      "Epoch: 3, Steps: 1 | Train Loss: 801.9657593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10996222496032715\n",
      "Epoch: 4, Steps: 1 | Train Loss: 798.7296753 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.10932660102844238\n",
      "Epoch: 5, Steps: 1 | Train Loss: 798.5161133 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11285090446472168\n",
      "Epoch: 6, Steps: 1 | Train Loss: 798.2504883 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11221861839294434\n",
      "Epoch: 7, Steps: 1 | Train Loss: 796.7086182 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.11069726943969727\n",
      "Epoch: 8, Steps: 1 | Train Loss: 796.8652344 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11063003540039062\n",
      "Epoch: 9, Steps: 1 | Train Loss: 797.5455322 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.11173248291015625\n",
      "Epoch: 10, Steps: 1 | Train Loss: 795.9430542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.11068439483642578\n",
      "Epoch: 11, Steps: 1 | Train Loss: 797.1237183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11066341400146484\n",
      "Epoch: 12, Steps: 1 | Train Loss: 798.2944336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11034846305847168\n",
      "Epoch: 13, Steps: 1 | Train Loss: 796.2813721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.11141800880432129\n",
      "Epoch: 14, Steps: 1 | Train Loss: 796.3651123 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.1097409725189209\n",
      "Epoch: 15, Steps: 1 | Train Loss: 795.5989990 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.12282562255859375\n",
      "Epoch: 16, Steps: 1 | Train Loss: 796.8881836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.12244963645935059\n",
      "Epoch: 17, Steps: 1 | Train Loss: 797.1066284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.12307548522949219\n",
      "Epoch: 18, Steps: 1 | Train Loss: 796.7442627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.11252140998840332\n",
      "Epoch: 19, Steps: 1 | Train Loss: 797.5001221 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.110687255859375\n",
      "Epoch: 20, Steps: 1 | Train Loss: 798.3538208 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.10864710807800293\n",
      "Epoch: 21, Steps: 1 | Train Loss: 797.6124878 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10748410224914551\n",
      "Epoch: 22, Steps: 1 | Train Loss: 797.4776611 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.10803461074829102\n",
      "Epoch: 23, Steps: 1 | Train Loss: 795.7987671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.10804224014282227\n",
      "Epoch: 24, Steps: 1 | Train Loss: 798.2817993 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.10791349411010742\n",
      "Epoch: 25, Steps: 1 | Train Loss: 796.9995728 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12220549583435059\n",
      "Epoch: 26, Steps: 1 | Train Loss: 796.8512573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11291146278381348\n",
      "Epoch: 27, Steps: 1 | Train Loss: 797.9837036 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.1095879077911377\n",
      "Epoch: 28, Steps: 1 | Train Loss: 797.9721069 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10862159729003906\n",
      "Epoch: 29, Steps: 1 | Train Loss: 797.5731201 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.10815858840942383\n",
      "Epoch: 30, Steps: 1 | Train Loss: 797.5295410 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.10756444931030273\n",
      "Epoch: 31, Steps: 1 | Train Loss: 797.6897583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10764384269714355\n",
      "Epoch: 32, Steps: 1 | Train Loss: 796.8779907 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10775923728942871\n",
      "Epoch: 33, Steps: 1 | Train Loss: 797.8832397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10772323608398438\n",
      "Epoch: 34, Steps: 1 | Train Loss: 797.2701416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10799026489257812\n",
      "Epoch: 35, Steps: 1 | Train Loss: 796.1605835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.1078333854675293\n",
      "Epoch: 36, Steps: 1 | Train Loss: 797.8515015 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.12218642234802246\n",
      "Epoch: 37, Steps: 1 | Train Loss: 797.7167358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11266636848449707\n",
      "Epoch: 38, Steps: 1 | Train Loss: 799.1203003 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10954141616821289\n",
      "Epoch: 39, Steps: 1 | Train Loss: 798.5708618 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.1093287467956543\n",
      "Epoch: 40, Steps: 1 | Train Loss: 796.7802734 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10806655883789062\n",
      "Epoch: 41, Steps: 1 | Train Loss: 796.8297729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10756325721740723\n",
      "Epoch: 42, Steps: 1 | Train Loss: 796.9021606 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10772490501403809\n",
      "Epoch: 43, Steps: 1 | Train Loss: 797.0631714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10785961151123047\n",
      "Epoch: 44, Steps: 1 | Train Loss: 797.9574585 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10828399658203125\n",
      "Epoch: 45, Steps: 1 | Train Loss: 797.8455811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.10891366004943848\n",
      "Epoch: 46, Steps: 1 | Train Loss: 797.6638184 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.1075446605682373\n",
      "Epoch: 47, Steps: 1 | Train Loss: 798.0190430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.1211707592010498\n",
      "Epoch: 48, Steps: 1 | Train Loss: 797.0048828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.12112212181091309\n",
      "Epoch: 49, Steps: 1 | Train Loss: 795.0086670 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.12136030197143555\n",
      "Epoch: 50, Steps: 1 | Train Loss: 797.9841919 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.11148357391357422\n",
      "Epoch: 51, Steps: 1 | Train Loss: 795.6193237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10947275161743164\n",
      "Epoch: 52, Steps: 1 | Train Loss: 797.8530273 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.10867094993591309\n",
      "Epoch: 53, Steps: 1 | Train Loss: 798.3775024 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10782003402709961\n",
      "Epoch: 54, Steps: 1 | Train Loss: 799.2372437 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.10768437385559082\n",
      "Epoch: 55, Steps: 1 | Train Loss: 798.4071655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.10773181915283203\n",
      "Epoch: 56, Steps: 1 | Train Loss: 797.6101685 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.10792231559753418\n",
      "Epoch: 57, Steps: 1 | Train Loss: 798.1786499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.10745763778686523\n",
      "Epoch: 58, Steps: 1 | Train Loss: 797.1161499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.10779666900634766\n",
      "Epoch: 59, Steps: 1 | Train Loss: 796.5184937 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10776138305664062\n",
      "Epoch: 60, Steps: 1 | Train Loss: 797.8723755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.1076040267944336\n",
      "Epoch: 61, Steps: 1 | Train Loss: 797.1707153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.10749411582946777\n",
      "Epoch: 62, Steps: 1 | Train Loss: 798.9817505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.12135553359985352\n",
      "Epoch: 63, Steps: 1 | Train Loss: 796.2714844 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.12090492248535156\n",
      "Epoch: 64, Steps: 1 | Train Loss: 796.9055786 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11662888526916504\n",
      "Epoch: 65, Steps: 1 | Train Loss: 797.4835815 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11053776741027832\n",
      "Epoch: 66, Steps: 1 | Train Loss: 798.6475830 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11040973663330078\n",
      "Epoch: 67, Steps: 1 | Train Loss: 796.7225342 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.10872340202331543\n",
      "Epoch: 68, Steps: 1 | Train Loss: 797.6381836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10796451568603516\n",
      "Epoch: 69, Steps: 1 | Train Loss: 799.3353882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.10791277885437012\n",
      "Epoch: 70, Steps: 1 | Train Loss: 798.9688721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10799813270568848\n",
      "Epoch: 71, Steps: 1 | Train Loss: 797.3539429 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10800623893737793\n",
      "Epoch: 72, Steps: 1 | Train Loss: 797.3106689 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.1077125072479248\n",
      "Epoch: 73, Steps: 1 | Train Loss: 796.7166748 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.11487007141113281\n",
      "Epoch: 74, Steps: 1 | Train Loss: 797.7372437 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.10997986793518066\n",
      "Epoch: 75, Steps: 1 | Train Loss: 796.9359131 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.12461018562316895\n",
      "Epoch: 76, Steps: 1 | Train Loss: 795.9091797 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.12558317184448242\n",
      "Epoch: 77, Steps: 1 | Train Loss: 796.9539185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.12580466270446777\n",
      "Epoch: 78, Steps: 1 | Train Loss: 798.3664551 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.11692953109741211\n",
      "Epoch: 79, Steps: 1 | Train Loss: 797.2375488 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.12850308418273926\n",
      "Epoch: 80, Steps: 1 | Train Loss: 798.6134644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.13351058959960938\n",
      "Epoch: 81, Steps: 1 | Train Loss: 796.9369507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.13661932945251465\n",
      "Epoch: 82, Steps: 1 | Train Loss: 797.3224487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.13773441314697266\n",
      "Epoch: 83, Steps: 1 | Train Loss: 797.1090698 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.13782238960266113\n",
      "Epoch: 84, Steps: 1 | Train Loss: 797.4345093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.13911819458007812\n",
      "Epoch: 85, Steps: 1 | Train Loss: 797.1362915 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.11799454689025879\n",
      "Epoch: 86, Steps: 1 | Train Loss: 797.2973633 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.12314057350158691\n",
      "Epoch: 87, Steps: 1 | Train Loss: 798.0170288 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.1155710220336914\n",
      "Epoch: 88, Steps: 1 | Train Loss: 797.4274902 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.10988521575927734\n",
      "Epoch: 89, Steps: 1 | Train Loss: 797.3908081 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10831141471862793\n",
      "Epoch: 90, Steps: 1 | Train Loss: 797.1360474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10820150375366211\n",
      "Epoch: 91, Steps: 1 | Train Loss: 796.9552612 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10781097412109375\n",
      "Epoch: 92, Steps: 1 | Train Loss: 796.2728271 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.11549925804138184\n",
      "Epoch: 93, Steps: 1 | Train Loss: 797.2502441 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.11656665802001953\n",
      "Epoch: 94, Steps: 1 | Train Loss: 795.4121704 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.13496088981628418\n",
      "Epoch: 95, Steps: 1 | Train Loss: 796.5095215 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.12541651725769043\n",
      "Epoch: 96, Steps: 1 | Train Loss: 797.3809814 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.1212000846862793\n",
      "Epoch: 97, Steps: 1 | Train Loss: 798.1478882 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.12484407424926758\n",
      "Epoch: 98, Steps: 1 | Train Loss: 797.1370239 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.12155723571777344\n",
      "Epoch: 99, Steps: 1 | Train Loss: 798.7945557 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.12239575386047363\n",
      "Epoch: 100, Steps: 1 | Train Loss: 798.2862549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.13315963745117188\n",
      "Epoch: 101, Steps: 1 | Train Loss: 797.7077026 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.14725875854492188\n",
      "Epoch: 102, Steps: 1 | Train Loss: 798.7769775 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.14568519592285156\n",
      "Epoch: 103, Steps: 1 | Train Loss: 796.5916138 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.1289687156677246\n",
      "Epoch: 104, Steps: 1 | Train Loss: 796.8881836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.1369943618774414\n",
      "Epoch: 105, Steps: 1 | Train Loss: 798.1438599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.1373450756072998\n",
      "Epoch: 106, Steps: 1 | Train Loss: 796.2459106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.13068628311157227\n",
      "Epoch: 107, Steps: 1 | Train Loss: 796.8027344 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.1412348747253418\n",
      "Epoch: 108, Steps: 1 | Train Loss: 796.8562012 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.14023566246032715\n",
      "Epoch: 109, Steps: 1 | Train Loss: 797.3766479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.14594316482543945\n",
      "Epoch: 110, Steps: 1 | Train Loss: 796.6641846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.14244580268859863\n",
      "Epoch: 111, Steps: 1 | Train Loss: 797.2413940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.12633419036865234\n",
      "Epoch: 112, Steps: 1 | Train Loss: 796.9383545 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.12849783897399902\n",
      "Epoch: 113, Steps: 1 | Train Loss: 796.9126587 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.12662148475646973\n",
      "Epoch: 114, Steps: 1 | Train Loss: 795.8840942 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.13784193992614746\n",
      "Epoch: 115, Steps: 1 | Train Loss: 796.9968872 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.13753175735473633\n",
      "Epoch: 116, Steps: 1 | Train Loss: 797.6213379 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.12831854820251465\n",
      "Epoch: 117, Steps: 1 | Train Loss: 797.6188965 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.12572383880615234\n",
      "Epoch: 118, Steps: 1 | Train Loss: 798.0087891 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.12333321571350098\n",
      "Epoch: 119, Steps: 1 | Train Loss: 796.6708374 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.13025641441345215\n",
      "Epoch: 120, Steps: 1 | Train Loss: 798.5119019 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.14551997184753418\n",
      "Epoch: 121, Steps: 1 | Train Loss: 797.0123291 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.12322735786437988\n",
      "Epoch: 122, Steps: 1 | Train Loss: 797.0100708 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.12451910972595215\n",
      "Epoch: 123, Steps: 1 | Train Loss: 796.1862183 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.1318364143371582\n",
      "Epoch: 124, Steps: 1 | Train Loss: 797.4052734 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.13155126571655273\n",
      "Epoch: 125, Steps: 1 | Train Loss: 797.4190674 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.1268291473388672\n",
      "Epoch: 126, Steps: 1 | Train Loss: 797.3339844 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.11498308181762695\n",
      "Epoch: 127, Steps: 1 | Train Loss: 797.4939575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12595129013061523\n",
      "Epoch: 128, Steps: 1 | Train Loss: 796.6450195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.12543964385986328\n",
      "Epoch: 129, Steps: 1 | Train Loss: 797.5274048 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.12520051002502441\n",
      "Epoch: 130, Steps: 1 | Train Loss: 796.9857788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.1313645839691162\n",
      "Epoch: 131, Steps: 1 | Train Loss: 799.6815796 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.12964391708374023\n",
      "Epoch: 132, Steps: 1 | Train Loss: 797.8217163 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.13014745712280273\n",
      "Epoch: 133, Steps: 1 | Train Loss: 796.7190552 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.13622689247131348\n",
      "Epoch: 134, Steps: 1 | Train Loss: 796.0828857 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.13702702522277832\n",
      "Epoch: 135, Steps: 1 | Train Loss: 796.3967285 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.13804125785827637\n",
      "Epoch: 136, Steps: 1 | Train Loss: 798.0332642 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.12400698661804199\n",
      "Epoch: 137, Steps: 1 | Train Loss: 796.9635620 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.1334056854248047\n",
      "Epoch: 138, Steps: 1 | Train Loss: 795.4307861 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.13562226295471191\n",
      "Epoch: 139, Steps: 1 | Train Loss: 797.8948364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.137986421585083\n",
      "Epoch: 140, Steps: 1 | Train Loss: 796.3067627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.14146661758422852\n",
      "Epoch: 141, Steps: 1 | Train Loss: 796.3243408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.12894725799560547\n",
      "Epoch: 142, Steps: 1 | Train Loss: 797.1614990 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.12428855895996094\n",
      "Epoch: 143, Steps: 1 | Train Loss: 798.8095093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.12098097801208496\n",
      "Epoch: 144, Steps: 1 | Train Loss: 797.1788940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.12313675880432129\n",
      "Epoch: 145, Steps: 1 | Train Loss: 798.6886597 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.12311029434204102\n",
      "Epoch: 146, Steps: 1 | Train Loss: 797.6409912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.12328290939331055\n",
      "Epoch: 147, Steps: 1 | Train Loss: 796.5643921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10965466499328613\n",
      "Epoch: 148, Steps: 1 | Train Loss: 797.3034058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11956906318664551\n",
      "Epoch: 149, Steps: 1 | Train Loss: 798.2868042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.12200069427490234\n",
      "Epoch: 150, Steps: 1 | Train Loss: 798.4724731 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.12171554565429688\n",
      "Epoch: 151, Steps: 1 | Train Loss: 797.4215088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.12185049057006836\n",
      "Epoch: 152, Steps: 1 | Train Loss: 797.2155151 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.12401294708251953\n",
      "Epoch: 153, Steps: 1 | Train Loss: 797.0183105 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.1235816478729248\n",
      "Epoch: 154, Steps: 1 | Train Loss: 797.2714844 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.12455964088439941\n",
      "Epoch: 155, Steps: 1 | Train Loss: 797.9317627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.12983322143554688\n",
      "Epoch: 156, Steps: 1 | Train Loss: 797.9835205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.12510943412780762\n",
      "Epoch: 157, Steps: 1 | Train Loss: 798.1873779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.13709115982055664\n",
      "Epoch: 158, Steps: 1 | Train Loss: 797.8757935 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.13532280921936035\n",
      "Epoch: 159, Steps: 1 | Train Loss: 797.9266968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.12930893898010254\n",
      "Epoch: 160, Steps: 1 | Train Loss: 796.7387085 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.12442493438720703\n",
      "Epoch: 161, Steps: 1 | Train Loss: 798.2479858 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.12644648551940918\n",
      "Epoch: 162, Steps: 1 | Train Loss: 798.2929077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.1284046173095703\n",
      "Epoch: 163, Steps: 1 | Train Loss: 795.7807617 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.12847018241882324\n",
      "Epoch: 164, Steps: 1 | Train Loss: 797.0951538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.13709115982055664\n",
      "Epoch: 165, Steps: 1 | Train Loss: 795.9921265 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.1123497486114502\n",
      "Epoch: 166, Steps: 1 | Train Loss: 797.1041870 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11082839965820312\n",
      "Epoch: 167, Steps: 1 | Train Loss: 797.4477539 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.1101694107055664\n",
      "Epoch: 168, Steps: 1 | Train Loss: 796.8301392 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.1107032299041748\n",
      "Epoch: 169, Steps: 1 | Train Loss: 796.9290771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11219024658203125\n",
      "Epoch: 170, Steps: 1 | Train Loss: 796.4857178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.1118767261505127\n",
      "Epoch: 171, Steps: 1 | Train Loss: 797.4615479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11189937591552734\n",
      "Epoch: 172, Steps: 1 | Train Loss: 797.2630005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.1109917163848877\n",
      "Epoch: 173, Steps: 1 | Train Loss: 798.5731812 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11055135726928711\n",
      "Epoch: 174, Steps: 1 | Train Loss: 797.3428955 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.10781359672546387\n",
      "Epoch: 175, Steps: 1 | Train Loss: 795.5023193 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.1220250129699707\n",
      "Epoch: 176, Steps: 1 | Train Loss: 796.6918945 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.12173771858215332\n",
      "Epoch: 177, Steps: 1 | Train Loss: 797.5989380 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.12119364738464355\n",
      "Epoch: 178, Steps: 1 | Train Loss: 796.2382812 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.1136617660522461\n",
      "Epoch: 179, Steps: 1 | Train Loss: 796.8444214 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11903071403503418\n",
      "Epoch: 180, Steps: 1 | Train Loss: 797.0347290 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11648774147033691\n",
      "Epoch: 181, Steps: 1 | Train Loss: 798.1832886 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.11756396293640137\n",
      "Epoch: 182, Steps: 1 | Train Loss: 797.2594604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11625528335571289\n",
      "Epoch: 183, Steps: 1 | Train Loss: 797.8787842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11699938774108887\n",
      "Epoch: 184, Steps: 1 | Train Loss: 797.0958252 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11658716201782227\n",
      "Epoch: 185, Steps: 1 | Train Loss: 797.9340820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11634016036987305\n",
      "Epoch: 186, Steps: 1 | Train Loss: 797.7325439 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11639022827148438\n",
      "Epoch: 187, Steps: 1 | Train Loss: 798.3272705 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11770868301391602\n",
      "Epoch: 188, Steps: 1 | Train Loss: 796.5830688 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.11600756645202637\n",
      "Epoch: 189, Steps: 1 | Train Loss: 796.7860718 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.12715888023376465\n",
      "Epoch: 190, Steps: 1 | Train Loss: 797.7719727 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.12222623825073242\n",
      "Epoch: 191, Steps: 1 | Train Loss: 796.4145508 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.13318252563476562\n",
      "Epoch: 192, Steps: 1 | Train Loss: 797.9356079 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.1196432113647461\n",
      "Epoch: 193, Steps: 1 | Train Loss: 797.5917358 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11951327323913574\n",
      "Epoch: 194, Steps: 1 | Train Loss: 796.2192993 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11818718910217285\n",
      "Epoch: 195, Steps: 1 | Train Loss: 797.4539185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11669135093688965\n",
      "Epoch: 196, Steps: 1 | Train Loss: 797.4405518 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11781573295593262\n",
      "Epoch: 197, Steps: 1 | Train Loss: 796.4358521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11703610420227051\n",
      "Epoch: 198, Steps: 1 | Train Loss: 797.8214111 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.12233567237854004\n",
      "Epoch: 199, Steps: 1 | Train Loss: 797.0791016 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11732625961303711\n",
      "Epoch: 200, Steps: 1 | Train Loss: 797.7556763 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11704111099243164\n",
      "Epoch: 201, Steps: 1 | Train Loss: 797.1473389 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.11549735069274902\n",
      "Epoch: 202, Steps: 1 | Train Loss: 797.8659668 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11640691757202148\n",
      "Epoch: 203, Steps: 1 | Train Loss: 797.1627808 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.11735296249389648\n",
      "Epoch: 204, Steps: 1 | Train Loss: 798.0173950 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.11756062507629395\n",
      "Epoch: 205, Steps: 1 | Train Loss: 798.4423828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.1164846420288086\n",
      "Epoch: 206, Steps: 1 | Train Loss: 795.9563599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.12429094314575195\n",
      "Epoch: 207, Steps: 1 | Train Loss: 797.8933716 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.12830519676208496\n",
      "Epoch: 208, Steps: 1 | Train Loss: 796.9319458 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.1290273666381836\n",
      "Epoch: 209, Steps: 1 | Train Loss: 797.5442505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.12674283981323242\n",
      "Epoch: 210, Steps: 1 | Train Loss: 797.5950317 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.12980246543884277\n",
      "Epoch: 211, Steps: 1 | Train Loss: 796.3126831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.12141036987304688\n",
      "Epoch: 212, Steps: 1 | Train Loss: 798.1016846 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11793971061706543\n",
      "Epoch: 213, Steps: 1 | Train Loss: 798.1877441 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11723160743713379\n",
      "Epoch: 214, Steps: 1 | Train Loss: 797.1370239 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.12683844566345215\n",
      "Epoch: 215, Steps: 1 | Train Loss: 796.8576050 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.12378549575805664\n",
      "Epoch: 216, Steps: 1 | Train Loss: 797.2261963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.12173700332641602\n",
      "Epoch: 217, Steps: 1 | Train Loss: 796.8045654 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.12159013748168945\n",
      "Epoch: 218, Steps: 1 | Train Loss: 796.3356323 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.1357574462890625\n",
      "Epoch: 219, Steps: 1 | Train Loss: 797.1093140 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.12460565567016602\n",
      "Epoch: 220, Steps: 1 | Train Loss: 798.8139648 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.12441349029541016\n",
      "Epoch: 221, Steps: 1 | Train Loss: 796.6804199 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.1259622573852539\n",
      "Epoch: 222, Steps: 1 | Train Loss: 797.2164307 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.12558507919311523\n",
      "Epoch: 223, Steps: 1 | Train Loss: 797.3777466 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.12156796455383301\n",
      "Epoch: 224, Steps: 1 | Train Loss: 796.8085327 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.124603271484375\n",
      "Epoch: 225, Steps: 1 | Train Loss: 796.4628296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.125748872756958\n",
      "Epoch: 226, Steps: 1 | Train Loss: 796.7522583 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10805320739746094\n",
      "Epoch: 227, Steps: 1 | Train Loss: 796.9270630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.13788533210754395\n",
      "Epoch: 228, Steps: 1 | Train Loss: 798.1684570 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.12918519973754883\n",
      "Epoch: 229, Steps: 1 | Train Loss: 798.8253784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.13560962677001953\n",
      "Epoch: 230, Steps: 1 | Train Loss: 796.8768921 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.12792015075683594\n",
      "Epoch: 231, Steps: 1 | Train Loss: 797.1305542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.1084897518157959\n",
      "Epoch: 232, Steps: 1 | Train Loss: 796.2386475 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.12605953216552734\n",
      "Epoch: 233, Steps: 1 | Train Loss: 799.0123901 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11138200759887695\n",
      "Epoch: 234, Steps: 1 | Train Loss: 797.8323975 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.1264815330505371\n",
      "Epoch: 235, Steps: 1 | Train Loss: 797.5719604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11946654319763184\n",
      "Epoch: 236, Steps: 1 | Train Loss: 797.6179810 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.12690019607543945\n",
      "Epoch: 237, Steps: 1 | Train Loss: 798.6334839 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.12065696716308594\n",
      "Epoch: 238, Steps: 1 | Train Loss: 796.9016724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.12069964408874512\n",
      "Epoch: 239, Steps: 1 | Train Loss: 798.6947632 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.12041068077087402\n",
      "Epoch: 240, Steps: 1 | Train Loss: 796.0628662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.10864138603210449\n",
      "Epoch: 241, Steps: 1 | Train Loss: 797.5381470 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.12587237358093262\n",
      "Epoch: 242, Steps: 1 | Train Loss: 795.9821167 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.12480497360229492\n",
      "Epoch: 243, Steps: 1 | Train Loss: 797.4968262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.12442207336425781\n",
      "Epoch: 244, Steps: 1 | Train Loss: 797.1411133 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.12446284294128418\n",
      "Epoch: 245, Steps: 1 | Train Loss: 797.3092651 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.12791085243225098\n",
      "Epoch: 246, Steps: 1 | Train Loss: 797.8210449 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.1262342929840088\n",
      "Epoch: 247, Steps: 1 | Train Loss: 796.0753784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.12925171852111816\n",
      "Epoch: 248, Steps: 1 | Train Loss: 798.5188599 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.12522101402282715\n",
      "Epoch: 249, Steps: 1 | Train Loss: 797.2919922 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.12877321243286133\n",
      "Epoch: 250, Steps: 1 | Train Loss: 798.3590088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "21\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_4>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11565065383911133\n",
      "Epoch: 1, Steps: 1 | Train Loss: 814.2157593 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.13317441940307617\n",
      "Epoch: 2, Steps: 1 | Train Loss: 807.9698486 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.1334378719329834\n",
      "Epoch: 3, Steps: 1 | Train Loss: 803.3876953 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.13770580291748047\n",
      "Epoch: 4, Steps: 1 | Train Loss: 801.0402222 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11912751197814941\n",
      "Epoch: 5, Steps: 1 | Train Loss: 800.0526733 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11823368072509766\n",
      "Epoch: 6, Steps: 1 | Train Loss: 799.3110352 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.1185004711151123\n",
      "Epoch: 7, Steps: 1 | Train Loss: 799.5128784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.12057971954345703\n",
      "Epoch: 8, Steps: 1 | Train Loss: 799.4931641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.1204993724822998\n",
      "Epoch: 9, Steps: 1 | Train Loss: 798.8429565 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.12058067321777344\n",
      "Epoch: 10, Steps: 1 | Train Loss: 799.0730591 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.12038064002990723\n",
      "Epoch: 11, Steps: 1 | Train Loss: 798.0202026 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.12255477905273438\n",
      "Epoch: 12, Steps: 1 | Train Loss: 798.1937256 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11424946784973145\n",
      "Epoch: 13, Steps: 1 | Train Loss: 800.0136108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.11127686500549316\n",
      "Epoch: 14, Steps: 1 | Train Loss: 798.9415283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10938262939453125\n",
      "Epoch: 15, Steps: 1 | Train Loss: 799.1161499 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10886430740356445\n",
      "Epoch: 16, Steps: 1 | Train Loss: 799.0841675 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.10783576965332031\n",
      "Epoch: 17, Steps: 1 | Train Loss: 797.7493896 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11121582984924316\n",
      "Epoch: 18, Steps: 1 | Train Loss: 798.3128662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.10990071296691895\n",
      "Epoch: 19, Steps: 1 | Train Loss: 798.2443237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.11081981658935547\n",
      "Epoch: 20, Steps: 1 | Train Loss: 797.5756836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11094069480895996\n",
      "Epoch: 21, Steps: 1 | Train Loss: 798.8043823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10651302337646484\n",
      "Epoch: 22, Steps: 1 | Train Loss: 798.6157227 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.10737729072570801\n",
      "Epoch: 23, Steps: 1 | Train Loss: 798.8281250 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.12113356590270996\n",
      "Epoch: 24, Steps: 1 | Train Loss: 798.1099854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.12314176559448242\n",
      "Epoch: 25, Steps: 1 | Train Loss: 799.5065308 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.12102270126342773\n",
      "Epoch: 26, Steps: 1 | Train Loss: 797.8439331 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11419844627380371\n",
      "Epoch: 27, Steps: 1 | Train Loss: 797.5318604 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.11149191856384277\n",
      "Epoch: 28, Steps: 1 | Train Loss: 797.7428589 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10753989219665527\n",
      "Epoch: 29, Steps: 1 | Train Loss: 799.5000000 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11069989204406738\n",
      "Epoch: 30, Steps: 1 | Train Loss: 798.8316040 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.1138448715209961\n",
      "Epoch: 31, Steps: 1 | Train Loss: 798.7749023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.1142737865447998\n",
      "Epoch: 32, Steps: 1 | Train Loss: 798.7250977 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10787725448608398\n",
      "Epoch: 33, Steps: 1 | Train Loss: 798.9368286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.116668701171875\n",
      "Epoch: 34, Steps: 1 | Train Loss: 800.5173950 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.11499214172363281\n",
      "Epoch: 35, Steps: 1 | Train Loss: 798.9527588 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10644292831420898\n",
      "Epoch: 36, Steps: 1 | Train Loss: 799.2902222 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.10768461227416992\n",
      "Epoch: 37, Steps: 1 | Train Loss: 798.8063354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.11127376556396484\n",
      "Epoch: 38, Steps: 1 | Train Loss: 798.3593750 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10886573791503906\n",
      "Epoch: 39, Steps: 1 | Train Loss: 798.8178711 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10797572135925293\n",
      "Epoch: 40, Steps: 1 | Train Loss: 799.8659058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.1080312728881836\n",
      "Epoch: 41, Steps: 1 | Train Loss: 798.3412476 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10788273811340332\n",
      "Epoch: 42, Steps: 1 | Train Loss: 799.2748413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10804581642150879\n",
      "Epoch: 43, Steps: 1 | Train Loss: 798.4049072 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10774707794189453\n",
      "Epoch: 44, Steps: 1 | Train Loss: 799.4777832 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.12263846397399902\n",
      "Epoch: 45, Steps: 1 | Train Loss: 798.3881226 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.1142113208770752\n",
      "Epoch: 46, Steps: 1 | Train Loss: 798.9968262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.11017584800720215\n",
      "Epoch: 47, Steps: 1 | Train Loss: 798.6220093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.10905575752258301\n",
      "Epoch: 48, Steps: 1 | Train Loss: 799.5968628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.10835933685302734\n",
      "Epoch: 49, Steps: 1 | Train Loss: 800.7211304 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.10805463790893555\n",
      "Epoch: 50, Steps: 1 | Train Loss: 798.1931763 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.10797905921936035\n",
      "Epoch: 51, Steps: 1 | Train Loss: 799.4973755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.10774111747741699\n",
      "Epoch: 52, Steps: 1 | Train Loss: 797.7171021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.1079864501953125\n",
      "Epoch: 53, Steps: 1 | Train Loss: 797.2178955 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.10818910598754883\n",
      "Epoch: 54, Steps: 1 | Train Loss: 798.7567139 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.10836219787597656\n",
      "Epoch: 55, Steps: 1 | Train Loss: 798.7111816 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.12202620506286621\n",
      "Epoch: 56, Steps: 1 | Train Loss: 797.9796753 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.12491941452026367\n",
      "Epoch: 57, Steps: 1 | Train Loss: 798.3803101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.11180472373962402\n",
      "Epoch: 58, Steps: 1 | Train Loss: 798.9467163 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.10957002639770508\n",
      "Epoch: 59, Steps: 1 | Train Loss: 798.7986450 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10866808891296387\n",
      "Epoch: 60, Steps: 1 | Train Loss: 797.9779663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.10883355140686035\n",
      "Epoch: 61, Steps: 1 | Train Loss: 797.6030273 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11497235298156738\n",
      "Epoch: 62, Steps: 1 | Train Loss: 799.5979614 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.1150050163269043\n",
      "Epoch: 63, Steps: 1 | Train Loss: 799.9996948 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.11498308181762695\n",
      "Epoch: 64, Steps: 1 | Train Loss: 797.7192383 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.11397671699523926\n",
      "Epoch: 65, Steps: 1 | Train Loss: 798.1176758 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.13046979904174805\n",
      "Epoch: 66, Steps: 1 | Train Loss: 798.8757935 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.1283106803894043\n",
      "Epoch: 67, Steps: 1 | Train Loss: 798.1080933 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.12856245040893555\n",
      "Epoch: 68, Steps: 1 | Train Loss: 798.5321655 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.1273670196533203\n",
      "Epoch: 69, Steps: 1 | Train Loss: 798.4140625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11842107772827148\n",
      "Epoch: 70, Steps: 1 | Train Loss: 799.1688232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.11877679824829102\n",
      "Epoch: 71, Steps: 1 | Train Loss: 800.0200195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.11835169792175293\n",
      "Epoch: 72, Steps: 1 | Train Loss: 799.2452393 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.11684393882751465\n",
      "Epoch: 73, Steps: 1 | Train Loss: 797.6215210 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.115386962890625\n",
      "Epoch: 74, Steps: 1 | Train Loss: 798.3866577 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.11287569999694824\n",
      "Epoch: 75, Steps: 1 | Train Loss: 798.7532349 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.11484980583190918\n",
      "Epoch: 76, Steps: 1 | Train Loss: 798.6028442 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.1126241683959961\n",
      "Epoch: 77, Steps: 1 | Train Loss: 798.6022339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.11442947387695312\n",
      "Epoch: 78, Steps: 1 | Train Loss: 798.7493896 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.11461329460144043\n",
      "Epoch: 79, Steps: 1 | Train Loss: 798.5245972 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.12722468376159668\n",
      "Epoch: 80, Steps: 1 | Train Loss: 798.9670410 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.12678861618041992\n",
      "Epoch: 81, Steps: 1 | Train Loss: 799.3739014 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.12599921226501465\n",
      "Epoch: 82, Steps: 1 | Train Loss: 799.2797241 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.12699556350708008\n",
      "Epoch: 83, Steps: 1 | Train Loss: 798.9262085 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.12023615837097168\n",
      "Epoch: 84, Steps: 1 | Train Loss: 799.3131714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.11132645606994629\n",
      "Epoch: 85, Steps: 1 | Train Loss: 798.7232666 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.1152958869934082\n",
      "Epoch: 86, Steps: 1 | Train Loss: 798.8306274 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.114501953125\n",
      "Epoch: 87, Steps: 1 | Train Loss: 799.7893066 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.11351323127746582\n",
      "Epoch: 88, Steps: 1 | Train Loss: 797.6309204 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.1291506290435791\n",
      "Epoch: 89, Steps: 1 | Train Loss: 798.4494019 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.1261122226715088\n",
      "Epoch: 90, Steps: 1 | Train Loss: 799.5101929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.12627696990966797\n",
      "Epoch: 91, Steps: 1 | Train Loss: 798.9307861 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.1261119842529297\n",
      "Epoch: 92, Steps: 1 | Train Loss: 798.9736328 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.13255691528320312\n",
      "Epoch: 93, Steps: 1 | Train Loss: 799.8019409 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.12894320487976074\n",
      "Epoch: 94, Steps: 1 | Train Loss: 799.2358398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.12864279747009277\n",
      "Epoch: 95, Steps: 1 | Train Loss: 798.1641235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.12569642066955566\n",
      "Epoch: 96, Steps: 1 | Train Loss: 798.6853638 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.12385439872741699\n",
      "Epoch: 97, Steps: 1 | Train Loss: 799.4915771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.11737203598022461\n",
      "Epoch: 98, Steps: 1 | Train Loss: 798.5947266 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.10991477966308594\n",
      "Epoch: 99, Steps: 1 | Train Loss: 798.2799683 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.11098408699035645\n",
      "Epoch: 100, Steps: 1 | Train Loss: 799.9374390 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.10990285873413086\n",
      "Epoch: 101, Steps: 1 | Train Loss: 798.6889038 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.11141514778137207\n",
      "Epoch: 102, Steps: 1 | Train Loss: 799.2843628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.11178350448608398\n",
      "Epoch: 103, Steps: 1 | Train Loss: 799.7839966 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.11049032211303711\n",
      "Epoch: 104, Steps: 1 | Train Loss: 798.9147339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10999917984008789\n",
      "Epoch: 105, Steps: 1 | Train Loss: 799.1349487 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.11049270629882812\n",
      "Epoch: 106, Steps: 1 | Train Loss: 798.1146851 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10963582992553711\n",
      "Epoch: 107, Steps: 1 | Train Loss: 798.6591187 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10994267463684082\n",
      "Epoch: 108, Steps: 1 | Train Loss: 797.3971558 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.1107320785522461\n",
      "Epoch: 109, Steps: 1 | Train Loss: 797.7422485 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.11080741882324219\n",
      "Epoch: 110, Steps: 1 | Train Loss: 799.7260132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.11093497276306152\n",
      "Epoch: 111, Steps: 1 | Train Loss: 797.6986694 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.11045145988464355\n",
      "Epoch: 112, Steps: 1 | Train Loss: 798.5289917 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11011528968811035\n",
      "Epoch: 113, Steps: 1 | Train Loss: 798.8344727 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10982322692871094\n",
      "Epoch: 114, Steps: 1 | Train Loss: 798.4723511 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.10722637176513672\n",
      "Epoch: 115, Steps: 1 | Train Loss: 798.5534668 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11040329933166504\n",
      "Epoch: 116, Steps: 1 | Train Loss: 799.9772339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.1097574234008789\n",
      "Epoch: 117, Steps: 1 | Train Loss: 798.7290039 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.11555218696594238\n",
      "Epoch: 118, Steps: 1 | Train Loss: 796.7878418 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.1112828254699707\n",
      "Epoch: 119, Steps: 1 | Train Loss: 799.5751343 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.1109769344329834\n",
      "Epoch: 120, Steps: 1 | Train Loss: 798.5556030 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11285018920898438\n",
      "Epoch: 121, Steps: 1 | Train Loss: 799.1134644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11081671714782715\n",
      "Epoch: 122, Steps: 1 | Train Loss: 798.1134033 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.1099853515625\n",
      "Epoch: 123, Steps: 1 | Train Loss: 797.9001465 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11135387420654297\n",
      "Epoch: 124, Steps: 1 | Train Loss: 798.2859497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.11028742790222168\n",
      "Epoch: 125, Steps: 1 | Train Loss: 798.8712769 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.1234135627746582\n",
      "Epoch: 126, Steps: 1 | Train Loss: 799.0397949 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.12264847755432129\n",
      "Epoch: 127, Steps: 1 | Train Loss: 798.1575317 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.11472463607788086\n",
      "Epoch: 128, Steps: 1 | Train Loss: 798.3615723 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10904955863952637\n",
      "Epoch: 129, Steps: 1 | Train Loss: 798.1705322 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11116242408752441\n",
      "Epoch: 130, Steps: 1 | Train Loss: 799.6531982 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11046385765075684\n",
      "Epoch: 131, Steps: 1 | Train Loss: 798.5955811 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11136317253112793\n",
      "Epoch: 132, Steps: 1 | Train Loss: 798.1058960 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11047124862670898\n",
      "Epoch: 133, Steps: 1 | Train Loss: 798.3546143 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.1108253002166748\n",
      "Epoch: 134, Steps: 1 | Train Loss: 798.6932983 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.11134147644042969\n",
      "Epoch: 135, Steps: 1 | Train Loss: 799.2718506 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10713052749633789\n",
      "Epoch: 136, Steps: 1 | Train Loss: 798.6840820 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.12050318717956543\n",
      "Epoch: 137, Steps: 1 | Train Loss: 798.7707520 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.12131381034851074\n",
      "Epoch: 138, Steps: 1 | Train Loss: 798.8650513 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.11021637916564941\n",
      "Epoch: 139, Steps: 1 | Train Loss: 799.8326416 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.11066126823425293\n",
      "Epoch: 140, Steps: 1 | Train Loss: 798.4894409 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.1088407039642334\n",
      "Epoch: 141, Steps: 1 | Train Loss: 798.8361206 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.10859870910644531\n",
      "Epoch: 142, Steps: 1 | Train Loss: 798.1383057 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.10842227935791016\n",
      "Epoch: 143, Steps: 1 | Train Loss: 797.6521606 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.10855722427368164\n",
      "Epoch: 144, Steps: 1 | Train Loss: 797.6288452 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.1084890365600586\n",
      "Epoch: 145, Steps: 1 | Train Loss: 798.8576050 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.12253332138061523\n",
      "Epoch: 146, Steps: 1 | Train Loss: 798.7237549 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.12358665466308594\n",
      "Epoch: 147, Steps: 1 | Train Loss: 797.5178223 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.11197662353515625\n",
      "Epoch: 148, Steps: 1 | Train Loss: 797.6218262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.11013102531433105\n",
      "Epoch: 149, Steps: 1 | Train Loss: 798.9713745 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10878849029541016\n",
      "Epoch: 150, Steps: 1 | Train Loss: 798.3720703 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.10860419273376465\n",
      "Epoch: 151, Steps: 1 | Train Loss: 798.9251099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10846304893493652\n",
      "Epoch: 152, Steps: 1 | Train Loss: 798.0767822 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10864806175231934\n",
      "Epoch: 153, Steps: 1 | Train Loss: 798.5494385 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.10824131965637207\n",
      "Epoch: 154, Steps: 1 | Train Loss: 799.0343018 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10832667350769043\n",
      "Epoch: 155, Steps: 1 | Train Loss: 799.2377319 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10841131210327148\n",
      "Epoch: 156, Steps: 1 | Train Loss: 798.3001099 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.1114499568939209\n",
      "Epoch: 157, Steps: 1 | Train Loss: 798.3686523 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10952043533325195\n",
      "Epoch: 158, Steps: 1 | Train Loss: 798.1134644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.10867595672607422\n",
      "Epoch: 159, Steps: 1 | Train Loss: 800.0164185 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.10872602462768555\n",
      "Epoch: 160, Steps: 1 | Train Loss: 799.3751831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.10840654373168945\n",
      "Epoch: 161, Steps: 1 | Train Loss: 797.9218750 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10835671424865723\n",
      "Epoch: 162, Steps: 1 | Train Loss: 798.8111572 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.10835981369018555\n",
      "Epoch: 163, Steps: 1 | Train Loss: 798.9111938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10862159729003906\n",
      "Epoch: 164, Steps: 1 | Train Loss: 798.8259277 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.1234896183013916\n",
      "Epoch: 165, Steps: 1 | Train Loss: 798.6884155 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11955404281616211\n",
      "Epoch: 166, Steps: 1 | Train Loss: 799.2748413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11505293846130371\n",
      "Epoch: 167, Steps: 1 | Train Loss: 798.7927856 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.1101372241973877\n",
      "Epoch: 168, Steps: 1 | Train Loss: 798.2128296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.10830068588256836\n",
      "Epoch: 169, Steps: 1 | Train Loss: 797.9893799 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.10864758491516113\n",
      "Epoch: 170, Steps: 1 | Train Loss: 798.9909058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.10836577415466309\n",
      "Epoch: 171, Steps: 1 | Train Loss: 798.3339233 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.1091470718383789\n",
      "Epoch: 172, Steps: 1 | Train Loss: 800.2954712 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.10854291915893555\n",
      "Epoch: 173, Steps: 1 | Train Loss: 798.8268433 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.10819864273071289\n",
      "Epoch: 174, Steps: 1 | Train Loss: 798.8743286 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.12247562408447266\n",
      "Epoch: 175, Steps: 1 | Train Loss: 798.8319092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.11141848564147949\n",
      "Epoch: 176, Steps: 1 | Train Loss: 798.9635620 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.1094207763671875\n",
      "Epoch: 177, Steps: 1 | Train Loss: 799.5872192 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.10877346992492676\n",
      "Epoch: 178, Steps: 1 | Train Loss: 799.8226929 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10802078247070312\n",
      "Epoch: 179, Steps: 1 | Train Loss: 798.8243408 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.10792827606201172\n",
      "Epoch: 180, Steps: 1 | Train Loss: 798.6378784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.10827994346618652\n",
      "Epoch: 181, Steps: 1 | Train Loss: 798.2749023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10823726654052734\n",
      "Epoch: 182, Steps: 1 | Train Loss: 799.1378784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.10819721221923828\n",
      "Epoch: 183, Steps: 1 | Train Loss: 798.3323364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.10824012756347656\n",
      "Epoch: 184, Steps: 1 | Train Loss: 799.7249756 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11812067031860352\n",
      "Epoch: 185, Steps: 1 | Train Loss: 798.9086304 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11150407791137695\n",
      "Epoch: 186, Steps: 1 | Train Loss: 799.3593750 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10949110984802246\n",
      "Epoch: 187, Steps: 1 | Train Loss: 799.0168457 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.10788321495056152\n",
      "Epoch: 188, Steps: 1 | Train Loss: 798.5715332 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.10808634757995605\n",
      "Epoch: 189, Steps: 1 | Train Loss: 797.2285767 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10810446739196777\n",
      "Epoch: 190, Steps: 1 | Train Loss: 798.2589722 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.10830283164978027\n",
      "Epoch: 191, Steps: 1 | Train Loss: 799.5782471 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10817837715148926\n",
      "Epoch: 192, Steps: 1 | Train Loss: 799.5403442 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.1091463565826416\n",
      "Epoch: 193, Steps: 1 | Train Loss: 797.6331177 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.10820341110229492\n",
      "Epoch: 194, Steps: 1 | Train Loss: 799.7981567 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.11675190925598145\n",
      "Epoch: 195, Steps: 1 | Train Loss: 799.9862671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11015748977661133\n",
      "Epoch: 196, Steps: 1 | Train Loss: 798.5548706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.10933613777160645\n",
      "Epoch: 197, Steps: 1 | Train Loss: 799.0670166 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11164450645446777\n",
      "Epoch: 198, Steps: 1 | Train Loss: 798.5125732 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.12790799140930176\n",
      "Epoch: 199, Steps: 1 | Train Loss: 798.6005249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11653375625610352\n",
      "Epoch: 200, Steps: 1 | Train Loss: 798.1024170 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11704421043395996\n",
      "Epoch: 201, Steps: 1 | Train Loss: 799.8856812 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.12158823013305664\n",
      "Epoch: 202, Steps: 1 | Train Loss: 798.8774414 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11066460609436035\n",
      "Epoch: 203, Steps: 1 | Train Loss: 797.6817627 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.10975861549377441\n",
      "Epoch: 204, Steps: 1 | Train Loss: 799.1078491 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.11235594749450684\n",
      "Epoch: 205, Steps: 1 | Train Loss: 796.5619507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.10991215705871582\n",
      "Epoch: 206, Steps: 1 | Train Loss: 798.1139526 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.10989236831665039\n",
      "Epoch: 207, Steps: 1 | Train Loss: 798.0545654 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.10771870613098145\n",
      "Epoch: 208, Steps: 1 | Train Loss: 798.2934570 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.10792779922485352\n",
      "Epoch: 209, Steps: 1 | Train Loss: 798.2896118 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.10758376121520996\n",
      "Epoch: 210, Steps: 1 | Train Loss: 798.8894043 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.1082465648651123\n",
      "Epoch: 211, Steps: 1 | Train Loss: 798.8378296 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11036086082458496\n",
      "Epoch: 212, Steps: 1 | Train Loss: 798.7319336 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11486625671386719\n",
      "Epoch: 213, Steps: 1 | Train Loss: 798.5245361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.10960578918457031\n",
      "Epoch: 214, Steps: 1 | Train Loss: 799.4456177 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.10880136489868164\n",
      "Epoch: 215, Steps: 1 | Train Loss: 799.0694580 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10829305648803711\n",
      "Epoch: 216, Steps: 1 | Train Loss: 798.8563232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.11522960662841797\n",
      "Epoch: 217, Steps: 1 | Train Loss: 798.1130371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10716509819030762\n",
      "Epoch: 218, Steps: 1 | Train Loss: 799.6071167 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10720109939575195\n",
      "Epoch: 219, Steps: 1 | Train Loss: 798.5751343 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10782217979431152\n",
      "Epoch: 220, Steps: 1 | Train Loss: 798.1842041 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.1078181266784668\n",
      "Epoch: 221, Steps: 1 | Train Loss: 799.6516724 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.10779428482055664\n",
      "Epoch: 222, Steps: 1 | Train Loss: 799.3421021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10786032676696777\n",
      "Epoch: 223, Steps: 1 | Train Loss: 799.5053101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.12115931510925293\n",
      "Epoch: 224, Steps: 1 | Train Loss: 800.1878052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.12062573432922363\n",
      "Epoch: 225, Steps: 1 | Train Loss: 799.0726318 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11287736892700195\n",
      "Epoch: 226, Steps: 1 | Train Loss: 799.0187988 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10945415496826172\n",
      "Epoch: 227, Steps: 1 | Train Loss: 799.6305542 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.1092827320098877\n",
      "Epoch: 228, Steps: 1 | Train Loss: 799.5547485 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.10917901992797852\n",
      "Epoch: 229, Steps: 1 | Train Loss: 798.4639893 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11072707176208496\n",
      "Epoch: 230, Steps: 1 | Train Loss: 798.8236084 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.11305451393127441\n",
      "Epoch: 231, Steps: 1 | Train Loss: 800.0931396 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.12433791160583496\n",
      "Epoch: 232, Steps: 1 | Train Loss: 799.1100464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.1165609359741211\n",
      "Epoch: 233, Steps: 1 | Train Loss: 797.5549316 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11198854446411133\n",
      "Epoch: 234, Steps: 1 | Train Loss: 798.3452759 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11131978034973145\n",
      "Epoch: 235, Steps: 1 | Train Loss: 798.5408325 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11204695701599121\n",
      "Epoch: 236, Steps: 1 | Train Loss: 799.5935669 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11141276359558105\n",
      "Epoch: 237, Steps: 1 | Train Loss: 798.9035645 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11097836494445801\n",
      "Epoch: 238, Steps: 1 | Train Loss: 799.6222534 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.1145632266998291\n",
      "Epoch: 239, Steps: 1 | Train Loss: 798.8306885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.11526989936828613\n",
      "Epoch: 240, Steps: 1 | Train Loss: 799.0568237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.11316299438476562\n",
      "Epoch: 241, Steps: 1 | Train Loss: 798.6660156 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11913299560546875\n",
      "Epoch: 242, Steps: 1 | Train Loss: 798.9311523 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.1108696460723877\n",
      "Epoch: 243, Steps: 1 | Train Loss: 798.1865234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.11011624336242676\n",
      "Epoch: 244, Steps: 1 | Train Loss: 797.9831543 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.1311483383178711\n",
      "Epoch: 245, Steps: 1 | Train Loss: 798.9481812 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.12754106521606445\n",
      "Epoch: 246, Steps: 1 | Train Loss: 799.8171997 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.12485361099243164\n",
      "Epoch: 247, Steps: 1 | Train Loss: 798.7265625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.11420822143554688\n",
      "Epoch: 248, Steps: 1 | Train Loss: 798.5302734 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11155843734741211\n",
      "Epoch: 249, Steps: 1 | Train Loss: 797.9213867 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11068415641784668\n",
      "Epoch: 250, Steps: 1 | Train Loss: 798.7722778 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "21\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_5>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11330699920654297\n",
      "Epoch: 1, Steps: 1 | Train Loss: 815.2597046 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.11123347282409668\n",
      "Epoch: 2, Steps: 1 | Train Loss: 811.1760864 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11100053787231445\n",
      "Epoch: 3, Steps: 1 | Train Loss: 806.0512085 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.11083483695983887\n",
      "Epoch: 4, Steps: 1 | Train Loss: 803.3627319 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11062240600585938\n",
      "Epoch: 5, Steps: 1 | Train Loss: 802.6737671 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11250042915344238\n",
      "Epoch: 6, Steps: 1 | Train Loss: 802.5941772 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11072444915771484\n",
      "Epoch: 7, Steps: 1 | Train Loss: 803.5639038 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.11035275459289551\n",
      "Epoch: 8, Steps: 1 | Train Loss: 803.5959473 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11037516593933105\n",
      "Epoch: 9, Steps: 1 | Train Loss: 802.0108643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.11057710647583008\n",
      "Epoch: 10, Steps: 1 | Train Loss: 801.6481323 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.11142945289611816\n",
      "Epoch: 11, Steps: 1 | Train Loss: 802.2687378 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.11012744903564453\n",
      "Epoch: 12, Steps: 1 | Train Loss: 802.1141968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11109399795532227\n",
      "Epoch: 13, Steps: 1 | Train Loss: 801.5578003 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.11077380180358887\n",
      "Epoch: 14, Steps: 1 | Train Loss: 801.5668945 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.11047673225402832\n",
      "Epoch: 15, Steps: 1 | Train Loss: 802.3654175 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10835456848144531\n",
      "Epoch: 16, Steps: 1 | Train Loss: 802.5538940 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.1077883243560791\n",
      "Epoch: 17, Steps: 1 | Train Loss: 802.3292236 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.1122431755065918\n",
      "Epoch: 18, Steps: 1 | Train Loss: 799.9938965 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.11004137992858887\n",
      "Epoch: 19, Steps: 1 | Train Loss: 801.8746338 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.1107327938079834\n",
      "Epoch: 20, Steps: 1 | Train Loss: 802.2697144 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.12421607971191406\n",
      "Epoch: 21, Steps: 1 | Train Loss: 801.4227905 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.12441349029541016\n",
      "Epoch: 22, Steps: 1 | Train Loss: 801.5639038 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.12475967407226562\n",
      "Epoch: 23, Steps: 1 | Train Loss: 802.1647949 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11364316940307617\n",
      "Epoch: 24, Steps: 1 | Train Loss: 802.7578125 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11156892776489258\n",
      "Epoch: 25, Steps: 1 | Train Loss: 802.1521606 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.11045670509338379\n",
      "Epoch: 26, Steps: 1 | Train Loss: 803.1245117 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.11061549186706543\n",
      "Epoch: 27, Steps: 1 | Train Loss: 800.1693726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.11154508590698242\n",
      "Epoch: 28, Steps: 1 | Train Loss: 802.8667603 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.10758066177368164\n",
      "Epoch: 29, Steps: 1 | Train Loss: 801.9682007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.10830044746398926\n",
      "Epoch: 30, Steps: 1 | Train Loss: 801.4672241 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.10753345489501953\n",
      "Epoch: 31, Steps: 1 | Train Loss: 802.5069580 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.10745620727539062\n",
      "Epoch: 32, Steps: 1 | Train Loss: 801.5794678 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.10742616653442383\n",
      "Epoch: 33, Steps: 1 | Train Loss: 801.9791870 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.1204671859741211\n",
      "Epoch: 34, Steps: 1 | Train Loss: 801.9769897 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.11999773979187012\n",
      "Epoch: 35, Steps: 1 | Train Loss: 802.5733032 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.12068700790405273\n",
      "Epoch: 36, Steps: 1 | Train Loss: 802.2579346 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.11339449882507324\n",
      "Epoch: 37, Steps: 1 | Train Loss: 801.1925049 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10919475555419922\n",
      "Epoch: 38, Steps: 1 | Train Loss: 802.3812866 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.10848259925842285\n",
      "Epoch: 39, Steps: 1 | Train Loss: 801.4129028 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.10797810554504395\n",
      "Epoch: 40, Steps: 1 | Train Loss: 801.8450317 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10780978202819824\n",
      "Epoch: 41, Steps: 1 | Train Loss: 802.8059692 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10737133026123047\n",
      "Epoch: 42, Steps: 1 | Train Loss: 801.0426025 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10721421241760254\n",
      "Epoch: 43, Steps: 1 | Train Loss: 801.9308472 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.11104035377502441\n",
      "Epoch: 44, Steps: 1 | Train Loss: 802.0047607 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10785722732543945\n",
      "Epoch: 45, Steps: 1 | Train Loss: 802.1531372 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.10741591453552246\n",
      "Epoch: 46, Steps: 1 | Train Loss: 800.3736572 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.10721993446350098\n",
      "Epoch: 47, Steps: 1 | Train Loss: 801.7897339 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.1071937084197998\n",
      "Epoch: 48, Steps: 1 | Train Loss: 803.4093628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.1075279712677002\n",
      "Epoch: 49, Steps: 1 | Train Loss: 801.2916260 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.10747909545898438\n",
      "Epoch: 50, Steps: 1 | Train Loss: 801.7074585 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.12050032615661621\n",
      "Epoch: 51, Steps: 1 | Train Loss: 801.9745483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.12104463577270508\n",
      "Epoch: 52, Steps: 1 | Train Loss: 801.1534424 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.1209557056427002\n",
      "Epoch: 53, Steps: 1 | Train Loss: 801.8659058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.11929583549499512\n",
      "Epoch: 54, Steps: 1 | Train Loss: 802.2702637 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.11183381080627441\n",
      "Epoch: 55, Steps: 1 | Train Loss: 801.5202026 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.11928391456604004\n",
      "Epoch: 56, Steps: 1 | Train Loss: 801.6686401 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.11506414413452148\n",
      "Epoch: 57, Steps: 1 | Train Loss: 801.8009033 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.11623668670654297\n",
      "Epoch: 58, Steps: 1 | Train Loss: 801.2882080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11449575424194336\n",
      "Epoch: 59, Steps: 1 | Train Loss: 802.2221680 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.10622954368591309\n",
      "Epoch: 60, Steps: 1 | Train Loss: 801.8985596 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.11408567428588867\n",
      "Epoch: 61, Steps: 1 | Train Loss: 801.3861694 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.11194467544555664\n",
      "Epoch: 62, Steps: 1 | Train Loss: 800.7821045 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.11476612091064453\n",
      "Epoch: 63, Steps: 1 | Train Loss: 803.8277588 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.11686325073242188\n",
      "Epoch: 64, Steps: 1 | Train Loss: 802.2942505 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.13157439231872559\n",
      "Epoch: 65, Steps: 1 | Train Loss: 801.7260132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.12847661972045898\n",
      "Epoch: 66, Steps: 1 | Train Loss: 802.0725098 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.12775111198425293\n",
      "Epoch: 67, Steps: 1 | Train Loss: 801.3688354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.11736798286437988\n",
      "Epoch: 68, Steps: 1 | Train Loss: 802.0520630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.10621809959411621\n",
      "Epoch: 69, Steps: 1 | Train Loss: 802.1442261 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.11809611320495605\n",
      "Epoch: 70, Steps: 1 | Train Loss: 801.2936401 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.1244964599609375\n",
      "Epoch: 71, Steps: 1 | Train Loss: 802.3065796 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.12588262557983398\n",
      "Epoch: 72, Steps: 1 | Train Loss: 802.3349609 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.12492036819458008\n",
      "Epoch: 73, Steps: 1 | Train Loss: 801.5825195 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12193489074707031\n",
      "Epoch: 74, Steps: 1 | Train Loss: 801.8935547 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.13801980018615723\n",
      "Epoch: 75, Steps: 1 | Train Loss: 801.9584351 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.13522648811340332\n",
      "Epoch: 76, Steps: 1 | Train Loss: 800.2093506 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.12868499755859375\n",
      "Epoch: 77, Steps: 1 | Train Loss: 801.1428223 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.12831854820251465\n",
      "Epoch: 78, Steps: 1 | Train Loss: 803.6536255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.1292724609375\n",
      "Epoch: 79, Steps: 1 | Train Loss: 802.2951660 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.14386773109436035\n",
      "Epoch: 80, Steps: 1 | Train Loss: 801.0108643 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.14101314544677734\n",
      "Epoch: 81, Steps: 1 | Train Loss: 802.6226807 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.12201213836669922\n",
      "Epoch: 82, Steps: 1 | Train Loss: 800.8723755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.14981317520141602\n",
      "Epoch: 83, Steps: 1 | Train Loss: 801.6412964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.13125157356262207\n",
      "Epoch: 84, Steps: 1 | Train Loss: 802.7883911 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.1335301399230957\n",
      "Epoch: 85, Steps: 1 | Train Loss: 802.3265991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.13544511795043945\n",
      "Epoch: 86, Steps: 1 | Train Loss: 801.7395630 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.12751555442810059\n",
      "Epoch: 87, Steps: 1 | Train Loss: 801.3305054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.1201937198638916\n",
      "Epoch: 88, Steps: 1 | Train Loss: 802.1226196 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.1305992603302002\n",
      "Epoch: 89, Steps: 1 | Train Loss: 801.5131836 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.11979961395263672\n",
      "Epoch: 90, Steps: 1 | Train Loss: 802.8134766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10588645935058594\n",
      "Epoch: 91, Steps: 1 | Train Loss: 802.3529663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10750842094421387\n",
      "Epoch: 92, Steps: 1 | Train Loss: 800.6763306 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10751461982727051\n",
      "Epoch: 93, Steps: 1 | Train Loss: 801.0275269 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.11646533012390137\n",
      "Epoch: 94, Steps: 1 | Train Loss: 801.5861206 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.11161541938781738\n",
      "Epoch: 95, Steps: 1 | Train Loss: 802.3111572 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.11645984649658203\n",
      "Epoch: 96, Steps: 1 | Train Loss: 802.5979614 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10688114166259766\n",
      "Epoch: 97, Steps: 1 | Train Loss: 802.1625366 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.12286877632141113\n",
      "Epoch: 98, Steps: 1 | Train Loss: 801.1781616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.12152886390686035\n",
      "Epoch: 99, Steps: 1 | Train Loss: 801.5747070 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.1455707550048828\n",
      "Epoch: 100, Steps: 1 | Train Loss: 803.3043823 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.12182235717773438\n",
      "Epoch: 101, Steps: 1 | Train Loss: 801.8370972 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.12176275253295898\n",
      "Epoch: 102, Steps: 1 | Train Loss: 802.3119507 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.12340974807739258\n",
      "Epoch: 103, Steps: 1 | Train Loss: 801.3247070 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10593032836914062\n",
      "Epoch: 104, Steps: 1 | Train Loss: 801.9475708 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.11059188842773438\n",
      "Epoch: 105, Steps: 1 | Train Loss: 801.8664551 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.10761117935180664\n",
      "Epoch: 106, Steps: 1 | Train Loss: 802.4204102 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10775113105773926\n",
      "Epoch: 107, Steps: 1 | Train Loss: 801.4024658 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.10800433158874512\n",
      "Epoch: 108, Steps: 1 | Train Loss: 801.9681396 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10822582244873047\n",
      "Epoch: 109, Steps: 1 | Train Loss: 804.4018555 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.12143659591674805\n",
      "Epoch: 110, Steps: 1 | Train Loss: 801.4578857 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.12109780311584473\n",
      "Epoch: 111, Steps: 1 | Train Loss: 802.8740234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.1168057918548584\n",
      "Epoch: 112, Steps: 1 | Train Loss: 803.0665283 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.11612200736999512\n",
      "Epoch: 113, Steps: 1 | Train Loss: 802.4584961 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.12054872512817383\n",
      "Epoch: 114, Steps: 1 | Train Loss: 803.0290527 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.11562871932983398\n",
      "Epoch: 115, Steps: 1 | Train Loss: 801.3563843 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.11562013626098633\n",
      "Epoch: 116, Steps: 1 | Train Loss: 801.6838989 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.11374998092651367\n",
      "Epoch: 117, Steps: 1 | Train Loss: 803.1210327 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.11029338836669922\n",
      "Epoch: 118, Steps: 1 | Train Loss: 801.5072632 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.1110849380493164\n",
      "Epoch: 119, Steps: 1 | Train Loss: 800.9990845 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.11415290832519531\n",
      "Epoch: 120, Steps: 1 | Train Loss: 801.3606567 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11154294013977051\n",
      "Epoch: 121, Steps: 1 | Train Loss: 801.6609497 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11015725135803223\n",
      "Epoch: 122, Steps: 1 | Train Loss: 802.3450317 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.11053800582885742\n",
      "Epoch: 123, Steps: 1 | Train Loss: 800.6327515 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.11047482490539551\n",
      "Epoch: 124, Steps: 1 | Train Loss: 800.7839355 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.10880589485168457\n",
      "Epoch: 125, Steps: 1 | Train Loss: 802.0516968 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.10988044738769531\n",
      "Epoch: 126, Steps: 1 | Train Loss: 803.0784912 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.12343454360961914\n",
      "Epoch: 127, Steps: 1 | Train Loss: 801.2564087 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.12380027770996094\n",
      "Epoch: 128, Steps: 1 | Train Loss: 801.1826172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.11559200286865234\n",
      "Epoch: 129, Steps: 1 | Train Loss: 801.1874390 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.11333274841308594\n",
      "Epoch: 130, Steps: 1 | Train Loss: 802.4818115 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.11086392402648926\n",
      "Epoch: 131, Steps: 1 | Train Loss: 802.5828247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.10893654823303223\n",
      "Epoch: 132, Steps: 1 | Train Loss: 802.3459473 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.1070547103881836\n",
      "Epoch: 133, Steps: 1 | Train Loss: 801.9797974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.11462211608886719\n",
      "Epoch: 134, Steps: 1 | Train Loss: 801.9462891 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.11521673202514648\n",
      "Epoch: 135, Steps: 1 | Train Loss: 801.9974976 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.11893868446350098\n",
      "Epoch: 136, Steps: 1 | Train Loss: 802.6253662 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.12459945678710938\n",
      "Epoch: 137, Steps: 1 | Train Loss: 800.2670898 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.11827850341796875\n",
      "Epoch: 138, Steps: 1 | Train Loss: 801.4832153 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.14661574363708496\n",
      "Epoch: 139, Steps: 1 | Train Loss: 802.0921021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.13911080360412598\n",
      "Epoch: 140, Steps: 1 | Train Loss: 801.7653198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.13294625282287598\n",
      "Epoch: 141, Steps: 1 | Train Loss: 802.4472656 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.13811254501342773\n",
      "Epoch: 142, Steps: 1 | Train Loss: 801.9448853 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.13639307022094727\n",
      "Epoch: 143, Steps: 1 | Train Loss: 801.4561768 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.13895511627197266\n",
      "Epoch: 144, Steps: 1 | Train Loss: 801.1724854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.14310026168823242\n",
      "Epoch: 145, Steps: 1 | Train Loss: 802.3144531 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.12772655487060547\n",
      "Epoch: 146, Steps: 1 | Train Loss: 801.4814453 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.12796473503112793\n",
      "Epoch: 147, Steps: 1 | Train Loss: 801.5450439 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.12184286117553711\n",
      "Epoch: 148, Steps: 1 | Train Loss: 802.3199463 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10790681838989258\n",
      "Epoch: 149, Steps: 1 | Train Loss: 801.3903198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10773301124572754\n",
      "Epoch: 150, Steps: 1 | Train Loss: 802.8748779 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.1079874038696289\n",
      "Epoch: 151, Steps: 1 | Train Loss: 801.7526245 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.11123919486999512\n",
      "Epoch: 152, Steps: 1 | Train Loss: 801.3261108 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10904955863952637\n",
      "Epoch: 153, Steps: 1 | Train Loss: 802.2330933 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.1079862117767334\n",
      "Epoch: 154, Steps: 1 | Train Loss: 801.5482788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10760974884033203\n",
      "Epoch: 155, Steps: 1 | Train Loss: 801.3610229 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10706710815429688\n",
      "Epoch: 156, Steps: 1 | Train Loss: 803.2644653 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10716080665588379\n",
      "Epoch: 157, Steps: 1 | Train Loss: 802.1407471 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10726284980773926\n",
      "Epoch: 158, Steps: 1 | Train Loss: 801.8357544 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.1072993278503418\n",
      "Epoch: 159, Steps: 1 | Train Loss: 801.6839600 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.10764455795288086\n",
      "Epoch: 160, Steps: 1 | Train Loss: 802.2533569 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.1179203987121582\n",
      "Epoch: 161, Steps: 1 | Train Loss: 802.1042480 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.11378026008605957\n",
      "Epoch: 162, Steps: 1 | Train Loss: 801.4127808 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.1324923038482666\n",
      "Epoch: 163, Steps: 1 | Train Loss: 801.5856323 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.13173270225524902\n",
      "Epoch: 164, Steps: 1 | Train Loss: 802.2003784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.12464737892150879\n",
      "Epoch: 165, Steps: 1 | Train Loss: 802.8903198 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.12181568145751953\n",
      "Epoch: 166, Steps: 1 | Train Loss: 802.5455322 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.12316036224365234\n",
      "Epoch: 167, Steps: 1 | Train Loss: 800.9279175 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11949849128723145\n",
      "Epoch: 168, Steps: 1 | Train Loss: 801.7135010 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11872673034667969\n",
      "Epoch: 169, Steps: 1 | Train Loss: 802.3568115 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.1108551025390625\n",
      "Epoch: 170, Steps: 1 | Train Loss: 802.8534546 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.11115908622741699\n",
      "Epoch: 171, Steps: 1 | Train Loss: 801.5204468 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11262774467468262\n",
      "Epoch: 172, Steps: 1 | Train Loss: 801.1607056 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.1201786994934082\n",
      "Epoch: 173, Steps: 1 | Train Loss: 802.2190552 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.11699557304382324\n",
      "Epoch: 174, Steps: 1 | Train Loss: 801.2763672 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.1150515079498291\n",
      "Epoch: 175, Steps: 1 | Train Loss: 801.4495850 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.11981678009033203\n",
      "Epoch: 176, Steps: 1 | Train Loss: 801.4400024 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.1333017349243164\n",
      "Epoch: 177, Steps: 1 | Train Loss: 802.7797241 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.13289284706115723\n",
      "Epoch: 178, Steps: 1 | Train Loss: 802.1784058 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.13210654258728027\n",
      "Epoch: 179, Steps: 1 | Train Loss: 801.8439331 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.12842583656311035\n",
      "Epoch: 180, Steps: 1 | Train Loss: 802.2255859 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.12750530242919922\n",
      "Epoch: 181, Steps: 1 | Train Loss: 801.8300171 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.10977029800415039\n",
      "Epoch: 182, Steps: 1 | Train Loss: 800.7099609 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.11798501014709473\n",
      "Epoch: 183, Steps: 1 | Train Loss: 802.7551880 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11823725700378418\n",
      "Epoch: 184, Steps: 1 | Train Loss: 802.0595093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11800932884216309\n",
      "Epoch: 185, Steps: 1 | Train Loss: 802.1293335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11005377769470215\n",
      "Epoch: 186, Steps: 1 | Train Loss: 802.1954956 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.11123013496398926\n",
      "Epoch: 187, Steps: 1 | Train Loss: 801.2218628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.1094508171081543\n",
      "Epoch: 188, Steps: 1 | Train Loss: 801.9766235 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.11291003227233887\n",
      "Epoch: 189, Steps: 1 | Train Loss: 801.8980103 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.10998964309692383\n",
      "Epoch: 190, Steps: 1 | Train Loss: 800.7223511 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.11486244201660156\n",
      "Epoch: 191, Steps: 1 | Train Loss: 801.3932495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10989761352539062\n",
      "Epoch: 192, Steps: 1 | Train Loss: 801.3434448 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.11174511909484863\n",
      "Epoch: 193, Steps: 1 | Train Loss: 802.1254272 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11091804504394531\n",
      "Epoch: 194, Steps: 1 | Train Loss: 802.7682495 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.10801005363464355\n",
      "Epoch: 195, Steps: 1 | Train Loss: 802.6329956 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11199235916137695\n",
      "Epoch: 196, Steps: 1 | Train Loss: 801.5823975 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11205649375915527\n",
      "Epoch: 197, Steps: 1 | Train Loss: 802.8428955 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.11859321594238281\n",
      "Epoch: 198, Steps: 1 | Train Loss: 800.7832642 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.10937070846557617\n",
      "Epoch: 199, Steps: 1 | Train Loss: 801.6809082 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.13389086723327637\n",
      "Epoch: 200, Steps: 1 | Train Loss: 801.2768555 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.11649513244628906\n",
      "Epoch: 201, Steps: 1 | Train Loss: 802.4181519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.11002230644226074\n",
      "Epoch: 202, Steps: 1 | Train Loss: 801.4536133 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11218118667602539\n",
      "Epoch: 203, Steps: 1 | Train Loss: 802.1181641 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.1115562915802002\n",
      "Epoch: 204, Steps: 1 | Train Loss: 802.0997925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.11630916595458984\n",
      "Epoch: 205, Steps: 1 | Train Loss: 801.1261597 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.11176204681396484\n",
      "Epoch: 206, Steps: 1 | Train Loss: 801.3361816 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.1163794994354248\n",
      "Epoch: 207, Steps: 1 | Train Loss: 801.4472046 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.11596179008483887\n",
      "Epoch: 208, Steps: 1 | Train Loss: 801.9597778 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.12025117874145508\n",
      "Epoch: 209, Steps: 1 | Train Loss: 801.3566284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.13605880737304688\n",
      "Epoch: 210, Steps: 1 | Train Loss: 802.3698120 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.13029170036315918\n",
      "Epoch: 211, Steps: 1 | Train Loss: 802.7313232 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11487340927124023\n",
      "Epoch: 212, Steps: 1 | Train Loss: 802.1656494 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.10822010040283203\n",
      "Epoch: 213, Steps: 1 | Train Loss: 801.1331177 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11197590827941895\n",
      "Epoch: 214, Steps: 1 | Train Loss: 801.5735474 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.1106407642364502\n",
      "Epoch: 215, Steps: 1 | Train Loss: 801.2418213 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.10673904418945312\n",
      "Epoch: 216, Steps: 1 | Train Loss: 801.1044312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.10722970962524414\n",
      "Epoch: 217, Steps: 1 | Train Loss: 801.4423828 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.10739707946777344\n",
      "Epoch: 218, Steps: 1 | Train Loss: 802.1107788 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.10743832588195801\n",
      "Epoch: 219, Steps: 1 | Train Loss: 801.7740479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.10754799842834473\n",
      "Epoch: 220, Steps: 1 | Train Loss: 803.5046997 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.12128663063049316\n",
      "Epoch: 221, Steps: 1 | Train Loss: 802.4599609 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.11180329322814941\n",
      "Epoch: 222, Steps: 1 | Train Loss: 801.8609009 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.10926413536071777\n",
      "Epoch: 223, Steps: 1 | Train Loss: 803.0093994 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.10846567153930664\n",
      "Epoch: 224, Steps: 1 | Train Loss: 802.6549683 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.10785174369812012\n",
      "Epoch: 225, Steps: 1 | Train Loss: 800.9359741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.10771918296813965\n",
      "Epoch: 226, Steps: 1 | Train Loss: 800.0441284 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.10757279396057129\n",
      "Epoch: 227, Steps: 1 | Train Loss: 801.4349365 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.1078634262084961\n",
      "Epoch: 228, Steps: 1 | Train Loss: 802.4315186 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.1070556640625\n",
      "Epoch: 229, Steps: 1 | Train Loss: 802.7062378 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11151933670043945\n",
      "Epoch: 230, Steps: 1 | Train Loss: 803.1848755 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.1117715835571289\n",
      "Epoch: 231, Steps: 1 | Train Loss: 802.5227661 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.12437272071838379\n",
      "Epoch: 232, Steps: 1 | Train Loss: 801.7350464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.12501835823059082\n",
      "Epoch: 233, Steps: 1 | Train Loss: 802.6041260 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.10940313339233398\n",
      "Epoch: 234, Steps: 1 | Train Loss: 800.4959717 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.13315796852111816\n",
      "Epoch: 235, Steps: 1 | Train Loss: 802.6555786 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11764121055603027\n",
      "Epoch: 236, Steps: 1 | Train Loss: 801.7142944 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11763477325439453\n",
      "Epoch: 237, Steps: 1 | Train Loss: 801.8654175 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11688065528869629\n",
      "Epoch: 238, Steps: 1 | Train Loss: 801.3905640 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.11704707145690918\n",
      "Epoch: 239, Steps: 1 | Train Loss: 802.0537109 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.12174367904663086\n",
      "Epoch: 240, Steps: 1 | Train Loss: 800.7506714 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.10750770568847656\n",
      "Epoch: 241, Steps: 1 | Train Loss: 800.8944092 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.10747599601745605\n",
      "Epoch: 242, Steps: 1 | Train Loss: 802.2973022 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.1072392463684082\n",
      "Epoch: 243, Steps: 1 | Train Loss: 802.0552979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.10753107070922852\n",
      "Epoch: 244, Steps: 1 | Train Loss: 801.5707397 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.10756516456604004\n",
      "Epoch: 245, Steps: 1 | Train Loss: 801.7681885 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.1074228286743164\n",
      "Epoch: 246, Steps: 1 | Train Loss: 802.7698364 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11101961135864258\n",
      "Epoch: 247, Steps: 1 | Train Loss: 800.6019897 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.10916018486022949\n",
      "Epoch: 248, Steps: 1 | Train Loss: 801.6206055 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.11832213401794434\n",
      "Epoch: 249, Steps: 1 | Train Loss: 801.1754150 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.10950040817260742\n",
      "Epoch: 250, Steps: 1 | Train Loss: 802.5746460 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "21\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_6>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11380314826965332\n",
      "Epoch: 1, Steps: 1 | Train Loss: 814.9334106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.10975217819213867\n",
      "Epoch: 2, Steps: 1 | Train Loss: 810.6942749 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11113953590393066\n",
      "Epoch: 3, Steps: 1 | Train Loss: 805.1859741 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.10750985145568848\n",
      "Epoch: 4, Steps: 1 | Train Loss: 803.7138062 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11085152626037598\n",
      "Epoch: 5, Steps: 1 | Train Loss: 802.2741699 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.1108396053314209\n",
      "Epoch: 6, Steps: 1 | Train Loss: 801.1445312 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.10889720916748047\n",
      "Epoch: 7, Steps: 1 | Train Loss: 800.9747925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.10822463035583496\n",
      "Epoch: 8, Steps: 1 | Train Loss: 800.0678101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.10777401924133301\n",
      "Epoch: 9, Steps: 1 | Train Loss: 800.6027222 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.10771584510803223\n",
      "Epoch: 10, Steps: 1 | Train Loss: 801.5750122 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.10764884948730469\n",
      "Epoch: 11, Steps: 1 | Train Loss: 799.0764771 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10786724090576172\n",
      "Epoch: 12, Steps: 1 | Train Loss: 800.4125366 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.10738563537597656\n",
      "Epoch: 13, Steps: 1 | Train Loss: 800.6822510 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.10722875595092773\n",
      "Epoch: 14, Steps: 1 | Train Loss: 800.1012573 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.10761022567749023\n",
      "Epoch: 15, Steps: 1 | Train Loss: 800.4594116 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.10778069496154785\n",
      "Epoch: 16, Steps: 1 | Train Loss: 800.9537354 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.10783958435058594\n",
      "Epoch: 17, Steps: 1 | Train Loss: 799.8710938 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.12149262428283691\n",
      "Epoch: 18, Steps: 1 | Train Loss: 800.4116211 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.11656975746154785\n",
      "Epoch: 19, Steps: 1 | Train Loss: 800.2489624 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.1103506088256836\n",
      "Epoch: 20, Steps: 1 | Train Loss: 801.6401367 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.10882806777954102\n",
      "Epoch: 21, Steps: 1 | Train Loss: 800.5001831 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.10787534713745117\n",
      "Epoch: 22, Steps: 1 | Train Loss: 800.5358887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.10772538185119629\n",
      "Epoch: 23, Steps: 1 | Train Loss: 798.6843872 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.1075143814086914\n",
      "Epoch: 24, Steps: 1 | Train Loss: 800.3826294 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.10750007629394531\n",
      "Epoch: 25, Steps: 1 | Train Loss: 800.7998047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.10792255401611328\n",
      "Epoch: 26, Steps: 1 | Train Loss: 799.7998047 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9802322387695315e-13\n",
      "Epoch: 27 cost time: 0.10808801651000977\n",
      "Epoch: 27, Steps: 1 | Train Loss: 801.2821045 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4901161193847657e-13\n",
      "Epoch: 28 cost time: 0.10769367218017578\n",
      "Epoch: 28, Steps: 1 | Train Loss: 800.4959106 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.450580596923829e-14\n",
      "Epoch: 29 cost time: 0.12193083763122559\n",
      "Epoch: 29, Steps: 1 | Train Loss: 800.6095581 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.7252902984619144e-14\n",
      "Epoch: 30 cost time: 0.11348319053649902\n",
      "Epoch: 30, Steps: 1 | Train Loss: 801.6490479 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8626451492309572e-14\n",
      "Epoch: 31 cost time: 0.1095583438873291\n",
      "Epoch: 31, Steps: 1 | Train Loss: 801.2881470 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.313225746154786e-15\n",
      "Epoch: 32 cost time: 0.1087956428527832\n",
      "Epoch: 32, Steps: 1 | Train Loss: 799.6342163 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.656612873077393e-15\n",
      "Epoch: 33 cost time: 0.1078498363494873\n",
      "Epoch: 33, Steps: 1 | Train Loss: 800.5537109 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3283064365386965e-15\n",
      "Epoch: 34 cost time: 0.10751175880432129\n",
      "Epoch: 34, Steps: 1 | Train Loss: 799.5977783 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1641532182693482e-15\n",
      "Epoch: 35 cost time: 0.10762166976928711\n",
      "Epoch: 35, Steps: 1 | Train Loss: 799.5657959 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.820766091346741e-16\n",
      "Epoch: 36 cost time: 0.10512304306030273\n",
      "Epoch: 36, Steps: 1 | Train Loss: 800.4412842 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9103830456733706e-16\n",
      "Epoch: 37 cost time: 0.11728167533874512\n",
      "Epoch: 37, Steps: 1 | Train Loss: 799.6852417 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4551915228366853e-16\n",
      "Epoch: 38 cost time: 0.10533809661865234\n",
      "Epoch: 38, Steps: 1 | Train Loss: 801.2443848 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.275957614183426e-17\n",
      "Epoch: 39 cost time: 0.1104128360748291\n",
      "Epoch: 39, Steps: 1 | Train Loss: 800.2136841 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.637978807091713e-17\n",
      "Epoch: 40 cost time: 0.11187219619750977\n",
      "Epoch: 40, Steps: 1 | Train Loss: 800.9478149 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8189894035458566e-17\n",
      "Epoch: 41 cost time: 0.10988521575927734\n",
      "Epoch: 41, Steps: 1 | Train Loss: 798.7731934 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.094947017729283e-18\n",
      "Epoch: 42 cost time: 0.10878920555114746\n",
      "Epoch: 42, Steps: 1 | Train Loss: 801.5106201 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5474735088646416e-18\n",
      "Epoch: 43 cost time: 0.10756349563598633\n",
      "Epoch: 43, Steps: 1 | Train Loss: 800.3132935 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2737367544323208e-18\n",
      "Epoch: 44 cost time: 0.10905981063842773\n",
      "Epoch: 44, Steps: 1 | Train Loss: 800.6430054 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1368683772161604e-18\n",
      "Epoch: 45 cost time: 0.10782980918884277\n",
      "Epoch: 45, Steps: 1 | Train Loss: 799.5209351 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.684341886080802e-19\n",
      "Epoch: 46 cost time: 0.10735821723937988\n",
      "Epoch: 46, Steps: 1 | Train Loss: 801.1127319 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.842170943040401e-19\n",
      "Epoch: 47 cost time: 0.10757660865783691\n",
      "Epoch: 47, Steps: 1 | Train Loss: 799.4799194 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4210854715202005e-19\n",
      "Epoch: 48 cost time: 0.10791182518005371\n",
      "Epoch: 48, Steps: 1 | Train Loss: 800.7679443 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 7.105427357601002e-20\n",
      "Epoch: 49 cost time: 0.10787558555603027\n",
      "Epoch: 49, Steps: 1 | Train Loss: 799.2614136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.552713678800501e-20\n",
      "Epoch: 50 cost time: 0.12120699882507324\n",
      "Epoch: 50, Steps: 1 | Train Loss: 799.4037476 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7763568394002506e-20\n",
      "Epoch: 51 cost time: 0.12679624557495117\n",
      "Epoch: 51, Steps: 1 | Train Loss: 800.5424805 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.881784197001253e-21\n",
      "Epoch: 52 cost time: 0.11644554138183594\n",
      "Epoch: 52, Steps: 1 | Train Loss: 801.5140991 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.4408920985006265e-21\n",
      "Epoch: 53 cost time: 0.118499755859375\n",
      "Epoch: 53, Steps: 1 | Train Loss: 801.8023071 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2204460492503133e-21\n",
      "Epoch: 54 cost time: 0.11626100540161133\n",
      "Epoch: 54, Steps: 1 | Train Loss: 800.2161255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1102230246251566e-21\n",
      "Epoch: 55 cost time: 0.1143946647644043\n",
      "Epoch: 55, Steps: 1 | Train Loss: 800.0537109 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.551115123125783e-22\n",
      "Epoch: 56 cost time: 0.1161656379699707\n",
      "Epoch: 56, Steps: 1 | Train Loss: 801.7881470 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7755575615628916e-22\n",
      "Epoch: 57 cost time: 0.11536312103271484\n",
      "Epoch: 57, Steps: 1 | Train Loss: 799.8527832 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3877787807814458e-22\n",
      "Epoch: 58 cost time: 0.11140203475952148\n",
      "Epoch: 58, Steps: 1 | Train Loss: 800.8609009 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.938893903907229e-23\n",
      "Epoch: 59 cost time: 0.11125731468200684\n",
      "Epoch: 59, Steps: 1 | Train Loss: 800.9122925 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4694469519536145e-23\n",
      "Epoch: 60 cost time: 0.11067748069763184\n",
      "Epoch: 60, Steps: 1 | Train Loss: 801.2493896 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7347234759768072e-23\n",
      "Epoch: 61 cost time: 0.11170220375061035\n",
      "Epoch: 61, Steps: 1 | Train Loss: 799.8684692 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.673617379884036e-24\n",
      "Epoch: 62 cost time: 0.1325228214263916\n",
      "Epoch: 62, Steps: 1 | Train Loss: 800.6395264 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.336808689942018e-24\n",
      "Epoch: 63 cost time: 0.12970805168151855\n",
      "Epoch: 63, Steps: 1 | Train Loss: 800.6353760 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.168404344971009e-24\n",
      "Epoch: 64 cost time: 0.12256693840026855\n",
      "Epoch: 64, Steps: 1 | Train Loss: 800.8419189 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0842021724855045e-24\n",
      "Epoch: 65 cost time: 0.12070751190185547\n",
      "Epoch: 65, Steps: 1 | Train Loss: 799.5479126 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.421010862427523e-25\n",
      "Epoch: 66 cost time: 0.11765909194946289\n",
      "Epoch: 66, Steps: 1 | Train Loss: 800.5785522 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.7105054312137613e-25\n",
      "Epoch: 67 cost time: 0.11046648025512695\n",
      "Epoch: 67, Steps: 1 | Train Loss: 801.1837769 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3552527156068807e-25\n",
      "Epoch: 68 cost time: 0.11693978309631348\n",
      "Epoch: 68, Steps: 1 | Train Loss: 801.9884644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.776263578034403e-26\n",
      "Epoch: 69 cost time: 0.11047601699829102\n",
      "Epoch: 69, Steps: 1 | Train Loss: 799.5355835 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.3881317890172016e-26\n",
      "Epoch: 70 cost time: 0.1080327033996582\n",
      "Epoch: 70, Steps: 1 | Train Loss: 802.0338745 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6940658945086008e-26\n",
      "Epoch: 71 cost time: 0.10788488388061523\n",
      "Epoch: 71, Steps: 1 | Train Loss: 800.0281372 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.470329472543004e-27\n",
      "Epoch: 72 cost time: 0.10827016830444336\n",
      "Epoch: 72, Steps: 1 | Train Loss: 799.0273438 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.235164736271502e-27\n",
      "Epoch: 73 cost time: 0.10808062553405762\n",
      "Epoch: 73, Steps: 1 | Train Loss: 801.8009033 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.117582368135751e-27\n",
      "Epoch: 74 cost time: 0.12150859832763672\n",
      "Epoch: 74, Steps: 1 | Train Loss: 799.5530396 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0587911840678755e-27\n",
      "Epoch: 75 cost time: 0.12071990966796875\n",
      "Epoch: 75, Steps: 1 | Train Loss: 800.2470703 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.293955920339378e-28\n",
      "Epoch: 76 cost time: 0.1341409683227539\n",
      "Epoch: 76, Steps: 1 | Train Loss: 801.1779175 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.646977960169689e-28\n",
      "Epoch: 77 cost time: 0.10445570945739746\n",
      "Epoch: 77, Steps: 1 | Train Loss: 801.4513550 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3234889800848444e-28\n",
      "Epoch: 78 cost time: 0.10136604309082031\n",
      "Epoch: 78, Steps: 1 | Train Loss: 800.1812744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.617444900424222e-29\n",
      "Epoch: 79 cost time: 0.10058307647705078\n",
      "Epoch: 79, Steps: 1 | Train Loss: 800.0872192 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.308722450212111e-29\n",
      "Epoch: 80 cost time: 0.10080933570861816\n",
      "Epoch: 80, Steps: 1 | Train Loss: 800.3083496 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6543612251060555e-29\n",
      "Epoch: 81 cost time: 0.10042881965637207\n",
      "Epoch: 81, Steps: 1 | Train Loss: 801.5905151 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.271806125530277e-30\n",
      "Epoch: 82 cost time: 0.10105562210083008\n",
      "Epoch: 82, Steps: 1 | Train Loss: 799.0678101 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.135903062765139e-30\n",
      "Epoch: 83 cost time: 0.10032868385314941\n",
      "Epoch: 83, Steps: 1 | Train Loss: 800.2583008 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0679515313825694e-30\n",
      "Epoch: 84 cost time: 0.11250615119934082\n",
      "Epoch: 84, Steps: 1 | Train Loss: 800.4343262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0339757656912847e-30\n",
      "Epoch: 85 cost time: 0.12704753875732422\n",
      "Epoch: 85, Steps: 1 | Train Loss: 800.8816528 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 5.169878828456423e-31\n",
      "Epoch: 86 cost time: 0.1261143684387207\n",
      "Epoch: 86, Steps: 1 | Train Loss: 800.0877075 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5849394142282117e-31\n",
      "Epoch: 87 cost time: 0.12104177474975586\n",
      "Epoch: 87, Steps: 1 | Train Loss: 801.4144287 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2924697071141058e-31\n",
      "Epoch: 88 cost time: 0.1224067211151123\n",
      "Epoch: 88, Steps: 1 | Train Loss: 800.7645264 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.462348535570529e-32\n",
      "Epoch: 89 cost time: 0.11042952537536621\n",
      "Epoch: 89, Steps: 1 | Train Loss: 800.6725464 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2311742677852646e-32\n",
      "Epoch: 90 cost time: 0.10848093032836914\n",
      "Epoch: 90, Steps: 1 | Train Loss: 800.5690308 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6155871338926323e-32\n",
      "Epoch: 91 cost time: 0.10807037353515625\n",
      "Epoch: 91, Steps: 1 | Train Loss: 801.5549927 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.077935669463162e-33\n",
      "Epoch: 92 cost time: 0.10878157615661621\n",
      "Epoch: 92, Steps: 1 | Train Loss: 798.3502808 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.038967834731581e-33\n",
      "Epoch: 93 cost time: 0.10876679420471191\n",
      "Epoch: 93, Steps: 1 | Train Loss: 801.7340088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.0194839173657904e-33\n",
      "Epoch: 94 cost time: 0.10857844352722168\n",
      "Epoch: 94, Steps: 1 | Train Loss: 800.0750122 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0097419586828952e-33\n",
      "Epoch: 95 cost time: 0.10849952697753906\n",
      "Epoch: 95, Steps: 1 | Train Loss: 799.8568726 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.048709793414476e-34\n",
      "Epoch: 96 cost time: 0.10865330696105957\n",
      "Epoch: 96, Steps: 1 | Train Loss: 802.2189941 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.524354896707238e-34\n",
      "Epoch: 97 cost time: 0.10894203186035156\n",
      "Epoch: 97, Steps: 1 | Train Loss: 800.3138428 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.262177448353619e-34\n",
      "Epoch: 98 cost time: 0.10853219032287598\n",
      "Epoch: 98, Steps: 1 | Train Loss: 800.2161255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.310887241768095e-35\n",
      "Epoch: 99 cost time: 0.12157249450683594\n",
      "Epoch: 99, Steps: 1 | Train Loss: 800.0430908 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.1554436208840475e-35\n",
      "Epoch: 100 cost time: 0.1242682933807373\n",
      "Epoch: 100, Steps: 1 | Train Loss: 800.4568481 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5777218104420237e-35\n",
      "Epoch: 101 cost time: 0.11438179016113281\n",
      "Epoch: 101, Steps: 1 | Train Loss: 800.2727051 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.888609052210119e-36\n",
      "Epoch: 102 cost time: 0.10924625396728516\n",
      "Epoch: 102, Steps: 1 | Train Loss: 801.5087280 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.9443045261050593e-36\n",
      "Epoch: 103 cost time: 0.10823273658752441\n",
      "Epoch: 103, Steps: 1 | Train Loss: 799.9423218 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.9721522630525297e-36\n",
      "Epoch: 104 cost time: 0.10752534866333008\n",
      "Epoch: 104, Steps: 1 | Train Loss: 802.5216064 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.860761315262648e-37\n",
      "Epoch: 105 cost time: 0.10753393173217773\n",
      "Epoch: 105, Steps: 1 | Train Loss: 800.0247192 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.930380657631324e-37\n",
      "Epoch: 106 cost time: 0.1076667308807373\n",
      "Epoch: 106, Steps: 1 | Train Loss: 802.7335205 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.465190328815662e-37\n",
      "Epoch: 107 cost time: 0.10778546333312988\n",
      "Epoch: 107, Steps: 1 | Train Loss: 800.1010132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.232595164407831e-37\n",
      "Epoch: 108 cost time: 0.11023807525634766\n",
      "Epoch: 108, Steps: 1 | Train Loss: 801.6322632 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.162975822039155e-38\n",
      "Epoch: 109 cost time: 0.10765504837036133\n",
      "Epoch: 109, Steps: 1 | Train Loss: 801.5548706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0814879110195776e-38\n",
      "Epoch: 110 cost time: 0.1077423095703125\n",
      "Epoch: 110, Steps: 1 | Train Loss: 798.5632935 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5407439555097888e-38\n",
      "Epoch: 111 cost time: 0.1111764907836914\n",
      "Epoch: 111, Steps: 1 | Train Loss: 802.1561890 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.703719777548944e-39\n",
      "Epoch: 112 cost time: 0.10857009887695312\n",
      "Epoch: 112, Steps: 1 | Train Loss: 798.7357178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.851859888774472e-39\n",
      "Epoch: 113 cost time: 0.10813426971435547\n",
      "Epoch: 113, Steps: 1 | Train Loss: 799.3395386 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.925929944387236e-39\n",
      "Epoch: 114 cost time: 0.10788869857788086\n",
      "Epoch: 114, Steps: 1 | Train Loss: 800.1510620 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.62964972193618e-40\n",
      "Epoch: 115 cost time: 0.1076207160949707\n",
      "Epoch: 115, Steps: 1 | Train Loss: 799.3518066 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.81482486096809e-40\n",
      "Epoch: 116 cost time: 0.10746598243713379\n",
      "Epoch: 116, Steps: 1 | Train Loss: 802.5938721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.407412430484045e-40\n",
      "Epoch: 117 cost time: 0.10764169692993164\n",
      "Epoch: 117, Steps: 1 | Train Loss: 801.5368042 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2037062152420225e-40\n",
      "Epoch: 118 cost time: 0.1072845458984375\n",
      "Epoch: 118, Steps: 1 | Train Loss: 799.9396362 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.018531076210113e-41\n",
      "Epoch: 119 cost time: 0.10772895812988281\n",
      "Epoch: 119, Steps: 1 | Train Loss: 800.2260132 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0092655381050563e-41\n",
      "Epoch: 120 cost time: 0.12159967422485352\n",
      "Epoch: 120, Steps: 1 | Train Loss: 801.0739136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5046327690525281e-41\n",
      "Epoch: 121 cost time: 0.11614489555358887\n",
      "Epoch: 121, Steps: 1 | Train Loss: 799.6818237 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.523163845262641e-42\n",
      "Epoch: 122 cost time: 0.11066365242004395\n",
      "Epoch: 122, Steps: 1 | Train Loss: 800.8673706 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 3.7615819226313203e-42\n",
      "Epoch: 123 cost time: 0.10905838012695312\n",
      "Epoch: 123, Steps: 1 | Train Loss: 799.5409546 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8807909613156602e-42\n",
      "Epoch: 124 cost time: 0.10782599449157715\n",
      "Epoch: 124, Steps: 1 | Train Loss: 800.7672729 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.403954806578301e-43\n",
      "Epoch: 125 cost time: 0.1076207160949707\n",
      "Epoch: 125, Steps: 1 | Train Loss: 799.0229492 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7019774032891504e-43\n",
      "Epoch: 126 cost time: 0.10752034187316895\n",
      "Epoch: 126, Steps: 1 | Train Loss: 800.2880249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3509887016445752e-43\n",
      "Epoch: 127 cost time: 0.10737252235412598\n",
      "Epoch: 127, Steps: 1 | Train Loss: 800.2754517 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1754943508222876e-43\n",
      "Epoch: 128 cost time: 0.1080617904663086\n",
      "Epoch: 128, Steps: 1 | Train Loss: 800.3800049 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.877471754111438e-44\n",
      "Epoch: 129 cost time: 0.10809135437011719\n",
      "Epoch: 129, Steps: 1 | Train Loss: 799.6156006 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.938735877055719e-44\n",
      "Epoch: 130 cost time: 0.10756373405456543\n",
      "Epoch: 130, Steps: 1 | Train Loss: 800.2660522 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4693679385278595e-44\n",
      "Epoch: 131 cost time: 0.12125992774963379\n",
      "Epoch: 131, Steps: 1 | Train Loss: 800.2953491 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.346839692639298e-45\n",
      "Epoch: 132 cost time: 0.11880898475646973\n",
      "Epoch: 132, Steps: 1 | Train Loss: 801.2593384 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.673419846319649e-45\n",
      "Epoch: 133 cost time: 0.11050105094909668\n",
      "Epoch: 133, Steps: 1 | Train Loss: 801.3428345 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8367099231598244e-45\n",
      "Epoch: 134 cost time: 0.10919928550720215\n",
      "Epoch: 134, Steps: 1 | Train Loss: 800.4884644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.183549615799122e-46\n",
      "Epoch: 135 cost time: 0.10834026336669922\n",
      "Epoch: 135, Steps: 1 | Train Loss: 800.4978027 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.591774807899561e-46\n",
      "Epoch: 136 cost time: 0.10776281356811523\n",
      "Epoch: 136, Steps: 1 | Train Loss: 800.3204956 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2958874039497805e-46\n",
      "Epoch: 137 cost time: 0.10764098167419434\n",
      "Epoch: 137, Steps: 1 | Train Loss: 799.9516602 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1479437019748902e-46\n",
      "Epoch: 138 cost time: 0.10780549049377441\n",
      "Epoch: 138, Steps: 1 | Train Loss: 800.2726440 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.739718509874451e-47\n",
      "Epoch: 139 cost time: 0.1077275276184082\n",
      "Epoch: 139, Steps: 1 | Train Loss: 800.4745483 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8698592549372256e-47\n",
      "Epoch: 140 cost time: 0.10770535469055176\n",
      "Epoch: 140, Steps: 1 | Train Loss: 798.8225098 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4349296274686128e-47\n",
      "Epoch: 141 cost time: 0.10749173164367676\n",
      "Epoch: 141, Steps: 1 | Train Loss: 799.3923340 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.174648137343064e-48\n",
      "Epoch: 142 cost time: 0.10767698287963867\n",
      "Epoch: 142, Steps: 1 | Train Loss: 800.4343262 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.587324068671532e-48\n",
      "Epoch: 143 cost time: 0.11800003051757812\n",
      "Epoch: 143, Steps: 1 | Train Loss: 800.1011963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.793662034335766e-48\n",
      "Epoch: 144 cost time: 0.11025285720825195\n",
      "Epoch: 144, Steps: 1 | Train Loss: 800.6687012 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.96831017167883e-49\n",
      "Epoch: 145 cost time: 0.10934138298034668\n",
      "Epoch: 145, Steps: 1 | Train Loss: 801.2242432 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.484155085839415e-49\n",
      "Epoch: 146 cost time: 0.10852384567260742\n",
      "Epoch: 146, Steps: 1 | Train Loss: 800.0739136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2420775429197075e-49\n",
      "Epoch: 147 cost time: 0.10779070854187012\n",
      "Epoch: 147, Steps: 1 | Train Loss: 800.2543335 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1210387714598537e-49\n",
      "Epoch: 148 cost time: 0.10755681991577148\n",
      "Epoch: 148, Steps: 1 | Train Loss: 800.4043579 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.605193857299269e-50\n",
      "Epoch: 149 cost time: 0.10780739784240723\n",
      "Epoch: 149, Steps: 1 | Train Loss: 801.1079712 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.8025969286496344e-50\n",
      "Epoch: 150 cost time: 0.10770344734191895\n",
      "Epoch: 150, Steps: 1 | Train Loss: 801.7340088 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4012984643248172e-50\n",
      "Epoch: 151 cost time: 0.1076195240020752\n",
      "Epoch: 151, Steps: 1 | Train Loss: 800.0908203 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.006492321624086e-51\n",
      "Epoch: 152 cost time: 0.10782098770141602\n",
      "Epoch: 152, Steps: 1 | Train Loss: 801.4722900 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.503246160812043e-51\n",
      "Epoch: 153 cost time: 0.10785102844238281\n",
      "Epoch: 153, Steps: 1 | Train Loss: 798.4513550 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7516230804060215e-51\n",
      "Epoch: 154 cost time: 0.11597871780395508\n",
      "Epoch: 154, Steps: 1 | Train Loss: 800.6134644 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.758115402030107e-52\n",
      "Epoch: 155 cost time: 0.10832571983337402\n",
      "Epoch: 155, Steps: 1 | Train Loss: 799.7290039 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.379057701015054e-52\n",
      "Epoch: 156 cost time: 0.10848712921142578\n",
      "Epoch: 156, Steps: 1 | Train Loss: 800.6500244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.189528850507527e-52\n",
      "Epoch: 157 cost time: 0.10816144943237305\n",
      "Epoch: 157, Steps: 1 | Train Loss: 800.0832520 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0947644252537634e-52\n",
      "Epoch: 158 cost time: 0.10829663276672363\n",
      "Epoch: 158, Steps: 1 | Train Loss: 801.7179565 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.473822126268817e-53\n",
      "Epoch: 159 cost time: 0.12155771255493164\n",
      "Epoch: 159, Steps: 1 | Train Loss: 799.6005249 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 2.7369110631344086e-53\n",
      "Epoch: 160 cost time: 0.11487460136413574\n",
      "Epoch: 160, Steps: 1 | Train Loss: 800.8239136 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.3684555315672043e-53\n",
      "Epoch: 161 cost time: 0.11004805564880371\n",
      "Epoch: 161, Steps: 1 | Train Loss: 800.9990234 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.842277657836021e-54\n",
      "Epoch: 162 cost time: 0.10865187644958496\n",
      "Epoch: 162, Steps: 1 | Train Loss: 799.1032715 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.4211388289180107e-54\n",
      "Epoch: 163 cost time: 0.1078643798828125\n",
      "Epoch: 163, Steps: 1 | Train Loss: 800.1939697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7105694144590054e-54\n",
      "Epoch: 164 cost time: 0.10672163963317871\n",
      "Epoch: 164, Steps: 1 | Train Loss: 800.7609253 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.552847072295027e-55\n",
      "Epoch: 165 cost time: 0.11071920394897461\n",
      "Epoch: 165, Steps: 1 | Train Loss: 798.3374023 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.2764235361475134e-55\n",
      "Epoch: 166 cost time: 0.11062932014465332\n",
      "Epoch: 166, Steps: 1 | Train Loss: 799.7470093 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.1382117680737567e-55\n",
      "Epoch: 167 cost time: 0.11061453819274902\n",
      "Epoch: 167, Steps: 1 | Train Loss: 801.0003052 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0691058840368783e-55\n",
      "Epoch: 168 cost time: 0.11092185974121094\n",
      "Epoch: 168, Steps: 1 | Train Loss: 800.6249390 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.345529420184392e-56\n",
      "Epoch: 169 cost time: 0.11121892929077148\n",
      "Epoch: 169, Steps: 1 | Train Loss: 800.0388794 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.672764710092196e-56\n",
      "Epoch: 170 cost time: 0.11119389533996582\n",
      "Epoch: 170, Steps: 1 | Train Loss: 800.9459229 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.336382355046098e-56\n",
      "Epoch: 171 cost time: 0.12449383735656738\n",
      "Epoch: 171, Steps: 1 | Train Loss: 799.6095581 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.68191177523049e-57\n",
      "Epoch: 172 cost time: 0.11866903305053711\n",
      "Epoch: 172, Steps: 1 | Train Loss: 801.2564697 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.340955887615245e-57\n",
      "Epoch: 173 cost time: 0.11386489868164062\n",
      "Epoch: 173, Steps: 1 | Train Loss: 802.4036255 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6704779438076224e-57\n",
      "Epoch: 174 cost time: 0.1111292839050293\n",
      "Epoch: 174, Steps: 1 | Train Loss: 801.2184448 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.352389719038112e-58\n",
      "Epoch: 175 cost time: 0.11028313636779785\n",
      "Epoch: 175, Steps: 1 | Train Loss: 802.2609253 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.176194859519056e-58\n",
      "Epoch: 176 cost time: 0.11107420921325684\n",
      "Epoch: 176, Steps: 1 | Train Loss: 800.4701538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.088097429759528e-58\n",
      "Epoch: 177 cost time: 0.11080551147460938\n",
      "Epoch: 177, Steps: 1 | Train Loss: 799.4425659 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.044048714879764e-58\n",
      "Epoch: 178 cost time: 0.11000418663024902\n",
      "Epoch: 178, Steps: 1 | Train Loss: 800.8656616 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.22024357439882e-59\n",
      "Epoch: 179 cost time: 0.10998201370239258\n",
      "Epoch: 179, Steps: 1 | Train Loss: 799.9181519 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.61012178719941e-59\n",
      "Epoch: 180 cost time: 0.11119341850280762\n",
      "Epoch: 180, Steps: 1 | Train Loss: 800.9314575 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.305060893599705e-59\n",
      "Epoch: 181 cost time: 0.11020779609680176\n",
      "Epoch: 181, Steps: 1 | Train Loss: 802.0418091 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.525304467998525e-60\n",
      "Epoch: 182 cost time: 0.12428092956542969\n",
      "Epoch: 182, Steps: 1 | Train Loss: 799.2257080 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.2626522339992625e-60\n",
      "Epoch: 183 cost time: 0.12437129020690918\n",
      "Epoch: 183, Steps: 1 | Train Loss: 800.6470947 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.6313261169996313e-60\n",
      "Epoch: 184 cost time: 0.11544203758239746\n",
      "Epoch: 184, Steps: 1 | Train Loss: 801.1222534 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.156630584998156e-61\n",
      "Epoch: 185 cost time: 0.11252045631408691\n",
      "Epoch: 185, Steps: 1 | Train Loss: 798.9718628 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.078315292499078e-61\n",
      "Epoch: 186 cost time: 0.11182785034179688\n",
      "Epoch: 186, Steps: 1 | Train Loss: 800.0582886 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.039157646249539e-61\n",
      "Epoch: 187 cost time: 0.10982394218444824\n",
      "Epoch: 187, Steps: 1 | Train Loss: 800.4804077 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.0195788231247695e-61\n",
      "Epoch: 188 cost time: 0.11089110374450684\n",
      "Epoch: 188, Steps: 1 | Train Loss: 800.3701172 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.097894115623848e-62\n",
      "Epoch: 189 cost time: 0.10991692543029785\n",
      "Epoch: 189, Steps: 1 | Train Loss: 802.0231323 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.548947057811924e-62\n",
      "Epoch: 190 cost time: 0.11059856414794922\n",
      "Epoch: 190, Steps: 1 | Train Loss: 800.5472412 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.274473528905962e-62\n",
      "Epoch: 191 cost time: 0.11220908164978027\n",
      "Epoch: 191, Steps: 1 | Train Loss: 801.6866455 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.37236764452981e-63\n",
      "Epoch: 192 cost time: 0.10970616340637207\n",
      "Epoch: 192, Steps: 1 | Train Loss: 800.9721069 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.186183822264905e-63\n",
      "Epoch: 193 cost time: 0.11048555374145508\n",
      "Epoch: 193, Steps: 1 | Train Loss: 800.9125977 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5930919111324524e-63\n",
      "Epoch: 194 cost time: 0.11089062690734863\n",
      "Epoch: 194, Steps: 1 | Train Loss: 801.0755005 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.965459555662262e-64\n",
      "Epoch: 195 cost time: 0.12522006034851074\n",
      "Epoch: 195, Steps: 1 | Train Loss: 800.7175293 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.982729777831131e-64\n",
      "Epoch: 196 cost time: 0.11472582817077637\n",
      "Epoch: 196, Steps: 1 | Train Loss: 800.3363037 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.9913648889155655e-64\n",
      "Epoch: 197 cost time: 0.11290121078491211\n",
      "Epoch: 197, Steps: 1 | Train Loss: 799.6807861 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.956824444577828e-65\n",
      "Epoch: 198 cost time: 0.1119532585144043\n",
      "Epoch: 198, Steps: 1 | Train Loss: 799.5347900 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.978412222288914e-65\n",
      "Epoch: 199 cost time: 0.11121511459350586\n",
      "Epoch: 199, Steps: 1 | Train Loss: 800.6940308 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.489206111144457e-65\n",
      "Epoch: 200 cost time: 0.11107540130615234\n",
      "Epoch: 200, Steps: 1 | Train Loss: 801.0386963 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2446030555722284e-65\n",
      "Epoch: 201 cost time: 0.10985302925109863\n",
      "Epoch: 201, Steps: 1 | Train Loss: 800.3058472 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.223015277861142e-66\n",
      "Epoch: 202 cost time: 0.11118507385253906\n",
      "Epoch: 202, Steps: 1 | Train Loss: 798.9076538 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.111507638930571e-66\n",
      "Epoch: 203 cost time: 0.11103606224060059\n",
      "Epoch: 203, Steps: 1 | Train Loss: 801.7612915 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5557538194652856e-66\n",
      "Epoch: 204 cost time: 0.11012792587280273\n",
      "Epoch: 204, Steps: 1 | Train Loss: 800.6372070 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.778769097326428e-67\n",
      "Epoch: 205 cost time: 0.11122417449951172\n",
      "Epoch: 205, Steps: 1 | Train Loss: 800.2125244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.889384548663214e-67\n",
      "Epoch: 206 cost time: 0.12437081336975098\n",
      "Epoch: 206, Steps: 1 | Train Loss: 801.3134766 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.944692274331607e-67\n",
      "Epoch: 207 cost time: 0.11790752410888672\n",
      "Epoch: 207, Steps: 1 | Train Loss: 800.1608887 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.723461371658035e-68\n",
      "Epoch: 208 cost time: 0.1125173568725586\n",
      "Epoch: 208, Steps: 1 | Train Loss: 801.1563721 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.861730685829017e-68\n",
      "Epoch: 209 cost time: 0.11101078987121582\n",
      "Epoch: 209, Steps: 1 | Train Loss: 800.3739624 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.4308653429145087e-68\n",
      "Epoch: 210 cost time: 0.11112189292907715\n",
      "Epoch: 210, Steps: 1 | Train Loss: 802.5190430 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.2154326714572543e-68\n",
      "Epoch: 211 cost time: 0.11096882820129395\n",
      "Epoch: 211, Steps: 1 | Train Loss: 801.6430664 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.077163357286272e-69\n",
      "Epoch: 212 cost time: 0.11079192161560059\n",
      "Epoch: 212, Steps: 1 | Train Loss: 800.8370361 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.038581678643136e-69\n",
      "Epoch: 213 cost time: 0.11122345924377441\n",
      "Epoch: 213, Steps: 1 | Train Loss: 800.9560547 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.519290839321568e-69\n",
      "Epoch: 214 cost time: 0.11046957969665527\n",
      "Epoch: 214, Steps: 1 | Train Loss: 798.5307007 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.59645419660784e-70\n",
      "Epoch: 215 cost time: 0.11060476303100586\n",
      "Epoch: 215, Steps: 1 | Train Loss: 799.0828247 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.79822709830392e-70\n",
      "Epoch: 216 cost time: 0.11067986488342285\n",
      "Epoch: 216, Steps: 1 | Train Loss: 800.1925049 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.89911354915196e-70\n",
      "Epoch: 217 cost time: 0.1107175350189209\n",
      "Epoch: 217, Steps: 1 | Train Loss: 801.9142456 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.4955677457598e-71\n",
      "Epoch: 218 cost time: 0.12386751174926758\n",
      "Epoch: 218, Steps: 1 | Train Loss: 800.9031372 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.7477838728799e-71\n",
      "Epoch: 219 cost time: 0.1251232624053955\n",
      "Epoch: 219, Steps: 1 | Train Loss: 799.6812744 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.37389193643995e-71\n",
      "Epoch: 220 cost time: 0.12317419052124023\n",
      "Epoch: 220, Steps: 1 | Train Loss: 800.6546021 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.186945968219975e-71\n",
      "Epoch: 221 cost time: 0.11851239204406738\n",
      "Epoch: 221, Steps: 1 | Train Loss: 801.1848145 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.934729841099875e-72\n",
      "Epoch: 222 cost time: 0.11342906951904297\n",
      "Epoch: 222, Steps: 1 | Train Loss: 800.0265503 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.9673649205499374e-72\n",
      "Epoch: 223 cost time: 0.11090993881225586\n",
      "Epoch: 223, Steps: 1 | Train Loss: 801.6527100 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.4836824602749687e-72\n",
      "Epoch: 224 cost time: 0.11144852638244629\n",
      "Epoch: 224, Steps: 1 | Train Loss: 799.3108521 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.418412301374843e-73\n",
      "Epoch: 225 cost time: 0.11152100563049316\n",
      "Epoch: 225, Steps: 1 | Train Loss: 801.1702881 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.709206150687422e-73\n",
      "Epoch: 226 cost time: 0.11005783081054688\n",
      "Epoch: 226, Steps: 1 | Train Loss: 799.2702637 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.854603075343711e-73\n",
      "Epoch: 227 cost time: 0.1108405590057373\n",
      "Epoch: 227, Steps: 1 | Train Loss: 800.0444946 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.273015376718554e-74\n",
      "Epoch: 228 cost time: 0.11061358451843262\n",
      "Epoch: 228, Steps: 1 | Train Loss: 800.3060913 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.636507688359277e-74\n",
      "Epoch: 229 cost time: 0.10985279083251953\n",
      "Epoch: 229, Steps: 1 | Train Loss: 800.7858276 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.3182538441796386e-74\n",
      "Epoch: 230 cost time: 0.11013603210449219\n",
      "Epoch: 230, Steps: 1 | Train Loss: 799.7772827 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1591269220898193e-74\n",
      "Epoch: 231 cost time: 0.12358522415161133\n",
      "Epoch: 231, Steps: 1 | Train Loss: 798.6378784 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.795634610449096e-75\n",
      "Epoch: 232 cost time: 0.11694931983947754\n",
      "Epoch: 232, Steps: 1 | Train Loss: 799.4208984 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.897817305224548e-75\n",
      "Epoch: 233 cost time: 0.11302638053894043\n",
      "Epoch: 233, Steps: 1 | Train Loss: 799.8630371 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.448908652612274e-75\n",
      "Epoch: 234 cost time: 0.11183714866638184\n",
      "Epoch: 234, Steps: 1 | Train Loss: 800.6511841 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.24454326306137e-76\n",
      "Epoch: 235 cost time: 0.11086273193359375\n",
      "Epoch: 235, Steps: 1 | Train Loss: 799.4915161 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.622271631530685e-76\n",
      "Epoch: 236 cost time: 0.11083865165710449\n",
      "Epoch: 236, Steps: 1 | Train Loss: 800.2284546 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.8111358157653426e-76\n",
      "Epoch: 237 cost time: 0.11078119277954102\n",
      "Epoch: 237, Steps: 1 | Train Loss: 800.8226318 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.055679078826713e-77\n",
      "Epoch: 238 cost time: 0.11135435104370117\n",
      "Epoch: 238, Steps: 1 | Train Loss: 800.2750244 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.5278395394133566e-77\n",
      "Epoch: 239 cost time: 0.11050868034362793\n",
      "Epoch: 239, Steps: 1 | Train Loss: 799.9172974 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.2639197697066783e-77\n",
      "Epoch: 240 cost time: 0.11107659339904785\n",
      "Epoch: 240, Steps: 1 | Train Loss: 801.1591187 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1319598848533391e-77\n",
      "Epoch: 241 cost time: 0.11060452461242676\n",
      "Epoch: 241, Steps: 1 | Train Loss: 800.6825562 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.659799424266696e-78\n",
      "Epoch: 242 cost time: 0.11057662963867188\n",
      "Epoch: 242, Steps: 1 | Train Loss: 799.5479126 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.829899712133348e-78\n",
      "Epoch: 243 cost time: 0.11089658737182617\n",
      "Epoch: 243, Steps: 1 | Train Loss: 800.9588623 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.414949856066674e-78\n",
      "Epoch: 244 cost time: 0.12397336959838867\n",
      "Epoch: 244, Steps: 1 | Train Loss: 800.9882202 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.07474928033337e-79\n",
      "Epoch: 245 cost time: 0.12497138977050781\n",
      "Epoch: 245, Steps: 1 | Train Loss: 800.7432861 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.537374640166685e-79\n",
      "Epoch: 246 cost time: 0.12339043617248535\n",
      "Epoch: 246, Steps: 1 | Train Loss: 800.7125854 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.7686873200833424e-79\n",
      "Epoch: 247 cost time: 0.11935782432556152\n",
      "Epoch: 247, Steps: 1 | Train Loss: 801.6278687 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 8.843436600416712e-80\n",
      "Epoch: 248 cost time: 0.11346936225891113\n",
      "Epoch: 248, Steps: 1 | Train Loss: 799.2736206 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.421718300208356e-80\n",
      "Epoch: 249 cost time: 0.1126108169555664\n",
      "Epoch: 249, Steps: 1 | Train Loss: 799.8259277 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.210859150104178e-80\n",
      "Epoch: 250 cost time: 0.11203360557556152\n",
      "Epoch: 250, Steps: 1 | Train Loss: 800.2684937 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.105429575052089e-80\n",
      "21\n",
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl96_ll48_pl1_dm512_nh8_el4_dl3_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_7>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.11424469947814941\n",
      "Epoch: 1, Steps: 1 | Train Loss: 814.3185425 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (inf --> nan).  Saving model ...\n",
      "Updating learning rate to 1e-05\n",
      "Epoch: 2 cost time: 0.11040329933166504\n",
      "Epoch: 2, Steps: 1 | Train Loss: 808.9636841 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5e-06\n",
      "Epoch: 3 cost time: 0.11050295829772949\n",
      "Epoch: 3, Steps: 1 | Train Loss: 800.8286133 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.5e-06\n",
      "Epoch: 4 cost time: 0.11028265953063965\n",
      "Epoch: 4, Steps: 1 | Train Loss: 800.6741943 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.25e-06\n",
      "Epoch: 5 cost time: 0.11060953140258789\n",
      "Epoch: 5, Steps: 1 | Train Loss: 796.5975952 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.25e-07\n",
      "Epoch: 6 cost time: 0.11365342140197754\n",
      "Epoch: 6, Steps: 1 | Train Loss: 796.8032227 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.125e-07\n",
      "Epoch: 7 cost time: 0.11205220222473145\n",
      "Epoch: 7, Steps: 1 | Train Loss: 797.1808472 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.5625e-07\n",
      "Epoch: 8 cost time: 0.1108391284942627\n",
      "Epoch: 8, Steps: 1 | Train Loss: 795.5149536 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.8125e-08\n",
      "Epoch: 9 cost time: 0.11067461967468262\n",
      "Epoch: 9, Steps: 1 | Train Loss: 797.2982178 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.90625e-08\n",
      "Epoch: 10 cost time: 0.11107206344604492\n",
      "Epoch: 10, Steps: 1 | Train Loss: 795.0219727 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.953125e-08\n",
      "Epoch: 11 cost time: 0.11077046394348145\n",
      "Epoch: 11, Steps: 1 | Train Loss: 797.2382202 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.765625e-09\n",
      "Epoch: 12 cost time: 0.10973453521728516\n",
      "Epoch: 12, Steps: 1 | Train Loss: 795.3052979 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.8828125e-09\n",
      "Epoch: 13 cost time: 0.11022663116455078\n",
      "Epoch: 13, Steps: 1 | Train Loss: 797.1383667 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.44140625e-09\n",
      "Epoch: 14 cost time: 0.11117362976074219\n",
      "Epoch: 14, Steps: 1 | Train Loss: 795.5483398 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.220703125e-09\n",
      "Epoch: 15 cost time: 0.11031079292297363\n",
      "Epoch: 15, Steps: 1 | Train Loss: 795.9879761 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 6.103515625e-10\n",
      "Epoch: 16 cost time: 0.11174225807189941\n",
      "Epoch: 16, Steps: 1 | Train Loss: 796.8977051 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.0517578125e-10\n",
      "Epoch: 17 cost time: 0.11034226417541504\n",
      "Epoch: 17, Steps: 1 | Train Loss: 796.4154663 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.52587890625e-10\n",
      "Epoch: 18 cost time: 0.11440348625183105\n",
      "Epoch: 18, Steps: 1 | Train Loss: 795.6412964 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 7.62939453125e-11\n",
      "Epoch: 19 cost time: 0.11208248138427734\n",
      "Epoch: 19, Steps: 1 | Train Loss: 796.8501587 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 3.814697265625e-11\n",
      "Epoch: 20 cost time: 0.1110377311706543\n",
      "Epoch: 20, Steps: 1 | Train Loss: 796.4577026 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating learning rate to 1.9073486328125e-11\n",
      "Epoch: 21 cost time: 0.11098170280456543\n",
      "Epoch: 21, Steps: 1 | Train Loss: 796.9623413 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 9.5367431640625e-12\n",
      "Epoch: 22 cost time: 0.11106181144714355\n",
      "Epoch: 22, Steps: 1 | Train Loss: 797.1640625 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 4.76837158203125e-12\n",
      "Epoch: 23 cost time: 0.11089038848876953\n",
      "Epoch: 23, Steps: 1 | Train Loss: 796.3163452 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 2.384185791015625e-12\n",
      "Epoch: 24 cost time: 0.11138129234313965\n",
      "Epoch: 24, Steps: 1 | Train Loss: 795.5793457 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 1.1920928955078126e-12\n",
      "Epoch: 25 cost time: 0.11130285263061523\n",
      "Epoch: 25, Steps: 1 | Train Loss: 796.7620239 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n",
      "Updating learning rate to 5.960464477539063e-13\n",
      "Epoch: 26 cost time: 0.10950589179992676\n",
      "Epoch: 26, Steps: 1 | Train Loss: 796.1966553 Vali Loss: nan Test Loss: nan\n",
      "Validation loss decreased (nan --> nan).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "exp.fit(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "839c11a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Event</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-12/2018-02-18</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19/2018-02-25</th>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26/2018-03-04</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05/2018-03-11</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Region                  AA                                                     \\\n",
       "Event                   01   02   03    04   05   06   07   08   09   10   11   \n",
       "2018-02-12/2018-02-18  4.0  2.0  3.0  10.0  2.0  1.0  1.0  1.0  1.0  0.0  1.0   \n",
       "2018-02-19/2018-02-25  4.0  2.0  3.0  10.0  2.0  1.0  1.0  1.0  0.0  0.0  2.0   \n",
       "2018-02-26/2018-03-04  3.0  1.0  3.0   9.0  1.0  1.0  1.0  1.0  0.0  0.0  2.0   \n",
       "2018-03-05/2018-03-11  3.0  1.0  3.0   9.0  2.0  1.0  1.0  1.0  0.0  1.0  1.0   \n",
       "\n",
       "Region                                                               AC       \\\n",
       "Event                   12   13   14   15   16   17   18   19   20   01   02   \n",
       "2018-02-12/2018-02-18  1.0  0.0  1.0  0.0  0.0  1.0  1.0  2.0  0.0  3.0  2.0   \n",
       "2018-02-19/2018-02-25  1.0  0.0  1.0  0.0  0.0  1.0  2.0  2.0  0.0  3.0  2.0   \n",
       "2018-02-26/2018-03-04  1.0  0.0  1.0  0.0  0.0  1.0  1.0  2.0  0.0  3.0  2.0   \n",
       "2018-03-05/2018-03-11  1.0  1.0  0.0  0.0  0.0  0.0  1.0  1.0  0.0  2.0  2.0   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   03   04   05   06   07   08   09   10   11   12   13   \n",
       "2018-02-12/2018-02-18  2.0  8.0  2.0  1.0  1.0  0.0  1.0  0.0  1.0  1.0  2.0   \n",
       "2018-02-19/2018-02-25  1.0  8.0  2.0  1.0  1.0  0.0  1.0  0.0  1.0  0.0  2.0   \n",
       "2018-02-26/2018-03-04  1.0  8.0  2.0  0.0  1.0  1.0  1.0  0.0  1.0  1.0  2.0   \n",
       "2018-03-05/2018-03-11  2.0  8.0  2.0  0.0  1.0  0.0  1.0  0.0  1.0  1.0  1.0   \n",
       "\n",
       "Region                                                    \n",
       "Event                   14   15   16   17   18   19   20  \n",
       "2018-02-12/2018-02-18  0.0  0.0  0.0  1.0  1.0  1.0  0.0  \n",
       "2018-02-19/2018-02-25  0.0  0.0  1.0  1.0  0.0  1.0  0.0  \n",
       "2018-02-26/2018-03-04  0.0  0.0  0.0  1.0  1.0  1.0  0.0  \n",
       "2018-03-05/2018-03-11  0.0  0.0  0.0  1.0  1.0  1.0  0.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aedb8d67",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AC</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Event</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-02-12/2018-02-18</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-19/2018-02-25</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-26/2018-03-04</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-03-05/2018-03-11</th>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Region                  AA                                                     \\\n",
       "Event                   01   02   03    04   05   06   07   08   09   10   11   \n",
       "2018-02-12/2018-02-18  9.0  5.0  7.0  18.0  5.0  2.0  1.0  2.0  2.0  1.0  3.0   \n",
       "2018-02-19/2018-02-25  9.0  5.0  7.0  18.0  6.0  2.0  1.0  2.0  2.0  1.0  3.0   \n",
       "2018-02-26/2018-03-04  9.0  5.0  7.0  18.0  5.0  2.0  1.0  2.0  2.0  1.0  3.0   \n",
       "2018-03-05/2018-03-11  9.0  5.0  7.0  18.0  6.0  2.0  2.0  2.0  2.0  1.0  3.0   \n",
       "\n",
       "Region                                                                AC  \\\n",
       "Event                   12   13   14   15   16   17   18   19   20    01   \n",
       "2018-02-12/2018-02-18  2.0  1.0  0.0  0.0  0.0  3.0  1.0  2.0  0.0  14.0   \n",
       "2018-02-19/2018-02-25  1.0  1.0  0.0  0.0  0.0  3.0  1.0  2.0  0.0  14.0   \n",
       "2018-02-26/2018-03-04  2.0  1.0  0.0  0.0  0.0  3.0  1.0  2.0  0.0  14.0   \n",
       "2018-03-05/2018-03-11  1.0  1.0  0.0  0.0  0.0  3.0  1.0  2.0  0.0  14.0   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                    02   03    04    05   06   07   08   09   10   11   \n",
       "2018-02-12/2018-02-18  10.0  9.0  21.0  11.0  4.0  5.0  5.0  3.0  3.0  8.0   \n",
       "2018-02-19/2018-02-25  10.0  9.0  21.0  11.0  4.0  5.0  5.0  3.0  3.0  8.0   \n",
       "2018-02-26/2018-03-04  10.0  9.0  21.0  11.0  4.0  5.0  5.0  3.0  3.0  8.0   \n",
       "2018-03-05/2018-03-11  10.0  9.0  21.0  11.0  4.0  5.0  5.0  3.0  3.0  8.0   \n",
       "\n",
       "Region                                                              \n",
       "Event                   12   13   14   15   16   17   18   19   20  \n",
       "2018-02-12/2018-02-18  4.0  3.0  1.0  0.0  1.0  7.0  1.0  5.0  0.0  \n",
       "2018-02-19/2018-02-25  4.0  3.0  1.0  0.0  1.0  7.0  1.0  5.0  0.0  \n",
       "2018-02-26/2018-03-04  4.0  3.0  1.0  0.0  1.0  7.0  1.0  5.0  0.0  \n",
       "2018-03-05/2018-03-11  4.0  3.0  1.0  0.0  1.0  7.0  1.0  5.0  0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32dfccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eede27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a48814",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "238552f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "912803ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "58e24b86",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadro RTX 8000\n",
      "Quadro RTX 8000\n",
      "Quadro RTX 5000\n",
      "Quadro RTX 5000\n",
      "Quadro RTX 5000\n",
      "Quadro RTX 5000\n",
      "Quadro RTX 5000\n",
      "Quadro RTX 5000\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(torch.cuda.get_device_name(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f71c20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee069e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59bc95b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = exp.model_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c98e6c9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d7ae40",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_mod = exp.model_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac9fc6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting=\"str\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2aefbd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x,y [[13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 7  6  7 49  6  0  1  2  2  2  6  0  1  0  0  0  2  0  4  0]] [[ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 7  6  7 49  6  0  1  2  2  2  6  0  1  0  0  0  2  0  4  0]]\n"
     ]
    }
   ],
   "source": [
    "pred =test_mod.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1be0554d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.490034  ,  4.462694  ,  5.8469057 , 13.973756  ,  4.458868  ,\n",
       "        1.6943204 ,  1.80356   ,  1.9661559 ,  1.8184396 ,  1.4654043 ,\n",
       "        2.014461  ,  1.276834  ,  1.0240163 ,  0.4990942 ,  0.05070882,\n",
       "        0.6254189 ,  1.1649    ,  0.27910122,  0.85286796, -0.02110814],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee2a897",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "437fdcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1=np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5b1f44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2=np.array([2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42f6e8c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 2, 3, 4])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate([a1,a2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be63afa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82ee22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a95c4858",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = atd2022.io.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "784bb4cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Region</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AF</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AJ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AL</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">AY</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BB</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BD</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BF</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BH</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BL</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BX</th>\n",
       "      <th colspan=\"20\" halign=\"left\">BY</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CB</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CD</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CF</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CH</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CJ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CW</th>\n",
       "      <th colspan=\"20\" halign=\"left\">CY</th>\n",
       "      <th colspan=\"20\" halign=\"left\">DA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">DJ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">DO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">DQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">DR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">EC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">EG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">EI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">EK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">EN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">ER</th>\n",
       "      <th colspan=\"20\" halign=\"left\">ES</th>\n",
       "      <th colspan=\"20\" halign=\"left\">ET</th>\n",
       "      <th colspan=\"20\" halign=\"left\">EU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">EZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FJ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">FR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GB</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GH</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GJ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GL</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GY</th>\n",
       "      <th colspan=\"20\" halign=\"left\">GZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">HA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">HK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">HM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">HO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">HQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">HR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">HU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">ID</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">IZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">JA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">JE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">JM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">JN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">JO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">JQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">KZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LH</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">LY</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MB</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MD</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MF</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MH</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MJ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">ML</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MX</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MY</th>\n",
       "      <th colspan=\"20\" halign=\"left\">MZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NF</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NH</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NL</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NR</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">NZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">OD</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PF</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PL</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">PU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">QA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">RW</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SB</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SF</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SH</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SL</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">ST</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SW</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SY</th>\n",
       "      <th colspan=\"20\" halign=\"left\">SZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TD</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TH</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TL</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TN</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TO</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TU</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TW</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TX</th>\n",
       "      <th colspan=\"20\" halign=\"left\">TZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">UG</th>\n",
       "      <th colspan=\"20\" halign=\"left\">UK</th>\n",
       "      <th colspan=\"20\" halign=\"left\">UP</th>\n",
       "      <th colspan=\"20\" halign=\"left\">US</th>\n",
       "      <th colspan=\"20\" halign=\"left\">UV</th>\n",
       "      <th colspan=\"20\" halign=\"left\">UY</th>\n",
       "      <th colspan=\"20\" halign=\"left\">UZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">VC</th>\n",
       "      <th colspan=\"20\" halign=\"left\">VE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">VI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">VM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">VQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">VT</th>\n",
       "      <th colspan=\"20\" halign=\"left\">WA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">WE</th>\n",
       "      <th colspan=\"20\" halign=\"left\">WF</th>\n",
       "      <th colspan=\"20\" halign=\"left\">WI</th>\n",
       "      <th colspan=\"20\" halign=\"left\">WQ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">WS</th>\n",
       "      <th colspan=\"20\" halign=\"left\">WZ</th>\n",
       "      <th colspan=\"20\" halign=\"left\">YM</th>\n",
       "      <th colspan=\"20\" halign=\"left\">ZA</th>\n",
       "      <th colspan=\"20\" halign=\"left\">ZI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Event</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>01</th>\n",
       "      <th>02</th>\n",
       "      <th>03</th>\n",
       "      <th>04</th>\n",
       "      <th>05</th>\n",
       "      <th>06</th>\n",
       "      <th>07</th>\n",
       "      <th>08</th>\n",
       "      <th>09</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2018-01-08/2018-01-14</th>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>85</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>421</td>\n",
       "      <td>204</td>\n",
       "      <td>340</td>\n",
       "      <td>975</td>\n",
       "      <td>494</td>\n",
       "      <td>87</td>\n",
       "      <td>111</td>\n",
       "      <td>82</td>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>158</td>\n",
       "      <td>67</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>169</td>\n",
       "      <td>20</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>964</td>\n",
       "      <td>357</td>\n",
       "      <td>419</td>\n",
       "      <td>1612</td>\n",
       "      <td>480</td>\n",
       "      <td>221</td>\n",
       "      <td>267</td>\n",
       "      <td>304</td>\n",
       "      <td>72</td>\n",
       "      <td>110</td>\n",
       "      <td>496</td>\n",
       "      <td>189</td>\n",
       "      <td>162</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>135</td>\n",
       "      <td>250</td>\n",
       "      <td>159</td>\n",
       "      <td>1245</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>73</td>\n",
       "      <td>17</td>\n",
       "      <td>195</td>\n",
       "      <td>56</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>161</td>\n",
       "      <td>70</td>\n",
       "      <td>130</td>\n",
       "      <td>366</td>\n",
       "      <td>139</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>70</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>113</td>\n",
       "      <td>128</td>\n",
       "      <td>106</td>\n",
       "      <td>237</td>\n",
       "      <td>79</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>64</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>273</td>\n",
       "      <td>65</td>\n",
       "      <td>114</td>\n",
       "      <td>448</td>\n",
       "      <td>219</td>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>54</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>73</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>48</td>\n",
       "      <td>18</td>\n",
       "      <td>129</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>68</td>\n",
       "      <td>134</td>\n",
       "      <td>228</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>163</td>\n",
       "      <td>62</td>\n",
       "      <td>162</td>\n",
       "      <td>295</td>\n",
       "      <td>81</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>54</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>83</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>4214</td>\n",
       "      <td>2183</td>\n",
       "      <td>1663</td>\n",
       "      <td>5506</td>\n",
       "      <td>2008</td>\n",
       "      <td>640</td>\n",
       "      <td>1057</td>\n",
       "      <td>848</td>\n",
       "      <td>797</td>\n",
       "      <td>295</td>\n",
       "      <td>1697</td>\n",
       "      <td>614</td>\n",
       "      <td>435</td>\n",
       "      <td>98</td>\n",
       "      <td>32</td>\n",
       "      <td>154</td>\n",
       "      <td>970</td>\n",
       "      <td>355</td>\n",
       "      <td>1109</td>\n",
       "      <td>0</td>\n",
       "      <td>326</td>\n",
       "      <td>148</td>\n",
       "      <td>138</td>\n",
       "      <td>479</td>\n",
       "      <td>220</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>51</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>93</td>\n",
       "      <td>39</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>72</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>76</td>\n",
       "      <td>165</td>\n",
       "      <td>407</td>\n",
       "      <td>146</td>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>86</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>65</td>\n",
       "      <td>30</td>\n",
       "      <td>140</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>62</td>\n",
       "      <td>19</td>\n",
       "      <td>223</td>\n",
       "      <td>44</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>222</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>108</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>521</td>\n",
       "      <td>246</td>\n",
       "      <td>458</td>\n",
       "      <td>1251</td>\n",
       "      <td>380</td>\n",
       "      <td>66</td>\n",
       "      <td>103</td>\n",
       "      <td>151</td>\n",
       "      <td>37</td>\n",
       "      <td>40</td>\n",
       "      <td>194</td>\n",
       "      <td>117</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>95</td>\n",
       "      <td>30</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>160</td>\n",
       "      <td>71</td>\n",
       "      <td>107</td>\n",
       "      <td>341</td>\n",
       "      <td>78</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>45</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>14</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>1092</td>\n",
       "      <td>453</td>\n",
       "      <td>433</td>\n",
       "      <td>1436</td>\n",
       "      <td>437</td>\n",
       "      <td>169</td>\n",
       "      <td>204</td>\n",
       "      <td>394</td>\n",
       "      <td>106</td>\n",
       "      <td>96</td>\n",
       "      <td>385</td>\n",
       "      <td>172</td>\n",
       "      <td>43</td>\n",
       "      <td>93</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>334</td>\n",
       "      <td>148</td>\n",
       "      <td>455</td>\n",
       "      <td>73</td>\n",
       "      <td>161</td>\n",
       "      <td>84</td>\n",
       "      <td>57</td>\n",
       "      <td>259</td>\n",
       "      <td>78</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>57</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>50</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>54</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>316</td>\n",
       "      <td>180</td>\n",
       "      <td>112</td>\n",
       "      <td>324</td>\n",
       "      <td>112</td>\n",
       "      <td>36</td>\n",
       "      <td>51</td>\n",
       "      <td>144</td>\n",
       "      <td>42</td>\n",
       "      <td>38</td>\n",
       "      <td>170</td>\n",
       "      <td>98</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>215</td>\n",
       "      <td>43</td>\n",
       "      <td>226</td>\n",
       "      <td>48</td>\n",
       "      <td>66</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>98</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>202</td>\n",
       "      <td>47</td>\n",
       "      <td>60</td>\n",
       "      <td>229</td>\n",
       "      <td>74</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>47</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>298</td>\n",
       "      <td>132</td>\n",
       "      <td>196</td>\n",
       "      <td>576</td>\n",
       "      <td>170</td>\n",
       "      <td>129</td>\n",
       "      <td>79</td>\n",
       "      <td>95</td>\n",
       "      <td>49</td>\n",
       "      <td>28</td>\n",
       "      <td>130</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>130</td>\n",
       "      <td>33</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>104</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>134</td>\n",
       "      <td>203</td>\n",
       "      <td>576</td>\n",
       "      <td>151</td>\n",
       "      <td>21</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>22</td>\n",
       "      <td>41</td>\n",
       "      <td>73</td>\n",
       "      <td>90</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>164</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>4924</td>\n",
       "      <td>2658</td>\n",
       "      <td>2144</td>\n",
       "      <td>6553</td>\n",
       "      <td>2182</td>\n",
       "      <td>809</td>\n",
       "      <td>1132</td>\n",
       "      <td>1467</td>\n",
       "      <td>772</td>\n",
       "      <td>331</td>\n",
       "      <td>1961</td>\n",
       "      <td>770</td>\n",
       "      <td>496</td>\n",
       "      <td>182</td>\n",
       "      <td>38</td>\n",
       "      <td>291</td>\n",
       "      <td>1418</td>\n",
       "      <td>370</td>\n",
       "      <td>1341</td>\n",
       "      <td>3</td>\n",
       "      <td>290</td>\n",
       "      <td>154</td>\n",
       "      <td>237</td>\n",
       "      <td>669</td>\n",
       "      <td>257</td>\n",
       "      <td>52</td>\n",
       "      <td>83</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>24</td>\n",
       "      <td>110</td>\n",
       "      <td>51</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>273</td>\n",
       "      <td>20</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>33</td>\n",
       "      <td>40</td>\n",
       "      <td>168</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>47</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>54</td>\n",
       "      <td>9</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>414</td>\n",
       "      <td>249</td>\n",
       "      <td>283</td>\n",
       "      <td>836</td>\n",
       "      <td>246</td>\n",
       "      <td>146</td>\n",
       "      <td>86</td>\n",
       "      <td>160</td>\n",
       "      <td>62</td>\n",
       "      <td>48</td>\n",
       "      <td>184</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>168</td>\n",
       "      <td>34</td>\n",
       "      <td>150</td>\n",
       "      <td>7</td>\n",
       "      <td>66</td>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>149</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>63</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>47</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>4342</td>\n",
       "      <td>1923</td>\n",
       "      <td>2432</td>\n",
       "      <td>7696</td>\n",
       "      <td>2973</td>\n",
       "      <td>1246</td>\n",
       "      <td>1192</td>\n",
       "      <td>1070</td>\n",
       "      <td>642</td>\n",
       "      <td>313</td>\n",
       "      <td>1865</td>\n",
       "      <td>621</td>\n",
       "      <td>468</td>\n",
       "      <td>186</td>\n",
       "      <td>148</td>\n",
       "      <td>260</td>\n",
       "      <td>1162</td>\n",
       "      <td>169</td>\n",
       "      <td>1039</td>\n",
       "      <td>7</td>\n",
       "      <td>384</td>\n",
       "      <td>128</td>\n",
       "      <td>372</td>\n",
       "      <td>893</td>\n",
       "      <td>179</td>\n",
       "      <td>50</td>\n",
       "      <td>35</td>\n",
       "      <td>71</td>\n",
       "      <td>60</td>\n",
       "      <td>20</td>\n",
       "      <td>202</td>\n",
       "      <td>38</td>\n",
       "      <td>34</td>\n",
       "      <td>46</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>108</td>\n",
       "      <td>42</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>68</td>\n",
       "      <td>53</td>\n",
       "      <td>231</td>\n",
       "      <td>66</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>95</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>107</td>\n",
       "      <td>33</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>95</td>\n",
       "      <td>231</td>\n",
       "      <td>641</td>\n",
       "      <td>227</td>\n",
       "      <td>50</td>\n",
       "      <td>55</td>\n",
       "      <td>52</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>146</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>95</td>\n",
       "      <td>45</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>284</td>\n",
       "      <td>68</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>341</td>\n",
       "      <td>143</td>\n",
       "      <td>145</td>\n",
       "      <td>773</td>\n",
       "      <td>229</td>\n",
       "      <td>51</td>\n",
       "      <td>81</td>\n",
       "      <td>93</td>\n",
       "      <td>102</td>\n",
       "      <td>55</td>\n",
       "      <td>213</td>\n",
       "      <td>70</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>49</td>\n",
       "      <td>94</td>\n",
       "      <td>29</td>\n",
       "      <td>137</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "      <td>238</td>\n",
       "      <td>67</td>\n",
       "      <td>23</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>86</td>\n",
       "      <td>91</td>\n",
       "      <td>386</td>\n",
       "      <td>109</td>\n",
       "      <td>60</td>\n",
       "      <td>69</td>\n",
       "      <td>58</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>24</td>\n",
       "      <td>43</td>\n",
       "      <td>175</td>\n",
       "      <td>55</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>36</td>\n",
       "      <td>205</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>238</td>\n",
       "      <td>178</td>\n",
       "      <td>135</td>\n",
       "      <td>332</td>\n",
       "      <td>89</td>\n",
       "      <td>25</td>\n",
       "      <td>130</td>\n",
       "      <td>56</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>110</td>\n",
       "      <td>110</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>125</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>734</td>\n",
       "      <td>376</td>\n",
       "      <td>727</td>\n",
       "      <td>1908</td>\n",
       "      <td>618</td>\n",
       "      <td>191</td>\n",
       "      <td>150</td>\n",
       "      <td>311</td>\n",
       "      <td>140</td>\n",
       "      <td>115</td>\n",
       "      <td>462</td>\n",
       "      <td>199</td>\n",
       "      <td>90</td>\n",
       "      <td>131</td>\n",
       "      <td>44</td>\n",
       "      <td>150</td>\n",
       "      <td>466</td>\n",
       "      <td>114</td>\n",
       "      <td>601</td>\n",
       "      <td>7</td>\n",
       "      <td>1509</td>\n",
       "      <td>766</td>\n",
       "      <td>689</td>\n",
       "      <td>2174</td>\n",
       "      <td>759</td>\n",
       "      <td>172</td>\n",
       "      <td>255</td>\n",
       "      <td>253</td>\n",
       "      <td>206</td>\n",
       "      <td>102</td>\n",
       "      <td>437</td>\n",
       "      <td>247</td>\n",
       "      <td>121</td>\n",
       "      <td>47</td>\n",
       "      <td>18</td>\n",
       "      <td>41</td>\n",
       "      <td>412</td>\n",
       "      <td>98</td>\n",
       "      <td>391</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>51</td>\n",
       "      <td>81</td>\n",
       "      <td>360</td>\n",
       "      <td>136</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>230</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>601</td>\n",
       "      <td>304</td>\n",
       "      <td>227</td>\n",
       "      <td>972</td>\n",
       "      <td>228</td>\n",
       "      <td>51</td>\n",
       "      <td>117</td>\n",
       "      <td>160</td>\n",
       "      <td>44</td>\n",
       "      <td>63</td>\n",
       "      <td>311</td>\n",
       "      <td>113</td>\n",
       "      <td>94</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>69</td>\n",
       "      <td>138</td>\n",
       "      <td>54</td>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "      <td>327</td>\n",
       "      <td>120</td>\n",
       "      <td>184</td>\n",
       "      <td>656</td>\n",
       "      <td>191</td>\n",
       "      <td>50</td>\n",
       "      <td>69</td>\n",
       "      <td>129</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>179</td>\n",
       "      <td>46</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>266</td>\n",
       "      <td>26</td>\n",
       "      <td>77</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>238</td>\n",
       "      <td>108</td>\n",
       "      <td>120</td>\n",
       "      <td>341</td>\n",
       "      <td>146</td>\n",
       "      <td>63</td>\n",
       "      <td>35</td>\n",
       "      <td>59</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>128</td>\n",
       "      <td>72</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>119</td>\n",
       "      <td>64</td>\n",
       "      <td>78</td>\n",
       "      <td>274</td>\n",
       "      <td>77</td>\n",
       "      <td>42</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>61</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>233</td>\n",
       "      <td>124</td>\n",
       "      <td>113</td>\n",
       "      <td>417</td>\n",
       "      <td>202</td>\n",
       "      <td>43</td>\n",
       "      <td>62</td>\n",
       "      <td>64</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>60</td>\n",
       "      <td>42</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2094</td>\n",
       "      <td>1027</td>\n",
       "      <td>1157</td>\n",
       "      <td>3999</td>\n",
       "      <td>1473</td>\n",
       "      <td>345</td>\n",
       "      <td>396</td>\n",
       "      <td>558</td>\n",
       "      <td>328</td>\n",
       "      <td>168</td>\n",
       "      <td>1023</td>\n",
       "      <td>292</td>\n",
       "      <td>218</td>\n",
       "      <td>135</td>\n",
       "      <td>20</td>\n",
       "      <td>152</td>\n",
       "      <td>572</td>\n",
       "      <td>204</td>\n",
       "      <td>664</td>\n",
       "      <td>15</td>\n",
       "      <td>125</td>\n",
       "      <td>90</td>\n",
       "      <td>70</td>\n",
       "      <td>152</td>\n",
       "      <td>79</td>\n",
       "      <td>29</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>52</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>98</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>1213</td>\n",
       "      <td>627</td>\n",
       "      <td>334</td>\n",
       "      <td>1249</td>\n",
       "      <td>670</td>\n",
       "      <td>167</td>\n",
       "      <td>236</td>\n",
       "      <td>158</td>\n",
       "      <td>125</td>\n",
       "      <td>92</td>\n",
       "      <td>583</td>\n",
       "      <td>161</td>\n",
       "      <td>145</td>\n",
       "      <td>26</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>293</td>\n",
       "      <td>61</td>\n",
       "      <td>193</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>44</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1711</td>\n",
       "      <td>844</td>\n",
       "      <td>1128</td>\n",
       "      <td>3058</td>\n",
       "      <td>1288</td>\n",
       "      <td>339</td>\n",
       "      <td>330</td>\n",
       "      <td>447</td>\n",
       "      <td>257</td>\n",
       "      <td>137</td>\n",
       "      <td>708</td>\n",
       "      <td>312</td>\n",
       "      <td>170</td>\n",
       "      <td>101</td>\n",
       "      <td>20</td>\n",
       "      <td>124</td>\n",
       "      <td>492</td>\n",
       "      <td>126</td>\n",
       "      <td>640</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>78</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>94</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>481</td>\n",
       "      <td>284</td>\n",
       "      <td>307</td>\n",
       "      <td>845</td>\n",
       "      <td>263</td>\n",
       "      <td>63</td>\n",
       "      <td>127</td>\n",
       "      <td>143</td>\n",
       "      <td>67</td>\n",
       "      <td>73</td>\n",
       "      <td>273</td>\n",
       "      <td>115</td>\n",
       "      <td>61</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>63</td>\n",
       "      <td>144</td>\n",
       "      <td>36</td>\n",
       "      <td>134</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>174</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>92</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>82</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>164</td>\n",
       "      <td>84</td>\n",
       "      <td>60</td>\n",
       "      <td>206</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>149</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "      <td>87</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1266</td>\n",
       "      <td>646</td>\n",
       "      <td>411</td>\n",
       "      <td>1532</td>\n",
       "      <td>608</td>\n",
       "      <td>86</td>\n",
       "      <td>260</td>\n",
       "      <td>175</td>\n",
       "      <td>107</td>\n",
       "      <td>116</td>\n",
       "      <td>942</td>\n",
       "      <td>281</td>\n",
       "      <td>79</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>188</td>\n",
       "      <td>66</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>227</td>\n",
       "      <td>112</td>\n",
       "      <td>102</td>\n",
       "      <td>401</td>\n",
       "      <td>92</td>\n",
       "      <td>64</td>\n",
       "      <td>56</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>17</td>\n",
       "      <td>129</td>\n",
       "      <td>44</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>84</td>\n",
       "      <td>41</td>\n",
       "      <td>207</td>\n",
       "      <td>72</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>72</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>23</td>\n",
       "      <td>75</td>\n",
       "      <td>116</td>\n",
       "      <td>50</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>237</td>\n",
       "      <td>82</td>\n",
       "      <td>104</td>\n",
       "      <td>367</td>\n",
       "      <td>101</td>\n",
       "      <td>24</td>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>122</td>\n",
       "      <td>81</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>43</td>\n",
       "      <td>79</td>\n",
       "      <td>186</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>488</td>\n",
       "      <td>276</td>\n",
       "      <td>279</td>\n",
       "      <td>962</td>\n",
       "      <td>255</td>\n",
       "      <td>123</td>\n",
       "      <td>132</td>\n",
       "      <td>106</td>\n",
       "      <td>69</td>\n",
       "      <td>28</td>\n",
       "      <td>170</td>\n",
       "      <td>88</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>313</td>\n",
       "      <td>26</td>\n",
       "      <td>111</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7118</td>\n",
       "      <td>3774</td>\n",
       "      <td>4398</td>\n",
       "      <td>12657</td>\n",
       "      <td>3267</td>\n",
       "      <td>1034</td>\n",
       "      <td>1295</td>\n",
       "      <td>1918</td>\n",
       "      <td>1305</td>\n",
       "      <td>952</td>\n",
       "      <td>3944</td>\n",
       "      <td>1541</td>\n",
       "      <td>638</td>\n",
       "      <td>789</td>\n",
       "      <td>163</td>\n",
       "      <td>335</td>\n",
       "      <td>2831</td>\n",
       "      <td>857</td>\n",
       "      <td>2742</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2593</td>\n",
       "      <td>1251</td>\n",
       "      <td>1164</td>\n",
       "      <td>3384</td>\n",
       "      <td>2417</td>\n",
       "      <td>380</td>\n",
       "      <td>408</td>\n",
       "      <td>1079</td>\n",
       "      <td>320</td>\n",
       "      <td>359</td>\n",
       "      <td>1889</td>\n",
       "      <td>620</td>\n",
       "      <td>726</td>\n",
       "      <td>661</td>\n",
       "      <td>47</td>\n",
       "      <td>603</td>\n",
       "      <td>1131</td>\n",
       "      <td>269</td>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "      <td>3214</td>\n",
       "      <td>1688</td>\n",
       "      <td>1941</td>\n",
       "      <td>5680</td>\n",
       "      <td>2050</td>\n",
       "      <td>397</td>\n",
       "      <td>770</td>\n",
       "      <td>762</td>\n",
       "      <td>513</td>\n",
       "      <td>359</td>\n",
       "      <td>2243</td>\n",
       "      <td>938</td>\n",
       "      <td>514</td>\n",
       "      <td>431</td>\n",
       "      <td>61</td>\n",
       "      <td>537</td>\n",
       "      <td>1340</td>\n",
       "      <td>435</td>\n",
       "      <td>3191</td>\n",
       "      <td>43</td>\n",
       "      <td>1037</td>\n",
       "      <td>498</td>\n",
       "      <td>613</td>\n",
       "      <td>2153</td>\n",
       "      <td>614</td>\n",
       "      <td>158</td>\n",
       "      <td>282</td>\n",
       "      <td>268</td>\n",
       "      <td>121</td>\n",
       "      <td>75</td>\n",
       "      <td>401</td>\n",
       "      <td>160</td>\n",
       "      <td>53</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>40</td>\n",
       "      <td>368</td>\n",
       "      <td>99</td>\n",
       "      <td>306</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>754</td>\n",
       "      <td>363</td>\n",
       "      <td>235</td>\n",
       "      <td>1423</td>\n",
       "      <td>416</td>\n",
       "      <td>181</td>\n",
       "      <td>187</td>\n",
       "      <td>246</td>\n",
       "      <td>77</td>\n",
       "      <td>90</td>\n",
       "      <td>325</td>\n",
       "      <td>119</td>\n",
       "      <td>113</td>\n",
       "      <td>53</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>252</td>\n",
       "      <td>139</td>\n",
       "      <td>725</td>\n",
       "      <td>7</td>\n",
       "      <td>1586</td>\n",
       "      <td>631</td>\n",
       "      <td>996</td>\n",
       "      <td>3231</td>\n",
       "      <td>1024</td>\n",
       "      <td>369</td>\n",
       "      <td>456</td>\n",
       "      <td>427</td>\n",
       "      <td>215</td>\n",
       "      <td>137</td>\n",
       "      <td>630</td>\n",
       "      <td>223</td>\n",
       "      <td>98</td>\n",
       "      <td>68</td>\n",
       "      <td>60</td>\n",
       "      <td>62</td>\n",
       "      <td>496</td>\n",
       "      <td>108</td>\n",
       "      <td>407</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>137</td>\n",
       "      <td>87</td>\n",
       "      <td>280</td>\n",
       "      <td>113</td>\n",
       "      <td>18</td>\n",
       "      <td>59</td>\n",
       "      <td>59</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>93</td>\n",
       "      <td>52</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>123</td>\n",
       "      <td>29</td>\n",
       "      <td>124</td>\n",
       "      <td>0</td>\n",
       "      <td>256</td>\n",
       "      <td>88</td>\n",
       "      <td>77</td>\n",
       "      <td>798</td>\n",
       "      <td>122</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "      <td>61</td>\n",
       "      <td>24</td>\n",
       "      <td>20</td>\n",
       "      <td>75</td>\n",
       "      <td>46</td>\n",
       "      <td>139</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>73</td>\n",
       "      <td>41</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>323</td>\n",
       "      <td>129</td>\n",
       "      <td>298</td>\n",
       "      <td>788</td>\n",
       "      <td>193</td>\n",
       "      <td>46</td>\n",
       "      <td>57</td>\n",
       "      <td>82</td>\n",
       "      <td>44</td>\n",
       "      <td>30</td>\n",
       "      <td>132</td>\n",
       "      <td>34</td>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>124</td>\n",
       "      <td>16</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1061</td>\n",
       "      <td>478</td>\n",
       "      <td>534</td>\n",
       "      <td>1720</td>\n",
       "      <td>458</td>\n",
       "      <td>178</td>\n",
       "      <td>229</td>\n",
       "      <td>202</td>\n",
       "      <td>92</td>\n",
       "      <td>107</td>\n",
       "      <td>645</td>\n",
       "      <td>200</td>\n",
       "      <td>98</td>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>85</td>\n",
       "      <td>269</td>\n",
       "      <td>84</td>\n",
       "      <td>445</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>32</td>\n",
       "      <td>34</td>\n",
       "      <td>162</td>\n",
       "      <td>46</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1243</td>\n",
       "      <td>561</td>\n",
       "      <td>711</td>\n",
       "      <td>2255</td>\n",
       "      <td>767</td>\n",
       "      <td>193</td>\n",
       "      <td>107</td>\n",
       "      <td>202</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>480</td>\n",
       "      <td>173</td>\n",
       "      <td>417</td>\n",
       "      <td>41</td>\n",
       "      <td>91</td>\n",
       "      <td>185</td>\n",
       "      <td>135</td>\n",
       "      <td>56</td>\n",
       "      <td>333</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1094</td>\n",
       "      <td>502</td>\n",
       "      <td>911</td>\n",
       "      <td>2452</td>\n",
       "      <td>755</td>\n",
       "      <td>186</td>\n",
       "      <td>230</td>\n",
       "      <td>203</td>\n",
       "      <td>146</td>\n",
       "      <td>102</td>\n",
       "      <td>344</td>\n",
       "      <td>156</td>\n",
       "      <td>150</td>\n",
       "      <td>35</td>\n",
       "      <td>85</td>\n",
       "      <td>86</td>\n",
       "      <td>277</td>\n",
       "      <td>40</td>\n",
       "      <td>240</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>82</td>\n",
       "      <td>87</td>\n",
       "      <td>321</td>\n",
       "      <td>112</td>\n",
       "      <td>25</td>\n",
       "      <td>31</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>59</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>77</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>52</td>\n",
       "      <td>59</td>\n",
       "      <td>73</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "      <td>45</td>\n",
       "      <td>115</td>\n",
       "      <td>394</td>\n",
       "      <td>131</td>\n",
       "      <td>37</td>\n",
       "      <td>51</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>22</td>\n",
       "      <td>54</td>\n",
       "      <td>230</td>\n",
       "      <td>68</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>143</td>\n",
       "      <td>128</td>\n",
       "      <td>766</td>\n",
       "      <td>203</td>\n",
       "      <td>71</td>\n",
       "      <td>103</td>\n",
       "      <td>110</td>\n",
       "      <td>58</td>\n",
       "      <td>43</td>\n",
       "      <td>161</td>\n",
       "      <td>69</td>\n",
       "      <td>33</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "      <td>137</td>\n",
       "      <td>71</td>\n",
       "      <td>253</td>\n",
       "      <td>3</td>\n",
       "      <td>123</td>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "      <td>160</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>61</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>43</td>\n",
       "      <td>124</td>\n",
       "      <td>305</td>\n",
       "      <td>125</td>\n",
       "      <td>29</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>78</td>\n",
       "      <td>92</td>\n",
       "      <td>209</td>\n",
       "      <td>195</td>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>107</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>91</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>71</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>35</td>\n",
       "      <td>45</td>\n",
       "      <td>101</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>101</td>\n",
       "      <td>118</td>\n",
       "      <td>767</td>\n",
       "      <td>193</td>\n",
       "      <td>61</td>\n",
       "      <td>146</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>122</td>\n",
       "      <td>77</td>\n",
       "      <td>54</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>180</td>\n",
       "      <td>38</td>\n",
       "      <td>118</td>\n",
       "      <td>2</td>\n",
       "      <td>70</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>91</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>64</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>127</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>277</td>\n",
       "      <td>101</td>\n",
       "      <td>39</td>\n",
       "      <td>269</td>\n",
       "      <td>158</td>\n",
       "      <td>20</td>\n",
       "      <td>67</td>\n",
       "      <td>55</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>116</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>139</td>\n",
       "      <td>29</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>38</td>\n",
       "      <td>87</td>\n",
       "      <td>147</td>\n",
       "      <td>48</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>84</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>64</td>\n",
       "      <td>99</td>\n",
       "      <td>431</td>\n",
       "      <td>157</td>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "      <td>44</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>53</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>71</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>222</td>\n",
       "      <td>112</td>\n",
       "      <td>81</td>\n",
       "      <td>360</td>\n",
       "      <td>112</td>\n",
       "      <td>32</td>\n",
       "      <td>39</td>\n",
       "      <td>36</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>77</td>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>47</td>\n",
       "      <td>94</td>\n",
       "      <td>274</td>\n",
       "      <td>170</td>\n",
       "      <td>16</td>\n",
       "      <td>39</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>66</td>\n",
       "      <td>40</td>\n",
       "      <td>131</td>\n",
       "      <td>221</td>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1111</td>\n",
       "      <td>529</td>\n",
       "      <td>618</td>\n",
       "      <td>2071</td>\n",
       "      <td>516</td>\n",
       "      <td>245</td>\n",
       "      <td>296</td>\n",
       "      <td>366</td>\n",
       "      <td>144</td>\n",
       "      <td>98</td>\n",
       "      <td>467</td>\n",
       "      <td>206</td>\n",
       "      <td>271</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>152</td>\n",
       "      <td>493</td>\n",
       "      <td>114</td>\n",
       "      <td>634</td>\n",
       "      <td>3</td>\n",
       "      <td>1084</td>\n",
       "      <td>486</td>\n",
       "      <td>650</td>\n",
       "      <td>1608</td>\n",
       "      <td>639</td>\n",
       "      <td>245</td>\n",
       "      <td>178</td>\n",
       "      <td>324</td>\n",
       "      <td>235</td>\n",
       "      <td>127</td>\n",
       "      <td>477</td>\n",
       "      <td>222</td>\n",
       "      <td>78</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>98</td>\n",
       "      <td>525</td>\n",
       "      <td>53</td>\n",
       "      <td>310</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>52</td>\n",
       "      <td>152</td>\n",
       "      <td>70</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>69</td>\n",
       "      <td>45</td>\n",
       "      <td>144</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>17</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5305</td>\n",
       "      <td>2773</td>\n",
       "      <td>1506</td>\n",
       "      <td>6327</td>\n",
       "      <td>2354</td>\n",
       "      <td>512</td>\n",
       "      <td>930</td>\n",
       "      <td>1200</td>\n",
       "      <td>470</td>\n",
       "      <td>585</td>\n",
       "      <td>2695</td>\n",
       "      <td>895</td>\n",
       "      <td>830</td>\n",
       "      <td>375</td>\n",
       "      <td>313</td>\n",
       "      <td>169</td>\n",
       "      <td>1703</td>\n",
       "      <td>578</td>\n",
       "      <td>2688</td>\n",
       "      <td>99</td>\n",
       "      <td>590</td>\n",
       "      <td>273</td>\n",
       "      <td>268</td>\n",
       "      <td>991</td>\n",
       "      <td>260</td>\n",
       "      <td>139</td>\n",
       "      <td>134</td>\n",
       "      <td>134</td>\n",
       "      <td>60</td>\n",
       "      <td>52</td>\n",
       "      <td>235</td>\n",
       "      <td>119</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>159</td>\n",
       "      <td>31</td>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>636</td>\n",
       "      <td>386</td>\n",
       "      <td>273</td>\n",
       "      <td>1375</td>\n",
       "      <td>314</td>\n",
       "      <td>84</td>\n",
       "      <td>171</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>52</td>\n",
       "      <td>315</td>\n",
       "      <td>137</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>81</td>\n",
       "      <td>12</td>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>335</td>\n",
       "      <td>142</td>\n",
       "      <td>120</td>\n",
       "      <td>597</td>\n",
       "      <td>178</td>\n",
       "      <td>39</td>\n",
       "      <td>71</td>\n",
       "      <td>84</td>\n",
       "      <td>36</td>\n",
       "      <td>71</td>\n",
       "      <td>115</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>47</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>150</td>\n",
       "      <td>32</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>109</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>668</td>\n",
       "      <td>354</td>\n",
       "      <td>248</td>\n",
       "      <td>1198</td>\n",
       "      <td>349</td>\n",
       "      <td>101</td>\n",
       "      <td>169</td>\n",
       "      <td>133</td>\n",
       "      <td>84</td>\n",
       "      <td>30</td>\n",
       "      <td>236</td>\n",
       "      <td>79</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>182</td>\n",
       "      <td>73</td>\n",
       "      <td>157</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>106</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>108</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>280</td>\n",
       "      <td>126</td>\n",
       "      <td>301</td>\n",
       "      <td>662</td>\n",
       "      <td>142</td>\n",
       "      <td>70</td>\n",
       "      <td>87</td>\n",
       "      <td>128</td>\n",
       "      <td>50</td>\n",
       "      <td>30</td>\n",
       "      <td>128</td>\n",
       "      <td>35</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>111</td>\n",
       "      <td>19</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3161</td>\n",
       "      <td>1433</td>\n",
       "      <td>950</td>\n",
       "      <td>4438</td>\n",
       "      <td>1414</td>\n",
       "      <td>471</td>\n",
       "      <td>574</td>\n",
       "      <td>909</td>\n",
       "      <td>398</td>\n",
       "      <td>412</td>\n",
       "      <td>1939</td>\n",
       "      <td>635</td>\n",
       "      <td>370</td>\n",
       "      <td>512</td>\n",
       "      <td>51</td>\n",
       "      <td>575</td>\n",
       "      <td>1067</td>\n",
       "      <td>882</td>\n",
       "      <td>2127</td>\n",
       "      <td>5</td>\n",
       "      <td>353</td>\n",
       "      <td>257</td>\n",
       "      <td>296</td>\n",
       "      <td>799</td>\n",
       "      <td>243</td>\n",
       "      <td>117</td>\n",
       "      <td>97</td>\n",
       "      <td>100</td>\n",
       "      <td>30</td>\n",
       "      <td>40</td>\n",
       "      <td>155</td>\n",
       "      <td>132</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>120</td>\n",
       "      <td>30</td>\n",
       "      <td>148</td>\n",
       "      <td>6</td>\n",
       "      <td>193</td>\n",
       "      <td>43</td>\n",
       "      <td>94</td>\n",
       "      <td>317</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>63</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>79</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>13</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>71</td>\n",
       "      <td>107</td>\n",
       "      <td>316</td>\n",
       "      <td>162</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "      <td>131</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>123</td>\n",
       "      <td>175</td>\n",
       "      <td>559</td>\n",
       "      <td>224</td>\n",
       "      <td>88</td>\n",
       "      <td>95</td>\n",
       "      <td>56</td>\n",
       "      <td>33</td>\n",
       "      <td>24</td>\n",
       "      <td>306</td>\n",
       "      <td>83</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>61</td>\n",
       "      <td>68</td>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>20</td>\n",
       "      <td>68</td>\n",
       "      <td>160</td>\n",
       "      <td>59</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>187</td>\n",
       "      <td>159</td>\n",
       "      <td>494</td>\n",
       "      <td>170</td>\n",
       "      <td>43</td>\n",
       "      <td>47</td>\n",
       "      <td>119</td>\n",
       "      <td>24</td>\n",
       "      <td>57</td>\n",
       "      <td>214</td>\n",
       "      <td>62</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>73</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1730</td>\n",
       "      <td>889</td>\n",
       "      <td>815</td>\n",
       "      <td>2290</td>\n",
       "      <td>1015</td>\n",
       "      <td>267</td>\n",
       "      <td>311</td>\n",
       "      <td>416</td>\n",
       "      <td>249</td>\n",
       "      <td>257</td>\n",
       "      <td>951</td>\n",
       "      <td>251</td>\n",
       "      <td>243</td>\n",
       "      <td>106</td>\n",
       "      <td>78</td>\n",
       "      <td>66</td>\n",
       "      <td>571</td>\n",
       "      <td>101</td>\n",
       "      <td>765</td>\n",
       "      <td>5</td>\n",
       "      <td>224</td>\n",
       "      <td>125</td>\n",
       "      <td>106</td>\n",
       "      <td>441</td>\n",
       "      <td>146</td>\n",
       "      <td>70</td>\n",
       "      <td>65</td>\n",
       "      <td>45</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>3485</td>\n",
       "      <td>1584</td>\n",
       "      <td>1396</td>\n",
       "      <td>5145</td>\n",
       "      <td>1722</td>\n",
       "      <td>659</td>\n",
       "      <td>649</td>\n",
       "      <td>1003</td>\n",
       "      <td>713</td>\n",
       "      <td>266</td>\n",
       "      <td>1907</td>\n",
       "      <td>715</td>\n",
       "      <td>469</td>\n",
       "      <td>165</td>\n",
       "      <td>118</td>\n",
       "      <td>303</td>\n",
       "      <td>859</td>\n",
       "      <td>268</td>\n",
       "      <td>1727</td>\n",
       "      <td>4</td>\n",
       "      <td>190</td>\n",
       "      <td>105</td>\n",
       "      <td>139</td>\n",
       "      <td>487</td>\n",
       "      <td>129</td>\n",
       "      <td>67</td>\n",
       "      <td>46</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>83</td>\n",
       "      <td>4</td>\n",
       "      <td>716</td>\n",
       "      <td>359</td>\n",
       "      <td>497</td>\n",
       "      <td>1619</td>\n",
       "      <td>595</td>\n",
       "      <td>179</td>\n",
       "      <td>200</td>\n",
       "      <td>290</td>\n",
       "      <td>96</td>\n",
       "      <td>112</td>\n",
       "      <td>427</td>\n",
       "      <td>137</td>\n",
       "      <td>75</td>\n",
       "      <td>79</td>\n",
       "      <td>18</td>\n",
       "      <td>89</td>\n",
       "      <td>390</td>\n",
       "      <td>90</td>\n",
       "      <td>536</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>130</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2251</td>\n",
       "      <td>1216</td>\n",
       "      <td>1048</td>\n",
       "      <td>3804</td>\n",
       "      <td>1030</td>\n",
       "      <td>344</td>\n",
       "      <td>408</td>\n",
       "      <td>437</td>\n",
       "      <td>359</td>\n",
       "      <td>212</td>\n",
       "      <td>1095</td>\n",
       "      <td>317</td>\n",
       "      <td>179</td>\n",
       "      <td>221</td>\n",
       "      <td>38</td>\n",
       "      <td>130</td>\n",
       "      <td>530</td>\n",
       "      <td>306</td>\n",
       "      <td>937</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>17</td>\n",
       "      <td>226</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>103</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>42</td>\n",
       "      <td>18</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>66</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>194</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>369</td>\n",
       "      <td>183</td>\n",
       "      <td>247</td>\n",
       "      <td>842</td>\n",
       "      <td>238</td>\n",
       "      <td>94</td>\n",
       "      <td>106</td>\n",
       "      <td>93</td>\n",
       "      <td>42</td>\n",
       "      <td>34</td>\n",
       "      <td>165</td>\n",
       "      <td>111</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>161</td>\n",
       "      <td>11</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>300</td>\n",
       "      <td>153</td>\n",
       "      <td>85</td>\n",
       "      <td>717</td>\n",
       "      <td>232</td>\n",
       "      <td>63</td>\n",
       "      <td>72</td>\n",
       "      <td>125</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>217</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>135</td>\n",
       "      <td>89</td>\n",
       "      <td>388</td>\n",
       "      <td>2</td>\n",
       "      <td>643</td>\n",
       "      <td>340</td>\n",
       "      <td>452</td>\n",
       "      <td>1563</td>\n",
       "      <td>507</td>\n",
       "      <td>117</td>\n",
       "      <td>209</td>\n",
       "      <td>241</td>\n",
       "      <td>124</td>\n",
       "      <td>62</td>\n",
       "      <td>331</td>\n",
       "      <td>158</td>\n",
       "      <td>65</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>264</td>\n",
       "      <td>61</td>\n",
       "      <td>225</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>326</td>\n",
       "      <td>159</td>\n",
       "      <td>167</td>\n",
       "      <td>915</td>\n",
       "      <td>323</td>\n",
       "      <td>55</td>\n",
       "      <td>72</td>\n",
       "      <td>123</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>278</td>\n",
       "      <td>79</td>\n",
       "      <td>46</td>\n",
       "      <td>126</td>\n",
       "      <td>26</td>\n",
       "      <td>88</td>\n",
       "      <td>205</td>\n",
       "      <td>51</td>\n",
       "      <td>225</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>393</td>\n",
       "      <td>218</td>\n",
       "      <td>193</td>\n",
       "      <td>576</td>\n",
       "      <td>238</td>\n",
       "      <td>60</td>\n",
       "      <td>106</td>\n",
       "      <td>177</td>\n",
       "      <td>41</td>\n",
       "      <td>37</td>\n",
       "      <td>141</td>\n",
       "      <td>85</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>158</td>\n",
       "      <td>126</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>1218</td>\n",
       "      <td>539</td>\n",
       "      <td>480</td>\n",
       "      <td>2114</td>\n",
       "      <td>696</td>\n",
       "      <td>183</td>\n",
       "      <td>323</td>\n",
       "      <td>660</td>\n",
       "      <td>225</td>\n",
       "      <td>158</td>\n",
       "      <td>716</td>\n",
       "      <td>235</td>\n",
       "      <td>408</td>\n",
       "      <td>145</td>\n",
       "      <td>84</td>\n",
       "      <td>72</td>\n",
       "      <td>390</td>\n",
       "      <td>350</td>\n",
       "      <td>2820</td>\n",
       "      <td>1</td>\n",
       "      <td>827</td>\n",
       "      <td>289</td>\n",
       "      <td>735</td>\n",
       "      <td>1748</td>\n",
       "      <td>389</td>\n",
       "      <td>105</td>\n",
       "      <td>228</td>\n",
       "      <td>158</td>\n",
       "      <td>94</td>\n",
       "      <td>47</td>\n",
       "      <td>264</td>\n",
       "      <td>147</td>\n",
       "      <td>40</td>\n",
       "      <td>101</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>217</td>\n",
       "      <td>47</td>\n",
       "      <td>147</td>\n",
       "      <td>7</td>\n",
       "      <td>178</td>\n",
       "      <td>68</td>\n",
       "      <td>53</td>\n",
       "      <td>326</td>\n",
       "      <td>136</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>67</td>\n",
       "      <td>20</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>571</td>\n",
       "      <td>284</td>\n",
       "      <td>585</td>\n",
       "      <td>1178</td>\n",
       "      <td>268</td>\n",
       "      <td>114</td>\n",
       "      <td>137</td>\n",
       "      <td>184</td>\n",
       "      <td>130</td>\n",
       "      <td>85</td>\n",
       "      <td>333</td>\n",
       "      <td>59</td>\n",
       "      <td>30</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>445</td>\n",
       "      <td>50</td>\n",
       "      <td>210</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>61</td>\n",
       "      <td>133</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>361</td>\n",
       "      <td>165</td>\n",
       "      <td>124</td>\n",
       "      <td>389</td>\n",
       "      <td>113</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>104</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>238</td>\n",
       "      <td>85</td>\n",
       "      <td>47</td>\n",
       "      <td>411</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>374</td>\n",
       "      <td>143</td>\n",
       "      <td>341</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1197</td>\n",
       "      <td>589</td>\n",
       "      <td>783</td>\n",
       "      <td>2625</td>\n",
       "      <td>1002</td>\n",
       "      <td>219</td>\n",
       "      <td>247</td>\n",
       "      <td>453</td>\n",
       "      <td>141</td>\n",
       "      <td>182</td>\n",
       "      <td>910</td>\n",
       "      <td>295</td>\n",
       "      <td>289</td>\n",
       "      <td>55</td>\n",
       "      <td>112</td>\n",
       "      <td>128</td>\n",
       "      <td>666</td>\n",
       "      <td>125</td>\n",
       "      <td>750</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>325</td>\n",
       "      <td>212</td>\n",
       "      <td>174</td>\n",
       "      <td>459</td>\n",
       "      <td>231</td>\n",
       "      <td>69</td>\n",
       "      <td>74</td>\n",
       "      <td>56</td>\n",
       "      <td>41</td>\n",
       "      <td>36</td>\n",
       "      <td>155</td>\n",
       "      <td>67</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>63</td>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>83</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>110</td>\n",
       "      <td>109</td>\n",
       "      <td>415</td>\n",
       "      <td>122</td>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>74</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>104</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "      <td>0</td>\n",
       "      <td>497</td>\n",
       "      <td>239</td>\n",
       "      <td>121</td>\n",
       "      <td>718</td>\n",
       "      <td>214</td>\n",
       "      <td>77</td>\n",
       "      <td>104</td>\n",
       "      <td>132</td>\n",
       "      <td>44</td>\n",
       "      <td>37</td>\n",
       "      <td>215</td>\n",
       "      <td>89</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>153</td>\n",
       "      <td>57</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>8128</td>\n",
       "      <td>4171</td>\n",
       "      <td>4051</td>\n",
       "      <td>13385</td>\n",
       "      <td>4880</td>\n",
       "      <td>1200</td>\n",
       "      <td>1719</td>\n",
       "      <td>2080</td>\n",
       "      <td>1135</td>\n",
       "      <td>676</td>\n",
       "      <td>3696</td>\n",
       "      <td>1454</td>\n",
       "      <td>892</td>\n",
       "      <td>356</td>\n",
       "      <td>120</td>\n",
       "      <td>613</td>\n",
       "      <td>2450</td>\n",
       "      <td>702</td>\n",
       "      <td>2402</td>\n",
       "      <td>27</td>\n",
       "      <td>516</td>\n",
       "      <td>174</td>\n",
       "      <td>239</td>\n",
       "      <td>822</td>\n",
       "      <td>290</td>\n",
       "      <td>128</td>\n",
       "      <td>108</td>\n",
       "      <td>118</td>\n",
       "      <td>58</td>\n",
       "      <td>41</td>\n",
       "      <td>190</td>\n",
       "      <td>91</td>\n",
       "      <td>46</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>60</td>\n",
       "      <td>147</td>\n",
       "      <td>41</td>\n",
       "      <td>320</td>\n",
       "      <td>1</td>\n",
       "      <td>61559</td>\n",
       "      <td>33163</td>\n",
       "      <td>30229</td>\n",
       "      <td>87743</td>\n",
       "      <td>30530</td>\n",
       "      <td>8138</td>\n",
       "      <td>16698</td>\n",
       "      <td>16986</td>\n",
       "      <td>9693</td>\n",
       "      <td>4860</td>\n",
       "      <td>27084</td>\n",
       "      <td>11439</td>\n",
       "      <td>5413</td>\n",
       "      <td>2056</td>\n",
       "      <td>514</td>\n",
       "      <td>3657</td>\n",
       "      <td>22324</td>\n",
       "      <td>6238</td>\n",
       "      <td>22093</td>\n",
       "      <td>72</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>58</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>151</td>\n",
       "      <td>64</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>41</td>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>250</td>\n",
       "      <td>142</td>\n",
       "      <td>129</td>\n",
       "      <td>469</td>\n",
       "      <td>78</td>\n",
       "      <td>66</td>\n",
       "      <td>47</td>\n",
       "      <td>91</td>\n",
       "      <td>66</td>\n",
       "      <td>19</td>\n",
       "      <td>106</td>\n",
       "      <td>54</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>135</td>\n",
       "      <td>36</td>\n",
       "      <td>109</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>558</td>\n",
       "      <td>305</td>\n",
       "      <td>648</td>\n",
       "      <td>1679</td>\n",
       "      <td>793</td>\n",
       "      <td>204</td>\n",
       "      <td>168</td>\n",
       "      <td>138</td>\n",
       "      <td>74</td>\n",
       "      <td>55</td>\n",
       "      <td>243</td>\n",
       "      <td>127</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>311</td>\n",
       "      <td>43</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>108</td>\n",
       "      <td>63</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>221</td>\n",
       "      <td>96</td>\n",
       "      <td>95</td>\n",
       "      <td>313</td>\n",
       "      <td>83</td>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>80</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "      <td>171</td>\n",
       "      <td>136</td>\n",
       "      <td>546</td>\n",
       "      <td>167</td>\n",
       "      <td>57</td>\n",
       "      <td>147</td>\n",
       "      <td>70</td>\n",
       "      <td>76</td>\n",
       "      <td>25</td>\n",
       "      <td>162</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>189</td>\n",
       "      <td>69</td>\n",
       "      <td>690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>147</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>285</td>\n",
       "      <td>142</td>\n",
       "      <td>128</td>\n",
       "      <td>523</td>\n",
       "      <td>173</td>\n",
       "      <td>44</td>\n",
       "      <td>92</td>\n",
       "      <td>91</td>\n",
       "      <td>18</td>\n",
       "      <td>34</td>\n",
       "      <td>161</td>\n",
       "      <td>44</td>\n",
       "      <td>94</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>67</td>\n",
       "      <td>61</td>\n",
       "      <td>599</td>\n",
       "      <td>0</td>\n",
       "      <td>402</td>\n",
       "      <td>196</td>\n",
       "      <td>110</td>\n",
       "      <td>390</td>\n",
       "      <td>196</td>\n",
       "      <td>55</td>\n",
       "      <td>104</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>44</td>\n",
       "      <td>157</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>236</td>\n",
       "      <td>14</td>\n",
       "      <td>120</td>\n",
       "      <td>0</td>\n",
       "      <td>725</td>\n",
       "      <td>381</td>\n",
       "      <td>275</td>\n",
       "      <td>869</td>\n",
       "      <td>379</td>\n",
       "      <td>109</td>\n",
       "      <td>130</td>\n",
       "      <td>207</td>\n",
       "      <td>55</td>\n",
       "      <td>106</td>\n",
       "      <td>417</td>\n",
       "      <td>128</td>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>45</td>\n",
       "      <td>265</td>\n",
       "      <td>79</td>\n",
       "      <td>192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-15/2018-01-21</th>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>474</td>\n",
       "      <td>182</td>\n",
       "      <td>596</td>\n",
       "      <td>1277</td>\n",
       "      <td>390</td>\n",
       "      <td>106</td>\n",
       "      <td>169</td>\n",
       "      <td>85</td>\n",
       "      <td>50</td>\n",
       "      <td>35</td>\n",
       "      <td>160</td>\n",
       "      <td>65</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>139</td>\n",
       "      <td>17</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>1608</td>\n",
       "      <td>441</td>\n",
       "      <td>522</td>\n",
       "      <td>2752</td>\n",
       "      <td>617</td>\n",
       "      <td>271</td>\n",
       "      <td>304</td>\n",
       "      <td>384</td>\n",
       "      <td>95</td>\n",
       "      <td>126</td>\n",
       "      <td>622</td>\n",
       "      <td>214</td>\n",
       "      <td>181</td>\n",
       "      <td>36</td>\n",
       "      <td>82</td>\n",
       "      <td>106</td>\n",
       "      <td>313</td>\n",
       "      <td>302</td>\n",
       "      <td>1790</td>\n",
       "      <td>3</td>\n",
       "      <td>69</td>\n",
       "      <td>42</td>\n",
       "      <td>51</td>\n",
       "      <td>182</td>\n",
       "      <td>66</td>\n",
       "      <td>45</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>77</td>\n",
       "      <td>205</td>\n",
       "      <td>670</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "      <td>59</td>\n",
       "      <td>43</td>\n",
       "      <td>55</td>\n",
       "      <td>37</td>\n",
       "      <td>75</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>34</td>\n",
       "      <td>67</td>\n",
       "      <td>15</td>\n",
       "      <td>110</td>\n",
       "      <td>82</td>\n",
       "      <td>69</td>\n",
       "      <td>242</td>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>77</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>121</td>\n",
       "      <td>0</td>\n",
       "      <td>358</td>\n",
       "      <td>124</td>\n",
       "      <td>152</td>\n",
       "      <td>605</td>\n",
       "      <td>261</td>\n",
       "      <td>61</td>\n",
       "      <td>51</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>38</td>\n",
       "      <td>105</td>\n",
       "      <td>44</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>73</td>\n",
       "      <td>112</td>\n",
       "      <td>197</td>\n",
       "      <td>120</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>95</td>\n",
       "      <td>226</td>\n",
       "      <td>733</td>\n",
       "      <td>144</td>\n",
       "      <td>69</td>\n",
       "      <td>37</td>\n",
       "      <td>64</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>109</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>102</td>\n",
       "      <td>24</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>4534</td>\n",
       "      <td>2256</td>\n",
       "      <td>2148</td>\n",
       "      <td>6491</td>\n",
       "      <td>2336</td>\n",
       "      <td>637</td>\n",
       "      <td>1033</td>\n",
       "      <td>817</td>\n",
       "      <td>638</td>\n",
       "      <td>306</td>\n",
       "      <td>1614</td>\n",
       "      <td>617</td>\n",
       "      <td>420</td>\n",
       "      <td>141</td>\n",
       "      <td>43</td>\n",
       "      <td>195</td>\n",
       "      <td>1183</td>\n",
       "      <td>474</td>\n",
       "      <td>1369</td>\n",
       "      <td>2</td>\n",
       "      <td>319</td>\n",
       "      <td>155</td>\n",
       "      <td>124</td>\n",
       "      <td>537</td>\n",
       "      <td>184</td>\n",
       "      <td>38</td>\n",
       "      <td>82</td>\n",
       "      <td>54</td>\n",
       "      <td>19</td>\n",
       "      <td>31</td>\n",
       "      <td>80</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>63</td>\n",
       "      <td>37</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>78</td>\n",
       "      <td>81</td>\n",
       "      <td>396</td>\n",
       "      <td>146</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>112</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "      <td>95</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>56</td>\n",
       "      <td>43</td>\n",
       "      <td>148</td>\n",
       "      <td>57</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>118</td>\n",
       "      <td>73</td>\n",
       "      <td>199</td>\n",
       "      <td>69</td>\n",
       "      <td>23</td>\n",
       "      <td>58</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>156</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>559</td>\n",
       "      <td>252</td>\n",
       "      <td>373</td>\n",
       "      <td>1011</td>\n",
       "      <td>231</td>\n",
       "      <td>72</td>\n",
       "      <td>124</td>\n",
       "      <td>157</td>\n",
       "      <td>85</td>\n",
       "      <td>58</td>\n",
       "      <td>191</td>\n",
       "      <td>103</td>\n",
       "      <td>75</td>\n",
       "      <td>39</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>142</td>\n",
       "      <td>73</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>105</td>\n",
       "      <td>57</td>\n",
       "      <td>340</td>\n",
       "      <td>69</td>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "      <td>23</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>41</td>\n",
       "      <td>25</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>58</td>\n",
       "      <td>28</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>1316</td>\n",
       "      <td>486</td>\n",
       "      <td>645</td>\n",
       "      <td>1909</td>\n",
       "      <td>587</td>\n",
       "      <td>167</td>\n",
       "      <td>278</td>\n",
       "      <td>408</td>\n",
       "      <td>115</td>\n",
       "      <td>133</td>\n",
       "      <td>409</td>\n",
       "      <td>160</td>\n",
       "      <td>89</td>\n",
       "      <td>116</td>\n",
       "      <td>28</td>\n",
       "      <td>62</td>\n",
       "      <td>359</td>\n",
       "      <td>151</td>\n",
       "      <td>467</td>\n",
       "      <td>69</td>\n",
       "      <td>147</td>\n",
       "      <td>62</td>\n",
       "      <td>61</td>\n",
       "      <td>207</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>56</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>57</td>\n",
       "      <td>23</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>226</td>\n",
       "      <td>43</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>371</td>\n",
       "      <td>138</td>\n",
       "      <td>112</td>\n",
       "      <td>499</td>\n",
       "      <td>129</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "      <td>99</td>\n",
       "      <td>75</td>\n",
       "      <td>20</td>\n",
       "      <td>139</td>\n",
       "      <td>84</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>161</td>\n",
       "      <td>57</td>\n",
       "      <td>309</td>\n",
       "      <td>75</td>\n",
       "      <td>59</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>100</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>204</td>\n",
       "      <td>78</td>\n",
       "      <td>134</td>\n",
       "      <td>406</td>\n",
       "      <td>88</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370</td>\n",
       "      <td>181</td>\n",
       "      <td>222</td>\n",
       "      <td>697</td>\n",
       "      <td>207</td>\n",
       "      <td>77</td>\n",
       "      <td>119</td>\n",
       "      <td>130</td>\n",
       "      <td>68</td>\n",
       "      <td>29</td>\n",
       "      <td>157</td>\n",
       "      <td>74</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>175</td>\n",
       "      <td>38</td>\n",
       "      <td>163</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>303</td>\n",
       "      <td>120</td>\n",
       "      <td>192</td>\n",
       "      <td>670</td>\n",
       "      <td>201</td>\n",
       "      <td>27</td>\n",
       "      <td>48</td>\n",
       "      <td>70</td>\n",
       "      <td>29</td>\n",
       "      <td>19</td>\n",
       "      <td>78</td>\n",
       "      <td>65</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>20</td>\n",
       "      <td>46</td>\n",
       "      <td>162</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>58</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>5629</td>\n",
       "      <td>3059</td>\n",
       "      <td>2654</td>\n",
       "      <td>8906</td>\n",
       "      <td>2582</td>\n",
       "      <td>919</td>\n",
       "      <td>1359</td>\n",
       "      <td>1762</td>\n",
       "      <td>872</td>\n",
       "      <td>456</td>\n",
       "      <td>2272</td>\n",
       "      <td>882</td>\n",
       "      <td>613</td>\n",
       "      <td>282</td>\n",
       "      <td>53</td>\n",
       "      <td>242</td>\n",
       "      <td>1841</td>\n",
       "      <td>655</td>\n",
       "      <td>1834</td>\n",
       "      <td>9</td>\n",
       "      <td>275</td>\n",
       "      <td>102</td>\n",
       "      <td>133</td>\n",
       "      <td>454</td>\n",
       "      <td>177</td>\n",
       "      <td>75</td>\n",
       "      <td>68</td>\n",
       "      <td>64</td>\n",
       "      <td>29</td>\n",
       "      <td>34</td>\n",
       "      <td>123</td>\n",
       "      <td>60</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>142</td>\n",
       "      <td>15</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>70</td>\n",
       "      <td>30</td>\n",
       "      <td>238</td>\n",
       "      <td>114</td>\n",
       "      <td>22</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>379</td>\n",
       "      <td>192</td>\n",
       "      <td>268</td>\n",
       "      <td>876</td>\n",
       "      <td>242</td>\n",
       "      <td>150</td>\n",
       "      <td>76</td>\n",
       "      <td>178</td>\n",
       "      <td>78</td>\n",
       "      <td>60</td>\n",
       "      <td>198</td>\n",
       "      <td>78</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>223</td>\n",
       "      <td>43</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>69</td>\n",
       "      <td>15</td>\n",
       "      <td>170</td>\n",
       "      <td>50</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>113</td>\n",
       "      <td>28</td>\n",
       "      <td>14</td>\n",
       "      <td>38</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>27</td>\n",
       "      <td>105</td>\n",
       "      <td>1</td>\n",
       "      <td>73</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>40</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>4362</td>\n",
       "      <td>1988</td>\n",
       "      <td>2019</td>\n",
       "      <td>6611</td>\n",
       "      <td>2387</td>\n",
       "      <td>986</td>\n",
       "      <td>1042</td>\n",
       "      <td>981</td>\n",
       "      <td>802</td>\n",
       "      <td>340</td>\n",
       "      <td>2100</td>\n",
       "      <td>721</td>\n",
       "      <td>520</td>\n",
       "      <td>152</td>\n",
       "      <td>159</td>\n",
       "      <td>262</td>\n",
       "      <td>1332</td>\n",
       "      <td>239</td>\n",
       "      <td>874</td>\n",
       "      <td>16</td>\n",
       "      <td>968</td>\n",
       "      <td>343</td>\n",
       "      <td>675</td>\n",
       "      <td>2558</td>\n",
       "      <td>554</td>\n",
       "      <td>50</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>558</td>\n",
       "      <td>131</td>\n",
       "      <td>87</td>\n",
       "      <td>142</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>322</td>\n",
       "      <td>61</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>46</td>\n",
       "      <td>12</td>\n",
       "      <td>28</td>\n",
       "      <td>78</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>192</td>\n",
       "      <td>55</td>\n",
       "      <td>18</td>\n",
       "      <td>29</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>85</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>31</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211</td>\n",
       "      <td>89</td>\n",
       "      <td>119</td>\n",
       "      <td>579</td>\n",
       "      <td>155</td>\n",
       "      <td>63</td>\n",
       "      <td>52</td>\n",
       "      <td>72</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>121</td>\n",
       "      <td>45</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>107</td>\n",
       "      <td>44</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>46</td>\n",
       "      <td>31</td>\n",
       "      <td>175</td>\n",
       "      <td>67</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>153</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>105</td>\n",
       "      <td>82</td>\n",
       "      <td>614</td>\n",
       "      <td>153</td>\n",
       "      <td>27</td>\n",
       "      <td>43</td>\n",
       "      <td>61</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>92</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>73</td>\n",
       "      <td>23</td>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>78</td>\n",
       "      <td>72</td>\n",
       "      <td>309</td>\n",
       "      <td>151</td>\n",
       "      <td>16</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>275</td>\n",
       "      <td>102</td>\n",
       "      <td>121</td>\n",
       "      <td>464</td>\n",
       "      <td>96</td>\n",
       "      <td>57</td>\n",
       "      <td>82</td>\n",
       "      <td>48</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>152</td>\n",
       "      <td>52</td>\n",
       "      <td>234</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>26</td>\n",
       "      <td>92</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>88</td>\n",
       "      <td>30</td>\n",
       "      <td>109</td>\n",
       "      <td>173</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>74</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>84</td>\n",
       "      <td>56</td>\n",
       "      <td>36</td>\n",
       "      <td>182</td>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>933</td>\n",
       "      <td>447</td>\n",
       "      <td>880</td>\n",
       "      <td>3131</td>\n",
       "      <td>700</td>\n",
       "      <td>141</td>\n",
       "      <td>168</td>\n",
       "      <td>278</td>\n",
       "      <td>114</td>\n",
       "      <td>98</td>\n",
       "      <td>537</td>\n",
       "      <td>256</td>\n",
       "      <td>161</td>\n",
       "      <td>68</td>\n",
       "      <td>77</td>\n",
       "      <td>208</td>\n",
       "      <td>456</td>\n",
       "      <td>136</td>\n",
       "      <td>436</td>\n",
       "      <td>2</td>\n",
       "      <td>1552</td>\n",
       "      <td>699</td>\n",
       "      <td>637</td>\n",
       "      <td>2290</td>\n",
       "      <td>764</td>\n",
       "      <td>162</td>\n",
       "      <td>260</td>\n",
       "      <td>303</td>\n",
       "      <td>143</td>\n",
       "      <td>123</td>\n",
       "      <td>577</td>\n",
       "      <td>231</td>\n",
       "      <td>149</td>\n",
       "      <td>97</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>345</td>\n",
       "      <td>131</td>\n",
       "      <td>333</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>46</td>\n",
       "      <td>69</td>\n",
       "      <td>194</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>91</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>125</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>63</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>23</td>\n",
       "      <td>56</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>244</td>\n",
       "      <td>140</td>\n",
       "      <td>72</td>\n",
       "      <td>519</td>\n",
       "      <td>106</td>\n",
       "      <td>30</td>\n",
       "      <td>82</td>\n",
       "      <td>72</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>147</td>\n",
       "      <td>40</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>84</td>\n",
       "      <td>20</td>\n",
       "      <td>92</td>\n",
       "      <td>10</td>\n",
       "      <td>392</td>\n",
       "      <td>191</td>\n",
       "      <td>223</td>\n",
       "      <td>952</td>\n",
       "      <td>387</td>\n",
       "      <td>62</td>\n",
       "      <td>82</td>\n",
       "      <td>359</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>222</td>\n",
       "      <td>63</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>199</td>\n",
       "      <td>25</td>\n",
       "      <td>108</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>262</td>\n",
       "      <td>102</td>\n",
       "      <td>173</td>\n",
       "      <td>557</td>\n",
       "      <td>147</td>\n",
       "      <td>63</td>\n",
       "      <td>60</td>\n",
       "      <td>72</td>\n",
       "      <td>50</td>\n",
       "      <td>11</td>\n",
       "      <td>102</td>\n",
       "      <td>72</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>75</td>\n",
       "      <td>138</td>\n",
       "      <td>581</td>\n",
       "      <td>167</td>\n",
       "      <td>35</td>\n",
       "      <td>47</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>234</td>\n",
       "      <td>133</td>\n",
       "      <td>113</td>\n",
       "      <td>418</td>\n",
       "      <td>187</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>35</td>\n",
       "      <td>29</td>\n",
       "      <td>67</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>54</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2138</td>\n",
       "      <td>1079</td>\n",
       "      <td>1770</td>\n",
       "      <td>4383</td>\n",
       "      <td>1379</td>\n",
       "      <td>341</td>\n",
       "      <td>462</td>\n",
       "      <td>611</td>\n",
       "      <td>292</td>\n",
       "      <td>140</td>\n",
       "      <td>1052</td>\n",
       "      <td>399</td>\n",
       "      <td>246</td>\n",
       "      <td>167</td>\n",
       "      <td>82</td>\n",
       "      <td>86</td>\n",
       "      <td>676</td>\n",
       "      <td>195</td>\n",
       "      <td>809</td>\n",
       "      <td>2</td>\n",
       "      <td>222</td>\n",
       "      <td>86</td>\n",
       "      <td>98</td>\n",
       "      <td>307</td>\n",
       "      <td>214</td>\n",
       "      <td>37</td>\n",
       "      <td>56</td>\n",
       "      <td>75</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>81</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>62</td>\n",
       "      <td>15</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>23</td>\n",
       "      <td>54</td>\n",
       "      <td>115</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>166</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>33</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1166</td>\n",
       "      <td>606</td>\n",
       "      <td>340</td>\n",
       "      <td>1205</td>\n",
       "      <td>671</td>\n",
       "      <td>137</td>\n",
       "      <td>231</td>\n",
       "      <td>119</td>\n",
       "      <td>130</td>\n",
       "      <td>90</td>\n",
       "      <td>474</td>\n",
       "      <td>128</td>\n",
       "      <td>127</td>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>305</td>\n",
       "      <td>77</td>\n",
       "      <td>224</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>63</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>59</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1648</td>\n",
       "      <td>800</td>\n",
       "      <td>991</td>\n",
       "      <td>2909</td>\n",
       "      <td>1157</td>\n",
       "      <td>360</td>\n",
       "      <td>376</td>\n",
       "      <td>464</td>\n",
       "      <td>257</td>\n",
       "      <td>158</td>\n",
       "      <td>751</td>\n",
       "      <td>350</td>\n",
       "      <td>180</td>\n",
       "      <td>151</td>\n",
       "      <td>22</td>\n",
       "      <td>132</td>\n",
       "      <td>576</td>\n",
       "      <td>211</td>\n",
       "      <td>744</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>131</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>547</td>\n",
       "      <td>259</td>\n",
       "      <td>333</td>\n",
       "      <td>1072</td>\n",
       "      <td>299</td>\n",
       "      <td>61</td>\n",
       "      <td>116</td>\n",
       "      <td>122</td>\n",
       "      <td>61</td>\n",
       "      <td>44</td>\n",
       "      <td>227</td>\n",
       "      <td>109</td>\n",
       "      <td>61</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>181</td>\n",
       "      <td>52</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>55</td>\n",
       "      <td>37</td>\n",
       "      <td>184</td>\n",
       "      <td>55</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>71</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>13</td>\n",
       "      <td>56</td>\n",
       "      <td>5</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>34</td>\n",
       "      <td>65</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>124</td>\n",
       "      <td>73</td>\n",
       "      <td>208</td>\n",
       "      <td>94</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>51</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>99</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>87</td>\n",
       "      <td>83</td>\n",
       "      <td>23</td>\n",
       "      <td>166</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>853</td>\n",
       "      <td>467</td>\n",
       "      <td>272</td>\n",
       "      <td>1334</td>\n",
       "      <td>479</td>\n",
       "      <td>69</td>\n",
       "      <td>204</td>\n",
       "      <td>122</td>\n",
       "      <td>64</td>\n",
       "      <td>67</td>\n",
       "      <td>736</td>\n",
       "      <td>197</td>\n",
       "      <td>57</td>\n",
       "      <td>76</td>\n",
       "      <td>1</td>\n",
       "      <td>80</td>\n",
       "      <td>148</td>\n",
       "      <td>54</td>\n",
       "      <td>165</td>\n",
       "      <td>1</td>\n",
       "      <td>221</td>\n",
       "      <td>121</td>\n",
       "      <td>123</td>\n",
       "      <td>471</td>\n",
       "      <td>79</td>\n",
       "      <td>67</td>\n",
       "      <td>44</td>\n",
       "      <td>67</td>\n",
       "      <td>69</td>\n",
       "      <td>14</td>\n",
       "      <td>99</td>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>158</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>94</td>\n",
       "      <td>49</td>\n",
       "      <td>193</td>\n",
       "      <td>79</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>89</td>\n",
       "      <td>8</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>18</td>\n",
       "      <td>55</td>\n",
       "      <td>189</td>\n",
       "      <td>56</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>200</td>\n",
       "      <td>110</td>\n",
       "      <td>144</td>\n",
       "      <td>425</td>\n",
       "      <td>119</td>\n",
       "      <td>39</td>\n",
       "      <td>70</td>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>61</td>\n",
       "      <td>141</td>\n",
       "      <td>52</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>138</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>2</td>\n",
       "      <td>141</td>\n",
       "      <td>70</td>\n",
       "      <td>85</td>\n",
       "      <td>318</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>649</td>\n",
       "      <td>227</td>\n",
       "      <td>282</td>\n",
       "      <td>1052</td>\n",
       "      <td>264</td>\n",
       "      <td>135</td>\n",
       "      <td>119</td>\n",
       "      <td>129</td>\n",
       "      <td>105</td>\n",
       "      <td>55</td>\n",
       "      <td>204</td>\n",
       "      <td>107</td>\n",
       "      <td>53</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>40</td>\n",
       "      <td>277</td>\n",
       "      <td>39</td>\n",
       "      <td>129</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7110</td>\n",
       "      <td>4254</td>\n",
       "      <td>4714</td>\n",
       "      <td>14132</td>\n",
       "      <td>3672</td>\n",
       "      <td>1012</td>\n",
       "      <td>1440</td>\n",
       "      <td>2319</td>\n",
       "      <td>1244</td>\n",
       "      <td>1077</td>\n",
       "      <td>3978</td>\n",
       "      <td>1435</td>\n",
       "      <td>753</td>\n",
       "      <td>1031</td>\n",
       "      <td>287</td>\n",
       "      <td>405</td>\n",
       "      <td>2882</td>\n",
       "      <td>1369</td>\n",
       "      <td>3378</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1787</td>\n",
       "      <td>808</td>\n",
       "      <td>1043</td>\n",
       "      <td>2792</td>\n",
       "      <td>1921</td>\n",
       "      <td>234</td>\n",
       "      <td>315</td>\n",
       "      <td>654</td>\n",
       "      <td>205</td>\n",
       "      <td>146</td>\n",
       "      <td>1006</td>\n",
       "      <td>477</td>\n",
       "      <td>455</td>\n",
       "      <td>271</td>\n",
       "      <td>46</td>\n",
       "      <td>229</td>\n",
       "      <td>707</td>\n",
       "      <td>210</td>\n",
       "      <td>657</td>\n",
       "      <td>2</td>\n",
       "      <td>3582</td>\n",
       "      <td>1807</td>\n",
       "      <td>2729</td>\n",
       "      <td>8122</td>\n",
       "      <td>2893</td>\n",
       "      <td>504</td>\n",
       "      <td>1238</td>\n",
       "      <td>897</td>\n",
       "      <td>325</td>\n",
       "      <td>373</td>\n",
       "      <td>2414</td>\n",
       "      <td>1112</td>\n",
       "      <td>646</td>\n",
       "      <td>487</td>\n",
       "      <td>84</td>\n",
       "      <td>795</td>\n",
       "      <td>1364</td>\n",
       "      <td>473</td>\n",
       "      <td>2671</td>\n",
       "      <td>77</td>\n",
       "      <td>867</td>\n",
       "      <td>457</td>\n",
       "      <td>543</td>\n",
       "      <td>1985</td>\n",
       "      <td>562</td>\n",
       "      <td>141</td>\n",
       "      <td>240</td>\n",
       "      <td>204</td>\n",
       "      <td>91</td>\n",
       "      <td>82</td>\n",
       "      <td>458</td>\n",
       "      <td>150</td>\n",
       "      <td>62</td>\n",
       "      <td>72</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>283</td>\n",
       "      <td>83</td>\n",
       "      <td>259</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>69</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>919</td>\n",
       "      <td>389</td>\n",
       "      <td>344</td>\n",
       "      <td>1720</td>\n",
       "      <td>573</td>\n",
       "      <td>170</td>\n",
       "      <td>205</td>\n",
       "      <td>291</td>\n",
       "      <td>75</td>\n",
       "      <td>98</td>\n",
       "      <td>376</td>\n",
       "      <td>170</td>\n",
       "      <td>180</td>\n",
       "      <td>51</td>\n",
       "      <td>56</td>\n",
       "      <td>67</td>\n",
       "      <td>279</td>\n",
       "      <td>180</td>\n",
       "      <td>886</td>\n",
       "      <td>10</td>\n",
       "      <td>1348</td>\n",
       "      <td>619</td>\n",
       "      <td>973</td>\n",
       "      <td>3237</td>\n",
       "      <td>824</td>\n",
       "      <td>316</td>\n",
       "      <td>351</td>\n",
       "      <td>372</td>\n",
       "      <td>209</td>\n",
       "      <td>82</td>\n",
       "      <td>531</td>\n",
       "      <td>209</td>\n",
       "      <td>164</td>\n",
       "      <td>51</td>\n",
       "      <td>72</td>\n",
       "      <td>122</td>\n",
       "      <td>435</td>\n",
       "      <td>102</td>\n",
       "      <td>476</td>\n",
       "      <td>1</td>\n",
       "      <td>309</td>\n",
       "      <td>120</td>\n",
       "      <td>99</td>\n",
       "      <td>387</td>\n",
       "      <td>139</td>\n",
       "      <td>23</td>\n",
       "      <td>79</td>\n",
       "      <td>69</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>110</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>362</td>\n",
       "      <td>145</td>\n",
       "      <td>124</td>\n",
       "      <td>470</td>\n",
       "      <td>210</td>\n",
       "      <td>39</td>\n",
       "      <td>49</td>\n",
       "      <td>56</td>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>134</td>\n",
       "      <td>21</td>\n",
       "      <td>219</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>474</td>\n",
       "      <td>197</td>\n",
       "      <td>416</td>\n",
       "      <td>1312</td>\n",
       "      <td>418</td>\n",
       "      <td>68</td>\n",
       "      <td>135</td>\n",
       "      <td>115</td>\n",
       "      <td>46</td>\n",
       "      <td>49</td>\n",
       "      <td>206</td>\n",
       "      <td>66</td>\n",
       "      <td>49</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>65</td>\n",
       "      <td>132</td>\n",
       "      <td>25</td>\n",
       "      <td>258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1131</td>\n",
       "      <td>788</td>\n",
       "      <td>505</td>\n",
       "      <td>2039</td>\n",
       "      <td>539</td>\n",
       "      <td>160</td>\n",
       "      <td>285</td>\n",
       "      <td>287</td>\n",
       "      <td>206</td>\n",
       "      <td>186</td>\n",
       "      <td>741</td>\n",
       "      <td>214</td>\n",
       "      <td>161</td>\n",
       "      <td>85</td>\n",
       "      <td>29</td>\n",
       "      <td>78</td>\n",
       "      <td>396</td>\n",
       "      <td>146</td>\n",
       "      <td>452</td>\n",
       "      <td>0</td>\n",
       "      <td>96</td>\n",
       "      <td>45</td>\n",
       "      <td>61</td>\n",
       "      <td>318</td>\n",
       "      <td>91</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1049</td>\n",
       "      <td>507</td>\n",
       "      <td>453</td>\n",
       "      <td>1830</td>\n",
       "      <td>509</td>\n",
       "      <td>123</td>\n",
       "      <td>116</td>\n",
       "      <td>297</td>\n",
       "      <td>96</td>\n",
       "      <td>127</td>\n",
       "      <td>501</td>\n",
       "      <td>210</td>\n",
       "      <td>486</td>\n",
       "      <td>40</td>\n",
       "      <td>150</td>\n",
       "      <td>220</td>\n",
       "      <td>150</td>\n",
       "      <td>77</td>\n",
       "      <td>374</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>729</td>\n",
       "      <td>438</td>\n",
       "      <td>557</td>\n",
       "      <td>1631</td>\n",
       "      <td>399</td>\n",
       "      <td>187</td>\n",
       "      <td>181</td>\n",
       "      <td>180</td>\n",
       "      <td>101</td>\n",
       "      <td>55</td>\n",
       "      <td>327</td>\n",
       "      <td>127</td>\n",
       "      <td>160</td>\n",
       "      <td>18</td>\n",
       "      <td>42</td>\n",
       "      <td>115</td>\n",
       "      <td>343</td>\n",
       "      <td>50</td>\n",
       "      <td>258</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>85</td>\n",
       "      <td>136</td>\n",
       "      <td>417</td>\n",
       "      <td>174</td>\n",
       "      <td>60</td>\n",
       "      <td>44</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>77</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>98</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>193</td>\n",
       "      <td>94</td>\n",
       "      <td>72</td>\n",
       "      <td>220</td>\n",
       "      <td>89</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>28</td>\n",
       "      <td>38</td>\n",
       "      <td>93</td>\n",
       "      <td>49</td>\n",
       "      <td>33</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>99</td>\n",
       "      <td>36</td>\n",
       "      <td>258</td>\n",
       "      <td>5</td>\n",
       "      <td>284</td>\n",
       "      <td>108</td>\n",
       "      <td>230</td>\n",
       "      <td>911</td>\n",
       "      <td>187</td>\n",
       "      <td>73</td>\n",
       "      <td>71</td>\n",
       "      <td>60</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>64</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>58</td>\n",
       "      <td>2</td>\n",
       "      <td>58</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>29</td>\n",
       "      <td>70</td>\n",
       "      <td>266</td>\n",
       "      <td>70</td>\n",
       "      <td>29</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>514</td>\n",
       "      <td>210</td>\n",
       "      <td>195</td>\n",
       "      <td>807</td>\n",
       "      <td>306</td>\n",
       "      <td>110</td>\n",
       "      <td>164</td>\n",
       "      <td>185</td>\n",
       "      <td>89</td>\n",
       "      <td>39</td>\n",
       "      <td>287</td>\n",
       "      <td>107</td>\n",
       "      <td>58</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>148</td>\n",
       "      <td>492</td>\n",
       "      <td>66</td>\n",
       "      <td>330</td>\n",
       "      <td>3</td>\n",
       "      <td>80</td>\n",
       "      <td>45</td>\n",
       "      <td>79</td>\n",
       "      <td>188</td>\n",
       "      <td>75</td>\n",
       "      <td>31</td>\n",
       "      <td>27</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>44</td>\n",
       "      <td>74</td>\n",
       "      <td>258</td>\n",
       "      <td>165</td>\n",
       "      <td>46</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>264</td>\n",
       "      <td>114</td>\n",
       "      <td>100</td>\n",
       "      <td>381</td>\n",
       "      <td>254</td>\n",
       "      <td>58</td>\n",
       "      <td>34</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "      <td>94</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>75</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>52</td>\n",
       "      <td>57</td>\n",
       "      <td>99</td>\n",
       "      <td>44</td>\n",
       "      <td>18</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>81</td>\n",
       "      <td>80</td>\n",
       "      <td>373</td>\n",
       "      <td>129</td>\n",
       "      <td>48</td>\n",
       "      <td>63</td>\n",
       "      <td>66</td>\n",
       "      <td>33</td>\n",
       "      <td>29</td>\n",
       "      <td>134</td>\n",
       "      <td>33</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>38</td>\n",
       "      <td>120</td>\n",
       "      <td>67</td>\n",
       "      <td>262</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>74</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>49</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>34</td>\n",
       "      <td>168</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>182</td>\n",
       "      <td>90</td>\n",
       "      <td>75</td>\n",
       "      <td>207</td>\n",
       "      <td>103</td>\n",
       "      <td>22</td>\n",
       "      <td>36</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>91</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>106</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>92</td>\n",
       "      <td>146</td>\n",
       "      <td>281</td>\n",
       "      <td>172</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>47</td>\n",
       "      <td>44</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>54</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>54</td>\n",
       "      <td>507</td>\n",
       "      <td>126</td>\n",
       "      <td>35</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>64</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>140</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>248</td>\n",
       "      <td>131</td>\n",
       "      <td>94</td>\n",
       "      <td>484</td>\n",
       "      <td>87</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>48</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>114</td>\n",
       "      <td>49</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>70</td>\n",
       "      <td>151</td>\n",
       "      <td>419</td>\n",
       "      <td>119</td>\n",
       "      <td>42</td>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>33</td>\n",
       "      <td>149</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>1247</td>\n",
       "      <td>581</td>\n",
       "      <td>549</td>\n",
       "      <td>1952</td>\n",
       "      <td>634</td>\n",
       "      <td>313</td>\n",
       "      <td>324</td>\n",
       "      <td>244</td>\n",
       "      <td>114</td>\n",
       "      <td>79</td>\n",
       "      <td>577</td>\n",
       "      <td>259</td>\n",
       "      <td>156</td>\n",
       "      <td>65</td>\n",
       "      <td>22</td>\n",
       "      <td>80</td>\n",
       "      <td>433</td>\n",
       "      <td>106</td>\n",
       "      <td>537</td>\n",
       "      <td>1</td>\n",
       "      <td>1173</td>\n",
       "      <td>553</td>\n",
       "      <td>529</td>\n",
       "      <td>1581</td>\n",
       "      <td>669</td>\n",
       "      <td>249</td>\n",
       "      <td>250</td>\n",
       "      <td>198</td>\n",
       "      <td>128</td>\n",
       "      <td>132</td>\n",
       "      <td>486</td>\n",
       "      <td>163</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>10</td>\n",
       "      <td>52</td>\n",
       "      <td>377</td>\n",
       "      <td>70</td>\n",
       "      <td>269</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>45</td>\n",
       "      <td>105</td>\n",
       "      <td>188</td>\n",
       "      <td>139</td>\n",
       "      <td>34</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>20</td>\n",
       "      <td>18</td>\n",
       "      <td>198</td>\n",
       "      <td>42</td>\n",
       "      <td>34</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "      <td>33</td>\n",
       "      <td>92</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>71</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5410</td>\n",
       "      <td>2733</td>\n",
       "      <td>1398</td>\n",
       "      <td>6942</td>\n",
       "      <td>2622</td>\n",
       "      <td>440</td>\n",
       "      <td>912</td>\n",
       "      <td>1288</td>\n",
       "      <td>390</td>\n",
       "      <td>706</td>\n",
       "      <td>2693</td>\n",
       "      <td>1027</td>\n",
       "      <td>696</td>\n",
       "      <td>246</td>\n",
       "      <td>171</td>\n",
       "      <td>169</td>\n",
       "      <td>1632</td>\n",
       "      <td>942</td>\n",
       "      <td>2441</td>\n",
       "      <td>77</td>\n",
       "      <td>533</td>\n",
       "      <td>273</td>\n",
       "      <td>272</td>\n",
       "      <td>1014</td>\n",
       "      <td>345</td>\n",
       "      <td>92</td>\n",
       "      <td>109</td>\n",
       "      <td>148</td>\n",
       "      <td>84</td>\n",
       "      <td>45</td>\n",
       "      <td>202</td>\n",
       "      <td>66</td>\n",
       "      <td>52</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>83</td>\n",
       "      <td>121</td>\n",
       "      <td>38</td>\n",
       "      <td>166</td>\n",
       "      <td>1</td>\n",
       "      <td>612</td>\n",
       "      <td>276</td>\n",
       "      <td>291</td>\n",
       "      <td>990</td>\n",
       "      <td>295</td>\n",
       "      <td>71</td>\n",
       "      <td>177</td>\n",
       "      <td>119</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>256</td>\n",
       "      <td>121</td>\n",
       "      <td>38</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>137</td>\n",
       "      <td>26</td>\n",
       "      <td>110</td>\n",
       "      <td>5</td>\n",
       "      <td>278</td>\n",
       "      <td>147</td>\n",
       "      <td>176</td>\n",
       "      <td>576</td>\n",
       "      <td>185</td>\n",
       "      <td>46</td>\n",
       "      <td>36</td>\n",
       "      <td>69</td>\n",
       "      <td>45</td>\n",
       "      <td>33</td>\n",
       "      <td>87</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>13</td>\n",
       "      <td>131</td>\n",
       "      <td>31</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>65</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>770</td>\n",
       "      <td>407</td>\n",
       "      <td>403</td>\n",
       "      <td>1403</td>\n",
       "      <td>448</td>\n",
       "      <td>103</td>\n",
       "      <td>215</td>\n",
       "      <td>154</td>\n",
       "      <td>80</td>\n",
       "      <td>46</td>\n",
       "      <td>270</td>\n",
       "      <td>109</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>191</td>\n",
       "      <td>73</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>50</td>\n",
       "      <td>46</td>\n",
       "      <td>224</td>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>39</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>87</td>\n",
       "      <td>24</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>100</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>65</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>632</td>\n",
       "      <td>296</td>\n",
       "      <td>494</td>\n",
       "      <td>1921</td>\n",
       "      <td>371</td>\n",
       "      <td>39</td>\n",
       "      <td>52</td>\n",
       "      <td>134</td>\n",
       "      <td>42</td>\n",
       "      <td>40</td>\n",
       "      <td>263</td>\n",
       "      <td>56</td>\n",
       "      <td>49</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>149</td>\n",
       "      <td>44</td>\n",
       "      <td>133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3487</td>\n",
       "      <td>1691</td>\n",
       "      <td>1127</td>\n",
       "      <td>5084</td>\n",
       "      <td>1300</td>\n",
       "      <td>452</td>\n",
       "      <td>654</td>\n",
       "      <td>943</td>\n",
       "      <td>502</td>\n",
       "      <td>601</td>\n",
       "      <td>2381</td>\n",
       "      <td>576</td>\n",
       "      <td>391</td>\n",
       "      <td>515</td>\n",
       "      <td>98</td>\n",
       "      <td>307</td>\n",
       "      <td>1300</td>\n",
       "      <td>975</td>\n",
       "      <td>2933</td>\n",
       "      <td>15</td>\n",
       "      <td>323</td>\n",
       "      <td>183</td>\n",
       "      <td>215</td>\n",
       "      <td>872</td>\n",
       "      <td>246</td>\n",
       "      <td>67</td>\n",
       "      <td>96</td>\n",
       "      <td>80</td>\n",
       "      <td>41</td>\n",
       "      <td>27</td>\n",
       "      <td>150</td>\n",
       "      <td>97</td>\n",
       "      <td>30</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>167</td>\n",
       "      <td>19</td>\n",
       "      <td>149</td>\n",
       "      <td>4</td>\n",
       "      <td>107</td>\n",
       "      <td>85</td>\n",
       "      <td>46</td>\n",
       "      <td>195</td>\n",
       "      <td>82</td>\n",
       "      <td>21</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>10</td>\n",
       "      <td>114</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>54</td>\n",
       "      <td>172</td>\n",
       "      <td>328</td>\n",
       "      <td>120</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>48</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>89</td>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>29</td>\n",
       "      <td>91</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>442</td>\n",
       "      <td>178</td>\n",
       "      <td>235</td>\n",
       "      <td>928</td>\n",
       "      <td>406</td>\n",
       "      <td>70</td>\n",
       "      <td>122</td>\n",
       "      <td>62</td>\n",
       "      <td>52</td>\n",
       "      <td>31</td>\n",
       "      <td>552</td>\n",
       "      <td>141</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>76</td>\n",
       "      <td>125</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>45</td>\n",
       "      <td>75</td>\n",
       "      <td>245</td>\n",
       "      <td>98</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>56</td>\n",
       "      <td>106</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>86</td>\n",
       "      <td>14</td>\n",
       "      <td>200</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "      <td>195</td>\n",
       "      <td>289</td>\n",
       "      <td>1076</td>\n",
       "      <td>231</td>\n",
       "      <td>94</td>\n",
       "      <td>44</td>\n",
       "      <td>176</td>\n",
       "      <td>35</td>\n",
       "      <td>26</td>\n",
       "      <td>132</td>\n",
       "      <td>59</td>\n",
       "      <td>35</td>\n",
       "      <td>92</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>46</td>\n",
       "      <td>19</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>1821</td>\n",
       "      <td>988</td>\n",
       "      <td>614</td>\n",
       "      <td>2249</td>\n",
       "      <td>944</td>\n",
       "      <td>218</td>\n",
       "      <td>317</td>\n",
       "      <td>601</td>\n",
       "      <td>248</td>\n",
       "      <td>247</td>\n",
       "      <td>995</td>\n",
       "      <td>353</td>\n",
       "      <td>281</td>\n",
       "      <td>106</td>\n",
       "      <td>24</td>\n",
       "      <td>134</td>\n",
       "      <td>623</td>\n",
       "      <td>86</td>\n",
       "      <td>692</td>\n",
       "      <td>1</td>\n",
       "      <td>171</td>\n",
       "      <td>121</td>\n",
       "      <td>102</td>\n",
       "      <td>320</td>\n",
       "      <td>105</td>\n",
       "      <td>43</td>\n",
       "      <td>108</td>\n",
       "      <td>43</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>92</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>3982</td>\n",
       "      <td>1675</td>\n",
       "      <td>1599</td>\n",
       "      <td>6135</td>\n",
       "      <td>2015</td>\n",
       "      <td>543</td>\n",
       "      <td>631</td>\n",
       "      <td>933</td>\n",
       "      <td>694</td>\n",
       "      <td>365</td>\n",
       "      <td>1994</td>\n",
       "      <td>734</td>\n",
       "      <td>733</td>\n",
       "      <td>176</td>\n",
       "      <td>165</td>\n",
       "      <td>425</td>\n",
       "      <td>910</td>\n",
       "      <td>336</td>\n",
       "      <td>1485</td>\n",
       "      <td>14</td>\n",
       "      <td>142</td>\n",
       "      <td>55</td>\n",
       "      <td>151</td>\n",
       "      <td>580</td>\n",
       "      <td>140</td>\n",
       "      <td>28</td>\n",
       "      <td>48</td>\n",
       "      <td>45</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>20</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>677</td>\n",
       "      <td>376</td>\n",
       "      <td>474</td>\n",
       "      <td>1467</td>\n",
       "      <td>597</td>\n",
       "      <td>192</td>\n",
       "      <td>199</td>\n",
       "      <td>256</td>\n",
       "      <td>68</td>\n",
       "      <td>118</td>\n",
       "      <td>411</td>\n",
       "      <td>150</td>\n",
       "      <td>64</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>114</td>\n",
       "      <td>383</td>\n",
       "      <td>54</td>\n",
       "      <td>523</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>93</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2231</td>\n",
       "      <td>1254</td>\n",
       "      <td>974</td>\n",
       "      <td>3732</td>\n",
       "      <td>1138</td>\n",
       "      <td>320</td>\n",
       "      <td>386</td>\n",
       "      <td>552</td>\n",
       "      <td>307</td>\n",
       "      <td>221</td>\n",
       "      <td>1114</td>\n",
       "      <td>410</td>\n",
       "      <td>223</td>\n",
       "      <td>339</td>\n",
       "      <td>25</td>\n",
       "      <td>125</td>\n",
       "      <td>765</td>\n",
       "      <td>281</td>\n",
       "      <td>942</td>\n",
       "      <td>4</td>\n",
       "      <td>68</td>\n",
       "      <td>26</td>\n",
       "      <td>36</td>\n",
       "      <td>171</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>62</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>45</td>\n",
       "      <td>59</td>\n",
       "      <td>184</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>51</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>394</td>\n",
       "      <td>220</td>\n",
       "      <td>336</td>\n",
       "      <td>919</td>\n",
       "      <td>281</td>\n",
       "      <td>139</td>\n",
       "      <td>122</td>\n",
       "      <td>95</td>\n",
       "      <td>51</td>\n",
       "      <td>31</td>\n",
       "      <td>174</td>\n",
       "      <td>56</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>205</td>\n",
       "      <td>16</td>\n",
       "      <td>67</td>\n",
       "      <td>1</td>\n",
       "      <td>341</td>\n",
       "      <td>107</td>\n",
       "      <td>100</td>\n",
       "      <td>633</td>\n",
       "      <td>209</td>\n",
       "      <td>71</td>\n",
       "      <td>119</td>\n",
       "      <td>165</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>183</td>\n",
       "      <td>57</td>\n",
       "      <td>34</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>125</td>\n",
       "      <td>86</td>\n",
       "      <td>434</td>\n",
       "      <td>0</td>\n",
       "      <td>752</td>\n",
       "      <td>339</td>\n",
       "      <td>547</td>\n",
       "      <td>1835</td>\n",
       "      <td>728</td>\n",
       "      <td>176</td>\n",
       "      <td>192</td>\n",
       "      <td>216</td>\n",
       "      <td>117</td>\n",
       "      <td>100</td>\n",
       "      <td>381</td>\n",
       "      <td>169</td>\n",
       "      <td>109</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>49</td>\n",
       "      <td>301</td>\n",
       "      <td>67</td>\n",
       "      <td>304</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>355</td>\n",
       "      <td>182</td>\n",
       "      <td>153</td>\n",
       "      <td>998</td>\n",
       "      <td>265</td>\n",
       "      <td>48</td>\n",
       "      <td>92</td>\n",
       "      <td>166</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>226</td>\n",
       "      <td>75</td>\n",
       "      <td>72</td>\n",
       "      <td>150</td>\n",
       "      <td>52</td>\n",
       "      <td>97</td>\n",
       "      <td>309</td>\n",
       "      <td>50</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>383</td>\n",
       "      <td>124</td>\n",
       "      <td>246</td>\n",
       "      <td>713</td>\n",
       "      <td>145</td>\n",
       "      <td>40</td>\n",
       "      <td>95</td>\n",
       "      <td>80</td>\n",
       "      <td>66</td>\n",
       "      <td>13</td>\n",
       "      <td>148</td>\n",
       "      <td>59</td>\n",
       "      <td>31</td>\n",
       "      <td>34</td>\n",
       "      <td>57</td>\n",
       "      <td>6</td>\n",
       "      <td>141</td>\n",
       "      <td>66</td>\n",
       "      <td>204</td>\n",
       "      <td>2</td>\n",
       "      <td>1986</td>\n",
       "      <td>957</td>\n",
       "      <td>952</td>\n",
       "      <td>3510</td>\n",
       "      <td>1229</td>\n",
       "      <td>250</td>\n",
       "      <td>693</td>\n",
       "      <td>766</td>\n",
       "      <td>167</td>\n",
       "      <td>287</td>\n",
       "      <td>1340</td>\n",
       "      <td>510</td>\n",
       "      <td>1014</td>\n",
       "      <td>204</td>\n",
       "      <td>321</td>\n",
       "      <td>218</td>\n",
       "      <td>514</td>\n",
       "      <td>412</td>\n",
       "      <td>3004</td>\n",
       "      <td>25</td>\n",
       "      <td>771</td>\n",
       "      <td>336</td>\n",
       "      <td>727</td>\n",
       "      <td>2096</td>\n",
       "      <td>335</td>\n",
       "      <td>160</td>\n",
       "      <td>172</td>\n",
       "      <td>165</td>\n",
       "      <td>63</td>\n",
       "      <td>43</td>\n",
       "      <td>279</td>\n",
       "      <td>88</td>\n",
       "      <td>49</td>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>39</td>\n",
       "      <td>136</td>\n",
       "      <td>37</td>\n",
       "      <td>145</td>\n",
       "      <td>3</td>\n",
       "      <td>128</td>\n",
       "      <td>74</td>\n",
       "      <td>61</td>\n",
       "      <td>215</td>\n",
       "      <td>67</td>\n",
       "      <td>14</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>43</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>496</td>\n",
       "      <td>264</td>\n",
       "      <td>358</td>\n",
       "      <td>1117</td>\n",
       "      <td>278</td>\n",
       "      <td>128</td>\n",
       "      <td>98</td>\n",
       "      <td>162</td>\n",
       "      <td>128</td>\n",
       "      <td>54</td>\n",
       "      <td>305</td>\n",
       "      <td>84</td>\n",
       "      <td>47</td>\n",
       "      <td>31</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>504</td>\n",
       "      <td>72</td>\n",
       "      <td>238</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>109</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>64</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>62</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>198</td>\n",
       "      <td>79</td>\n",
       "      <td>114</td>\n",
       "      <td>193</td>\n",
       "      <td>75</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>105</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>97</td>\n",
       "      <td>12</td>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2190</td>\n",
       "      <td>815</td>\n",
       "      <td>1000</td>\n",
       "      <td>3727</td>\n",
       "      <td>1290</td>\n",
       "      <td>285</td>\n",
       "      <td>361</td>\n",
       "      <td>550</td>\n",
       "      <td>185</td>\n",
       "      <td>265</td>\n",
       "      <td>1469</td>\n",
       "      <td>480</td>\n",
       "      <td>1109</td>\n",
       "      <td>181</td>\n",
       "      <td>224</td>\n",
       "      <td>235</td>\n",
       "      <td>723</td>\n",
       "      <td>240</td>\n",
       "      <td>1970</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>134</td>\n",
       "      <td>115</td>\n",
       "      <td>430</td>\n",
       "      <td>170</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>27</td>\n",
       "      <td>25</td>\n",
       "      <td>99</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>92</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>22</td>\n",
       "      <td>140</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>72</td>\n",
       "      <td>97</td>\n",
       "      <td>291</td>\n",
       "      <td>83</td>\n",
       "      <td>30</td>\n",
       "      <td>63</td>\n",
       "      <td>22</td>\n",
       "      <td>27</td>\n",
       "      <td>30</td>\n",
       "      <td>86</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>106</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>454</td>\n",
       "      <td>243</td>\n",
       "      <td>118</td>\n",
       "      <td>765</td>\n",
       "      <td>222</td>\n",
       "      <td>100</td>\n",
       "      <td>138</td>\n",
       "      <td>180</td>\n",
       "      <td>52</td>\n",
       "      <td>35</td>\n",
       "      <td>319</td>\n",
       "      <td>89</td>\n",
       "      <td>60</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>240</td>\n",
       "      <td>63</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>8290</td>\n",
       "      <td>4441</td>\n",
       "      <td>4685</td>\n",
       "      <td>13545</td>\n",
       "      <td>4417</td>\n",
       "      <td>1217</td>\n",
       "      <td>1869</td>\n",
       "      <td>1859</td>\n",
       "      <td>1299</td>\n",
       "      <td>733</td>\n",
       "      <td>3437</td>\n",
       "      <td>1327</td>\n",
       "      <td>932</td>\n",
       "      <td>346</td>\n",
       "      <td>114</td>\n",
       "      <td>499</td>\n",
       "      <td>2323</td>\n",
       "      <td>848</td>\n",
       "      <td>2345</td>\n",
       "      <td>13</td>\n",
       "      <td>579</td>\n",
       "      <td>222</td>\n",
       "      <td>345</td>\n",
       "      <td>1039</td>\n",
       "      <td>340</td>\n",
       "      <td>120</td>\n",
       "      <td>121</td>\n",
       "      <td>128</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>231</td>\n",
       "      <td>155</td>\n",
       "      <td>73</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>65</td>\n",
       "      <td>158</td>\n",
       "      <td>55</td>\n",
       "      <td>381</td>\n",
       "      <td>3</td>\n",
       "      <td>60048</td>\n",
       "      <td>31165</td>\n",
       "      <td>28045</td>\n",
       "      <td>90734</td>\n",
       "      <td>30159</td>\n",
       "      <td>8178</td>\n",
       "      <td>16976</td>\n",
       "      <td>15079</td>\n",
       "      <td>8982</td>\n",
       "      <td>4148</td>\n",
       "      <td>26144</td>\n",
       "      <td>10801</td>\n",
       "      <td>5161</td>\n",
       "      <td>3145</td>\n",
       "      <td>671</td>\n",
       "      <td>3376</td>\n",
       "      <td>21459</td>\n",
       "      <td>7305</td>\n",
       "      <td>22013</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>344</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>90</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>32</td>\n",
       "      <td>43</td>\n",
       "      <td>312</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>312</td>\n",
       "      <td>167</td>\n",
       "      <td>89</td>\n",
       "      <td>320</td>\n",
       "      <td>107</td>\n",
       "      <td>58</td>\n",
       "      <td>62</td>\n",
       "      <td>65</td>\n",
       "      <td>46</td>\n",
       "      <td>42</td>\n",
       "      <td>263</td>\n",
       "      <td>90</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>5</td>\n",
       "      <td>56</td>\n",
       "      <td>166</td>\n",
       "      <td>40</td>\n",
       "      <td>345</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>602</td>\n",
       "      <td>316</td>\n",
       "      <td>592</td>\n",
       "      <td>1605</td>\n",
       "      <td>549</td>\n",
       "      <td>209</td>\n",
       "      <td>217</td>\n",
       "      <td>151</td>\n",
       "      <td>79</td>\n",
       "      <td>52</td>\n",
       "      <td>277</td>\n",
       "      <td>92</td>\n",
       "      <td>56</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>290</td>\n",
       "      <td>51</td>\n",
       "      <td>264</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>247</td>\n",
       "      <td>86</td>\n",
       "      <td>85</td>\n",
       "      <td>298</td>\n",
       "      <td>100</td>\n",
       "      <td>32</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>16</td>\n",
       "      <td>62</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>171</td>\n",
       "      <td>173</td>\n",
       "      <td>554</td>\n",
       "      <td>172</td>\n",
       "      <td>78</td>\n",
       "      <td>264</td>\n",
       "      <td>80</td>\n",
       "      <td>56</td>\n",
       "      <td>35</td>\n",
       "      <td>182</td>\n",
       "      <td>78</td>\n",
       "      <td>40</td>\n",
       "      <td>57</td>\n",
       "      <td>5</td>\n",
       "      <td>67</td>\n",
       "      <td>193</td>\n",
       "      <td>68</td>\n",
       "      <td>463</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>43</td>\n",
       "      <td>23</td>\n",
       "      <td>162</td>\n",
       "      <td>35</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>223</td>\n",
       "      <td>200</td>\n",
       "      <td>74</td>\n",
       "      <td>455</td>\n",
       "      <td>179</td>\n",
       "      <td>129</td>\n",
       "      <td>138</td>\n",
       "      <td>92</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>171</td>\n",
       "      <td>53</td>\n",
       "      <td>64</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>148</td>\n",
       "      <td>73</td>\n",
       "      <td>420</td>\n",
       "      <td>3</td>\n",
       "      <td>273</td>\n",
       "      <td>142</td>\n",
       "      <td>179</td>\n",
       "      <td>412</td>\n",
       "      <td>173</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>66</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>83</td>\n",
       "      <td>28</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>903</td>\n",
       "      <td>351</td>\n",
       "      <td>351</td>\n",
       "      <td>1306</td>\n",
       "      <td>377</td>\n",
       "      <td>131</td>\n",
       "      <td>157</td>\n",
       "      <td>198</td>\n",
       "      <td>90</td>\n",
       "      <td>107</td>\n",
       "      <td>356</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>18</td>\n",
       "      <td>45</td>\n",
       "      <td>210</td>\n",
       "      <td>87</td>\n",
       "      <td>192</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-22/2018-01-28</th>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>516</td>\n",
       "      <td>201</td>\n",
       "      <td>710</td>\n",
       "      <td>1301</td>\n",
       "      <td>444</td>\n",
       "      <td>135</td>\n",
       "      <td>156</td>\n",
       "      <td>98</td>\n",
       "      <td>93</td>\n",
       "      <td>59</td>\n",
       "      <td>270</td>\n",
       "      <td>62</td>\n",
       "      <td>33</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>153</td>\n",
       "      <td>32</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>1801</td>\n",
       "      <td>532</td>\n",
       "      <td>377</td>\n",
       "      <td>1778</td>\n",
       "      <td>586</td>\n",
       "      <td>272</td>\n",
       "      <td>374</td>\n",
       "      <td>450</td>\n",
       "      <td>154</td>\n",
       "      <td>128</td>\n",
       "      <td>1141</td>\n",
       "      <td>198</td>\n",
       "      <td>256</td>\n",
       "      <td>62</td>\n",
       "      <td>98</td>\n",
       "      <td>166</td>\n",
       "      <td>306</td>\n",
       "      <td>556</td>\n",
       "      <td>2571</td>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>31</td>\n",
       "      <td>52</td>\n",
       "      <td>239</td>\n",
       "      <td>78</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>21</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>84</td>\n",
       "      <td>198</td>\n",
       "      <td>838</td>\n",
       "      <td>242</td>\n",
       "      <td>55</td>\n",
       "      <td>44</td>\n",
       "      <td>46</td>\n",
       "      <td>55</td>\n",
       "      <td>16</td>\n",
       "      <td>94</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>98</td>\n",
       "      <td>12</td>\n",
       "      <td>69</td>\n",
       "      <td>8</td>\n",
       "      <td>119</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>224</td>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>55</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>324</td>\n",
       "      <td>123</td>\n",
       "      <td>265</td>\n",
       "      <td>1036</td>\n",
       "      <td>285</td>\n",
       "      <td>69</td>\n",
       "      <td>60</td>\n",
       "      <td>58</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "      <td>92</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>31</td>\n",
       "      <td>16</td>\n",
       "      <td>69</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>49</td>\n",
       "      <td>69</td>\n",
       "      <td>261</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>102</td>\n",
       "      <td>143</td>\n",
       "      <td>513</td>\n",
       "      <td>134</td>\n",
       "      <td>53</td>\n",
       "      <td>31</td>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>5008</td>\n",
       "      <td>2264</td>\n",
       "      <td>2227</td>\n",
       "      <td>7515</td>\n",
       "      <td>3170</td>\n",
       "      <td>723</td>\n",
       "      <td>1078</td>\n",
       "      <td>900</td>\n",
       "      <td>600</td>\n",
       "      <td>261</td>\n",
       "      <td>1674</td>\n",
       "      <td>605</td>\n",
       "      <td>434</td>\n",
       "      <td>215</td>\n",
       "      <td>43</td>\n",
       "      <td>158</td>\n",
       "      <td>1426</td>\n",
       "      <td>369</td>\n",
       "      <td>1570</td>\n",
       "      <td>14</td>\n",
       "      <td>380</td>\n",
       "      <td>135</td>\n",
       "      <td>202</td>\n",
       "      <td>648</td>\n",
       "      <td>245</td>\n",
       "      <td>32</td>\n",
       "      <td>62</td>\n",
       "      <td>77</td>\n",
       "      <td>46</td>\n",
       "      <td>47</td>\n",
       "      <td>140</td>\n",
       "      <td>80</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>67</td>\n",
       "      <td>21</td>\n",
       "      <td>85</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>83</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>53</td>\n",
       "      <td>43</td>\n",
       "      <td>195</td>\n",
       "      <td>127</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>99</td>\n",
       "      <td>29</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>72</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>118</td>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>129</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>53</td>\n",
       "      <td>30</td>\n",
       "      <td>201</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>48</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>113</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>561</td>\n",
       "      <td>341</td>\n",
       "      <td>429</td>\n",
       "      <td>1672</td>\n",
       "      <td>285</td>\n",
       "      <td>92</td>\n",
       "      <td>161</td>\n",
       "      <td>180</td>\n",
       "      <td>37</td>\n",
       "      <td>73</td>\n",
       "      <td>335</td>\n",
       "      <td>105</td>\n",
       "      <td>103</td>\n",
       "      <td>44</td>\n",
       "      <td>22</td>\n",
       "      <td>70</td>\n",
       "      <td>231</td>\n",
       "      <td>29</td>\n",
       "      <td>197</td>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "      <td>82</td>\n",
       "      <td>76</td>\n",
       "      <td>315</td>\n",
       "      <td>79</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>63</td>\n",
       "      <td>23</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>88</td>\n",
       "      <td>46</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>1185</td>\n",
       "      <td>571</td>\n",
       "      <td>637</td>\n",
       "      <td>2049</td>\n",
       "      <td>481</td>\n",
       "      <td>138</td>\n",
       "      <td>260</td>\n",
       "      <td>540</td>\n",
       "      <td>103</td>\n",
       "      <td>148</td>\n",
       "      <td>513</td>\n",
       "      <td>129</td>\n",
       "      <td>77</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>55</td>\n",
       "      <td>381</td>\n",
       "      <td>198</td>\n",
       "      <td>400</td>\n",
       "      <td>107</td>\n",
       "      <td>123</td>\n",
       "      <td>58</td>\n",
       "      <td>37</td>\n",
       "      <td>209</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>45</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>69</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>149</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>408</td>\n",
       "      <td>146</td>\n",
       "      <td>160</td>\n",
       "      <td>570</td>\n",
       "      <td>142</td>\n",
       "      <td>32</td>\n",
       "      <td>37</td>\n",
       "      <td>217</td>\n",
       "      <td>36</td>\n",
       "      <td>58</td>\n",
       "      <td>162</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>119</td>\n",
       "      <td>23</td>\n",
       "      <td>207</td>\n",
       "      <td>71</td>\n",
       "      <td>102</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>127</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>67</td>\n",
       "      <td>140</td>\n",
       "      <td>400</td>\n",
       "      <td>166</td>\n",
       "      <td>53</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>44</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>547</td>\n",
       "      <td>289</td>\n",
       "      <td>220</td>\n",
       "      <td>859</td>\n",
       "      <td>194</td>\n",
       "      <td>106</td>\n",
       "      <td>119</td>\n",
       "      <td>104</td>\n",
       "      <td>74</td>\n",
       "      <td>33</td>\n",
       "      <td>211</td>\n",
       "      <td>77</td>\n",
       "      <td>47</td>\n",
       "      <td>38</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>411</td>\n",
       "      <td>51</td>\n",
       "      <td>310</td>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>62</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>306</td>\n",
       "      <td>156</td>\n",
       "      <td>126</td>\n",
       "      <td>411</td>\n",
       "      <td>170</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>33</td>\n",
       "      <td>89</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>49</td>\n",
       "      <td>233</td>\n",
       "      <td>500</td>\n",
       "      <td>318</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>5658</td>\n",
       "      <td>2946</td>\n",
       "      <td>2930</td>\n",
       "      <td>9058</td>\n",
       "      <td>2997</td>\n",
       "      <td>1014</td>\n",
       "      <td>1445</td>\n",
       "      <td>1760</td>\n",
       "      <td>856</td>\n",
       "      <td>487</td>\n",
       "      <td>2037</td>\n",
       "      <td>967</td>\n",
       "      <td>537</td>\n",
       "      <td>202</td>\n",
       "      <td>45</td>\n",
       "      <td>206</td>\n",
       "      <td>1393</td>\n",
       "      <td>483</td>\n",
       "      <td>1769</td>\n",
       "      <td>6</td>\n",
       "      <td>281</td>\n",
       "      <td>111</td>\n",
       "      <td>215</td>\n",
       "      <td>641</td>\n",
       "      <td>211</td>\n",
       "      <td>83</td>\n",
       "      <td>34</td>\n",
       "      <td>71</td>\n",
       "      <td>38</td>\n",
       "      <td>45</td>\n",
       "      <td>177</td>\n",
       "      <td>40</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>254</td>\n",
       "      <td>12</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>122</td>\n",
       "      <td>50</td>\n",
       "      <td>47</td>\n",
       "      <td>262</td>\n",
       "      <td>98</td>\n",
       "      <td>33</td>\n",
       "      <td>52</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>59</td>\n",
       "      <td>19</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>308</td>\n",
       "      <td>233</td>\n",
       "      <td>238</td>\n",
       "      <td>902</td>\n",
       "      <td>277</td>\n",
       "      <td>148</td>\n",
       "      <td>71</td>\n",
       "      <td>137</td>\n",
       "      <td>65</td>\n",
       "      <td>38</td>\n",
       "      <td>167</td>\n",
       "      <td>66</td>\n",
       "      <td>24</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>155</td>\n",
       "      <td>46</td>\n",
       "      <td>95</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>100</td>\n",
       "      <td>42</td>\n",
       "      <td>123</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>29</td>\n",
       "      <td>73</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>94</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>77</td>\n",
       "      <td>14</td>\n",
       "      <td>114</td>\n",
       "      <td>3</td>\n",
       "      <td>77</td>\n",
       "      <td>94</td>\n",
       "      <td>17</td>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>57</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>4518</td>\n",
       "      <td>2072</td>\n",
       "      <td>2411</td>\n",
       "      <td>6837</td>\n",
       "      <td>2695</td>\n",
       "      <td>1255</td>\n",
       "      <td>963</td>\n",
       "      <td>1045</td>\n",
       "      <td>591</td>\n",
       "      <td>378</td>\n",
       "      <td>2119</td>\n",
       "      <td>685</td>\n",
       "      <td>441</td>\n",
       "      <td>148</td>\n",
       "      <td>110</td>\n",
       "      <td>276</td>\n",
       "      <td>1194</td>\n",
       "      <td>173</td>\n",
       "      <td>829</td>\n",
       "      <td>10</td>\n",
       "      <td>449</td>\n",
       "      <td>143</td>\n",
       "      <td>200</td>\n",
       "      <td>1109</td>\n",
       "      <td>664</td>\n",
       "      <td>55</td>\n",
       "      <td>68</td>\n",
       "      <td>88</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>334</td>\n",
       "      <td>71</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>114</td>\n",
       "      <td>19</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>63</td>\n",
       "      <td>48</td>\n",
       "      <td>228</td>\n",
       "      <td>62</td>\n",
       "      <td>13</td>\n",
       "      <td>36</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>75</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>84</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>215</td>\n",
       "      <td>121</td>\n",
       "      <td>113</td>\n",
       "      <td>332</td>\n",
       "      <td>119</td>\n",
       "      <td>47</td>\n",
       "      <td>46</td>\n",
       "      <td>67</td>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "      <td>136</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>103</td>\n",
       "      <td>38</td>\n",
       "      <td>179</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>28</td>\n",
       "      <td>42</td>\n",
       "      <td>179</td>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>65</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>236</td>\n",
       "      <td>98</td>\n",
       "      <td>108</td>\n",
       "      <td>580</td>\n",
       "      <td>204</td>\n",
       "      <td>46</td>\n",
       "      <td>59</td>\n",
       "      <td>82</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>86</td>\n",
       "      <td>51</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>116</td>\n",
       "      <td>14</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>136</td>\n",
       "      <td>95</td>\n",
       "      <td>61</td>\n",
       "      <td>188</td>\n",
       "      <td>100</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>19</td>\n",
       "      <td>23</td>\n",
       "      <td>110</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>229</td>\n",
       "      <td>178</td>\n",
       "      <td>176</td>\n",
       "      <td>625</td>\n",
       "      <td>138</td>\n",
       "      <td>26</td>\n",
       "      <td>60</td>\n",
       "      <td>90</td>\n",
       "      <td>22</td>\n",
       "      <td>33</td>\n",
       "      <td>116</td>\n",
       "      <td>72</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>113</td>\n",
       "      <td>33</td>\n",
       "      <td>114</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>159</td>\n",
       "      <td>22</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>134</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>179</td>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>222</td>\n",
       "      <td>59</td>\n",
       "      <td>25</td>\n",
       "      <td>65</td>\n",
       "      <td>62</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>49</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>66</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>917</td>\n",
       "      <td>434</td>\n",
       "      <td>499</td>\n",
       "      <td>2320</td>\n",
       "      <td>651</td>\n",
       "      <td>152</td>\n",
       "      <td>167</td>\n",
       "      <td>302</td>\n",
       "      <td>108</td>\n",
       "      <td>102</td>\n",
       "      <td>587</td>\n",
       "      <td>184</td>\n",
       "      <td>93</td>\n",
       "      <td>82</td>\n",
       "      <td>16</td>\n",
       "      <td>121</td>\n",
       "      <td>513</td>\n",
       "      <td>174</td>\n",
       "      <td>418</td>\n",
       "      <td>2</td>\n",
       "      <td>1679</td>\n",
       "      <td>740</td>\n",
       "      <td>834</td>\n",
       "      <td>2469</td>\n",
       "      <td>811</td>\n",
       "      <td>198</td>\n",
       "      <td>334</td>\n",
       "      <td>277</td>\n",
       "      <td>198</td>\n",
       "      <td>111</td>\n",
       "      <td>453</td>\n",
       "      <td>224</td>\n",
       "      <td>140</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>365</td>\n",
       "      <td>122</td>\n",
       "      <td>322</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>121</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>190</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>44</td>\n",
       "      <td>6</td>\n",
       "      <td>88</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>110</td>\n",
       "      <td>66</td>\n",
       "      <td>40</td>\n",
       "      <td>249</td>\n",
       "      <td>63</td>\n",
       "      <td>27</td>\n",
       "      <td>24</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>72</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>61</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>359</td>\n",
       "      <td>209</td>\n",
       "      <td>305</td>\n",
       "      <td>1342</td>\n",
       "      <td>272</td>\n",
       "      <td>28</td>\n",
       "      <td>84</td>\n",
       "      <td>162</td>\n",
       "      <td>29</td>\n",
       "      <td>44</td>\n",
       "      <td>191</td>\n",
       "      <td>73</td>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>148</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>349</td>\n",
       "      <td>163</td>\n",
       "      <td>163</td>\n",
       "      <td>476</td>\n",
       "      <td>229</td>\n",
       "      <td>50</td>\n",
       "      <td>64</td>\n",
       "      <td>114</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>168</td>\n",
       "      <td>115</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>47</td>\n",
       "      <td>7</td>\n",
       "      <td>84</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>157</td>\n",
       "      <td>92</td>\n",
       "      <td>95</td>\n",
       "      <td>263</td>\n",
       "      <td>141</td>\n",
       "      <td>29</td>\n",
       "      <td>46</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>70</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>180</td>\n",
       "      <td>89</td>\n",
       "      <td>89</td>\n",
       "      <td>438</td>\n",
       "      <td>173</td>\n",
       "      <td>91</td>\n",
       "      <td>64</td>\n",
       "      <td>51</td>\n",
       "      <td>66</td>\n",
       "      <td>24</td>\n",
       "      <td>54</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2300</td>\n",
       "      <td>1165</td>\n",
       "      <td>1403</td>\n",
       "      <td>4864</td>\n",
       "      <td>1373</td>\n",
       "      <td>293</td>\n",
       "      <td>465</td>\n",
       "      <td>553</td>\n",
       "      <td>252</td>\n",
       "      <td>173</td>\n",
       "      <td>1072</td>\n",
       "      <td>397</td>\n",
       "      <td>305</td>\n",
       "      <td>159</td>\n",
       "      <td>51</td>\n",
       "      <td>212</td>\n",
       "      <td>656</td>\n",
       "      <td>236</td>\n",
       "      <td>869</td>\n",
       "      <td>3</td>\n",
       "      <td>163</td>\n",
       "      <td>67</td>\n",
       "      <td>45</td>\n",
       "      <td>256</td>\n",
       "      <td>76</td>\n",
       "      <td>12</td>\n",
       "      <td>51</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>85</td>\n",
       "      <td>17</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>26</td>\n",
       "      <td>50</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>214</td>\n",
       "      <td>69</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>31</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>14</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>1232</td>\n",
       "      <td>604</td>\n",
       "      <td>414</td>\n",
       "      <td>1543</td>\n",
       "      <td>658</td>\n",
       "      <td>128</td>\n",
       "      <td>251</td>\n",
       "      <td>190</td>\n",
       "      <td>129</td>\n",
       "      <td>100</td>\n",
       "      <td>459</td>\n",
       "      <td>149</td>\n",
       "      <td>84</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>36</td>\n",
       "      <td>405</td>\n",
       "      <td>69</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>93</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>70</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>1974</td>\n",
       "      <td>865</td>\n",
       "      <td>986</td>\n",
       "      <td>3307</td>\n",
       "      <td>1140</td>\n",
       "      <td>328</td>\n",
       "      <td>402</td>\n",
       "      <td>499</td>\n",
       "      <td>231</td>\n",
       "      <td>158</td>\n",
       "      <td>880</td>\n",
       "      <td>340</td>\n",
       "      <td>212</td>\n",
       "      <td>154</td>\n",
       "      <td>28</td>\n",
       "      <td>120</td>\n",
       "      <td>720</td>\n",
       "      <td>170</td>\n",
       "      <td>854</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>44</td>\n",
       "      <td>26</td>\n",
       "      <td>90</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>546</td>\n",
       "      <td>307</td>\n",
       "      <td>483</td>\n",
       "      <td>1209</td>\n",
       "      <td>311</td>\n",
       "      <td>89</td>\n",
       "      <td>100</td>\n",
       "      <td>119</td>\n",
       "      <td>40</td>\n",
       "      <td>63</td>\n",
       "      <td>221</td>\n",
       "      <td>189</td>\n",
       "      <td>38</td>\n",
       "      <td>99</td>\n",
       "      <td>9</td>\n",
       "      <td>33</td>\n",
       "      <td>174</td>\n",
       "      <td>29</td>\n",
       "      <td>152</td>\n",
       "      <td>7</td>\n",
       "      <td>82</td>\n",
       "      <td>43</td>\n",
       "      <td>24</td>\n",
       "      <td>160</td>\n",
       "      <td>45</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>73</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>87</td>\n",
       "      <td>20</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "      <td>67</td>\n",
       "      <td>180</td>\n",
       "      <td>49</td>\n",
       "      <td>9</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>168</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>250</td>\n",
       "      <td>45</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>37</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>60</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>62</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>95</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>377</td>\n",
       "      <td>144</td>\n",
       "      <td>139</td>\n",
       "      <td>647</td>\n",
       "      <td>158</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>50</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>247</td>\n",
       "      <td>64</td>\n",
       "      <td>22</td>\n",
       "      <td>90</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>65</td>\n",
       "      <td>22</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>145</td>\n",
       "      <td>117</td>\n",
       "      <td>339</td>\n",
       "      <td>153</td>\n",
       "      <td>34</td>\n",
       "      <td>51</td>\n",
       "      <td>75</td>\n",
       "      <td>46</td>\n",
       "      <td>35</td>\n",
       "      <td>107</td>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>123</td>\n",
       "      <td>22</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>175</td>\n",
       "      <td>78</td>\n",
       "      <td>32</td>\n",
       "      <td>210</td>\n",
       "      <td>66</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>120</td>\n",
       "      <td>19</td>\n",
       "      <td>40</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>106</td>\n",
       "      <td>21</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>184</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>152</td>\n",
       "      <td>135</td>\n",
       "      <td>96</td>\n",
       "      <td>372</td>\n",
       "      <td>162</td>\n",
       "      <td>17</td>\n",
       "      <td>60</td>\n",
       "      <td>53</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>117</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>50</td>\n",
       "      <td>65</td>\n",
       "      <td>167</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>979</td>\n",
       "      <td>468</td>\n",
       "      <td>618</td>\n",
       "      <td>2665</td>\n",
       "      <td>658</td>\n",
       "      <td>230</td>\n",
       "      <td>222</td>\n",
       "      <td>159</td>\n",
       "      <td>104</td>\n",
       "      <td>89</td>\n",
       "      <td>253</td>\n",
       "      <td>102</td>\n",
       "      <td>72</td>\n",
       "      <td>54</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>400</td>\n",
       "      <td>78</td>\n",
       "      <td>259</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7080</td>\n",
       "      <td>3882</td>\n",
       "      <td>3980</td>\n",
       "      <td>13414</td>\n",
       "      <td>4152</td>\n",
       "      <td>829</td>\n",
       "      <td>1233</td>\n",
       "      <td>2465</td>\n",
       "      <td>1062</td>\n",
       "      <td>904</td>\n",
       "      <td>4129</td>\n",
       "      <td>1528</td>\n",
       "      <td>738</td>\n",
       "      <td>1658</td>\n",
       "      <td>417</td>\n",
       "      <td>376</td>\n",
       "      <td>3067</td>\n",
       "      <td>1270</td>\n",
       "      <td>2804</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1419</td>\n",
       "      <td>605</td>\n",
       "      <td>845</td>\n",
       "      <td>2300</td>\n",
       "      <td>1249</td>\n",
       "      <td>292</td>\n",
       "      <td>255</td>\n",
       "      <td>488</td>\n",
       "      <td>159</td>\n",
       "      <td>130</td>\n",
       "      <td>913</td>\n",
       "      <td>345</td>\n",
       "      <td>411</td>\n",
       "      <td>192</td>\n",
       "      <td>80</td>\n",
       "      <td>186</td>\n",
       "      <td>500</td>\n",
       "      <td>132</td>\n",
       "      <td>654</td>\n",
       "      <td>3</td>\n",
       "      <td>3672</td>\n",
       "      <td>1805</td>\n",
       "      <td>2270</td>\n",
       "      <td>8022</td>\n",
       "      <td>2620</td>\n",
       "      <td>345</td>\n",
       "      <td>949</td>\n",
       "      <td>837</td>\n",
       "      <td>327</td>\n",
       "      <td>351</td>\n",
       "      <td>2283</td>\n",
       "      <td>993</td>\n",
       "      <td>601</td>\n",
       "      <td>506</td>\n",
       "      <td>66</td>\n",
       "      <td>708</td>\n",
       "      <td>1212</td>\n",
       "      <td>442</td>\n",
       "      <td>1661</td>\n",
       "      <td>50</td>\n",
       "      <td>1040</td>\n",
       "      <td>451</td>\n",
       "      <td>545</td>\n",
       "      <td>1950</td>\n",
       "      <td>600</td>\n",
       "      <td>154</td>\n",
       "      <td>247</td>\n",
       "      <td>208</td>\n",
       "      <td>109</td>\n",
       "      <td>86</td>\n",
       "      <td>492</td>\n",
       "      <td>162</td>\n",
       "      <td>86</td>\n",
       "      <td>53</td>\n",
       "      <td>22</td>\n",
       "      <td>58</td>\n",
       "      <td>326</td>\n",
       "      <td>89</td>\n",
       "      <td>544</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>19</td>\n",
       "      <td>71</td>\n",
       "      <td>89</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>778</td>\n",
       "      <td>340</td>\n",
       "      <td>331</td>\n",
       "      <td>1582</td>\n",
       "      <td>444</td>\n",
       "      <td>138</td>\n",
       "      <td>211</td>\n",
       "      <td>234</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>307</td>\n",
       "      <td>128</td>\n",
       "      <td>149</td>\n",
       "      <td>26</td>\n",
       "      <td>55</td>\n",
       "      <td>77</td>\n",
       "      <td>312</td>\n",
       "      <td>152</td>\n",
       "      <td>809</td>\n",
       "      <td>14</td>\n",
       "      <td>1460</td>\n",
       "      <td>629</td>\n",
       "      <td>961</td>\n",
       "      <td>2769</td>\n",
       "      <td>956</td>\n",
       "      <td>441</td>\n",
       "      <td>422</td>\n",
       "      <td>361</td>\n",
       "      <td>211</td>\n",
       "      <td>79</td>\n",
       "      <td>541</td>\n",
       "      <td>220</td>\n",
       "      <td>149</td>\n",
       "      <td>52</td>\n",
       "      <td>73</td>\n",
       "      <td>115</td>\n",
       "      <td>347</td>\n",
       "      <td>121</td>\n",
       "      <td>459</td>\n",
       "      <td>0</td>\n",
       "      <td>217</td>\n",
       "      <td>122</td>\n",
       "      <td>98</td>\n",
       "      <td>333</td>\n",
       "      <td>78</td>\n",
       "      <td>36</td>\n",
       "      <td>62</td>\n",
       "      <td>58</td>\n",
       "      <td>32</td>\n",
       "      <td>9</td>\n",
       "      <td>87</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>97</td>\n",
       "      <td>27</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>390</td>\n",
       "      <td>155</td>\n",
       "      <td>159</td>\n",
       "      <td>539</td>\n",
       "      <td>208</td>\n",
       "      <td>49</td>\n",
       "      <td>61</td>\n",
       "      <td>49</td>\n",
       "      <td>41</td>\n",
       "      <td>18</td>\n",
       "      <td>95</td>\n",
       "      <td>26</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>119</td>\n",
       "      <td>41</td>\n",
       "      <td>170</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>438</td>\n",
       "      <td>248</td>\n",
       "      <td>296</td>\n",
       "      <td>1379</td>\n",
       "      <td>374</td>\n",
       "      <td>58</td>\n",
       "      <td>106</td>\n",
       "      <td>87</td>\n",
       "      <td>32</td>\n",
       "      <td>50</td>\n",
       "      <td>178</td>\n",
       "      <td>81</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>43</td>\n",
       "      <td>95</td>\n",
       "      <td>40</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>864</td>\n",
       "      <td>519</td>\n",
       "      <td>405</td>\n",
       "      <td>1511</td>\n",
       "      <td>425</td>\n",
       "      <td>139</td>\n",
       "      <td>227</td>\n",
       "      <td>226</td>\n",
       "      <td>118</td>\n",
       "      <td>157</td>\n",
       "      <td>553</td>\n",
       "      <td>178</td>\n",
       "      <td>112</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>70</td>\n",
       "      <td>293</td>\n",
       "      <td>109</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>109</td>\n",
       "      <td>59</td>\n",
       "      <td>37</td>\n",
       "      <td>219</td>\n",
       "      <td>74</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>664</td>\n",
       "      <td>369</td>\n",
       "      <td>316</td>\n",
       "      <td>997</td>\n",
       "      <td>323</td>\n",
       "      <td>92</td>\n",
       "      <td>120</td>\n",
       "      <td>183</td>\n",
       "      <td>122</td>\n",
       "      <td>72</td>\n",
       "      <td>310</td>\n",
       "      <td>159</td>\n",
       "      <td>250</td>\n",
       "      <td>23</td>\n",
       "      <td>54</td>\n",
       "      <td>235</td>\n",
       "      <td>163</td>\n",
       "      <td>70</td>\n",
       "      <td>294</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>804</td>\n",
       "      <td>414</td>\n",
       "      <td>501</td>\n",
       "      <td>1380</td>\n",
       "      <td>356</td>\n",
       "      <td>227</td>\n",
       "      <td>194</td>\n",
       "      <td>133</td>\n",
       "      <td>163</td>\n",
       "      <td>56</td>\n",
       "      <td>451</td>\n",
       "      <td>126</td>\n",
       "      <td>132</td>\n",
       "      <td>28</td>\n",
       "      <td>24</td>\n",
       "      <td>107</td>\n",
       "      <td>270</td>\n",
       "      <td>138</td>\n",
       "      <td>332</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>125</td>\n",
       "      <td>136</td>\n",
       "      <td>612</td>\n",
       "      <td>177</td>\n",
       "      <td>22</td>\n",
       "      <td>61</td>\n",
       "      <td>60</td>\n",
       "      <td>27</td>\n",
       "      <td>39</td>\n",
       "      <td>70</td>\n",
       "      <td>26</td>\n",
       "      <td>20</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>145</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>21</td>\n",
       "      <td>42</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>64</td>\n",
       "      <td>76</td>\n",
       "      <td>410</td>\n",
       "      <td>98</td>\n",
       "      <td>39</td>\n",
       "      <td>44</td>\n",
       "      <td>38</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>414</td>\n",
       "      <td>86</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>514</td>\n",
       "      <td>249</td>\n",
       "      <td>141</td>\n",
       "      <td>1187</td>\n",
       "      <td>295</td>\n",
       "      <td>110</td>\n",
       "      <td>147</td>\n",
       "      <td>98</td>\n",
       "      <td>106</td>\n",
       "      <td>39</td>\n",
       "      <td>242</td>\n",
       "      <td>46</td>\n",
       "      <td>91</td>\n",
       "      <td>61</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "      <td>282</td>\n",
       "      <td>76</td>\n",
       "      <td>310</td>\n",
       "      <td>7</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>135</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>31</td>\n",
       "      <td>33</td>\n",
       "      <td>141</td>\n",
       "      <td>64</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>249</td>\n",
       "      <td>112</td>\n",
       "      <td>126</td>\n",
       "      <td>338</td>\n",
       "      <td>158</td>\n",
       "      <td>64</td>\n",
       "      <td>28</td>\n",
       "      <td>32</td>\n",
       "      <td>17</td>\n",
       "      <td>43</td>\n",
       "      <td>62</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>76</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>123</td>\n",
       "      <td>51</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "      <td>68</td>\n",
       "      <td>148</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>40</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>240</td>\n",
       "      <td>116</td>\n",
       "      <td>83</td>\n",
       "      <td>422</td>\n",
       "      <td>134</td>\n",
       "      <td>41</td>\n",
       "      <td>98</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>47</td>\n",
       "      <td>217</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>18</td>\n",
       "      <td>101</td>\n",
       "      <td>147</td>\n",
       "      <td>307</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>19</td>\n",
       "      <td>12</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>125</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>26</td>\n",
       "      <td>16</td>\n",
       "      <td>216</td>\n",
       "      <td>60</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>265</td>\n",
       "      <td>125</td>\n",
       "      <td>74</td>\n",
       "      <td>282</td>\n",
       "      <td>71</td>\n",
       "      <td>30</td>\n",
       "      <td>53</td>\n",
       "      <td>37</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>82</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>115</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>81</td>\n",
       "      <td>113</td>\n",
       "      <td>430</td>\n",
       "      <td>65</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>70</td>\n",
       "      <td>84</td>\n",
       "      <td>17</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>26</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>174</td>\n",
       "      <td>26</td>\n",
       "      <td>55</td>\n",
       "      <td>103</td>\n",
       "      <td>96</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>68</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>19</td>\n",
       "      <td>230</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>72</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>131</td>\n",
       "      <td>88</td>\n",
       "      <td>57</td>\n",
       "      <td>493</td>\n",
       "      <td>163</td>\n",
       "      <td>54</td>\n",
       "      <td>27</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>65</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "      <td>23</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>155</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>77</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>192</td>\n",
       "      <td>141</td>\n",
       "      <td>121</td>\n",
       "      <td>290</td>\n",
       "      <td>145</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>56</td>\n",
       "      <td>28</td>\n",
       "      <td>27</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>126</td>\n",
       "      <td>62</td>\n",
       "      <td>256</td>\n",
       "      <td>541</td>\n",
       "      <td>118</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>135</td>\n",
       "      <td>41</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1036</td>\n",
       "      <td>573</td>\n",
       "      <td>771</td>\n",
       "      <td>2231</td>\n",
       "      <td>569</td>\n",
       "      <td>311</td>\n",
       "      <td>305</td>\n",
       "      <td>354</td>\n",
       "      <td>162</td>\n",
       "      <td>69</td>\n",
       "      <td>553</td>\n",
       "      <td>265</td>\n",
       "      <td>128</td>\n",
       "      <td>44</td>\n",
       "      <td>17</td>\n",
       "      <td>87</td>\n",
       "      <td>460</td>\n",
       "      <td>87</td>\n",
       "      <td>533</td>\n",
       "      <td>0</td>\n",
       "      <td>1120</td>\n",
       "      <td>511</td>\n",
       "      <td>430</td>\n",
       "      <td>1606</td>\n",
       "      <td>478</td>\n",
       "      <td>215</td>\n",
       "      <td>214</td>\n",
       "      <td>197</td>\n",
       "      <td>197</td>\n",
       "      <td>128</td>\n",
       "      <td>568</td>\n",
       "      <td>172</td>\n",
       "      <td>86</td>\n",
       "      <td>29</td>\n",
       "      <td>11</td>\n",
       "      <td>37</td>\n",
       "      <td>355</td>\n",
       "      <td>76</td>\n",
       "      <td>195</td>\n",
       "      <td>5</td>\n",
       "      <td>71</td>\n",
       "      <td>35</td>\n",
       "      <td>43</td>\n",
       "      <td>93</td>\n",
       "      <td>93</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>116</td>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>221</td>\n",
       "      <td>68</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>22</td>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5697</td>\n",
       "      <td>3080</td>\n",
       "      <td>1405</td>\n",
       "      <td>7015</td>\n",
       "      <td>2617</td>\n",
       "      <td>508</td>\n",
       "      <td>982</td>\n",
       "      <td>998</td>\n",
       "      <td>374</td>\n",
       "      <td>613</td>\n",
       "      <td>2739</td>\n",
       "      <td>903</td>\n",
       "      <td>687</td>\n",
       "      <td>331</td>\n",
       "      <td>91</td>\n",
       "      <td>177</td>\n",
       "      <td>1416</td>\n",
       "      <td>554</td>\n",
       "      <td>1858</td>\n",
       "      <td>64</td>\n",
       "      <td>557</td>\n",
       "      <td>257</td>\n",
       "      <td>298</td>\n",
       "      <td>1038</td>\n",
       "      <td>258</td>\n",
       "      <td>119</td>\n",
       "      <td>128</td>\n",
       "      <td>134</td>\n",
       "      <td>83</td>\n",
       "      <td>32</td>\n",
       "      <td>185</td>\n",
       "      <td>104</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>192</td>\n",
       "      <td>46</td>\n",
       "      <td>170</td>\n",
       "      <td>0</td>\n",
       "      <td>283</td>\n",
       "      <td>182</td>\n",
       "      <td>189</td>\n",
       "      <td>712</td>\n",
       "      <td>234</td>\n",
       "      <td>79</td>\n",
       "      <td>111</td>\n",
       "      <td>73</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>122</td>\n",
       "      <td>95</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>88</td>\n",
       "      <td>14</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>141</td>\n",
       "      <td>542</td>\n",
       "      <td>242</td>\n",
       "      <td>42</td>\n",
       "      <td>109</td>\n",
       "      <td>87</td>\n",
       "      <td>40</td>\n",
       "      <td>28</td>\n",
       "      <td>77</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>125</td>\n",
       "      <td>20</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>46</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>47</td>\n",
       "      <td>39</td>\n",
       "      <td>110</td>\n",
       "      <td>55</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>810</td>\n",
       "      <td>394</td>\n",
       "      <td>529</td>\n",
       "      <td>1523</td>\n",
       "      <td>762</td>\n",
       "      <td>144</td>\n",
       "      <td>240</td>\n",
       "      <td>165</td>\n",
       "      <td>109</td>\n",
       "      <td>56</td>\n",
       "      <td>233</td>\n",
       "      <td>119</td>\n",
       "      <td>79</td>\n",
       "      <td>54</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>178</td>\n",
       "      <td>55</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>100</td>\n",
       "      <td>59</td>\n",
       "      <td>305</td>\n",
       "      <td>103</td>\n",
       "      <td>52</td>\n",
       "      <td>30</td>\n",
       "      <td>30</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>92</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>41</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>173</td>\n",
       "      <td>142</td>\n",
       "      <td>871</td>\n",
       "      <td>176</td>\n",
       "      <td>21</td>\n",
       "      <td>50</td>\n",
       "      <td>78</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>197</td>\n",
       "      <td>69</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>128</td>\n",
       "      <td>27</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3459</td>\n",
       "      <td>1661</td>\n",
       "      <td>1104</td>\n",
       "      <td>5085</td>\n",
       "      <td>1330</td>\n",
       "      <td>413</td>\n",
       "      <td>690</td>\n",
       "      <td>880</td>\n",
       "      <td>531</td>\n",
       "      <td>526</td>\n",
       "      <td>2090</td>\n",
       "      <td>574</td>\n",
       "      <td>313</td>\n",
       "      <td>371</td>\n",
       "      <td>38</td>\n",
       "      <td>350</td>\n",
       "      <td>1314</td>\n",
       "      <td>734</td>\n",
       "      <td>2518</td>\n",
       "      <td>2</td>\n",
       "      <td>673</td>\n",
       "      <td>263</td>\n",
       "      <td>327</td>\n",
       "      <td>1154</td>\n",
       "      <td>421</td>\n",
       "      <td>60</td>\n",
       "      <td>167</td>\n",
       "      <td>133</td>\n",
       "      <td>53</td>\n",
       "      <td>29</td>\n",
       "      <td>388</td>\n",
       "      <td>126</td>\n",
       "      <td>91</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>209</td>\n",
       "      <td>37</td>\n",
       "      <td>242</td>\n",
       "      <td>4</td>\n",
       "      <td>132</td>\n",
       "      <td>72</td>\n",
       "      <td>63</td>\n",
       "      <td>170</td>\n",
       "      <td>99</td>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>91</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>63</td>\n",
       "      <td>9</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>66</td>\n",
       "      <td>105</td>\n",
       "      <td>437</td>\n",
       "      <td>132</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>32</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>67</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>89</td>\n",
       "      <td>35</td>\n",
       "      <td>59</td>\n",
       "      <td>210</td>\n",
       "      <td>34</td>\n",
       "      <td>11</td>\n",
       "      <td>39</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>332</td>\n",
       "      <td>111</td>\n",
       "      <td>185</td>\n",
       "      <td>676</td>\n",
       "      <td>307</td>\n",
       "      <td>41</td>\n",
       "      <td>93</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>408</td>\n",
       "      <td>56</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>38</td>\n",
       "      <td>46</td>\n",
       "      <td>129</td>\n",
       "      <td>42</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>219</td>\n",
       "      <td>129</td>\n",
       "      <td>135</td>\n",
       "      <td>523</td>\n",
       "      <td>115</td>\n",
       "      <td>30</td>\n",
       "      <td>64</td>\n",
       "      <td>71</td>\n",
       "      <td>21</td>\n",
       "      <td>44</td>\n",
       "      <td>81</td>\n",
       "      <td>37</td>\n",
       "      <td>41</td>\n",
       "      <td>43</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>11</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>2162</td>\n",
       "      <td>1067</td>\n",
       "      <td>973</td>\n",
       "      <td>3428</td>\n",
       "      <td>1122</td>\n",
       "      <td>381</td>\n",
       "      <td>414</td>\n",
       "      <td>534</td>\n",
       "      <td>244</td>\n",
       "      <td>283</td>\n",
       "      <td>1064</td>\n",
       "      <td>318</td>\n",
       "      <td>277</td>\n",
       "      <td>92</td>\n",
       "      <td>39</td>\n",
       "      <td>154</td>\n",
       "      <td>945</td>\n",
       "      <td>187</td>\n",
       "      <td>1196</td>\n",
       "      <td>7</td>\n",
       "      <td>208</td>\n",
       "      <td>101</td>\n",
       "      <td>158</td>\n",
       "      <td>388</td>\n",
       "      <td>101</td>\n",
       "      <td>46</td>\n",
       "      <td>93</td>\n",
       "      <td>27</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>65</td>\n",
       "      <td>43</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>37</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>4261</td>\n",
       "      <td>2070</td>\n",
       "      <td>1938</td>\n",
       "      <td>6839</td>\n",
       "      <td>2054</td>\n",
       "      <td>731</td>\n",
       "      <td>676</td>\n",
       "      <td>1221</td>\n",
       "      <td>822</td>\n",
       "      <td>435</td>\n",
       "      <td>2199</td>\n",
       "      <td>952</td>\n",
       "      <td>759</td>\n",
       "      <td>261</td>\n",
       "      <td>214</td>\n",
       "      <td>586</td>\n",
       "      <td>1197</td>\n",
       "      <td>260</td>\n",
       "      <td>1378</td>\n",
       "      <td>15</td>\n",
       "      <td>300</td>\n",
       "      <td>149</td>\n",
       "      <td>232</td>\n",
       "      <td>741</td>\n",
       "      <td>220</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>29</td>\n",
       "      <td>85</td>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>76</td>\n",
       "      <td>10</td>\n",
       "      <td>41</td>\n",
       "      <td>2</td>\n",
       "      <td>924</td>\n",
       "      <td>409</td>\n",
       "      <td>566</td>\n",
       "      <td>2092</td>\n",
       "      <td>515</td>\n",
       "      <td>171</td>\n",
       "      <td>228</td>\n",
       "      <td>419</td>\n",
       "      <td>74</td>\n",
       "      <td>66</td>\n",
       "      <td>470</td>\n",
       "      <td>138</td>\n",
       "      <td>96</td>\n",
       "      <td>32</td>\n",
       "      <td>16</td>\n",
       "      <td>81</td>\n",
       "      <td>459</td>\n",
       "      <td>100</td>\n",
       "      <td>595</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "      <td>77</td>\n",
       "      <td>78</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2357</td>\n",
       "      <td>1187</td>\n",
       "      <td>1152</td>\n",
       "      <td>4017</td>\n",
       "      <td>1069</td>\n",
       "      <td>344</td>\n",
       "      <td>459</td>\n",
       "      <td>517</td>\n",
       "      <td>315</td>\n",
       "      <td>232</td>\n",
       "      <td>983</td>\n",
       "      <td>315</td>\n",
       "      <td>211</td>\n",
       "      <td>268</td>\n",
       "      <td>28</td>\n",
       "      <td>133</td>\n",
       "      <td>799</td>\n",
       "      <td>240</td>\n",
       "      <td>739</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>128</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>23</td>\n",
       "      <td>46</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>105</td>\n",
       "      <td>47</td>\n",
       "      <td>46</td>\n",
       "      <td>130</td>\n",
       "      <td>64</td>\n",
       "      <td>15</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>82</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>429</td>\n",
       "      <td>227</td>\n",
       "      <td>360</td>\n",
       "      <td>1181</td>\n",
       "      <td>393</td>\n",
       "      <td>175</td>\n",
       "      <td>130</td>\n",
       "      <td>118</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>142</td>\n",
       "      <td>69</td>\n",
       "      <td>40</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>185</td>\n",
       "      <td>15</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>185</td>\n",
       "      <td>136</td>\n",
       "      <td>778</td>\n",
       "      <td>187</td>\n",
       "      <td>61</td>\n",
       "      <td>113</td>\n",
       "      <td>115</td>\n",
       "      <td>19</td>\n",
       "      <td>25</td>\n",
       "      <td>133</td>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>43</td>\n",
       "      <td>357</td>\n",
       "      <td>0</td>\n",
       "      <td>982</td>\n",
       "      <td>546</td>\n",
       "      <td>490</td>\n",
       "      <td>2171</td>\n",
       "      <td>558</td>\n",
       "      <td>127</td>\n",
       "      <td>183</td>\n",
       "      <td>247</td>\n",
       "      <td>120</td>\n",
       "      <td>192</td>\n",
       "      <td>490</td>\n",
       "      <td>209</td>\n",
       "      <td>66</td>\n",
       "      <td>57</td>\n",
       "      <td>23</td>\n",
       "      <td>104</td>\n",
       "      <td>459</td>\n",
       "      <td>125</td>\n",
       "      <td>301</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>339</td>\n",
       "      <td>165</td>\n",
       "      <td>280</td>\n",
       "      <td>812</td>\n",
       "      <td>318</td>\n",
       "      <td>49</td>\n",
       "      <td>74</td>\n",
       "      <td>123</td>\n",
       "      <td>14</td>\n",
       "      <td>53</td>\n",
       "      <td>223</td>\n",
       "      <td>86</td>\n",
       "      <td>40</td>\n",
       "      <td>59</td>\n",
       "      <td>16</td>\n",
       "      <td>52</td>\n",
       "      <td>173</td>\n",
       "      <td>72</td>\n",
       "      <td>91</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>568</td>\n",
       "      <td>255</td>\n",
       "      <td>191</td>\n",
       "      <td>796</td>\n",
       "      <td>239</td>\n",
       "      <td>79</td>\n",
       "      <td>126</td>\n",
       "      <td>159</td>\n",
       "      <td>77</td>\n",
       "      <td>54</td>\n",
       "      <td>190</td>\n",
       "      <td>60</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>181</td>\n",
       "      <td>105</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>2494</td>\n",
       "      <td>1295</td>\n",
       "      <td>1161</td>\n",
       "      <td>4556</td>\n",
       "      <td>1379</td>\n",
       "      <td>304</td>\n",
       "      <td>558</td>\n",
       "      <td>1012</td>\n",
       "      <td>200</td>\n",
       "      <td>353</td>\n",
       "      <td>1727</td>\n",
       "      <td>641</td>\n",
       "      <td>718</td>\n",
       "      <td>177</td>\n",
       "      <td>223</td>\n",
       "      <td>327</td>\n",
       "      <td>643</td>\n",
       "      <td>443</td>\n",
       "      <td>3624</td>\n",
       "      <td>21</td>\n",
       "      <td>2280</td>\n",
       "      <td>941</td>\n",
       "      <td>1799</td>\n",
       "      <td>7134</td>\n",
       "      <td>1063</td>\n",
       "      <td>178</td>\n",
       "      <td>353</td>\n",
       "      <td>400</td>\n",
       "      <td>115</td>\n",
       "      <td>184</td>\n",
       "      <td>712</td>\n",
       "      <td>341</td>\n",
       "      <td>281</td>\n",
       "      <td>246</td>\n",
       "      <td>32</td>\n",
       "      <td>195</td>\n",
       "      <td>243</td>\n",
       "      <td>52</td>\n",
       "      <td>249</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>82</td>\n",
       "      <td>47</td>\n",
       "      <td>312</td>\n",
       "      <td>47</td>\n",
       "      <td>31</td>\n",
       "      <td>18</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>66</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>21</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>636</td>\n",
       "      <td>240</td>\n",
       "      <td>409</td>\n",
       "      <td>1583</td>\n",
       "      <td>363</td>\n",
       "      <td>152</td>\n",
       "      <td>129</td>\n",
       "      <td>197</td>\n",
       "      <td>98</td>\n",
       "      <td>42</td>\n",
       "      <td>303</td>\n",
       "      <td>73</td>\n",
       "      <td>49</td>\n",
       "      <td>45</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>357</td>\n",
       "      <td>72</td>\n",
       "      <td>213</td>\n",
       "      <td>7</td>\n",
       "      <td>39</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>99</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>48</td>\n",
       "      <td>42</td>\n",
       "      <td>185</td>\n",
       "      <td>194</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>60</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>59</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2785</td>\n",
       "      <td>1415</td>\n",
       "      <td>1128</td>\n",
       "      <td>4095</td>\n",
       "      <td>1690</td>\n",
       "      <td>307</td>\n",
       "      <td>419</td>\n",
       "      <td>665</td>\n",
       "      <td>215</td>\n",
       "      <td>347</td>\n",
       "      <td>1880</td>\n",
       "      <td>545</td>\n",
       "      <td>736</td>\n",
       "      <td>283</td>\n",
       "      <td>188</td>\n",
       "      <td>253</td>\n",
       "      <td>885</td>\n",
       "      <td>384</td>\n",
       "      <td>3052</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>283</td>\n",
       "      <td>112</td>\n",
       "      <td>176</td>\n",
       "      <td>441</td>\n",
       "      <td>151</td>\n",
       "      <td>81</td>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>138</td>\n",
       "      <td>67</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>87</td>\n",
       "      <td>10</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>127</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>185</td>\n",
       "      <td>86</td>\n",
       "      <td>78</td>\n",
       "      <td>349</td>\n",
       "      <td>116</td>\n",
       "      <td>30</td>\n",
       "      <td>60</td>\n",
       "      <td>29</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>83</td>\n",
       "      <td>28</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>103</td>\n",
       "      <td>13</td>\n",
       "      <td>92</td>\n",
       "      <td>0</td>\n",
       "      <td>522</td>\n",
       "      <td>284</td>\n",
       "      <td>182</td>\n",
       "      <td>898</td>\n",
       "      <td>344</td>\n",
       "      <td>74</td>\n",
       "      <td>114</td>\n",
       "      <td>130</td>\n",
       "      <td>74</td>\n",
       "      <td>77</td>\n",
       "      <td>280</td>\n",
       "      <td>78</td>\n",
       "      <td>37</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>256</td>\n",
       "      <td>65</td>\n",
       "      <td>218</td>\n",
       "      <td>3</td>\n",
       "      <td>8418</td>\n",
       "      <td>4098</td>\n",
       "      <td>4274</td>\n",
       "      <td>14483</td>\n",
       "      <td>4710</td>\n",
       "      <td>1054</td>\n",
       "      <td>1831</td>\n",
       "      <td>1917</td>\n",
       "      <td>1060</td>\n",
       "      <td>686</td>\n",
       "      <td>3656</td>\n",
       "      <td>1295</td>\n",
       "      <td>1060</td>\n",
       "      <td>356</td>\n",
       "      <td>122</td>\n",
       "      <td>490</td>\n",
       "      <td>2294</td>\n",
       "      <td>762</td>\n",
       "      <td>2428</td>\n",
       "      <td>11</td>\n",
       "      <td>564</td>\n",
       "      <td>176</td>\n",
       "      <td>393</td>\n",
       "      <td>1071</td>\n",
       "      <td>289</td>\n",
       "      <td>146</td>\n",
       "      <td>99</td>\n",
       "      <td>124</td>\n",
       "      <td>48</td>\n",
       "      <td>39</td>\n",
       "      <td>188</td>\n",
       "      <td>96</td>\n",
       "      <td>59</td>\n",
       "      <td>30</td>\n",
       "      <td>27</td>\n",
       "      <td>60</td>\n",
       "      <td>178</td>\n",
       "      <td>61</td>\n",
       "      <td>383</td>\n",
       "      <td>1</td>\n",
       "      <td>61676</td>\n",
       "      <td>32423</td>\n",
       "      <td>29077</td>\n",
       "      <td>91994</td>\n",
       "      <td>29578</td>\n",
       "      <td>8423</td>\n",
       "      <td>17333</td>\n",
       "      <td>15835</td>\n",
       "      <td>9053</td>\n",
       "      <td>4421</td>\n",
       "      <td>26226</td>\n",
       "      <td>10984</td>\n",
       "      <td>5237</td>\n",
       "      <td>2379</td>\n",
       "      <td>513</td>\n",
       "      <td>3631</td>\n",
       "      <td>22263</td>\n",
       "      <td>7108</td>\n",
       "      <td>24139</td>\n",
       "      <td>79</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>102</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>54</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>25</td>\n",
       "      <td>44</td>\n",
       "      <td>210</td>\n",
       "      <td>88</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>282</td>\n",
       "      <td>161</td>\n",
       "      <td>118</td>\n",
       "      <td>430</td>\n",
       "      <td>108</td>\n",
       "      <td>34</td>\n",
       "      <td>45</td>\n",
       "      <td>63</td>\n",
       "      <td>34</td>\n",
       "      <td>41</td>\n",
       "      <td>302</td>\n",
       "      <td>117</td>\n",
       "      <td>63</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>129</td>\n",
       "      <td>195</td>\n",
       "      <td>58</td>\n",
       "      <td>199</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>794</td>\n",
       "      <td>399</td>\n",
       "      <td>1002</td>\n",
       "      <td>3338</td>\n",
       "      <td>995</td>\n",
       "      <td>238</td>\n",
       "      <td>235</td>\n",
       "      <td>203</td>\n",
       "      <td>86</td>\n",
       "      <td>60</td>\n",
       "      <td>301</td>\n",
       "      <td>130</td>\n",
       "      <td>49</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>359</td>\n",
       "      <td>87</td>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>84</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>177</td>\n",
       "      <td>69</td>\n",
       "      <td>66</td>\n",
       "      <td>239</td>\n",
       "      <td>54</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>30</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>55</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>212</td>\n",
       "      <td>126</td>\n",
       "      <td>157</td>\n",
       "      <td>362</td>\n",
       "      <td>156</td>\n",
       "      <td>32</td>\n",
       "      <td>174</td>\n",
       "      <td>49</td>\n",
       "      <td>30</td>\n",
       "      <td>28</td>\n",
       "      <td>159</td>\n",
       "      <td>60</td>\n",
       "      <td>26</td>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>51</td>\n",
       "      <td>129</td>\n",
       "      <td>42</td>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>38</td>\n",
       "      <td>22</td>\n",
       "      <td>254</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>65</td>\n",
       "      <td>49</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>195</td>\n",
       "      <td>160</td>\n",
       "      <td>690</td>\n",
       "      <td>262</td>\n",
       "      <td>75</td>\n",
       "      <td>136</td>\n",
       "      <td>145</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>228</td>\n",
       "      <td>39</td>\n",
       "      <td>55</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>31</td>\n",
       "      <td>145</td>\n",
       "      <td>46</td>\n",
       "      <td>638</td>\n",
       "      <td>3</td>\n",
       "      <td>380</td>\n",
       "      <td>178</td>\n",
       "      <td>98</td>\n",
       "      <td>409</td>\n",
       "      <td>143</td>\n",
       "      <td>30</td>\n",
       "      <td>71</td>\n",
       "      <td>51</td>\n",
       "      <td>25</td>\n",
       "      <td>47</td>\n",
       "      <td>116</td>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>86</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>902</td>\n",
       "      <td>372</td>\n",
       "      <td>423</td>\n",
       "      <td>1263</td>\n",
       "      <td>393</td>\n",
       "      <td>129</td>\n",
       "      <td>165</td>\n",
       "      <td>193</td>\n",
       "      <td>70</td>\n",
       "      <td>100</td>\n",
       "      <td>338</td>\n",
       "      <td>134</td>\n",
       "      <td>82</td>\n",
       "      <td>30</td>\n",
       "      <td>13</td>\n",
       "      <td>51</td>\n",
       "      <td>245</td>\n",
       "      <td>45</td>\n",
       "      <td>191</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-01-29/2018-02-04</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>46</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>419</td>\n",
       "      <td>190</td>\n",
       "      <td>411</td>\n",
       "      <td>1011</td>\n",
       "      <td>441</td>\n",
       "      <td>101</td>\n",
       "      <td>164</td>\n",
       "      <td>172</td>\n",
       "      <td>61</td>\n",
       "      <td>40</td>\n",
       "      <td>214</td>\n",
       "      <td>70</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>37</td>\n",
       "      <td>183</td>\n",
       "      <td>32</td>\n",
       "      <td>139</td>\n",
       "      <td>0</td>\n",
       "      <td>1629</td>\n",
       "      <td>587</td>\n",
       "      <td>491</td>\n",
       "      <td>2672</td>\n",
       "      <td>627</td>\n",
       "      <td>281</td>\n",
       "      <td>342</td>\n",
       "      <td>492</td>\n",
       "      <td>142</td>\n",
       "      <td>106</td>\n",
       "      <td>1052</td>\n",
       "      <td>266</td>\n",
       "      <td>212</td>\n",
       "      <td>52</td>\n",
       "      <td>91</td>\n",
       "      <td>94</td>\n",
       "      <td>294</td>\n",
       "      <td>411</td>\n",
       "      <td>1820</td>\n",
       "      <td>6</td>\n",
       "      <td>76</td>\n",
       "      <td>42</td>\n",
       "      <td>43</td>\n",
       "      <td>187</td>\n",
       "      <td>56</td>\n",
       "      <td>36</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>87</td>\n",
       "      <td>3</td>\n",
       "      <td>232</td>\n",
       "      <td>64</td>\n",
       "      <td>168</td>\n",
       "      <td>683</td>\n",
       "      <td>230</td>\n",
       "      <td>85</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>67</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>59</td>\n",
       "      <td>7</td>\n",
       "      <td>56</td>\n",
       "      <td>4</td>\n",
       "      <td>112</td>\n",
       "      <td>63</td>\n",
       "      <td>42</td>\n",
       "      <td>193</td>\n",
       "      <td>91</td>\n",
       "      <td>25</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>81</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>357</td>\n",
       "      <td>123</td>\n",
       "      <td>198</td>\n",
       "      <td>873</td>\n",
       "      <td>238</td>\n",
       "      <td>88</td>\n",
       "      <td>50</td>\n",
       "      <td>44</td>\n",
       "      <td>44</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>42</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>59</td>\n",
       "      <td>17</td>\n",
       "      <td>83</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>40</td>\n",
       "      <td>49</td>\n",
       "      <td>198</td>\n",
       "      <td>58</td>\n",
       "      <td>25</td>\n",
       "      <td>22</td>\n",
       "      <td>21</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>188</td>\n",
       "      <td>79</td>\n",
       "      <td>200</td>\n",
       "      <td>653</td>\n",
       "      <td>185</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>71</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>5386</td>\n",
       "      <td>2351</td>\n",
       "      <td>2254</td>\n",
       "      <td>6973</td>\n",
       "      <td>2363</td>\n",
       "      <td>841</td>\n",
       "      <td>1324</td>\n",
       "      <td>1020</td>\n",
       "      <td>748</td>\n",
       "      <td>411</td>\n",
       "      <td>1697</td>\n",
       "      <td>877</td>\n",
       "      <td>399</td>\n",
       "      <td>141</td>\n",
       "      <td>37</td>\n",
       "      <td>157</td>\n",
       "      <td>1410</td>\n",
       "      <td>388</td>\n",
       "      <td>1324</td>\n",
       "      <td>5</td>\n",
       "      <td>284</td>\n",
       "      <td>126</td>\n",
       "      <td>182</td>\n",
       "      <td>592</td>\n",
       "      <td>184</td>\n",
       "      <td>52</td>\n",
       "      <td>81</td>\n",
       "      <td>57</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>79</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>72</td>\n",
       "      <td>11</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>65</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>71</td>\n",
       "      <td>68</td>\n",
       "      <td>228</td>\n",
       "      <td>144</td>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>92</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>170</td>\n",
       "      <td>16</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>102</td>\n",
       "      <td>68</td>\n",
       "      <td>20</td>\n",
       "      <td>155</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>117</td>\n",
       "      <td>39</td>\n",
       "      <td>43</td>\n",
       "      <td>106</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>82</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>556</td>\n",
       "      <td>254</td>\n",
       "      <td>359</td>\n",
       "      <td>1438</td>\n",
       "      <td>269</td>\n",
       "      <td>92</td>\n",
       "      <td>119</td>\n",
       "      <td>124</td>\n",
       "      <td>52</td>\n",
       "      <td>55</td>\n",
       "      <td>208</td>\n",
       "      <td>94</td>\n",
       "      <td>69</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>130</td>\n",
       "      <td>36</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "      <td>171</td>\n",
       "      <td>101</td>\n",
       "      <td>57</td>\n",
       "      <td>277</td>\n",
       "      <td>72</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>59</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>62</td>\n",
       "      <td>16</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>1063</td>\n",
       "      <td>429</td>\n",
       "      <td>471</td>\n",
       "      <td>1797</td>\n",
       "      <td>514</td>\n",
       "      <td>161</td>\n",
       "      <td>186</td>\n",
       "      <td>374</td>\n",
       "      <td>149</td>\n",
       "      <td>114</td>\n",
       "      <td>341</td>\n",
       "      <td>120</td>\n",
       "      <td>61</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>44</td>\n",
       "      <td>476</td>\n",
       "      <td>110</td>\n",
       "      <td>332</td>\n",
       "      <td>52</td>\n",
       "      <td>149</td>\n",
       "      <td>76</td>\n",
       "      <td>56</td>\n",
       "      <td>345</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>62</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>67</td>\n",
       "      <td>20</td>\n",
       "      <td>125</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>135</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>354</td>\n",
       "      <td>112</td>\n",
       "      <td>87</td>\n",
       "      <td>441</td>\n",
       "      <td>73</td>\n",
       "      <td>56</td>\n",
       "      <td>55</td>\n",
       "      <td>100</td>\n",
       "      <td>49</td>\n",
       "      <td>29</td>\n",
       "      <td>123</td>\n",
       "      <td>62</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>170</td>\n",
       "      <td>54</td>\n",
       "      <td>227</td>\n",
       "      <td>38</td>\n",
       "      <td>103</td>\n",
       "      <td>41</td>\n",
       "      <td>46</td>\n",
       "      <td>266</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>200</td>\n",
       "      <td>89</td>\n",
       "      <td>151</td>\n",
       "      <td>604</td>\n",
       "      <td>158</td>\n",
       "      <td>45</td>\n",
       "      <td>31</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>40</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>18</td>\n",
       "      <td>46</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>436</td>\n",
       "      <td>177</td>\n",
       "      <td>271</td>\n",
       "      <td>697</td>\n",
       "      <td>222</td>\n",
       "      <td>121</td>\n",
       "      <td>83</td>\n",
       "      <td>87</td>\n",
       "      <td>62</td>\n",
       "      <td>39</td>\n",
       "      <td>171</td>\n",
       "      <td>79</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>170</td>\n",
       "      <td>21</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>163</td>\n",
       "      <td>50</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>229</td>\n",
       "      <td>95</td>\n",
       "      <td>119</td>\n",
       "      <td>315</td>\n",
       "      <td>116</td>\n",
       "      <td>26</td>\n",
       "      <td>42</td>\n",
       "      <td>39</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>70</td>\n",
       "      <td>67</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>29</td>\n",
       "      <td>31</td>\n",
       "      <td>112</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>77</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>37</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>5398</td>\n",
       "      <td>3032</td>\n",
       "      <td>2433</td>\n",
       "      <td>7499</td>\n",
       "      <td>2607</td>\n",
       "      <td>923</td>\n",
       "      <td>1250</td>\n",
       "      <td>1506</td>\n",
       "      <td>951</td>\n",
       "      <td>383</td>\n",
       "      <td>2204</td>\n",
       "      <td>924</td>\n",
       "      <td>445</td>\n",
       "      <td>213</td>\n",
       "      <td>50</td>\n",
       "      <td>200</td>\n",
       "      <td>1341</td>\n",
       "      <td>536</td>\n",
       "      <td>1710</td>\n",
       "      <td>5</td>\n",
       "      <td>367</td>\n",
       "      <td>103</td>\n",
       "      <td>157</td>\n",
       "      <td>500</td>\n",
       "      <td>249</td>\n",
       "      <td>81</td>\n",
       "      <td>63</td>\n",
       "      <td>81</td>\n",
       "      <td>42</td>\n",
       "      <td>15</td>\n",
       "      <td>284</td>\n",
       "      <td>84</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>408</td>\n",
       "      <td>8</td>\n",
       "      <td>154</td>\n",
       "      <td>1</td>\n",
       "      <td>93</td>\n",
       "      <td>56</td>\n",
       "      <td>86</td>\n",
       "      <td>191</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>46</td>\n",
       "      <td>46</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>97</td>\n",
       "      <td>0</td>\n",
       "      <td>470</td>\n",
       "      <td>279</td>\n",
       "      <td>242</td>\n",
       "      <td>907</td>\n",
       "      <td>352</td>\n",
       "      <td>111</td>\n",
       "      <td>97</td>\n",
       "      <td>198</td>\n",
       "      <td>61</td>\n",
       "      <td>75</td>\n",
       "      <td>293</td>\n",
       "      <td>153</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>33</td>\n",
       "      <td>276</td>\n",
       "      <td>80</td>\n",
       "      <td>151</td>\n",
       "      <td>4</td>\n",
       "      <td>96</td>\n",
       "      <td>59</td>\n",
       "      <td>49</td>\n",
       "      <td>147</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>41</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>50</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "      <td>59</td>\n",
       "      <td>2</td>\n",
       "      <td>59</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>69</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>2</td>\n",
       "      <td>4176</td>\n",
       "      <td>2069</td>\n",
       "      <td>2735</td>\n",
       "      <td>7762</td>\n",
       "      <td>2747</td>\n",
       "      <td>1074</td>\n",
       "      <td>1002</td>\n",
       "      <td>1020</td>\n",
       "      <td>630</td>\n",
       "      <td>355</td>\n",
       "      <td>1740</td>\n",
       "      <td>711</td>\n",
       "      <td>563</td>\n",
       "      <td>146</td>\n",
       "      <td>121</td>\n",
       "      <td>279</td>\n",
       "      <td>1278</td>\n",
       "      <td>188</td>\n",
       "      <td>790</td>\n",
       "      <td>1</td>\n",
       "      <td>351</td>\n",
       "      <td>169</td>\n",
       "      <td>194</td>\n",
       "      <td>715</td>\n",
       "      <td>325</td>\n",
       "      <td>42</td>\n",
       "      <td>105</td>\n",
       "      <td>83</td>\n",
       "      <td>101</td>\n",
       "      <td>33</td>\n",
       "      <td>190</td>\n",
       "      <td>76</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>97</td>\n",
       "      <td>22</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>112</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>151</td>\n",
       "      <td>130</td>\n",
       "      <td>81</td>\n",
       "      <td>306</td>\n",
       "      <td>105</td>\n",
       "      <td>69</td>\n",
       "      <td>45</td>\n",
       "      <td>94</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>68</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>162</td>\n",
       "      <td>16</td>\n",
       "      <td>218</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>272</td>\n",
       "      <td>117</td>\n",
       "      <td>233</td>\n",
       "      <td>589</td>\n",
       "      <td>185</td>\n",
       "      <td>50</td>\n",
       "      <td>49</td>\n",
       "      <td>94</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>90</td>\n",
       "      <td>32</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "      <td>76</td>\n",
       "      <td>241</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "      <td>176</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>41</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>63</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>29</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>161</td>\n",
       "      <td>132</td>\n",
       "      <td>866</td>\n",
       "      <td>229</td>\n",
       "      <td>51</td>\n",
       "      <td>70</td>\n",
       "      <td>104</td>\n",
       "      <td>40</td>\n",
       "      <td>44</td>\n",
       "      <td>119</td>\n",
       "      <td>60</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>37</td>\n",
       "      <td>162</td>\n",
       "      <td>30</td>\n",
       "      <td>143</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>230</td>\n",
       "      <td>145</td>\n",
       "      <td>49</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>69</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>115</td>\n",
       "      <td>113</td>\n",
       "      <td>369</td>\n",
       "      <td>103</td>\n",
       "      <td>35</td>\n",
       "      <td>79</td>\n",
       "      <td>43</td>\n",
       "      <td>38</td>\n",
       "      <td>11</td>\n",
       "      <td>73</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>66</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>68</td>\n",
       "      <td>27</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>258</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>60</td>\n",
       "      <td>45</td>\n",
       "      <td>206</td>\n",
       "      <td>92</td>\n",
       "      <td>26</td>\n",
       "      <td>31</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>31</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>1038</td>\n",
       "      <td>487</td>\n",
       "      <td>562</td>\n",
       "      <td>1766</td>\n",
       "      <td>685</td>\n",
       "      <td>214</td>\n",
       "      <td>151</td>\n",
       "      <td>295</td>\n",
       "      <td>132</td>\n",
       "      <td>66</td>\n",
       "      <td>526</td>\n",
       "      <td>152</td>\n",
       "      <td>125</td>\n",
       "      <td>94</td>\n",
       "      <td>15</td>\n",
       "      <td>128</td>\n",
       "      <td>484</td>\n",
       "      <td>215</td>\n",
       "      <td>546</td>\n",
       "      <td>1</td>\n",
       "      <td>1474</td>\n",
       "      <td>716</td>\n",
       "      <td>662</td>\n",
       "      <td>2331</td>\n",
       "      <td>762</td>\n",
       "      <td>229</td>\n",
       "      <td>279</td>\n",
       "      <td>328</td>\n",
       "      <td>227</td>\n",
       "      <td>107</td>\n",
       "      <td>514</td>\n",
       "      <td>214</td>\n",
       "      <td>134</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>51</td>\n",
       "      <td>421</td>\n",
       "      <td>118</td>\n",
       "      <td>423</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>41</td>\n",
       "      <td>71</td>\n",
       "      <td>145</td>\n",
       "      <td>62</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>134</td>\n",
       "      <td>203</td>\n",
       "      <td>27</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>59</td>\n",
       "      <td>48</td>\n",
       "      <td>291</td>\n",
       "      <td>62</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>27</td>\n",
       "      <td>16</td>\n",
       "      <td>15</td>\n",
       "      <td>37</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>63</td>\n",
       "      <td>36</td>\n",
       "      <td>129</td>\n",
       "      <td>0</td>\n",
       "      <td>401</td>\n",
       "      <td>181</td>\n",
       "      <td>367</td>\n",
       "      <td>1083</td>\n",
       "      <td>445</td>\n",
       "      <td>39</td>\n",
       "      <td>80</td>\n",
       "      <td>90</td>\n",
       "      <td>29</td>\n",
       "      <td>37</td>\n",
       "      <td>149</td>\n",
       "      <td>87</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>55</td>\n",
       "      <td>73</td>\n",
       "      <td>16</td>\n",
       "      <td>126</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>209</td>\n",
       "      <td>109</td>\n",
       "      <td>90</td>\n",
       "      <td>347</td>\n",
       "      <td>187</td>\n",
       "      <td>32</td>\n",
       "      <td>49</td>\n",
       "      <td>41</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>87</td>\n",
       "      <td>41</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>46</td>\n",
       "      <td>14</td>\n",
       "      <td>66</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>63</td>\n",
       "      <td>161</td>\n",
       "      <td>285</td>\n",
       "      <td>130</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>39</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>131</td>\n",
       "      <td>114</td>\n",
       "      <td>383</td>\n",
       "      <td>132</td>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>42</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>82</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>56</td>\n",
       "      <td>42</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2061</td>\n",
       "      <td>826</td>\n",
       "      <td>1058</td>\n",
       "      <td>3626</td>\n",
       "      <td>1221</td>\n",
       "      <td>318</td>\n",
       "      <td>375</td>\n",
       "      <td>531</td>\n",
       "      <td>367</td>\n",
       "      <td>164</td>\n",
       "      <td>1084</td>\n",
       "      <td>316</td>\n",
       "      <td>226</td>\n",
       "      <td>138</td>\n",
       "      <td>32</td>\n",
       "      <td>129</td>\n",
       "      <td>662</td>\n",
       "      <td>487</td>\n",
       "      <td>874</td>\n",
       "      <td>5</td>\n",
       "      <td>189</td>\n",
       "      <td>80</td>\n",
       "      <td>80</td>\n",
       "      <td>318</td>\n",
       "      <td>90</td>\n",
       "      <td>25</td>\n",
       "      <td>55</td>\n",
       "      <td>79</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>68</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>55</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>80</td>\n",
       "      <td>37</td>\n",
       "      <td>59</td>\n",
       "      <td>291</td>\n",
       "      <td>89</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>39</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>1064</td>\n",
       "      <td>454</td>\n",
       "      <td>357</td>\n",
       "      <td>1534</td>\n",
       "      <td>639</td>\n",
       "      <td>151</td>\n",
       "      <td>211</td>\n",
       "      <td>175</td>\n",
       "      <td>123</td>\n",
       "      <td>94</td>\n",
       "      <td>384</td>\n",
       "      <td>140</td>\n",
       "      <td>63</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>23</td>\n",
       "      <td>339</td>\n",
       "      <td>63</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>11</td>\n",
       "      <td>48</td>\n",
       "      <td>47</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>23</td>\n",
       "      <td>17</td>\n",
       "      <td>53</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>18</td>\n",
       "      <td>13</td>\n",
       "      <td>84</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1734</td>\n",
       "      <td>783</td>\n",
       "      <td>998</td>\n",
       "      <td>3127</td>\n",
       "      <td>1112</td>\n",
       "      <td>303</td>\n",
       "      <td>454</td>\n",
       "      <td>464</td>\n",
       "      <td>283</td>\n",
       "      <td>105</td>\n",
       "      <td>884</td>\n",
       "      <td>267</td>\n",
       "      <td>220</td>\n",
       "      <td>187</td>\n",
       "      <td>19</td>\n",
       "      <td>157</td>\n",
       "      <td>515</td>\n",
       "      <td>146</td>\n",
       "      <td>677</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>26</td>\n",
       "      <td>28</td>\n",
       "      <td>101</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>736</td>\n",
       "      <td>289</td>\n",
       "      <td>423</td>\n",
       "      <td>1271</td>\n",
       "      <td>285</td>\n",
       "      <td>106</td>\n",
       "      <td>151</td>\n",
       "      <td>151</td>\n",
       "      <td>67</td>\n",
       "      <td>76</td>\n",
       "      <td>235</td>\n",
       "      <td>175</td>\n",
       "      <td>49</td>\n",
       "      <td>89</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>265</td>\n",
       "      <td>49</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>220</td>\n",
       "      <td>106</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>24</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>86</td>\n",
       "      <td>66</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>141</td>\n",
       "      <td>57</td>\n",
       "      <td>41</td>\n",
       "      <td>209</td>\n",
       "      <td>101</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "      <td>39</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>15</td>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>76</td>\n",
       "      <td>39</td>\n",
       "      <td>22</td>\n",
       "      <td>92</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>82</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>88</td>\n",
       "      <td>118</td>\n",
       "      <td>437</td>\n",
       "      <td>131</td>\n",
       "      <td>22</td>\n",
       "      <td>76</td>\n",
       "      <td>48</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>98</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>58</td>\n",
       "      <td>22</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>295</td>\n",
       "      <td>170</td>\n",
       "      <td>116</td>\n",
       "      <td>443</td>\n",
       "      <td>185</td>\n",
       "      <td>46</td>\n",
       "      <td>53</td>\n",
       "      <td>65</td>\n",
       "      <td>24</td>\n",
       "      <td>38</td>\n",
       "      <td>114</td>\n",
       "      <td>53</td>\n",
       "      <td>23</td>\n",
       "      <td>35</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>163</td>\n",
       "      <td>15</td>\n",
       "      <td>69</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "      <td>46</td>\n",
       "      <td>18</td>\n",
       "      <td>233</td>\n",
       "      <td>53</td>\n",
       "      <td>16</td>\n",
       "      <td>35</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>23</td>\n",
       "      <td>98</td>\n",
       "      <td>224</td>\n",
       "      <td>126</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>69</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "      <td>110</td>\n",
       "      <td>113</td>\n",
       "      <td>565</td>\n",
       "      <td>157</td>\n",
       "      <td>17</td>\n",
       "      <td>52</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "      <td>23</td>\n",
       "      <td>88</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>52</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>26</td>\n",
       "      <td>47</td>\n",
       "      <td>168</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>727</td>\n",
       "      <td>315</td>\n",
       "      <td>413</td>\n",
       "      <td>1451</td>\n",
       "      <td>477</td>\n",
       "      <td>120</td>\n",
       "      <td>160</td>\n",
       "      <td>141</td>\n",
       "      <td>123</td>\n",
       "      <td>80</td>\n",
       "      <td>302</td>\n",
       "      <td>71</td>\n",
       "      <td>76</td>\n",
       "      <td>36</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>300</td>\n",
       "      <td>40</td>\n",
       "      <td>161</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6349</td>\n",
       "      <td>3714</td>\n",
       "      <td>2902</td>\n",
       "      <td>10281</td>\n",
       "      <td>3164</td>\n",
       "      <td>903</td>\n",
       "      <td>1461</td>\n",
       "      <td>1913</td>\n",
       "      <td>1163</td>\n",
       "      <td>825</td>\n",
       "      <td>3806</td>\n",
       "      <td>1273</td>\n",
       "      <td>547</td>\n",
       "      <td>823</td>\n",
       "      <td>199</td>\n",
       "      <td>332</td>\n",
       "      <td>2348</td>\n",
       "      <td>1011</td>\n",
       "      <td>2452</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1327</td>\n",
       "      <td>667</td>\n",
       "      <td>662</td>\n",
       "      <td>2404</td>\n",
       "      <td>930</td>\n",
       "      <td>327</td>\n",
       "      <td>289</td>\n",
       "      <td>494</td>\n",
       "      <td>140</td>\n",
       "      <td>127</td>\n",
       "      <td>915</td>\n",
       "      <td>316</td>\n",
       "      <td>334</td>\n",
       "      <td>392</td>\n",
       "      <td>56</td>\n",
       "      <td>144</td>\n",
       "      <td>729</td>\n",
       "      <td>131</td>\n",
       "      <td>549</td>\n",
       "      <td>0</td>\n",
       "      <td>3369</td>\n",
       "      <td>1741</td>\n",
       "      <td>1479</td>\n",
       "      <td>5563</td>\n",
       "      <td>2113</td>\n",
       "      <td>501</td>\n",
       "      <td>921</td>\n",
       "      <td>806</td>\n",
       "      <td>385</td>\n",
       "      <td>324</td>\n",
       "      <td>1993</td>\n",
       "      <td>783</td>\n",
       "      <td>712</td>\n",
       "      <td>416</td>\n",
       "      <td>73</td>\n",
       "      <td>735</td>\n",
       "      <td>1257</td>\n",
       "      <td>386</td>\n",
       "      <td>2296</td>\n",
       "      <td>33</td>\n",
       "      <td>1076</td>\n",
       "      <td>618</td>\n",
       "      <td>675</td>\n",
       "      <td>2135</td>\n",
       "      <td>707</td>\n",
       "      <td>157</td>\n",
       "      <td>287</td>\n",
       "      <td>204</td>\n",
       "      <td>137</td>\n",
       "      <td>93</td>\n",
       "      <td>364</td>\n",
       "      <td>187</td>\n",
       "      <td>87</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>39</td>\n",
       "      <td>411</td>\n",
       "      <td>96</td>\n",
       "      <td>605</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>26</td>\n",
       "      <td>17</td>\n",
       "      <td>125</td>\n",
       "      <td>30</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>645</td>\n",
       "      <td>215</td>\n",
       "      <td>241</td>\n",
       "      <td>1061</td>\n",
       "      <td>320</td>\n",
       "      <td>131</td>\n",
       "      <td>176</td>\n",
       "      <td>236</td>\n",
       "      <td>59</td>\n",
       "      <td>70</td>\n",
       "      <td>305</td>\n",
       "      <td>101</td>\n",
       "      <td>113</td>\n",
       "      <td>34</td>\n",
       "      <td>33</td>\n",
       "      <td>61</td>\n",
       "      <td>277</td>\n",
       "      <td>102</td>\n",
       "      <td>696</td>\n",
       "      <td>3</td>\n",
       "      <td>1198</td>\n",
       "      <td>549</td>\n",
       "      <td>811</td>\n",
       "      <td>2637</td>\n",
       "      <td>788</td>\n",
       "      <td>366</td>\n",
       "      <td>328</td>\n",
       "      <td>436</td>\n",
       "      <td>219</td>\n",
       "      <td>86</td>\n",
       "      <td>470</td>\n",
       "      <td>206</td>\n",
       "      <td>117</td>\n",
       "      <td>59</td>\n",
       "      <td>47</td>\n",
       "      <td>65</td>\n",
       "      <td>434</td>\n",
       "      <td>120</td>\n",
       "      <td>505</td>\n",
       "      <td>0</td>\n",
       "      <td>187</td>\n",
       "      <td>89</td>\n",
       "      <td>57</td>\n",
       "      <td>266</td>\n",
       "      <td>102</td>\n",
       "      <td>35</td>\n",
       "      <td>59</td>\n",
       "      <td>63</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>72</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>78</td>\n",
       "      <td>20</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>396</td>\n",
       "      <td>198</td>\n",
       "      <td>234</td>\n",
       "      <td>736</td>\n",
       "      <td>239</td>\n",
       "      <td>54</td>\n",
       "      <td>88</td>\n",
       "      <td>61</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>106</td>\n",
       "      <td>60</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>180</td>\n",
       "      <td>30</td>\n",
       "      <td>142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>440</td>\n",
       "      <td>206</td>\n",
       "      <td>186</td>\n",
       "      <td>965</td>\n",
       "      <td>298</td>\n",
       "      <td>99</td>\n",
       "      <td>133</td>\n",
       "      <td>104</td>\n",
       "      <td>41</td>\n",
       "      <td>39</td>\n",
       "      <td>205</td>\n",
       "      <td>42</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>164</td>\n",
       "      <td>57</td>\n",
       "      <td>202</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1438</td>\n",
       "      <td>625</td>\n",
       "      <td>414</td>\n",
       "      <td>2004</td>\n",
       "      <td>646</td>\n",
       "      <td>97</td>\n",
       "      <td>239</td>\n",
       "      <td>437</td>\n",
       "      <td>129</td>\n",
       "      <td>195</td>\n",
       "      <td>976</td>\n",
       "      <td>388</td>\n",
       "      <td>208</td>\n",
       "      <td>127</td>\n",
       "      <td>28</td>\n",
       "      <td>102</td>\n",
       "      <td>827</td>\n",
       "      <td>133</td>\n",
       "      <td>471</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>52</td>\n",
       "      <td>58</td>\n",
       "      <td>378</td>\n",
       "      <td>45</td>\n",
       "      <td>17</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>1196</td>\n",
       "      <td>427</td>\n",
       "      <td>320</td>\n",
       "      <td>1586</td>\n",
       "      <td>392</td>\n",
       "      <td>185</td>\n",
       "      <td>182</td>\n",
       "      <td>269</td>\n",
       "      <td>131</td>\n",
       "      <td>91</td>\n",
       "      <td>547</td>\n",
       "      <td>169</td>\n",
       "      <td>330</td>\n",
       "      <td>34</td>\n",
       "      <td>58</td>\n",
       "      <td>155</td>\n",
       "      <td>244</td>\n",
       "      <td>71</td>\n",
       "      <td>315</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>207</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>73</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>794</td>\n",
       "      <td>364</td>\n",
       "      <td>538</td>\n",
       "      <td>1413</td>\n",
       "      <td>471</td>\n",
       "      <td>207</td>\n",
       "      <td>200</td>\n",
       "      <td>164</td>\n",
       "      <td>111</td>\n",
       "      <td>79</td>\n",
       "      <td>339</td>\n",
       "      <td>100</td>\n",
       "      <td>131</td>\n",
       "      <td>18</td>\n",
       "      <td>53</td>\n",
       "      <td>78</td>\n",
       "      <td>201</td>\n",
       "      <td>58</td>\n",
       "      <td>229</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>92</td>\n",
       "      <td>137</td>\n",
       "      <td>399</td>\n",
       "      <td>152</td>\n",
       "      <td>32</td>\n",
       "      <td>51</td>\n",
       "      <td>51</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>42</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>90</td>\n",
       "      <td>16</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>11</td>\n",
       "      <td>54</td>\n",
       "      <td>86</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>123</td>\n",
       "      <td>53</td>\n",
       "      <td>72</td>\n",
       "      <td>557</td>\n",
       "      <td>135</td>\n",
       "      <td>60</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>54</td>\n",
       "      <td>24</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>531</td>\n",
       "      <td>247</td>\n",
       "      <td>202</td>\n",
       "      <td>1125</td>\n",
       "      <td>388</td>\n",
       "      <td>110</td>\n",
       "      <td>174</td>\n",
       "      <td>130</td>\n",
       "      <td>67</td>\n",
       "      <td>37</td>\n",
       "      <td>246</td>\n",
       "      <td>98</td>\n",
       "      <td>142</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>36</td>\n",
       "      <td>136</td>\n",
       "      <td>52</td>\n",
       "      <td>275</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>14</td>\n",
       "      <td>119</td>\n",
       "      <td>144</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>77</td>\n",
       "      <td>50</td>\n",
       "      <td>81</td>\n",
       "      <td>241</td>\n",
       "      <td>71</td>\n",
       "      <td>30</td>\n",
       "      <td>24</td>\n",
       "      <td>17</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>48</td>\n",
       "      <td>4</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>252</td>\n",
       "      <td>117</td>\n",
       "      <td>77</td>\n",
       "      <td>370</td>\n",
       "      <td>150</td>\n",
       "      <td>47</td>\n",
       "      <td>40</td>\n",
       "      <td>52</td>\n",
       "      <td>21</td>\n",
       "      <td>26</td>\n",
       "      <td>90</td>\n",
       "      <td>34</td>\n",
       "      <td>29</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>26</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>116</td>\n",
       "      <td>51</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>72</td>\n",
       "      <td>43</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>54</td>\n",
       "      <td>46</td>\n",
       "      <td>37</td>\n",
       "      <td>113</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>260</td>\n",
       "      <td>131</td>\n",
       "      <td>79</td>\n",
       "      <td>616</td>\n",
       "      <td>128</td>\n",
       "      <td>53</td>\n",
       "      <td>138</td>\n",
       "      <td>117</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>113</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>72</td>\n",
       "      <td>47</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>20</td>\n",
       "      <td>14</td>\n",
       "      <td>66</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>45</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>36</td>\n",
       "      <td>15</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>123</td>\n",
       "      <td>96</td>\n",
       "      <td>347</td>\n",
       "      <td>112</td>\n",
       "      <td>19</td>\n",
       "      <td>56</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>87</td>\n",
       "      <td>32</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>126</td>\n",
       "      <td>15</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>159</td>\n",
       "      <td>67</td>\n",
       "      <td>108</td>\n",
       "      <td>242</td>\n",
       "      <td>76</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>39</td>\n",
       "      <td>83</td>\n",
       "      <td>26</td>\n",
       "      <td>44</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>44</td>\n",
       "      <td>90</td>\n",
       "      <td>63</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>99</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>91</td>\n",
       "      <td>107</td>\n",
       "      <td>351</td>\n",
       "      <td>96</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>76</td>\n",
       "      <td>11</td>\n",
       "      <td>99</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>42</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>145</td>\n",
       "      <td>108</td>\n",
       "      <td>465</td>\n",
       "      <td>129</td>\n",
       "      <td>34</td>\n",
       "      <td>47</td>\n",
       "      <td>33</td>\n",
       "      <td>60</td>\n",
       "      <td>30</td>\n",
       "      <td>75</td>\n",
       "      <td>48</td>\n",
       "      <td>27</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "      <td>17</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>121</td>\n",
       "      <td>57</td>\n",
       "      <td>175</td>\n",
       "      <td>303</td>\n",
       "      <td>185</td>\n",
       "      <td>45</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>27</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>42</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>203</td>\n",
       "      <td>155</td>\n",
       "      <td>93</td>\n",
       "      <td>228</td>\n",
       "      <td>156</td>\n",
       "      <td>19</td>\n",
       "      <td>17</td>\n",
       "      <td>241</td>\n",
       "      <td>34</td>\n",
       "      <td>61</td>\n",
       "      <td>122</td>\n",
       "      <td>64</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>157</td>\n",
       "      <td>4</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1100</td>\n",
       "      <td>612</td>\n",
       "      <td>884</td>\n",
       "      <td>2591</td>\n",
       "      <td>571</td>\n",
       "      <td>221</td>\n",
       "      <td>232</td>\n",
       "      <td>274</td>\n",
       "      <td>137</td>\n",
       "      <td>105</td>\n",
       "      <td>496</td>\n",
       "      <td>195</td>\n",
       "      <td>108</td>\n",
       "      <td>45</td>\n",
       "      <td>30</td>\n",
       "      <td>78</td>\n",
       "      <td>429</td>\n",
       "      <td>107</td>\n",
       "      <td>443</td>\n",
       "      <td>1</td>\n",
       "      <td>1001</td>\n",
       "      <td>550</td>\n",
       "      <td>376</td>\n",
       "      <td>1568</td>\n",
       "      <td>409</td>\n",
       "      <td>175</td>\n",
       "      <td>176</td>\n",
       "      <td>185</td>\n",
       "      <td>120</td>\n",
       "      <td>166</td>\n",
       "      <td>478</td>\n",
       "      <td>201</td>\n",
       "      <td>69</td>\n",
       "      <td>36</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>426</td>\n",
       "      <td>95</td>\n",
       "      <td>200</td>\n",
       "      <td>3</td>\n",
       "      <td>63</td>\n",
       "      <td>24</td>\n",
       "      <td>39</td>\n",
       "      <td>93</td>\n",
       "      <td>47</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>21</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>143</td>\n",
       "      <td>80</td>\n",
       "      <td>143</td>\n",
       "      <td>278</td>\n",
       "      <td>92</td>\n",
       "      <td>32</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>31</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5770</td>\n",
       "      <td>2917</td>\n",
       "      <td>1524</td>\n",
       "      <td>7037</td>\n",
       "      <td>2683</td>\n",
       "      <td>539</td>\n",
       "      <td>1039</td>\n",
       "      <td>925</td>\n",
       "      <td>446</td>\n",
       "      <td>513</td>\n",
       "      <td>2734</td>\n",
       "      <td>966</td>\n",
       "      <td>616</td>\n",
       "      <td>181</td>\n",
       "      <td>158</td>\n",
       "      <td>174</td>\n",
       "      <td>1432</td>\n",
       "      <td>550</td>\n",
       "      <td>2159</td>\n",
       "      <td>12</td>\n",
       "      <td>601</td>\n",
       "      <td>288</td>\n",
       "      <td>320</td>\n",
       "      <td>1077</td>\n",
       "      <td>226</td>\n",
       "      <td>151</td>\n",
       "      <td>165</td>\n",
       "      <td>133</td>\n",
       "      <td>90</td>\n",
       "      <td>31</td>\n",
       "      <td>230</td>\n",
       "      <td>96</td>\n",
       "      <td>39</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>168</td>\n",
       "      <td>26</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "      <td>317</td>\n",
       "      <td>160</td>\n",
       "      <td>348</td>\n",
       "      <td>850</td>\n",
       "      <td>231</td>\n",
       "      <td>80</td>\n",
       "      <td>83</td>\n",
       "      <td>57</td>\n",
       "      <td>46</td>\n",
       "      <td>25</td>\n",
       "      <td>71</td>\n",
       "      <td>58</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>128</td>\n",
       "      <td>23</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>274</td>\n",
       "      <td>201</td>\n",
       "      <td>280</td>\n",
       "      <td>881</td>\n",
       "      <td>193</td>\n",
       "      <td>56</td>\n",
       "      <td>51</td>\n",
       "      <td>62</td>\n",
       "      <td>45</td>\n",
       "      <td>23</td>\n",
       "      <td>96</td>\n",
       "      <td>51</td>\n",
       "      <td>29</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>131</td>\n",
       "      <td>30</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>80</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>109</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1002</td>\n",
       "      <td>587</td>\n",
       "      <td>555</td>\n",
       "      <td>1848</td>\n",
       "      <td>692</td>\n",
       "      <td>158</td>\n",
       "      <td>263</td>\n",
       "      <td>224</td>\n",
       "      <td>208</td>\n",
       "      <td>59</td>\n",
       "      <td>283</td>\n",
       "      <td>132</td>\n",
       "      <td>74</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>25</td>\n",
       "      <td>281</td>\n",
       "      <td>31</td>\n",
       "      <td>310</td>\n",
       "      <td>0</td>\n",
       "      <td>155</td>\n",
       "      <td>100</td>\n",
       "      <td>54</td>\n",
       "      <td>294</td>\n",
       "      <td>116</td>\n",
       "      <td>28</td>\n",
       "      <td>35</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>109</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>69</td>\n",
       "      <td>34</td>\n",
       "      <td>16</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>207</td>\n",
       "      <td>98</td>\n",
       "      <td>234</td>\n",
       "      <td>508</td>\n",
       "      <td>99</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>124</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>116</td>\n",
       "      <td>44</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>113</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3350</td>\n",
       "      <td>1513</td>\n",
       "      <td>1040</td>\n",
       "      <td>4813</td>\n",
       "      <td>1381</td>\n",
       "      <td>459</td>\n",
       "      <td>604</td>\n",
       "      <td>758</td>\n",
       "      <td>538</td>\n",
       "      <td>454</td>\n",
       "      <td>2049</td>\n",
       "      <td>631</td>\n",
       "      <td>285</td>\n",
       "      <td>370</td>\n",
       "      <td>56</td>\n",
       "      <td>244</td>\n",
       "      <td>1391</td>\n",
       "      <td>632</td>\n",
       "      <td>2300</td>\n",
       "      <td>5</td>\n",
       "      <td>952</td>\n",
       "      <td>533</td>\n",
       "      <td>374</td>\n",
       "      <td>1490</td>\n",
       "      <td>481</td>\n",
       "      <td>106</td>\n",
       "      <td>144</td>\n",
       "      <td>153</td>\n",
       "      <td>60</td>\n",
       "      <td>71</td>\n",
       "      <td>686</td>\n",
       "      <td>246</td>\n",
       "      <td>104</td>\n",
       "      <td>41</td>\n",
       "      <td>8</td>\n",
       "      <td>137</td>\n",
       "      <td>245</td>\n",
       "      <td>68</td>\n",
       "      <td>477</td>\n",
       "      <td>35</td>\n",
       "      <td>110</td>\n",
       "      <td>63</td>\n",
       "      <td>80</td>\n",
       "      <td>237</td>\n",
       "      <td>59</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "      <td>80</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>90</td>\n",
       "      <td>17</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "      <td>66</td>\n",
       "      <td>84</td>\n",
       "      <td>356</td>\n",
       "      <td>149</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td>18</td>\n",
       "      <td>57</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>67</td>\n",
       "      <td>34</td>\n",
       "      <td>35</td>\n",
       "      <td>106</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>408</td>\n",
       "      <td>181</td>\n",
       "      <td>378</td>\n",
       "      <td>1091</td>\n",
       "      <td>618</td>\n",
       "      <td>124</td>\n",
       "      <td>118</td>\n",
       "      <td>74</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>300</td>\n",
       "      <td>78</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>28</td>\n",
       "      <td>64</td>\n",
       "      <td>47</td>\n",
       "      <td>16</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>74</td>\n",
       "      <td>132</td>\n",
       "      <td>57</td>\n",
       "      <td>23</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>231</td>\n",
       "      <td>125</td>\n",
       "      <td>221</td>\n",
       "      <td>638</td>\n",
       "      <td>194</td>\n",
       "      <td>68</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>45</td>\n",
       "      <td>32</td>\n",
       "      <td>125</td>\n",
       "      <td>51</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>2040</td>\n",
       "      <td>972</td>\n",
       "      <td>733</td>\n",
       "      <td>2636</td>\n",
       "      <td>870</td>\n",
       "      <td>247</td>\n",
       "      <td>403</td>\n",
       "      <td>646</td>\n",
       "      <td>319</td>\n",
       "      <td>342</td>\n",
       "      <td>1154</td>\n",
       "      <td>386</td>\n",
       "      <td>240</td>\n",
       "      <td>84</td>\n",
       "      <td>30</td>\n",
       "      <td>105</td>\n",
       "      <td>811</td>\n",
       "      <td>142</td>\n",
       "      <td>577</td>\n",
       "      <td>4</td>\n",
       "      <td>233</td>\n",
       "      <td>129</td>\n",
       "      <td>132</td>\n",
       "      <td>414</td>\n",
       "      <td>64</td>\n",
       "      <td>46</td>\n",
       "      <td>151</td>\n",
       "      <td>57</td>\n",
       "      <td>31</td>\n",
       "      <td>19</td>\n",
       "      <td>91</td>\n",
       "      <td>27</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>23</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>5227</td>\n",
       "      <td>2143</td>\n",
       "      <td>1853</td>\n",
       "      <td>7801</td>\n",
       "      <td>2211</td>\n",
       "      <td>899</td>\n",
       "      <td>736</td>\n",
       "      <td>1864</td>\n",
       "      <td>1104</td>\n",
       "      <td>495</td>\n",
       "      <td>2676</td>\n",
       "      <td>1096</td>\n",
       "      <td>848</td>\n",
       "      <td>266</td>\n",
       "      <td>250</td>\n",
       "      <td>666</td>\n",
       "      <td>1260</td>\n",
       "      <td>279</td>\n",
       "      <td>2054</td>\n",
       "      <td>6</td>\n",
       "      <td>284</td>\n",
       "      <td>124</td>\n",
       "      <td>151</td>\n",
       "      <td>530</td>\n",
       "      <td>163</td>\n",
       "      <td>55</td>\n",
       "      <td>34</td>\n",
       "      <td>90</td>\n",
       "      <td>17</td>\n",
       "      <td>13</td>\n",
       "      <td>79</td>\n",
       "      <td>51</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>77</td>\n",
       "      <td>28</td>\n",
       "      <td>63</td>\n",
       "      <td>2</td>\n",
       "      <td>743</td>\n",
       "      <td>421</td>\n",
       "      <td>372</td>\n",
       "      <td>1394</td>\n",
       "      <td>573</td>\n",
       "      <td>213</td>\n",
       "      <td>212</td>\n",
       "      <td>330</td>\n",
       "      <td>104</td>\n",
       "      <td>104</td>\n",
       "      <td>371</td>\n",
       "      <td>166</td>\n",
       "      <td>59</td>\n",
       "      <td>40</td>\n",
       "      <td>22</td>\n",
       "      <td>56</td>\n",
       "      <td>384</td>\n",
       "      <td>42</td>\n",
       "      <td>636</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>117</td>\n",
       "      <td>45</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2458</td>\n",
       "      <td>1193</td>\n",
       "      <td>1059</td>\n",
       "      <td>3681</td>\n",
       "      <td>1044</td>\n",
       "      <td>319</td>\n",
       "      <td>522</td>\n",
       "      <td>479</td>\n",
       "      <td>352</td>\n",
       "      <td>214</td>\n",
       "      <td>968</td>\n",
       "      <td>355</td>\n",
       "      <td>242</td>\n",
       "      <td>155</td>\n",
       "      <td>25</td>\n",
       "      <td>120</td>\n",
       "      <td>690</td>\n",
       "      <td>255</td>\n",
       "      <td>765</td>\n",
       "      <td>4</td>\n",
       "      <td>78</td>\n",
       "      <td>42</td>\n",
       "      <td>98</td>\n",
       "      <td>374</td>\n",
       "      <td>64</td>\n",
       "      <td>18</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>36</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>53</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>135</td>\n",
       "      <td>38</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>59</td>\n",
       "      <td>71</td>\n",
       "      <td>184</td>\n",
       "      <td>67</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>44</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>438</td>\n",
       "      <td>224</td>\n",
       "      <td>273</td>\n",
       "      <td>783</td>\n",
       "      <td>285</td>\n",
       "      <td>81</td>\n",
       "      <td>150</td>\n",
       "      <td>111</td>\n",
       "      <td>49</td>\n",
       "      <td>26</td>\n",
       "      <td>166</td>\n",
       "      <td>59</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>57</td>\n",
       "      <td>155</td>\n",
       "      <td>30</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>214</td>\n",
       "      <td>130</td>\n",
       "      <td>124</td>\n",
       "      <td>719</td>\n",
       "      <td>169</td>\n",
       "      <td>37</td>\n",
       "      <td>61</td>\n",
       "      <td>78</td>\n",
       "      <td>33</td>\n",
       "      <td>30</td>\n",
       "      <td>130</td>\n",
       "      <td>28</td>\n",
       "      <td>51</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>76</td>\n",
       "      <td>44</td>\n",
       "      <td>211</td>\n",
       "      <td>2</td>\n",
       "      <td>850</td>\n",
       "      <td>386</td>\n",
       "      <td>360</td>\n",
       "      <td>1564</td>\n",
       "      <td>540</td>\n",
       "      <td>150</td>\n",
       "      <td>204</td>\n",
       "      <td>244</td>\n",
       "      <td>112</td>\n",
       "      <td>75</td>\n",
       "      <td>330</td>\n",
       "      <td>174</td>\n",
       "      <td>87</td>\n",
       "      <td>60</td>\n",
       "      <td>24</td>\n",
       "      <td>52</td>\n",
       "      <td>350</td>\n",
       "      <td>63</td>\n",
       "      <td>270</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>112</td>\n",
       "      <td>31</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>387</td>\n",
       "      <td>249</td>\n",
       "      <td>405</td>\n",
       "      <td>1006</td>\n",
       "      <td>345</td>\n",
       "      <td>73</td>\n",
       "      <td>86</td>\n",
       "      <td>219</td>\n",
       "      <td>34</td>\n",
       "      <td>40</td>\n",
       "      <td>211</td>\n",
       "      <td>47</td>\n",
       "      <td>50</td>\n",
       "      <td>81</td>\n",
       "      <td>9</td>\n",
       "      <td>126</td>\n",
       "      <td>210</td>\n",
       "      <td>59</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>439</td>\n",
       "      <td>222</td>\n",
       "      <td>507</td>\n",
       "      <td>1007</td>\n",
       "      <td>257</td>\n",
       "      <td>78</td>\n",
       "      <td>103</td>\n",
       "      <td>118</td>\n",
       "      <td>80</td>\n",
       "      <td>31</td>\n",
       "      <td>209</td>\n",
       "      <td>86</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>35</td>\n",
       "      <td>96</td>\n",
       "      <td>45</td>\n",
       "      <td>131</td>\n",
       "      <td>0</td>\n",
       "      <td>2026</td>\n",
       "      <td>970</td>\n",
       "      <td>786</td>\n",
       "      <td>3948</td>\n",
       "      <td>1058</td>\n",
       "      <td>268</td>\n",
       "      <td>526</td>\n",
       "      <td>827</td>\n",
       "      <td>301</td>\n",
       "      <td>234</td>\n",
       "      <td>1387</td>\n",
       "      <td>524</td>\n",
       "      <td>566</td>\n",
       "      <td>141</td>\n",
       "      <td>117</td>\n",
       "      <td>252</td>\n",
       "      <td>684</td>\n",
       "      <td>400</td>\n",
       "      <td>3660</td>\n",
       "      <td>20</td>\n",
       "      <td>917</td>\n",
       "      <td>398</td>\n",
       "      <td>508</td>\n",
       "      <td>1796</td>\n",
       "      <td>378</td>\n",
       "      <td>88</td>\n",
       "      <td>199</td>\n",
       "      <td>180</td>\n",
       "      <td>95</td>\n",
       "      <td>57</td>\n",
       "      <td>379</td>\n",
       "      <td>176</td>\n",
       "      <td>39</td>\n",
       "      <td>28</td>\n",
       "      <td>7</td>\n",
       "      <td>38</td>\n",
       "      <td>190</td>\n",
       "      <td>106</td>\n",
       "      <td>108</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>92</td>\n",
       "      <td>53</td>\n",
       "      <td>295</td>\n",
       "      <td>81</td>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>54</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>63</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>526</td>\n",
       "      <td>251</td>\n",
       "      <td>289</td>\n",
       "      <td>1033</td>\n",
       "      <td>320</td>\n",
       "      <td>143</td>\n",
       "      <td>117</td>\n",
       "      <td>169</td>\n",
       "      <td>93</td>\n",
       "      <td>75</td>\n",
       "      <td>279</td>\n",
       "      <td>80</td>\n",
       "      <td>49</td>\n",
       "      <td>54</td>\n",
       "      <td>22</td>\n",
       "      <td>34</td>\n",
       "      <td>397</td>\n",
       "      <td>35</td>\n",
       "      <td>154</td>\n",
       "      <td>2</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>275</td>\n",
       "      <td>49</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>152</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>53</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>166</td>\n",
       "      <td>57</td>\n",
       "      <td>116</td>\n",
       "      <td>324</td>\n",
       "      <td>91</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>35</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>32</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1645</td>\n",
       "      <td>672</td>\n",
       "      <td>925</td>\n",
       "      <td>3294</td>\n",
       "      <td>971</td>\n",
       "      <td>407</td>\n",
       "      <td>335</td>\n",
       "      <td>536</td>\n",
       "      <td>166</td>\n",
       "      <td>249</td>\n",
       "      <td>1468</td>\n",
       "      <td>373</td>\n",
       "      <td>465</td>\n",
       "      <td>106</td>\n",
       "      <td>102</td>\n",
       "      <td>147</td>\n",
       "      <td>904</td>\n",
       "      <td>386</td>\n",
       "      <td>2226</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>290</td>\n",
       "      <td>217</td>\n",
       "      <td>230</td>\n",
       "      <td>758</td>\n",
       "      <td>208</td>\n",
       "      <td>84</td>\n",
       "      <td>62</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>33</td>\n",
       "      <td>144</td>\n",
       "      <td>56</td>\n",
       "      <td>52</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>93</td>\n",
       "      <td>124</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>181</td>\n",
       "      <td>53</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>149</td>\n",
       "      <td>119</td>\n",
       "      <td>78</td>\n",
       "      <td>246</td>\n",
       "      <td>90</td>\n",
       "      <td>20</td>\n",
       "      <td>56</td>\n",
       "      <td>43</td>\n",
       "      <td>15</td>\n",
       "      <td>19</td>\n",
       "      <td>65</td>\n",
       "      <td>21</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>56</td>\n",
       "      <td>8</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>469</td>\n",
       "      <td>283</td>\n",
       "      <td>155</td>\n",
       "      <td>1022</td>\n",
       "      <td>291</td>\n",
       "      <td>80</td>\n",
       "      <td>182</td>\n",
       "      <td>109</td>\n",
       "      <td>38</td>\n",
       "      <td>73</td>\n",
       "      <td>219</td>\n",
       "      <td>63</td>\n",
       "      <td>73</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>41</td>\n",
       "      <td>225</td>\n",
       "      <td>55</td>\n",
       "      <td>214</td>\n",
       "      <td>1</td>\n",
       "      <td>8294</td>\n",
       "      <td>4360</td>\n",
       "      <td>4356</td>\n",
       "      <td>12954</td>\n",
       "      <td>4495</td>\n",
       "      <td>1252</td>\n",
       "      <td>1902</td>\n",
       "      <td>1797</td>\n",
       "      <td>1284</td>\n",
       "      <td>737</td>\n",
       "      <td>3570</td>\n",
       "      <td>1301</td>\n",
       "      <td>1014</td>\n",
       "      <td>394</td>\n",
       "      <td>84</td>\n",
       "      <td>385</td>\n",
       "      <td>2466</td>\n",
       "      <td>827</td>\n",
       "      <td>2371</td>\n",
       "      <td>7</td>\n",
       "      <td>425</td>\n",
       "      <td>225</td>\n",
       "      <td>208</td>\n",
       "      <td>1078</td>\n",
       "      <td>299</td>\n",
       "      <td>145</td>\n",
       "      <td>117</td>\n",
       "      <td>131</td>\n",
       "      <td>66</td>\n",
       "      <td>42</td>\n",
       "      <td>223</td>\n",
       "      <td>80</td>\n",
       "      <td>40</td>\n",
       "      <td>37</td>\n",
       "      <td>26</td>\n",
       "      <td>40</td>\n",
       "      <td>112</td>\n",
       "      <td>20</td>\n",
       "      <td>296</td>\n",
       "      <td>18</td>\n",
       "      <td>61920</td>\n",
       "      <td>32114</td>\n",
       "      <td>29224</td>\n",
       "      <td>91856</td>\n",
       "      <td>29239</td>\n",
       "      <td>8608</td>\n",
       "      <td>16672</td>\n",
       "      <td>17134</td>\n",
       "      <td>9658</td>\n",
       "      <td>4351</td>\n",
       "      <td>25975</td>\n",
       "      <td>10960</td>\n",
       "      <td>5088</td>\n",
       "      <td>1976</td>\n",
       "      <td>559</td>\n",
       "      <td>3245</td>\n",
       "      <td>22586</td>\n",
       "      <td>6915</td>\n",
       "      <td>23598</td>\n",
       "      <td>74</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>97</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>132</td>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>204</td>\n",
       "      <td>77</td>\n",
       "      <td>36</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>45</td>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>369</td>\n",
       "      <td>166</td>\n",
       "      <td>212</td>\n",
       "      <td>598</td>\n",
       "      <td>197</td>\n",
       "      <td>77</td>\n",
       "      <td>60</td>\n",
       "      <td>65</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>318</td>\n",
       "      <td>106</td>\n",
       "      <td>55</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>70</td>\n",
       "      <td>147</td>\n",
       "      <td>49</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>662</td>\n",
       "      <td>296</td>\n",
       "      <td>637</td>\n",
       "      <td>1817</td>\n",
       "      <td>490</td>\n",
       "      <td>235</td>\n",
       "      <td>207</td>\n",
       "      <td>207</td>\n",
       "      <td>106</td>\n",
       "      <td>41</td>\n",
       "      <td>285</td>\n",
       "      <td>104</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>48</td>\n",
       "      <td>23</td>\n",
       "      <td>305</td>\n",
       "      <td>79</td>\n",
       "      <td>701</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>34</td>\n",
       "      <td>15</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>26</td>\n",
       "      <td>12</td>\n",
       "      <td>67</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>32</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>77</td>\n",
       "      <td>48</td>\n",
       "      <td>299</td>\n",
       "      <td>74</td>\n",
       "      <td>28</td>\n",
       "      <td>30</td>\n",
       "      <td>41</td>\n",
       "      <td>31</td>\n",
       "      <td>9</td>\n",
       "      <td>52</td>\n",
       "      <td>28</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>77</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>316</td>\n",
       "      <td>172</td>\n",
       "      <td>96</td>\n",
       "      <td>537</td>\n",
       "      <td>137</td>\n",
       "      <td>50</td>\n",
       "      <td>157</td>\n",
       "      <td>124</td>\n",
       "      <td>73</td>\n",
       "      <td>22</td>\n",
       "      <td>140</td>\n",
       "      <td>27</td>\n",
       "      <td>34</td>\n",
       "      <td>74</td>\n",
       "      <td>7</td>\n",
       "      <td>24</td>\n",
       "      <td>204</td>\n",
       "      <td>96</td>\n",
       "      <td>449</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>130</td>\n",
       "      <td>49</td>\n",
       "      <td>24</td>\n",
       "      <td>136</td>\n",
       "      <td>53</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>8</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>411</td>\n",
       "      <td>200</td>\n",
       "      <td>117</td>\n",
       "      <td>621</td>\n",
       "      <td>262</td>\n",
       "      <td>82</td>\n",
       "      <td>125</td>\n",
       "      <td>105</td>\n",
       "      <td>31</td>\n",
       "      <td>38</td>\n",
       "      <td>296</td>\n",
       "      <td>51</td>\n",
       "      <td>67</td>\n",
       "      <td>51</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>111</td>\n",
       "      <td>60</td>\n",
       "      <td>840</td>\n",
       "      <td>0</td>\n",
       "      <td>242</td>\n",
       "      <td>181</td>\n",
       "      <td>76</td>\n",
       "      <td>266</td>\n",
       "      <td>133</td>\n",
       "      <td>31</td>\n",
       "      <td>59</td>\n",
       "      <td>28</td>\n",
       "      <td>34</td>\n",
       "      <td>29</td>\n",
       "      <td>76</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>75</td>\n",
       "      <td>17</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>964</td>\n",
       "      <td>450</td>\n",
       "      <td>475</td>\n",
       "      <td>1450</td>\n",
       "      <td>487</td>\n",
       "      <td>110</td>\n",
       "      <td>146</td>\n",
       "      <td>199</td>\n",
       "      <td>75</td>\n",
       "      <td>100</td>\n",
       "      <td>389</td>\n",
       "      <td>85</td>\n",
       "      <td>80</td>\n",
       "      <td>39</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>264</td>\n",
       "      <td>139</td>\n",
       "      <td>175</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018-02-05/2018-02-11</th>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>49</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>64</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>28</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>639</td>\n",
       "      <td>250</td>\n",
       "      <td>946</td>\n",
       "      <td>2613</td>\n",
       "      <td>603</td>\n",
       "      <td>125</td>\n",
       "      <td>161</td>\n",
       "      <td>126</td>\n",
       "      <td>57</td>\n",
       "      <td>58</td>\n",
       "      <td>190</td>\n",
       "      <td>76</td>\n",
       "      <td>35</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>33</td>\n",
       "      <td>217</td>\n",
       "      <td>26</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1175</td>\n",
       "      <td>368</td>\n",
       "      <td>453</td>\n",
       "      <td>1805</td>\n",
       "      <td>703</td>\n",
       "      <td>178</td>\n",
       "      <td>310</td>\n",
       "      <td>344</td>\n",
       "      <td>100</td>\n",
       "      <td>124</td>\n",
       "      <td>614</td>\n",
       "      <td>173</td>\n",
       "      <td>117</td>\n",
       "      <td>42</td>\n",
       "      <td>94</td>\n",
       "      <td>76</td>\n",
       "      <td>322</td>\n",
       "      <td>232</td>\n",
       "      <td>1445</td>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>38</td>\n",
       "      <td>29</td>\n",
       "      <td>241</td>\n",
       "      <td>88</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>16</td>\n",
       "      <td>69</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>335</td>\n",
       "      <td>75</td>\n",
       "      <td>191</td>\n",
       "      <td>887</td>\n",
       "      <td>219</td>\n",
       "      <td>110</td>\n",
       "      <td>71</td>\n",
       "      <td>48</td>\n",
       "      <td>56</td>\n",
       "      <td>60</td>\n",
       "      <td>83</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>71</td>\n",
       "      <td>14</td>\n",
       "      <td>103</td>\n",
       "      <td>12</td>\n",
       "      <td>74</td>\n",
       "      <td>54</td>\n",
       "      <td>83</td>\n",
       "      <td>148</td>\n",
       "      <td>75</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>317</td>\n",
       "      <td>90</td>\n",
       "      <td>187</td>\n",
       "      <td>936</td>\n",
       "      <td>207</td>\n",
       "      <td>104</td>\n",
       "      <td>62</td>\n",
       "      <td>63</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>87</td>\n",
       "      <td>39</td>\n",
       "      <td>13</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>68</td>\n",
       "      <td>13</td>\n",
       "      <td>67</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "      <td>159</td>\n",
       "      <td>82</td>\n",
       "      <td>25</td>\n",
       "      <td>39</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>186</td>\n",
       "      <td>77</td>\n",
       "      <td>157</td>\n",
       "      <td>676</td>\n",
       "      <td>126</td>\n",
       "      <td>60</td>\n",
       "      <td>49</td>\n",
       "      <td>41</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>67</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>37</td>\n",
       "      <td>15</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>5245</td>\n",
       "      <td>2804</td>\n",
       "      <td>2756</td>\n",
       "      <td>7174</td>\n",
       "      <td>2495</td>\n",
       "      <td>854</td>\n",
       "      <td>1311</td>\n",
       "      <td>993</td>\n",
       "      <td>832</td>\n",
       "      <td>351</td>\n",
       "      <td>1936</td>\n",
       "      <td>828</td>\n",
       "      <td>390</td>\n",
       "      <td>143</td>\n",
       "      <td>49</td>\n",
       "      <td>164</td>\n",
       "      <td>1552</td>\n",
       "      <td>417</td>\n",
       "      <td>1432</td>\n",
       "      <td>5</td>\n",
       "      <td>204</td>\n",
       "      <td>88</td>\n",
       "      <td>264</td>\n",
       "      <td>611</td>\n",
       "      <td>206</td>\n",
       "      <td>24</td>\n",
       "      <td>45</td>\n",
       "      <td>50</td>\n",
       "      <td>54</td>\n",
       "      <td>18</td>\n",
       "      <td>51</td>\n",
       "      <td>42</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>81</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>108</td>\n",
       "      <td>69</td>\n",
       "      <td>75</td>\n",
       "      <td>295</td>\n",
       "      <td>176</td>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>91</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>82</td>\n",
       "      <td>21</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>64</td>\n",
       "      <td>30</td>\n",
       "      <td>199</td>\n",
       "      <td>55</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>10</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>79</td>\n",
       "      <td>80</td>\n",
       "      <td>56</td>\n",
       "      <td>210</td>\n",
       "      <td>55</td>\n",
       "      <td>20</td>\n",
       "      <td>26</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>103</td>\n",
       "      <td>78</td>\n",
       "      <td>46</td>\n",
       "      <td>104</td>\n",
       "      <td>74</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>51</td>\n",
       "      <td>9</td>\n",
       "      <td>27</td>\n",
       "      <td>68</td>\n",
       "      <td>31</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>573</td>\n",
       "      <td>270</td>\n",
       "      <td>312</td>\n",
       "      <td>1092</td>\n",
       "      <td>215</td>\n",
       "      <td>53</td>\n",
       "      <td>102</td>\n",
       "      <td>129</td>\n",
       "      <td>53</td>\n",
       "      <td>49</td>\n",
       "      <td>334</td>\n",
       "      <td>149</td>\n",
       "      <td>45</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>21</td>\n",
       "      <td>227</td>\n",
       "      <td>47</td>\n",
       "      <td>299</td>\n",
       "      <td>1</td>\n",
       "      <td>162</td>\n",
       "      <td>96</td>\n",
       "      <td>51</td>\n",
       "      <td>269</td>\n",
       "      <td>71</td>\n",
       "      <td>18</td>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>62</td>\n",
       "      <td>32</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>61</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>1211</td>\n",
       "      <td>550</td>\n",
       "      <td>645</td>\n",
       "      <td>2294</td>\n",
       "      <td>578</td>\n",
       "      <td>163</td>\n",
       "      <td>366</td>\n",
       "      <td>373</td>\n",
       "      <td>117</td>\n",
       "      <td>157</td>\n",
       "      <td>529</td>\n",
       "      <td>148</td>\n",
       "      <td>60</td>\n",
       "      <td>145</td>\n",
       "      <td>74</td>\n",
       "      <td>38</td>\n",
       "      <td>901</td>\n",
       "      <td>132</td>\n",
       "      <td>476</td>\n",
       "      <td>75</td>\n",
       "      <td>172</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "      <td>332</td>\n",
       "      <td>54</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>5</td>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>46</td>\n",
       "      <td>24</td>\n",
       "      <td>75</td>\n",
       "      <td>1</td>\n",
       "      <td>27</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>29</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>15</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>335</td>\n",
       "      <td>128</td>\n",
       "      <td>155</td>\n",
       "      <td>577</td>\n",
       "      <td>123</td>\n",
       "      <td>64</td>\n",
       "      <td>99</td>\n",
       "      <td>93</td>\n",
       "      <td>69</td>\n",
       "      <td>25</td>\n",
       "      <td>109</td>\n",
       "      <td>88</td>\n",
       "      <td>20</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>184</td>\n",
       "      <td>61</td>\n",
       "      <td>349</td>\n",
       "      <td>46</td>\n",
       "      <td>118</td>\n",
       "      <td>101</td>\n",
       "      <td>36</td>\n",
       "      <td>241</td>\n",
       "      <td>37</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>70</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>128</td>\n",
       "      <td>59</td>\n",
       "      <td>58</td>\n",
       "      <td>290</td>\n",
       "      <td>142</td>\n",
       "      <td>25</td>\n",
       "      <td>29</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>36</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>35</td>\n",
       "      <td>9</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>91</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>383</td>\n",
       "      <td>191</td>\n",
       "      <td>245</td>\n",
       "      <td>746</td>\n",
       "      <td>231</td>\n",
       "      <td>119</td>\n",
       "      <td>109</td>\n",
       "      <td>126</td>\n",
       "      <td>61</td>\n",
       "      <td>27</td>\n",
       "      <td>132</td>\n",
       "      <td>81</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>163</td>\n",
       "      <td>17</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>20</td>\n",
       "      <td>38</td>\n",
       "      <td>89</td>\n",
       "      <td>40</td>\n",
       "      <td>13</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>111</td>\n",
       "      <td>198</td>\n",
       "      <td>463</td>\n",
       "      <td>115</td>\n",
       "      <td>36</td>\n",
       "      <td>40</td>\n",
       "      <td>23</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>85</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>26</td>\n",
       "      <td>44</td>\n",
       "      <td>200</td>\n",
       "      <td>56</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>36</td>\n",
       "      <td>10</td>\n",
       "      <td>38</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>51</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>5840</td>\n",
       "      <td>3167</td>\n",
       "      <td>2477</td>\n",
       "      <td>8090</td>\n",
       "      <td>2631</td>\n",
       "      <td>961</td>\n",
       "      <td>1194</td>\n",
       "      <td>1690</td>\n",
       "      <td>759</td>\n",
       "      <td>365</td>\n",
       "      <td>2190</td>\n",
       "      <td>817</td>\n",
       "      <td>571</td>\n",
       "      <td>257</td>\n",
       "      <td>25</td>\n",
       "      <td>287</td>\n",
       "      <td>1756</td>\n",
       "      <td>493</td>\n",
       "      <td>1808</td>\n",
       "      <td>1</td>\n",
       "      <td>313</td>\n",
       "      <td>142</td>\n",
       "      <td>107</td>\n",
       "      <td>519</td>\n",
       "      <td>133</td>\n",
       "      <td>65</td>\n",
       "      <td>30</td>\n",
       "      <td>190</td>\n",
       "      <td>63</td>\n",
       "      <td>46</td>\n",
       "      <td>188</td>\n",
       "      <td>42</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>246</td>\n",
       "      <td>6</td>\n",
       "      <td>110</td>\n",
       "      <td>0</td>\n",
       "      <td>97</td>\n",
       "      <td>42</td>\n",
       "      <td>28</td>\n",
       "      <td>179</td>\n",
       "      <td>43</td>\n",
       "      <td>10</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>576</td>\n",
       "      <td>308</td>\n",
       "      <td>251</td>\n",
       "      <td>973</td>\n",
       "      <td>378</td>\n",
       "      <td>125</td>\n",
       "      <td>134</td>\n",
       "      <td>201</td>\n",
       "      <td>113</td>\n",
       "      <td>83</td>\n",
       "      <td>283</td>\n",
       "      <td>141</td>\n",
       "      <td>56</td>\n",
       "      <td>67</td>\n",
       "      <td>37</td>\n",
       "      <td>38</td>\n",
       "      <td>320</td>\n",
       "      <td>49</td>\n",
       "      <td>140</td>\n",
       "      <td>2</td>\n",
       "      <td>109</td>\n",
       "      <td>58</td>\n",
       "      <td>27</td>\n",
       "      <td>130</td>\n",
       "      <td>44</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>45</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>68</td>\n",
       "      <td>20</td>\n",
       "      <td>25</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>51</td>\n",
       "      <td>32</td>\n",
       "      <td>6</td>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>0</td>\n",
       "      <td>4204</td>\n",
       "      <td>1999</td>\n",
       "      <td>2288</td>\n",
       "      <td>7653</td>\n",
       "      <td>3146</td>\n",
       "      <td>1219</td>\n",
       "      <td>955</td>\n",
       "      <td>1043</td>\n",
       "      <td>659</td>\n",
       "      <td>404</td>\n",
       "      <td>1710</td>\n",
       "      <td>700</td>\n",
       "      <td>525</td>\n",
       "      <td>169</td>\n",
       "      <td>166</td>\n",
       "      <td>236</td>\n",
       "      <td>1286</td>\n",
       "      <td>239</td>\n",
       "      <td>1030</td>\n",
       "      <td>1</td>\n",
       "      <td>351</td>\n",
       "      <td>117</td>\n",
       "      <td>123</td>\n",
       "      <td>544</td>\n",
       "      <td>93</td>\n",
       "      <td>42</td>\n",
       "      <td>52</td>\n",
       "      <td>56</td>\n",
       "      <td>54</td>\n",
       "      <td>7</td>\n",
       "      <td>214</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>56</td>\n",
       "      <td>12</td>\n",
       "      <td>59</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>47</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>206</td>\n",
       "      <td>138</td>\n",
       "      <td>70</td>\n",
       "      <td>420</td>\n",
       "      <td>137</td>\n",
       "      <td>50</td>\n",
       "      <td>69</td>\n",
       "      <td>94</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>103</td>\n",
       "      <td>38</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>84</td>\n",
       "      <td>18</td>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>162</td>\n",
       "      <td>184</td>\n",
       "      <td>634</td>\n",
       "      <td>211</td>\n",
       "      <td>50</td>\n",
       "      <td>80</td>\n",
       "      <td>74</td>\n",
       "      <td>33</td>\n",
       "      <td>39</td>\n",
       "      <td>204</td>\n",
       "      <td>42</td>\n",
       "      <td>42</td>\n",
       "      <td>34</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>144</td>\n",
       "      <td>51</td>\n",
       "      <td>144</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>44</td>\n",
       "      <td>62</td>\n",
       "      <td>103</td>\n",
       "      <td>42</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>24</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>29</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>39</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>116</td>\n",
       "      <td>114</td>\n",
       "      <td>789</td>\n",
       "      <td>193</td>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>58</td>\n",
       "      <td>23</td>\n",
       "      <td>27</td>\n",
       "      <td>167</td>\n",
       "      <td>43</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>26</td>\n",
       "      <td>53</td>\n",
       "      <td>17</td>\n",
       "      <td>87</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>53</td>\n",
       "      <td>63</td>\n",
       "      <td>193</td>\n",
       "      <td>126</td>\n",
       "      <td>26</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>50</td>\n",
       "      <td>20</td>\n",
       "      <td>73</td>\n",
       "      <td>36</td>\n",
       "      <td>31</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>232</td>\n",
       "      <td>94</td>\n",
       "      <td>116</td>\n",
       "      <td>356</td>\n",
       "      <td>121</td>\n",
       "      <td>48</td>\n",
       "      <td>85</td>\n",
       "      <td>53</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>69</td>\n",
       "      <td>36</td>\n",
       "      <td>25</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>28</td>\n",
       "      <td>25</td>\n",
       "      <td>104</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>32</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>118</td>\n",
       "      <td>58</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>79</td>\n",
       "      <td>65</td>\n",
       "      <td>128</td>\n",
       "      <td>87</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>70</td>\n",
       "      <td>83</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>86</td>\n",
       "      <td>21</td>\n",
       "      <td>38</td>\n",
       "      <td>0</td>\n",
       "      <td>785</td>\n",
       "      <td>311</td>\n",
       "      <td>613</td>\n",
       "      <td>1973</td>\n",
       "      <td>597</td>\n",
       "      <td>166</td>\n",
       "      <td>174</td>\n",
       "      <td>226</td>\n",
       "      <td>91</td>\n",
       "      <td>70</td>\n",
       "      <td>423</td>\n",
       "      <td>148</td>\n",
       "      <td>147</td>\n",
       "      <td>49</td>\n",
       "      <td>106</td>\n",
       "      <td>110</td>\n",
       "      <td>435</td>\n",
       "      <td>128</td>\n",
       "      <td>691</td>\n",
       "      <td>0</td>\n",
       "      <td>1450</td>\n",
       "      <td>723</td>\n",
       "      <td>851</td>\n",
       "      <td>2662</td>\n",
       "      <td>747</td>\n",
       "      <td>173</td>\n",
       "      <td>295</td>\n",
       "      <td>369</td>\n",
       "      <td>214</td>\n",
       "      <td>106</td>\n",
       "      <td>446</td>\n",
       "      <td>249</td>\n",
       "      <td>178</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>67</td>\n",
       "      <td>361</td>\n",
       "      <td>97</td>\n",
       "      <td>350</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>44</td>\n",
       "      <td>41</td>\n",
       "      <td>95</td>\n",
       "      <td>219</td>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>25</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>43</td>\n",
       "      <td>7</td>\n",
       "      <td>268</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>25</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>43</td>\n",
       "      <td>9</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>188</td>\n",
       "      <td>26</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>115</td>\n",
       "      <td>187</td>\n",
       "      <td>516</td>\n",
       "      <td>248</td>\n",
       "      <td>37</td>\n",
       "      <td>47</td>\n",
       "      <td>191</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>120</td>\n",
       "      <td>49</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>42</td>\n",
       "      <td>148</td>\n",
       "      <td>12</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>145</td>\n",
       "      <td>50</td>\n",
       "      <td>88</td>\n",
       "      <td>273</td>\n",
       "      <td>94</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>38</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>147</td>\n",
       "      <td>80</td>\n",
       "      <td>82</td>\n",
       "      <td>249</td>\n",
       "      <td>62</td>\n",
       "      <td>23</td>\n",
       "      <td>68</td>\n",
       "      <td>43</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>243</td>\n",
       "      <td>110</td>\n",
       "      <td>149</td>\n",
       "      <td>540</td>\n",
       "      <td>137</td>\n",
       "      <td>43</td>\n",
       "      <td>66</td>\n",
       "      <td>47</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>72</td>\n",
       "      <td>19</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2178</td>\n",
       "      <td>1080</td>\n",
       "      <td>1113</td>\n",
       "      <td>4328</td>\n",
       "      <td>1239</td>\n",
       "      <td>293</td>\n",
       "      <td>456</td>\n",
       "      <td>581</td>\n",
       "      <td>188</td>\n",
       "      <td>182</td>\n",
       "      <td>1106</td>\n",
       "      <td>435</td>\n",
       "      <td>222</td>\n",
       "      <td>128</td>\n",
       "      <td>70</td>\n",
       "      <td>104</td>\n",
       "      <td>766</td>\n",
       "      <td>351</td>\n",
       "      <td>967</td>\n",
       "      <td>1</td>\n",
       "      <td>191</td>\n",
       "      <td>58</td>\n",
       "      <td>102</td>\n",
       "      <td>244</td>\n",
       "      <td>113</td>\n",
       "      <td>25</td>\n",
       "      <td>65</td>\n",
       "      <td>31</td>\n",
       "      <td>23</td>\n",
       "      <td>15</td>\n",
       "      <td>64</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>49</td>\n",
       "      <td>8</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>26</td>\n",
       "      <td>198</td>\n",
       "      <td>95</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>39</td>\n",
       "      <td>22</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>19</td>\n",
       "      <td>6</td>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>1202</td>\n",
       "      <td>547</td>\n",
       "      <td>336</td>\n",
       "      <td>1223</td>\n",
       "      <td>620</td>\n",
       "      <td>126</td>\n",
       "      <td>210</td>\n",
       "      <td>147</td>\n",
       "      <td>59</td>\n",
       "      <td>101</td>\n",
       "      <td>410</td>\n",
       "      <td>92</td>\n",
       "      <td>68</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>37</td>\n",
       "      <td>258</td>\n",
       "      <td>43</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>78</td>\n",
       "      <td>134</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>66</td>\n",
       "      <td>77</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>64</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1711</td>\n",
       "      <td>812</td>\n",
       "      <td>1127</td>\n",
       "      <td>2722</td>\n",
       "      <td>1262</td>\n",
       "      <td>350</td>\n",
       "      <td>391</td>\n",
       "      <td>573</td>\n",
       "      <td>274</td>\n",
       "      <td>153</td>\n",
       "      <td>739</td>\n",
       "      <td>290</td>\n",
       "      <td>148</td>\n",
       "      <td>121</td>\n",
       "      <td>30</td>\n",
       "      <td>111</td>\n",
       "      <td>402</td>\n",
       "      <td>156</td>\n",
       "      <td>575</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>99</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>643</td>\n",
       "      <td>317</td>\n",
       "      <td>360</td>\n",
       "      <td>1423</td>\n",
       "      <td>357</td>\n",
       "      <td>103</td>\n",
       "      <td>122</td>\n",
       "      <td>131</td>\n",
       "      <td>99</td>\n",
       "      <td>62</td>\n",
       "      <td>324</td>\n",
       "      <td>143</td>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>217</td>\n",
       "      <td>83</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>119</td>\n",
       "      <td>37</td>\n",
       "      <td>18</td>\n",
       "      <td>266</td>\n",
       "      <td>128</td>\n",
       "      <td>29</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>85</td>\n",
       "      <td>32</td>\n",
       "      <td>30</td>\n",
       "      <td>45</td>\n",
       "      <td>101</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>45</td>\n",
       "      <td>5</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>225</td>\n",
       "      <td>77</td>\n",
       "      <td>25</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>48</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>44</td>\n",
       "      <td>25</td>\n",
       "      <td>73</td>\n",
       "      <td>48</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>13</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>267</td>\n",
       "      <td>102</td>\n",
       "      <td>112</td>\n",
       "      <td>393</td>\n",
       "      <td>96</td>\n",
       "      <td>17</td>\n",
       "      <td>101</td>\n",
       "      <td>56</td>\n",
       "      <td>29</td>\n",
       "      <td>18</td>\n",
       "      <td>112</td>\n",
       "      <td>51</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>62</td>\n",
       "      <td>36</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>247</td>\n",
       "      <td>154</td>\n",
       "      <td>108</td>\n",
       "      <td>382</td>\n",
       "      <td>98</td>\n",
       "      <td>70</td>\n",
       "      <td>52</td>\n",
       "      <td>82</td>\n",
       "      <td>37</td>\n",
       "      <td>36</td>\n",
       "      <td>118</td>\n",
       "      <td>41</td>\n",
       "      <td>40</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>139</td>\n",
       "      <td>10</td>\n",
       "      <td>80</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>144</td>\n",
       "      <td>25</td>\n",
       "      <td>21</td>\n",
       "      <td>23</td>\n",
       "      <td>19</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>42</td>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>133</td>\n",
       "      <td>47</td>\n",
       "      <td>49</td>\n",
       "      <td>163</td>\n",
       "      <td>136</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>32</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>25</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>204</td>\n",
       "      <td>79</td>\n",
       "      <td>128</td>\n",
       "      <td>320</td>\n",
       "      <td>152</td>\n",
       "      <td>68</td>\n",
       "      <td>75</td>\n",
       "      <td>45</td>\n",
       "      <td>20</td>\n",
       "      <td>19</td>\n",
       "      <td>86</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "      <td>3</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>74</td>\n",
       "      <td>42</td>\n",
       "      <td>59</td>\n",
       "      <td>160</td>\n",
       "      <td>41</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>17</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>615</td>\n",
       "      <td>312</td>\n",
       "      <td>371</td>\n",
       "      <td>1495</td>\n",
       "      <td>414</td>\n",
       "      <td>176</td>\n",
       "      <td>139</td>\n",
       "      <td>168</td>\n",
       "      <td>117</td>\n",
       "      <td>96</td>\n",
       "      <td>233</td>\n",
       "      <td>103</td>\n",
       "      <td>101</td>\n",
       "      <td>20</td>\n",
       "      <td>24</td>\n",
       "      <td>37</td>\n",
       "      <td>377</td>\n",
       "      <td>55</td>\n",
       "      <td>250</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7196</td>\n",
       "      <td>4130</td>\n",
       "      <td>3990</td>\n",
       "      <td>12857</td>\n",
       "      <td>3401</td>\n",
       "      <td>1044</td>\n",
       "      <td>1668</td>\n",
       "      <td>2171</td>\n",
       "      <td>1295</td>\n",
       "      <td>940</td>\n",
       "      <td>4492</td>\n",
       "      <td>1455</td>\n",
       "      <td>577</td>\n",
       "      <td>1004</td>\n",
       "      <td>199</td>\n",
       "      <td>402</td>\n",
       "      <td>2590</td>\n",
       "      <td>853</td>\n",
       "      <td>2887</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1792</td>\n",
       "      <td>740</td>\n",
       "      <td>675</td>\n",
       "      <td>2643</td>\n",
       "      <td>1254</td>\n",
       "      <td>329</td>\n",
       "      <td>325</td>\n",
       "      <td>442</td>\n",
       "      <td>173</td>\n",
       "      <td>180</td>\n",
       "      <td>1009</td>\n",
       "      <td>473</td>\n",
       "      <td>492</td>\n",
       "      <td>265</td>\n",
       "      <td>55</td>\n",
       "      <td>126</td>\n",
       "      <td>527</td>\n",
       "      <td>214</td>\n",
       "      <td>1305</td>\n",
       "      <td>5</td>\n",
       "      <td>3938</td>\n",
       "      <td>1827</td>\n",
       "      <td>2283</td>\n",
       "      <td>8075</td>\n",
       "      <td>2491</td>\n",
       "      <td>589</td>\n",
       "      <td>1010</td>\n",
       "      <td>869</td>\n",
       "      <td>498</td>\n",
       "      <td>364</td>\n",
       "      <td>2312</td>\n",
       "      <td>919</td>\n",
       "      <td>947</td>\n",
       "      <td>371</td>\n",
       "      <td>131</td>\n",
       "      <td>390</td>\n",
       "      <td>1417</td>\n",
       "      <td>686</td>\n",
       "      <td>4378</td>\n",
       "      <td>36</td>\n",
       "      <td>1132</td>\n",
       "      <td>538</td>\n",
       "      <td>805</td>\n",
       "      <td>2650</td>\n",
       "      <td>689</td>\n",
       "      <td>195</td>\n",
       "      <td>223</td>\n",
       "      <td>247</td>\n",
       "      <td>127</td>\n",
       "      <td>54</td>\n",
       "      <td>540</td>\n",
       "      <td>144</td>\n",
       "      <td>112</td>\n",
       "      <td>119</td>\n",
       "      <td>29</td>\n",
       "      <td>51</td>\n",
       "      <td>512</td>\n",
       "      <td>127</td>\n",
       "      <td>574</td>\n",
       "      <td>8</td>\n",
       "      <td>34</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>62</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>917</td>\n",
       "      <td>392</td>\n",
       "      <td>319</td>\n",
       "      <td>1533</td>\n",
       "      <td>520</td>\n",
       "      <td>193</td>\n",
       "      <td>243</td>\n",
       "      <td>294</td>\n",
       "      <td>61</td>\n",
       "      <td>101</td>\n",
       "      <td>404</td>\n",
       "      <td>123</td>\n",
       "      <td>116</td>\n",
       "      <td>37</td>\n",
       "      <td>73</td>\n",
       "      <td>54</td>\n",
       "      <td>333</td>\n",
       "      <td>123</td>\n",
       "      <td>925</td>\n",
       "      <td>7</td>\n",
       "      <td>1382</td>\n",
       "      <td>564</td>\n",
       "      <td>938</td>\n",
       "      <td>2842</td>\n",
       "      <td>816</td>\n",
       "      <td>357</td>\n",
       "      <td>344</td>\n",
       "      <td>374</td>\n",
       "      <td>173</td>\n",
       "      <td>90</td>\n",
       "      <td>515</td>\n",
       "      <td>178</td>\n",
       "      <td>142</td>\n",
       "      <td>35</td>\n",
       "      <td>34</td>\n",
       "      <td>61</td>\n",
       "      <td>336</td>\n",
       "      <td>130</td>\n",
       "      <td>379</td>\n",
       "      <td>0</td>\n",
       "      <td>199</td>\n",
       "      <td>112</td>\n",
       "      <td>95</td>\n",
       "      <td>235</td>\n",
       "      <td>76</td>\n",
       "      <td>25</td>\n",
       "      <td>61</td>\n",
       "      <td>37</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>85</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>190</td>\n",
       "      <td>156</td>\n",
       "      <td>882</td>\n",
       "      <td>182</td>\n",
       "      <td>58</td>\n",
       "      <td>63</td>\n",
       "      <td>59</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>92</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>93</td>\n",
       "      <td>20</td>\n",
       "      <td>117</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>408</td>\n",
       "      <td>157</td>\n",
       "      <td>640</td>\n",
       "      <td>1863</td>\n",
       "      <td>342</td>\n",
       "      <td>84</td>\n",
       "      <td>60</td>\n",
       "      <td>112</td>\n",
       "      <td>48</td>\n",
       "      <td>22</td>\n",
       "      <td>106</td>\n",
       "      <td>43</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>147</td>\n",
       "      <td>26</td>\n",
       "      <td>135</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1079</td>\n",
       "      <td>508</td>\n",
       "      <td>422</td>\n",
       "      <td>1565</td>\n",
       "      <td>476</td>\n",
       "      <td>239</td>\n",
       "      <td>188</td>\n",
       "      <td>412</td>\n",
       "      <td>130</td>\n",
       "      <td>208</td>\n",
       "      <td>798</td>\n",
       "      <td>296</td>\n",
       "      <td>156</td>\n",
       "      <td>115</td>\n",
       "      <td>26</td>\n",
       "      <td>100</td>\n",
       "      <td>730</td>\n",
       "      <td>119</td>\n",
       "      <td>408</td>\n",
       "      <td>1</td>\n",
       "      <td>63</td>\n",
       "      <td>32</td>\n",
       "      <td>59</td>\n",
       "      <td>322</td>\n",
       "      <td>58</td>\n",
       "      <td>20</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>13</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>859</td>\n",
       "      <td>377</td>\n",
       "      <td>813</td>\n",
       "      <td>2152</td>\n",
       "      <td>539</td>\n",
       "      <td>130</td>\n",
       "      <td>153</td>\n",
       "      <td>189</td>\n",
       "      <td>69</td>\n",
       "      <td>83</td>\n",
       "      <td>401</td>\n",
       "      <td>157</td>\n",
       "      <td>268</td>\n",
       "      <td>26</td>\n",
       "      <td>73</td>\n",
       "      <td>164</td>\n",
       "      <td>208</td>\n",
       "      <td>75</td>\n",
       "      <td>246</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1017</td>\n",
       "      <td>507</td>\n",
       "      <td>1031</td>\n",
       "      <td>2539</td>\n",
       "      <td>613</td>\n",
       "      <td>193</td>\n",
       "      <td>231</td>\n",
       "      <td>440</td>\n",
       "      <td>128</td>\n",
       "      <td>74</td>\n",
       "      <td>445</td>\n",
       "      <td>188</td>\n",
       "      <td>149</td>\n",
       "      <td>48</td>\n",
       "      <td>59</td>\n",
       "      <td>132</td>\n",
       "      <td>422</td>\n",
       "      <td>98</td>\n",
       "      <td>255</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>166</td>\n",
       "      <td>241</td>\n",
       "      <td>523</td>\n",
       "      <td>136</td>\n",
       "      <td>47</td>\n",
       "      <td>87</td>\n",
       "      <td>49</td>\n",
       "      <td>48</td>\n",
       "      <td>51</td>\n",
       "      <td>116</td>\n",
       "      <td>30</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>43</td>\n",
       "      <td>155</td>\n",
       "      <td>24</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>19</td>\n",
       "      <td>53</td>\n",
       "      <td>175</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>74</td>\n",
       "      <td>32</td>\n",
       "      <td>77</td>\n",
       "      <td>322</td>\n",
       "      <td>84</td>\n",
       "      <td>29</td>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>33</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>64</td>\n",
       "      <td>21</td>\n",
       "      <td>36</td>\n",
       "      <td>267</td>\n",
       "      <td>71</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>595</td>\n",
       "      <td>235</td>\n",
       "      <td>279</td>\n",
       "      <td>1473</td>\n",
       "      <td>277</td>\n",
       "      <td>96</td>\n",
       "      <td>130</td>\n",
       "      <td>146</td>\n",
       "      <td>101</td>\n",
       "      <td>27</td>\n",
       "      <td>342</td>\n",
       "      <td>129</td>\n",
       "      <td>153</td>\n",
       "      <td>40</td>\n",
       "      <td>27</td>\n",
       "      <td>20</td>\n",
       "      <td>174</td>\n",
       "      <td>57</td>\n",
       "      <td>477</td>\n",
       "      <td>1</td>\n",
       "      <td>78</td>\n",
       "      <td>30</td>\n",
       "      <td>63</td>\n",
       "      <td>252</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>94</td>\n",
       "      <td>41</td>\n",
       "      <td>21</td>\n",
       "      <td>149</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>36</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>197</td>\n",
       "      <td>86</td>\n",
       "      <td>91</td>\n",
       "      <td>374</td>\n",
       "      <td>200</td>\n",
       "      <td>47</td>\n",
       "      <td>37</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>17</td>\n",
       "      <td>42</td>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>19</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>69</td>\n",
       "      <td>35</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>49</td>\n",
       "      <td>33</td>\n",
       "      <td>25</td>\n",
       "      <td>122</td>\n",
       "      <td>44</td>\n",
       "      <td>8</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>18</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>47</td>\n",
       "      <td>35</td>\n",
       "      <td>105</td>\n",
       "      <td>37</td>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>7</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>246</td>\n",
       "      <td>106</td>\n",
       "      <td>65</td>\n",
       "      <td>543</td>\n",
       "      <td>146</td>\n",
       "      <td>57</td>\n",
       "      <td>101</td>\n",
       "      <td>94</td>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>130</td>\n",
       "      <td>32</td>\n",
       "      <td>29</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>34</td>\n",
       "      <td>109</td>\n",
       "      <td>55</td>\n",
       "      <td>212</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>18</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>21</td>\n",
       "      <td>25</td>\n",
       "      <td>57</td>\n",
       "      <td>82</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>75</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>259</td>\n",
       "      <td>117</td>\n",
       "      <td>69</td>\n",
       "      <td>218</td>\n",
       "      <td>156</td>\n",
       "      <td>30</td>\n",
       "      <td>95</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>62</td>\n",
       "      <td>20</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>94</td>\n",
       "      <td>12</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>44</td>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "      <td>66</td>\n",
       "      <td>105</td>\n",
       "      <td>173</td>\n",
       "      <td>86</td>\n",
       "      <td>7</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>44</td>\n",
       "      <td>54</td>\n",
       "      <td>29</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>14</td>\n",
       "      <td>29</td>\n",
       "      <td>75</td>\n",
       "      <td>34</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>30</td>\n",
       "      <td>8</td>\n",
       "      <td>18</td>\n",
       "      <td>66</td>\n",
       "      <td>13</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>63</td>\n",
       "      <td>84</td>\n",
       "      <td>347</td>\n",
       "      <td>124</td>\n",
       "      <td>37</td>\n",
       "      <td>27</td>\n",
       "      <td>26</td>\n",
       "      <td>18</td>\n",
       "      <td>10</td>\n",
       "      <td>89</td>\n",
       "      <td>32</td>\n",
       "      <td>11</td>\n",
       "      <td>19</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>85</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>66</td>\n",
       "      <td>33</td>\n",
       "      <td>21</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>11</td>\n",
       "      <td>24</td>\n",
       "      <td>92</td>\n",
       "      <td>26</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>279</td>\n",
       "      <td>128</td>\n",
       "      <td>93</td>\n",
       "      <td>371</td>\n",
       "      <td>135</td>\n",
       "      <td>49</td>\n",
       "      <td>42</td>\n",
       "      <td>53</td>\n",
       "      <td>36</td>\n",
       "      <td>29</td>\n",
       "      <td>90</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>142</td>\n",
       "      <td>63</td>\n",
       "      <td>561</td>\n",
       "      <td>1033</td>\n",
       "      <td>243</td>\n",
       "      <td>33</td>\n",
       "      <td>64</td>\n",
       "      <td>26</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>567</td>\n",
       "      <td>481</td>\n",
       "      <td>204</td>\n",
       "      <td>1166</td>\n",
       "      <td>187</td>\n",
       "      <td>72</td>\n",
       "      <td>129</td>\n",
       "      <td>355</td>\n",
       "      <td>60</td>\n",
       "      <td>192</td>\n",
       "      <td>286</td>\n",
       "      <td>166</td>\n",
       "      <td>118</td>\n",
       "      <td>52</td>\n",
       "      <td>25</td>\n",
       "      <td>74</td>\n",
       "      <td>616</td>\n",
       "      <td>30</td>\n",
       "      <td>115</td>\n",
       "      <td>0</td>\n",
       "      <td>1008</td>\n",
       "      <td>503</td>\n",
       "      <td>460</td>\n",
       "      <td>1854</td>\n",
       "      <td>423</td>\n",
       "      <td>257</td>\n",
       "      <td>256</td>\n",
       "      <td>219</td>\n",
       "      <td>127</td>\n",
       "      <td>56</td>\n",
       "      <td>485</td>\n",
       "      <td>212</td>\n",
       "      <td>136</td>\n",
       "      <td>23</td>\n",
       "      <td>66</td>\n",
       "      <td>77</td>\n",
       "      <td>412</td>\n",
       "      <td>124</td>\n",
       "      <td>549</td>\n",
       "      <td>0</td>\n",
       "      <td>1092</td>\n",
       "      <td>524</td>\n",
       "      <td>556</td>\n",
       "      <td>1328</td>\n",
       "      <td>521</td>\n",
       "      <td>213</td>\n",
       "      <td>245</td>\n",
       "      <td>200</td>\n",
       "      <td>194</td>\n",
       "      <td>130</td>\n",
       "      <td>430</td>\n",
       "      <td>165</td>\n",
       "      <td>75</td>\n",
       "      <td>17</td>\n",
       "      <td>8</td>\n",
       "      <td>26</td>\n",
       "      <td>472</td>\n",
       "      <td>34</td>\n",
       "      <td>149</td>\n",
       "      <td>7</td>\n",
       "      <td>93</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>112</td>\n",
       "      <td>52</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>138</td>\n",
       "      <td>88</td>\n",
       "      <td>41</td>\n",
       "      <td>185</td>\n",
       "      <td>53</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>19</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "      <td>73</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>15</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>30</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6554</td>\n",
       "      <td>3241</td>\n",
       "      <td>1761</td>\n",
       "      <td>7673</td>\n",
       "      <td>3399</td>\n",
       "      <td>567</td>\n",
       "      <td>1180</td>\n",
       "      <td>1254</td>\n",
       "      <td>514</td>\n",
       "      <td>773</td>\n",
       "      <td>2872</td>\n",
       "      <td>907</td>\n",
       "      <td>685</td>\n",
       "      <td>291</td>\n",
       "      <td>226</td>\n",
       "      <td>141</td>\n",
       "      <td>1736</td>\n",
       "      <td>634</td>\n",
       "      <td>1903</td>\n",
       "      <td>35</td>\n",
       "      <td>547</td>\n",
       "      <td>284</td>\n",
       "      <td>334</td>\n",
       "      <td>1207</td>\n",
       "      <td>287</td>\n",
       "      <td>95</td>\n",
       "      <td>146</td>\n",
       "      <td>130</td>\n",
       "      <td>105</td>\n",
       "      <td>58</td>\n",
       "      <td>212</td>\n",
       "      <td>112</td>\n",
       "      <td>30</td>\n",
       "      <td>33</td>\n",
       "      <td>3</td>\n",
       "      <td>95</td>\n",
       "      <td>203</td>\n",
       "      <td>39</td>\n",
       "      <td>222</td>\n",
       "      <td>0</td>\n",
       "      <td>269</td>\n",
       "      <td>116</td>\n",
       "      <td>179</td>\n",
       "      <td>502</td>\n",
       "      <td>222</td>\n",
       "      <td>61</td>\n",
       "      <td>65</td>\n",
       "      <td>61</td>\n",
       "      <td>39</td>\n",
       "      <td>17</td>\n",
       "      <td>76</td>\n",
       "      <td>43</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>15</td>\n",
       "      <td>49</td>\n",
       "      <td>2</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>165</td>\n",
       "      <td>221</td>\n",
       "      <td>810</td>\n",
       "      <td>204</td>\n",
       "      <td>137</td>\n",
       "      <td>79</td>\n",
       "      <td>69</td>\n",
       "      <td>37</td>\n",
       "      <td>28</td>\n",
       "      <td>82</td>\n",
       "      <td>51</td>\n",
       "      <td>13</td>\n",
       "      <td>20</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>111</td>\n",
       "      <td>33</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>17</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>125</td>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>1001</td>\n",
       "      <td>527</td>\n",
       "      <td>499</td>\n",
       "      <td>1662</td>\n",
       "      <td>536</td>\n",
       "      <td>139</td>\n",
       "      <td>249</td>\n",
       "      <td>249</td>\n",
       "      <td>92</td>\n",
       "      <td>71</td>\n",
       "      <td>316</td>\n",
       "      <td>113</td>\n",
       "      <td>80</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>41</td>\n",
       "      <td>248</td>\n",
       "      <td>43</td>\n",
       "      <td>233</td>\n",
       "      <td>2</td>\n",
       "      <td>172</td>\n",
       "      <td>125</td>\n",
       "      <td>46</td>\n",
       "      <td>299</td>\n",
       "      <td>103</td>\n",
       "      <td>43</td>\n",
       "      <td>33</td>\n",
       "      <td>63</td>\n",
       "      <td>7</td>\n",
       "      <td>31</td>\n",
       "      <td>109</td>\n",
       "      <td>38</td>\n",
       "      <td>32</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>25</td>\n",
       "      <td>26</td>\n",
       "      <td>84</td>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>50</td>\n",
       "      <td>26</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>222</td>\n",
       "      <td>85</td>\n",
       "      <td>189</td>\n",
       "      <td>517</td>\n",
       "      <td>108</td>\n",
       "      <td>40</td>\n",
       "      <td>51</td>\n",
       "      <td>43</td>\n",
       "      <td>36</td>\n",
       "      <td>8</td>\n",
       "      <td>73</td>\n",
       "      <td>35</td>\n",
       "      <td>23</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>83</td>\n",
       "      <td>14</td>\n",
       "      <td>52</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3995</td>\n",
       "      <td>1566</td>\n",
       "      <td>1214</td>\n",
       "      <td>5089</td>\n",
       "      <td>1738</td>\n",
       "      <td>522</td>\n",
       "      <td>647</td>\n",
       "      <td>885</td>\n",
       "      <td>416</td>\n",
       "      <td>506</td>\n",
       "      <td>2235</td>\n",
       "      <td>609</td>\n",
       "      <td>346</td>\n",
       "      <td>414</td>\n",
       "      <td>69</td>\n",
       "      <td>340</td>\n",
       "      <td>1726</td>\n",
       "      <td>546</td>\n",
       "      <td>3138</td>\n",
       "      <td>10</td>\n",
       "      <td>732</td>\n",
       "      <td>412</td>\n",
       "      <td>425</td>\n",
       "      <td>1044</td>\n",
       "      <td>462</td>\n",
       "      <td>55</td>\n",
       "      <td>106</td>\n",
       "      <td>126</td>\n",
       "      <td>39</td>\n",
       "      <td>50</td>\n",
       "      <td>773</td>\n",
       "      <td>158</td>\n",
       "      <td>73</td>\n",
       "      <td>69</td>\n",
       "      <td>11</td>\n",
       "      <td>125</td>\n",
       "      <td>260</td>\n",
       "      <td>63</td>\n",
       "      <td>274</td>\n",
       "      <td>11</td>\n",
       "      <td>101</td>\n",
       "      <td>76</td>\n",
       "      <td>54</td>\n",
       "      <td>219</td>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>15</td>\n",
       "      <td>22</td>\n",
       "      <td>17</td>\n",
       "      <td>70</td>\n",
       "      <td>21</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>47</td>\n",
       "      <td>3</td>\n",
       "      <td>52</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>59</td>\n",
       "      <td>102</td>\n",
       "      <td>303</td>\n",
       "      <td>110</td>\n",
       "      <td>34</td>\n",
       "      <td>51</td>\n",
       "      <td>59</td>\n",
       "      <td>22</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>74</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>107</td>\n",
       "      <td>52</td>\n",
       "      <td>82</td>\n",
       "      <td>260</td>\n",
       "      <td>60</td>\n",
       "      <td>14</td>\n",
       "      <td>24</td>\n",
       "      <td>18</td>\n",
       "      <td>14</td>\n",
       "      <td>7</td>\n",
       "      <td>30</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>10</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>251</td>\n",
       "      <td>127</td>\n",
       "      <td>190</td>\n",
       "      <td>781</td>\n",
       "      <td>395</td>\n",
       "      <td>116</td>\n",
       "      <td>123</td>\n",
       "      <td>33</td>\n",
       "      <td>14</td>\n",
       "      <td>15</td>\n",
       "      <td>162</td>\n",
       "      <td>43</td>\n",
       "      <td>14</td>\n",
       "      <td>12</td>\n",
       "      <td>6</td>\n",
       "      <td>53</td>\n",
       "      <td>53</td>\n",
       "      <td>5</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>38</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>48</td>\n",
       "      <td>78</td>\n",
       "      <td>175</td>\n",
       "      <td>72</td>\n",
       "      <td>45</td>\n",
       "      <td>6</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>29</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>27</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>234</td>\n",
       "      <td>82</td>\n",
       "      <td>176</td>\n",
       "      <td>428</td>\n",
       "      <td>113</td>\n",
       "      <td>60</td>\n",
       "      <td>43</td>\n",
       "      <td>58</td>\n",
       "      <td>34</td>\n",
       "      <td>24</td>\n",
       "      <td>72</td>\n",
       "      <td>49</td>\n",
       "      <td>21</td>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>84</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>0</td>\n",
       "      <td>2116</td>\n",
       "      <td>1116</td>\n",
       "      <td>930</td>\n",
       "      <td>2593</td>\n",
       "      <td>1097</td>\n",
       "      <td>265</td>\n",
       "      <td>369</td>\n",
       "      <td>408</td>\n",
       "      <td>470</td>\n",
       "      <td>304</td>\n",
       "      <td>1095</td>\n",
       "      <td>293</td>\n",
       "      <td>316</td>\n",
       "      <td>71</td>\n",
       "      <td>45</td>\n",
       "      <td>157</td>\n",
       "      <td>631</td>\n",
       "      <td>114</td>\n",
       "      <td>806</td>\n",
       "      <td>1</td>\n",
       "      <td>221</td>\n",
       "      <td>115</td>\n",
       "      <td>192</td>\n",
       "      <td>284</td>\n",
       "      <td>113</td>\n",
       "      <td>53</td>\n",
       "      <td>172</td>\n",
       "      <td>57</td>\n",
       "      <td>29</td>\n",
       "      <td>6</td>\n",
       "      <td>72</td>\n",
       "      <td>52</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>33</td>\n",
       "      <td>15</td>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>4207</td>\n",
       "      <td>1782</td>\n",
       "      <td>1585</td>\n",
       "      <td>6399</td>\n",
       "      <td>2092</td>\n",
       "      <td>730</td>\n",
       "      <td>646</td>\n",
       "      <td>1208</td>\n",
       "      <td>856</td>\n",
       "      <td>370</td>\n",
       "      <td>2178</td>\n",
       "      <td>801</td>\n",
       "      <td>615</td>\n",
       "      <td>125</td>\n",
       "      <td>163</td>\n",
       "      <td>315</td>\n",
       "      <td>1124</td>\n",
       "      <td>271</td>\n",
       "      <td>2232</td>\n",
       "      <td>21</td>\n",
       "      <td>157</td>\n",
       "      <td>96</td>\n",
       "      <td>69</td>\n",
       "      <td>410</td>\n",
       "      <td>126</td>\n",
       "      <td>77</td>\n",
       "      <td>105</td>\n",
       "      <td>44</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>92</td>\n",
       "      <td>54</td>\n",
       "      <td>23</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>108</td>\n",
       "      <td>18</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>679</td>\n",
       "      <td>341</td>\n",
       "      <td>420</td>\n",
       "      <td>1753</td>\n",
       "      <td>564</td>\n",
       "      <td>181</td>\n",
       "      <td>191</td>\n",
       "      <td>208</td>\n",
       "      <td>102</td>\n",
       "      <td>73</td>\n",
       "      <td>381</td>\n",
       "      <td>121</td>\n",
       "      <td>78</td>\n",
       "      <td>22</td>\n",
       "      <td>7</td>\n",
       "      <td>61</td>\n",
       "      <td>404</td>\n",
       "      <td>88</td>\n",
       "      <td>485</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>26</td>\n",
       "      <td>129</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2814</td>\n",
       "      <td>1444</td>\n",
       "      <td>1271</td>\n",
       "      <td>4715</td>\n",
       "      <td>1262</td>\n",
       "      <td>299</td>\n",
       "      <td>524</td>\n",
       "      <td>976</td>\n",
       "      <td>262</td>\n",
       "      <td>261</td>\n",
       "      <td>1164</td>\n",
       "      <td>502</td>\n",
       "      <td>241</td>\n",
       "      <td>225</td>\n",
       "      <td>17</td>\n",
       "      <td>227</td>\n",
       "      <td>698</td>\n",
       "      <td>285</td>\n",
       "      <td>947</td>\n",
       "      <td>10</td>\n",
       "      <td>56</td>\n",
       "      <td>22</td>\n",
       "      <td>44</td>\n",
       "      <td>166</td>\n",
       "      <td>51</td>\n",
       "      <td>20</td>\n",
       "      <td>35</td>\n",
       "      <td>20</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26</td>\n",
       "      <td>14</td>\n",
       "      <td>20</td>\n",
       "      <td>103</td>\n",
       "      <td>17</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>124</td>\n",
       "      <td>57</td>\n",
       "      <td>68</td>\n",
       "      <td>228</td>\n",
       "      <td>99</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>56</td>\n",
       "      <td>24</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>52</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>584</td>\n",
       "      <td>247</td>\n",
       "      <td>360</td>\n",
       "      <td>1251</td>\n",
       "      <td>381</td>\n",
       "      <td>182</td>\n",
       "      <td>177</td>\n",
       "      <td>128</td>\n",
       "      <td>96</td>\n",
       "      <td>41</td>\n",
       "      <td>179</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>32</td>\n",
       "      <td>143</td>\n",
       "      <td>22</td>\n",
       "      <td>93</td>\n",
       "      <td>0</td>\n",
       "      <td>226</td>\n",
       "      <td>82</td>\n",
       "      <td>94</td>\n",
       "      <td>536</td>\n",
       "      <td>199</td>\n",
       "      <td>40</td>\n",
       "      <td>76</td>\n",
       "      <td>107</td>\n",
       "      <td>26</td>\n",
       "      <td>19</td>\n",
       "      <td>128</td>\n",
       "      <td>21</td>\n",
       "      <td>19</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>19</td>\n",
       "      <td>127</td>\n",
       "      <td>74</td>\n",
       "      <td>278</td>\n",
       "      <td>11</td>\n",
       "      <td>745</td>\n",
       "      <td>403</td>\n",
       "      <td>551</td>\n",
       "      <td>1412</td>\n",
       "      <td>378</td>\n",
       "      <td>131</td>\n",
       "      <td>150</td>\n",
       "      <td>184</td>\n",
       "      <td>87</td>\n",
       "      <td>66</td>\n",
       "      <td>310</td>\n",
       "      <td>122</td>\n",
       "      <td>54</td>\n",
       "      <td>44</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>310</td>\n",
       "      <td>72</td>\n",
       "      <td>237</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>47</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>328</td>\n",
       "      <td>225</td>\n",
       "      <td>220</td>\n",
       "      <td>794</td>\n",
       "      <td>193</td>\n",
       "      <td>47</td>\n",
       "      <td>75</td>\n",
       "      <td>143</td>\n",
       "      <td>31</td>\n",
       "      <td>51</td>\n",
       "      <td>213</td>\n",
       "      <td>71</td>\n",
       "      <td>50</td>\n",
       "      <td>84</td>\n",
       "      <td>12</td>\n",
       "      <td>108</td>\n",
       "      <td>204</td>\n",
       "      <td>49</td>\n",
       "      <td>159</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>533</td>\n",
       "      <td>268</td>\n",
       "      <td>204</td>\n",
       "      <td>784</td>\n",
       "      <td>235</td>\n",
       "      <td>81</td>\n",
       "      <td>107</td>\n",
       "      <td>145</td>\n",
       "      <td>53</td>\n",
       "      <td>51</td>\n",
       "      <td>197</td>\n",
       "      <td>103</td>\n",
       "      <td>37</td>\n",
       "      <td>46</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>202</td>\n",
       "      <td>146</td>\n",
       "      <td>82</td>\n",
       "      <td>0</td>\n",
       "      <td>2433</td>\n",
       "      <td>1138</td>\n",
       "      <td>872</td>\n",
       "      <td>4590</td>\n",
       "      <td>1279</td>\n",
       "      <td>440</td>\n",
       "      <td>610</td>\n",
       "      <td>911</td>\n",
       "      <td>422</td>\n",
       "      <td>317</td>\n",
       "      <td>1965</td>\n",
       "      <td>515</td>\n",
       "      <td>846</td>\n",
       "      <td>163</td>\n",
       "      <td>171</td>\n",
       "      <td>187</td>\n",
       "      <td>770</td>\n",
       "      <td>570</td>\n",
       "      <td>6002</td>\n",
       "      <td>11</td>\n",
       "      <td>689</td>\n",
       "      <td>314</td>\n",
       "      <td>399</td>\n",
       "      <td>1478</td>\n",
       "      <td>419</td>\n",
       "      <td>125</td>\n",
       "      <td>229</td>\n",
       "      <td>123</td>\n",
       "      <td>54</td>\n",
       "      <td>55</td>\n",
       "      <td>232</td>\n",
       "      <td>110</td>\n",
       "      <td>35</td>\n",
       "      <td>24</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>125</td>\n",
       "      <td>57</td>\n",
       "      <td>87</td>\n",
       "      <td>2</td>\n",
       "      <td>201</td>\n",
       "      <td>74</td>\n",
       "      <td>58</td>\n",
       "      <td>265</td>\n",
       "      <td>91</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>37</td>\n",
       "      <td>58</td>\n",
       "      <td>20</td>\n",
       "      <td>53</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>85</td>\n",
       "      <td>15</td>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>624</td>\n",
       "      <td>324</td>\n",
       "      <td>510</td>\n",
       "      <td>1221</td>\n",
       "      <td>283</td>\n",
       "      <td>152</td>\n",
       "      <td>75</td>\n",
       "      <td>173</td>\n",
       "      <td>115</td>\n",
       "      <td>61</td>\n",
       "      <td>276</td>\n",
       "      <td>72</td>\n",
       "      <td>54</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>474</td>\n",
       "      <td>69</td>\n",
       "      <td>186</td>\n",
       "      <td>14</td>\n",
       "      <td>58</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>189</td>\n",
       "      <td>32</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>28</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>13</td>\n",
       "      <td>6</td>\n",
       "      <td>61</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71</td>\n",
       "      <td>54</td>\n",
       "      <td>53</td>\n",
       "      <td>166</td>\n",
       "      <td>66</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>42</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>34</td>\n",
       "      <td>13</td>\n",
       "      <td>47</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1643</td>\n",
       "      <td>736</td>\n",
       "      <td>1311</td>\n",
       "      <td>4330</td>\n",
       "      <td>1129</td>\n",
       "      <td>360</td>\n",
       "      <td>308</td>\n",
       "      <td>507</td>\n",
       "      <td>155</td>\n",
       "      <td>206</td>\n",
       "      <td>1292</td>\n",
       "      <td>388</td>\n",
       "      <td>433</td>\n",
       "      <td>137</td>\n",
       "      <td>118</td>\n",
       "      <td>202</td>\n",
       "      <td>897</td>\n",
       "      <td>233</td>\n",
       "      <td>1788</td>\n",
       "      <td>16</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>426</td>\n",
       "      <td>182</td>\n",
       "      <td>246</td>\n",
       "      <td>672</td>\n",
       "      <td>299</td>\n",
       "      <td>106</td>\n",
       "      <td>259</td>\n",
       "      <td>104</td>\n",
       "      <td>85</td>\n",
       "      <td>22</td>\n",
       "      <td>145</td>\n",
       "      <td>47</td>\n",
       "      <td>37</td>\n",
       "      <td>22</td>\n",
       "      <td>18</td>\n",
       "      <td>24</td>\n",
       "      <td>102</td>\n",
       "      <td>34</td>\n",
       "      <td>100</td>\n",
       "      <td>0</td>\n",
       "      <td>21</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>103</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>120</td>\n",
       "      <td>72</td>\n",
       "      <td>256</td>\n",
       "      <td>135</td>\n",
       "      <td>41</td>\n",
       "      <td>30</td>\n",
       "      <td>55</td>\n",
       "      <td>9</td>\n",
       "      <td>16</td>\n",
       "      <td>82</td>\n",
       "      <td>17</td>\n",
       "      <td>24</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>593</td>\n",
       "      <td>265</td>\n",
       "      <td>206</td>\n",
       "      <td>952</td>\n",
       "      <td>311</td>\n",
       "      <td>115</td>\n",
       "      <td>184</td>\n",
       "      <td>166</td>\n",
       "      <td>78</td>\n",
       "      <td>64</td>\n",
       "      <td>260</td>\n",
       "      <td>85</td>\n",
       "      <td>46</td>\n",
       "      <td>11</td>\n",
       "      <td>8</td>\n",
       "      <td>19</td>\n",
       "      <td>210</td>\n",
       "      <td>62</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td>8455</td>\n",
       "      <td>4561</td>\n",
       "      <td>4283</td>\n",
       "      <td>13050</td>\n",
       "      <td>4593</td>\n",
       "      <td>1214</td>\n",
       "      <td>1999</td>\n",
       "      <td>2206</td>\n",
       "      <td>1312</td>\n",
       "      <td>777</td>\n",
       "      <td>3709</td>\n",
       "      <td>1509</td>\n",
       "      <td>1016</td>\n",
       "      <td>470</td>\n",
       "      <td>115</td>\n",
       "      <td>488</td>\n",
       "      <td>2882</td>\n",
       "      <td>841</td>\n",
       "      <td>2248</td>\n",
       "      <td>16</td>\n",
       "      <td>583</td>\n",
       "      <td>248</td>\n",
       "      <td>299</td>\n",
       "      <td>837</td>\n",
       "      <td>432</td>\n",
       "      <td>112</td>\n",
       "      <td>129</td>\n",
       "      <td>126</td>\n",
       "      <td>76</td>\n",
       "      <td>53</td>\n",
       "      <td>298</td>\n",
       "      <td>128</td>\n",
       "      <td>43</td>\n",
       "      <td>33</td>\n",
       "      <td>17</td>\n",
       "      <td>21</td>\n",
       "      <td>211</td>\n",
       "      <td>28</td>\n",
       "      <td>310</td>\n",
       "      <td>4</td>\n",
       "      <td>58154</td>\n",
       "      <td>31163</td>\n",
       "      <td>27435</td>\n",
       "      <td>87506</td>\n",
       "      <td>28738</td>\n",
       "      <td>8340</td>\n",
       "      <td>16209</td>\n",
       "      <td>14962</td>\n",
       "      <td>8736</td>\n",
       "      <td>4217</td>\n",
       "      <td>25077</td>\n",
       "      <td>10227</td>\n",
       "      <td>4775</td>\n",
       "      <td>2152</td>\n",
       "      <td>605</td>\n",
       "      <td>2706</td>\n",
       "      <td>21819</td>\n",
       "      <td>6488</td>\n",
       "      <td>23614</td>\n",
       "      <td>79</td>\n",
       "      <td>22</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>68</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>5</td>\n",
       "      <td>35</td>\n",
       "      <td>87</td>\n",
       "      <td>106</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>15</td>\n",
       "      <td>67</td>\n",
       "      <td>274</td>\n",
       "      <td>62</td>\n",
       "      <td>66</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>4</td>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>51</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>378</td>\n",
       "      <td>278</td>\n",
       "      <td>115</td>\n",
       "      <td>689</td>\n",
       "      <td>211</td>\n",
       "      <td>133</td>\n",
       "      <td>66</td>\n",
       "      <td>68</td>\n",
       "      <td>63</td>\n",
       "      <td>40</td>\n",
       "      <td>250</td>\n",
       "      <td>110</td>\n",
       "      <td>63</td>\n",
       "      <td>16</td>\n",
       "      <td>14</td>\n",
       "      <td>93</td>\n",
       "      <td>184</td>\n",
       "      <td>46</td>\n",
       "      <td>151</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>545</td>\n",
       "      <td>272</td>\n",
       "      <td>324</td>\n",
       "      <td>1623</td>\n",
       "      <td>599</td>\n",
       "      <td>215</td>\n",
       "      <td>179</td>\n",
       "      <td>165</td>\n",
       "      <td>80</td>\n",
       "      <td>36</td>\n",
       "      <td>217</td>\n",
       "      <td>109</td>\n",
       "      <td>24</td>\n",
       "      <td>36</td>\n",
       "      <td>19</td>\n",
       "      <td>27</td>\n",
       "      <td>284</td>\n",
       "      <td>91</td>\n",
       "      <td>334</td>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>16</td>\n",
       "      <td>21</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>63</td>\n",
       "      <td>31</td>\n",
       "      <td>123</td>\n",
       "      <td>216</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>20</td>\n",
       "      <td>23</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>136</td>\n",
       "      <td>72</td>\n",
       "      <td>44</td>\n",
       "      <td>252</td>\n",
       "      <td>108</td>\n",
       "      <td>20</td>\n",
       "      <td>28</td>\n",
       "      <td>15</td>\n",
       "      <td>20</td>\n",
       "      <td>7</td>\n",
       "      <td>76</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>35</td>\n",
       "      <td>8</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>423</td>\n",
       "      <td>189</td>\n",
       "      <td>383</td>\n",
       "      <td>1220</td>\n",
       "      <td>234</td>\n",
       "      <td>26</td>\n",
       "      <td>154</td>\n",
       "      <td>64</td>\n",
       "      <td>59</td>\n",
       "      <td>24</td>\n",
       "      <td>189</td>\n",
       "      <td>38</td>\n",
       "      <td>31</td>\n",
       "      <td>84</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>164</td>\n",
       "      <td>142</td>\n",
       "      <td>681</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>48</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>135</td>\n",
       "      <td>42</td>\n",
       "      <td>47</td>\n",
       "      <td>175</td>\n",
       "      <td>56</td>\n",
       "      <td>16</td>\n",
       "      <td>23</td>\n",
       "      <td>14</td>\n",
       "      <td>22</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>48</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>276</td>\n",
       "      <td>100</td>\n",
       "      <td>47</td>\n",
       "      <td>510</td>\n",
       "      <td>153</td>\n",
       "      <td>31</td>\n",
       "      <td>108</td>\n",
       "      <td>63</td>\n",
       "      <td>38</td>\n",
       "      <td>42</td>\n",
       "      <td>129</td>\n",
       "      <td>47</td>\n",
       "      <td>55</td>\n",
       "      <td>15</td>\n",
       "      <td>14</td>\n",
       "      <td>25</td>\n",
       "      <td>94</td>\n",
       "      <td>62</td>\n",
       "      <td>539</td>\n",
       "      <td>2</td>\n",
       "      <td>277</td>\n",
       "      <td>173</td>\n",
       "      <td>128</td>\n",
       "      <td>297</td>\n",
       "      <td>183</td>\n",
       "      <td>31</td>\n",
       "      <td>77</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>37</td>\n",
       "      <td>95</td>\n",
       "      <td>16</td>\n",
       "      <td>24</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>4</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>722</td>\n",
       "      <td>337</td>\n",
       "      <td>349</td>\n",
       "      <td>999</td>\n",
       "      <td>453</td>\n",
       "      <td>118</td>\n",
       "      <td>152</td>\n",
       "      <td>152</td>\n",
       "      <td>55</td>\n",
       "      <td>89</td>\n",
       "      <td>315</td>\n",
       "      <td>204</td>\n",
       "      <td>51</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>31</td>\n",
       "      <td>205</td>\n",
       "      <td>73</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Region                 AA                                                     \\\n",
       "Event                  01 02  03  04 05  06 07 08 09 10 11 12 13 14 15 16 17   \n",
       "2018-01-08/2018-01-14  13  9  20  85  4   2  0  2  8  0  0  0  0  0  0  1  8   \n",
       "2018-01-15/2018-01-21  14  5   8  10  1   0  0  1  0  1  0  1  0  0  0  0  3   \n",
       "2018-01-22/2018-01-28   5  2   0  23  3   2  5  1  0  0  2  0  0  0  0  3  4   \n",
       "2018-01-29/2018-02-04   3  2   7  37  6  10  4  7  0  1  1  1  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11   7  6   7  49  6   0  1  2  2  2  6  0  1  0  0  0  2   \n",
       "\n",
       "Region                          AC                                           \\\n",
       "Event                 18 19 20  01  02  03   04  05 06  07  08 09 10  11 12   \n",
       "2018-01-08/2018-01-14  0  0  0  34   9  14  107   3  2   6   3  0  1   8  7   \n",
       "2018-01-15/2018-01-21  0  2  0  36  11  13   67  18  6   7  12  7  5  20  2   \n",
       "2018-01-22/2018-01-28  0  4  0  24  13   9   40  12  9   4   5  1  3  21  5   \n",
       "2018-01-29/2018-02-04  0  0  0  21  13  12   46  17  6  10   6  1  4  17  1   \n",
       "2018-02-05/2018-02-11  0  4  0  32  12  23   64  13  8   7  28  4  4  12  3   \n",
       "\n",
       "Region                                             AE                       \\\n",
       "Event                  13 14 15 16  17 18  19 20   01   02   03    04   05   \n",
       "2018-01-08/2018-01-14  16  1  0  2   3  0  18  0  421  204  340   975  494   \n",
       "2018-01-15/2018-01-21   3  0  0  0   1  0   6  0  474  182  596  1277  390   \n",
       "2018-01-22/2018-01-28   1  1  0  1   2  0   6  0  516  201  710  1301  444   \n",
       "2018-01-29/2018-02-04   2  1  0  0   9  0   4  0  419  190  411  1011  441   \n",
       "2018-02-05/2018-02-11   4  0  0  0  15  0   1  0  639  250  946  2613  603   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   06   07   08  09  10   11  12  13  14  15  16   17   \n",
       "2018-01-08/2018-01-14   87  111   82  58  31  158  67  33  25  13  23  169   \n",
       "2018-01-15/2018-01-21  106  169   85  50  35  160  65  21  14   6  55  139   \n",
       "2018-01-22/2018-01-28  135  156   98  93  59  270  62  33  29   6  33  153   \n",
       "2018-01-29/2018-02-04  101  164  172  61  40  214  70  18  12   4  37  183   \n",
       "2018-02-05/2018-02-11  125  161  126  57  58  190  76  35  25  13  33  217   \n",
       "\n",
       "Region                               AF                                      \\\n",
       "Event                  18   19 20    01   02   03    04   05   06   07   08   \n",
       "2018-01-08/2018-01-14  20   96  0   964  357  419  1612  480  221  267  304   \n",
       "2018-01-15/2018-01-21  17   79  0  1608  441  522  2752  617  271  304  384   \n",
       "2018-01-22/2018-01-28  32  131  0  1801  532  377  1778  586  272  374  450   \n",
       "2018-01-29/2018-02-04  32  139  0  1629  587  491  2672  627  281  342  492   \n",
       "2018-02-05/2018-02-11  26  115  0  1175  368  453  1805  703  178  310  344   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   09   10    11   12   13  14  15   16   17   18    19   \n",
       "2018-01-08/2018-01-14   72  110   496  189  162  17  42  135  250  159  1245   \n",
       "2018-01-15/2018-01-21   95  126   622  214  181  36  82  106  313  302  1790   \n",
       "2018-01-22/2018-01-28  154  128  1141  198  256  62  98  166  306  556  2571   \n",
       "2018-01-29/2018-02-04  142  106  1052  266  212  52  91   94  294  411  1820   \n",
       "2018-02-05/2018-02-11  100  124   614  173  117  42  94   76  322  232  1445   \n",
       "\n",
       "Region                    AG                                                  \\\n",
       "Event                 20  01  02  03   04  05  06  07  08  09  10  11  12 13   \n",
       "2018-01-08/2018-01-14  0  94  73  17  195  56  26  23  12   8  19  36  17  1   \n",
       "2018-01-15/2018-01-21  3  69  42  51  182  66  45  29  24   9   9  31  27  7   \n",
       "2018-01-22/2018-01-28  1  68  31  52  239  78  23  24  33   5   8  34  12  6   \n",
       "2018-01-29/2018-02-04  6  76  42  43  187  56  36  18  13  25  14  23  27  6   \n",
       "2018-02-05/2018-02-11  4  83  38  29  241  88  22  17  21  20  16  69  32  7   \n",
       "\n",
       "Region                                            AJ                          \\\n",
       "Event                 14  15  16  17  18  19 20   01  02   03   04   05   06   \n",
       "2018-01-08/2018-01-14  9   6  18  49  32  36  0  161  70  130  366  139   55   \n",
       "2018-01-15/2018-01-21  8   2  11  51  14  40  0  284  77  205  670  174   74   \n",
       "2018-01-22/2018-01-28  4   2   0  43  21  53  0  276  84  198  838  242   55   \n",
       "2018-01-29/2018-02-04  8   1   4  25  22  87  3  232  64  168  683  230   85   \n",
       "2018-02-05/2018-02-11  5  11   0  31   5  70  0  335  75  191  887  219  110   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                  07  08  09  10  11  12  13  14 15  16   17  18   19   \n",
       "2018-01-08/2018-01-14  45  37  10  25  70  18  12   3  3   3  111  18   63   \n",
       "2018-01-15/2018-01-21  59  43  55  37  75  48   8   6  3   5  101  34   67   \n",
       "2018-01-22/2018-01-28  44  46  55  16  94  39  14  10  2  10   98  12   69   \n",
       "2018-01-29/2018-02-04  37  25  11  45  67  32   0  15  3   4   59   7   56   \n",
       "2018-02-05/2018-02-11  71  48  56  60  83  19  21  25  8   8   71  14  103   \n",
       "\n",
       "Region                      AL                                                 \\\n",
       "Event                  20   01   02   03   04  05  06  07  08  09  10  11  12   \n",
       "2018-01-08/2018-01-14   3  113  128  106  237  79  15   8  20  10  14  64  20   \n",
       "2018-01-15/2018-01-21  15  110   82   69  242  42  26  26  22  15  19  77  45   \n",
       "2018-01-22/2018-01-28   8  119   74   77  224  96  28  20  21  18   5  55  38   \n",
       "2018-01-29/2018-02-04   4  112   63   42  193  91  25  28  20  15  14  81  36   \n",
       "2018-02-05/2018-02-11  12   74   54   83  148  75   8  10  14   8   3  18  22   \n",
       "\n",
       "Region                                                 AM                  \\\n",
       "Event                  13  14 15  16  17  18   19 20   01   02   03    04   \n",
       "2018-01-08/2018-01-14  15  19  0  15  17  15   30  0  273   65  114   448   \n",
       "2018-01-15/2018-01-21   8  10  0   6  37  17  121  0  358  124  152   605   \n",
       "2018-01-22/2018-01-28  13  32  1   6  31   9   45  0  324  123  265  1036   \n",
       "2018-01-29/2018-02-04  14  28  0  10  63  16   46  0  357  123  198   873   \n",
       "2018-02-05/2018-02-11   7   5  4   6  27   3   36  1  317   90  187   936   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   05   06  07  08  09  10   11  12  13  14 15  16  17   \n",
       "2018-01-08/2018-01-14  219   65  40  54  36  13   73  34  12  29  0  21  48   \n",
       "2018-01-15/2018-01-21  261   61  51  58  26  38  105  44   2  14  1   9  42   \n",
       "2018-01-22/2018-01-28  285   69  60  58  39  21   92  51   8  13  2   8  31   \n",
       "2018-01-29/2018-02-04  238   88  50  44  44  11   73  42  31  25  3  18  59   \n",
       "2018-02-05/2018-02-11  207  104  62  63  24  25   87  39  13  27  2   6  68   \n",
       "\n",
       "Region                             AN                                         \\\n",
       "Event                  18   19  20 01 02 03 04 05 06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14  18  129   3  1  1  0  4  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  12  100  17  0  0  4  5  0  0  0  2  0  0  2  0  0  0   \n",
       "2018-01-22/2018-01-28  16   69   4  0  0  0  5  0  0  0  0  0  0  0  1  0  0   \n",
       "2018-01-29/2018-02-04  17   83   1  0  0  0  0  4  0  0  0  0  0  1  0  0  0   \n",
       "2018-02-05/2018-02-11  13   67  11  0  0  0  2  0  0  0  0  0  0  1  0  0  0   \n",
       "\n",
       "Region                                    AO                                 \\\n",
       "Event                 15 16 17 18 19 20   01  02   03   04   05  06  07  08   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  110  68  134  228   67  14  40  32   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0   86  73  112  197  120  14  26  32   \n",
       "2018-01-22/2018-01-28  0  0  6  0  1  0  110  49   69  261   59  10  25  32   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0   62  40   49  198   58  25  22  21   \n",
       "2018-02-05/2018-02-11  0  0  0  0  1  0   59  53   51  159   82  25  39   6   \n",
       "\n",
       "Region                                                              AQ        \\\n",
       "Event                  09  10  11  12  13 14 15  16  17  18  19 20  01 02 03   \n",
       "2018-01-08/2018-01-14   9  19  45  18  13  5  0   4  24   4  15  0   5  5  2   \n",
       "2018-01-15/2018-01-21   8   7  45  11   3  8  0   0  27   5  33  0  13  5  1   \n",
       "2018-01-22/2018-01-28  12   8  25  11   7  6  0  10  44   7  35  2  16  3  2   \n",
       "2018-01-29/2018-02-04  17  14  23   9   7  8  2   2  37  14  15  0   2  4  0   \n",
       "2018-02-05/2018-02-11   8   2  10  13   4  8  1   1  15  10  27  0  10  4  8   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                  04 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   \n",
       "2018-01-08/2018-01-14   2  1  0  0  4  2  0  0  5  0  0  0  0  5  0  0  0   \n",
       "2018-01-15/2018-01-21  13  2  0  5  1  1  0  2  2  2  0  0  0  1  0  1  0   \n",
       "2018-01-22/2018-01-28   9  3  0  8  0  0  0  1  2  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  16  1  0  1  0  0  0  0  2  0  0  0  0  2  0  1  0   \n",
       "2018-02-05/2018-02-11  17  7  0  2  0  1  0  1  0  2  0  0  0  0  1  0  0   \n",
       "\n",
       "Region                  AR                                                   \\\n",
       "Event                   01   02   03   04   05  06  07  08  09  10   11  12   \n",
       "2018-01-08/2018-01-14  163   62  162  295   81  48  22  54  28  17   83  14   \n",
       "2018-01-15/2018-01-21  207   95  226  733  144  69  37  64  46  14  109  44   \n",
       "2018-01-22/2018-01-28  157  102  143  513  134  53  31  41  22  11   39  32   \n",
       "2018-01-29/2018-02-04  188   79  200  653  185  49  54  71  52   5   60  38   \n",
       "2018-02-05/2018-02-11  186   77  157  676  126  60  49  41  35   6   67  16   \n",
       "\n",
       "Region                                                  AS                    \\\n",
       "Event                  13  14 15  16   17  18  19 20    01    02    03    04   \n",
       "2018-01-08/2018-01-14   3  15  1   5   58  10  33  0  4214  2183  1663  5506   \n",
       "2018-01-15/2018-01-21  13  14  0   8  102  24  91  0  4534  2256  2148  6491   \n",
       "2018-01-22/2018-01-28  20  16  0   9   36  24  70  0  5008  2264  2227  7515   \n",
       "2018-01-29/2018-02-04  21  10  1  10   37   5  53  0  5386  2351  2254  6973   \n",
       "2018-02-05/2018-02-11  10   7  0   7   37  15  82  0  5245  2804  2756  7174   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                    05   06    07    08   09   10    11   12   13   14   \n",
       "2018-01-08/2018-01-14  2008  640  1057   848  797  295  1697  614  435   98   \n",
       "2018-01-15/2018-01-21  2336  637  1033   817  638  306  1614  617  420  141   \n",
       "2018-01-22/2018-01-28  3170  723  1078   900  600  261  1674  605  434  215   \n",
       "2018-01-29/2018-02-04  2363  841  1324  1020  748  411  1697  877  399  141   \n",
       "2018-02-05/2018-02-11  2495  854  1311   993  832  351  1936  828  390  143   \n",
       "\n",
       "Region                                                AU                      \\\n",
       "Event                  15   16    17   18    19  20   01   02   03   04   05   \n",
       "2018-01-08/2018-01-14  32  154   970  355  1109   0  326  148  138  479  220   \n",
       "2018-01-15/2018-01-21  43  195  1183  474  1369   2  319  155  124  537  184   \n",
       "2018-01-22/2018-01-28  43  158  1426  369  1570  14  380  135  202  648  245   \n",
       "2018-01-29/2018-02-04  37  157  1410  388  1324   5  284  126  182  592  184   \n",
       "2018-02-05/2018-02-11  49  164  1552  417  1432   5  204   88  264  611  206   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  06  07  08  09  10   11  12  13  14 15  16  17  18  19   \n",
       "2018-01-08/2018-01-14  41  43  51  24  21   93  39  22  44  8  18  72   7  50   \n",
       "2018-01-15/2018-01-21  38  82  54  19  31   80  46  16  21  0  28  63  37  63   \n",
       "2018-01-22/2018-01-28  32  62  77  46  47  140  80  27  24  2  32  67  21  85   \n",
       "2018-01-29/2018-02-04  52  81  57  39  28   79  33  19  28  3  44  72  11  60   \n",
       "2018-02-05/2018-02-11  24  45  50  54  18   51  42  12   8  5   5  58   7  41   \n",
       "\n",
       "Region                   AV                                                  \\\n",
       "Event                 20 01 02  03  04 05 06  07 08 09 10 11 12 13 14 15 16   \n",
       "2018-01-08/2018-01-14  0  9  1   3  21  2  4  11  0  1  1  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  2  1   5   6  1  0   3  0  2  0  2  1  0  0  0  1   \n",
       "2018-01-22/2018-01-28  3  9  4   8   3  2  1   0  0  3  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  5  5   2  27  9  0   3  1  1  0  1  1  1  0  0  0   \n",
       "2018-02-05/2018-02-11  0  9  3  10  20  6  0   1  4  0  0  0  1  0  0  0  0   \n",
       "\n",
       "Region                             AY                                         \\\n",
       "Event                 17 18 19 20  01  02  03  04  05 06 07  08 09 10  11 12   \n",
       "2018-01-08/2018-01-14  1  0  2  0  19  11  26  60  14  4  3   5  2  0   4  5   \n",
       "2018-01-15/2018-01-21  4  0  0  0  12  23  28  50   8  0  2  10  3  0   6  1   \n",
       "2018-01-22/2018-01-28  1  0  3  0  42  25  30  83  14  0  7  12  6  0   5  6   \n",
       "2018-01-29/2018-02-04  0  0  0  0  36  17  22  65   9  1  8  15  3  1  14  3   \n",
       "2018-02-05/2018-02-11  1  0  0  0  21  15  20  81  10  6  4   3  9  0   1  3   \n",
       "\n",
       "Region                                           BA                         \\\n",
       "Event                  13 14 15 16 17 18 19 20   01  02   03   04   05  06   \n",
       "2018-01-08/2018-01-14   5  0  0  0  4  0  1  0  128  76  165  407  146  49   \n",
       "2018-01-15/2018-01-21   0  1  0  0  1  2  2  0  128  78   81  396  146  39   \n",
       "2018-01-22/2018-01-28  10  1  1  0  3  0  4  0  122  53   43  195  127  30   \n",
       "2018-01-29/2018-02-04   3  1  2  0  0  1  6  0  111  71   68  228  144  49   \n",
       "2018-02-05/2018-02-11   5  0  0  1  5  0  0  0  108  69   75  295  176  29   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  07  08  09  10   11  12  13  14 15  16   17  18  19 20   \n",
       "2018-01-08/2018-01-14  29  42  11  18   86  15   6  23  2  32   44  13  24  0   \n",
       "2018-01-15/2018-01-21  15  39  13  11  112  29   9  26  6  45   95  14  22  0   \n",
       "2018-01-22/2018-01-28  32  26  20   5   99  29   8  11  2  15   72  17  24  0   \n",
       "2018-01-29/2018-02-04  26  41  11  13   92  31  12  20  1  45  170  16  49  0   \n",
       "2018-02-05/2018-02-11  35  24   8  25   91  10  11  24  5  14   82  21  32  0   \n",
       "\n",
       "Region                  BB                                                   \\\n",
       "Event                   01  02  03   04  05  06  07  08  09  10  11  12  13   \n",
       "2018-01-08/2018-01-14  100  65  30  140  58  10  18  10   6   6  33   8  11   \n",
       "2018-01-15/2018-01-21  134  56  43  148  57  12  21   9   4   8  51  18  13   \n",
       "2018-01-22/2018-01-28  118  39  34  129  42  10  15  11   9   5  22   5  10   \n",
       "2018-01-29/2018-02-04  102  68  20  155  33  14  13   5   8  11  33  19  10   \n",
       "2018-02-05/2018-02-11  147  64  30  199  55  13  19   9  20  10  54  29  10   \n",
       "\n",
       "Region                                          BC                            \\\n",
       "Event                 14 15  16  17 18  19 20   01   02  03   04  05  06  07   \n",
       "2018-01-08/2018-01-14  0  4   9  11  0  13  0  206   62  19  223  44   4   7   \n",
       "2018-01-15/2018-01-21  4  0  10  13  4   9  0  226  118  73  199  69  23  58   \n",
       "2018-01-22/2018-01-28  2  0   9   9  2   8  0  126   53  30  201  46  11  14   \n",
       "2018-01-29/2018-02-04  4  0  12  20  2  16  0  117   39  43  106  24  10  14   \n",
       "2018-02-05/2018-02-11  2  1   2  31  0  15  0   79   80  56  210  55  20  26   \n",
       "\n",
       "Region                                                                    BD  \\\n",
       "Event                  08  09  10   11  12  13 14 15  16  17  18  19 20   01   \n",
       "2018-01-08/2018-01-14  15  14   5  222  21   4  9  0   8  29  10  11  0   94   \n",
       "2018-01-15/2018-01-21  22  13  17  156  35  11  9  0   7  45  17  56  0   38   \n",
       "2018-01-22/2018-01-28  27   2   8   48  13  13  3  0   9  29   3  23  0   52   \n",
       "2018-01-29/2018-02-04  27   8   3   17   8   5  9  0   4  44  14  16  0   77   \n",
       "2018-02-05/2018-02-11  29  18   7   45  16   9  9  0  15  15   4  16  0  103   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  02  03   04  05  06  07  08 09  10  11  12  13 14 15   \n",
       "2018-01-08/2018-01-14  17  12  108  24  13  10   4  9   3  21  25   2  1  1   \n",
       "2018-01-15/2018-01-21  14  12   59  36   5  12  15  3   7  16   6   1  3  2   \n",
       "2018-01-22/2018-01-28  35  39  113  38   7  11  15  4   8  11  16   2  1  0   \n",
       "2018-01-29/2018-02-04  31  26   82  32   3   8  10  8   4  19  21   4  0  1   \n",
       "2018-02-05/2018-02-11  78  46  104  74  10  15  51  9  27  68  31  11  4  0   \n",
       "\n",
       "Region                                    BE                                \\\n",
       "Event                  16  17 18  19 20   01   02   03    04   05  06   07   \n",
       "2018-01-08/2018-01-14   0  25  1   5  0  521  246  458  1251  380  66  103   \n",
       "2018-01-15/2018-01-21   2   4  0   3  0  559  252  373  1011  231  72  124   \n",
       "2018-01-22/2018-01-28   8  24  2  12  0  561  341  429  1672  285  92  161   \n",
       "2018-01-29/2018-02-04   2  13  2  10  0  556  254  359  1438  269  92  119   \n",
       "2018-02-05/2018-02-11  12  34  3  29  0  573  270  312  1092  215  53  102   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   08  09  10   11   12   13  14  15  16   17  18   19   \n",
       "2018-01-08/2018-01-14  151  37  40  194  117   64  23   1  55   95  30   86   \n",
       "2018-01-15/2018-01-21  157  85  58  191  103   75  39  46  46  142  73  162   \n",
       "2018-01-22/2018-01-28  180  37  73  335  105  103  44  22  70  231  29  197   \n",
       "2018-01-29/2018-02-04  124  52  55  208   94   69  24   9  24  130  36  174   \n",
       "2018-02-05/2018-02-11  129  53  49  334  149   45  28   9  21  227  47  299   \n",
       "\n",
       "Region                     BF                                                 \\\n",
       "Event                 20   01   02   03   04  05  06  07  08  09  10  11  12   \n",
       "2018-01-08/2018-01-14  0  160   71  107  341  78  28  29  29  26  15  45  22   \n",
       "2018-01-15/2018-01-21  0  166  105   57  340  69  31  50  23  34  16  41  25   \n",
       "2018-01-22/2018-01-28  0  239   82   76  315  79  17  21  24  31   9  63  23   \n",
       "2018-01-29/2018-02-04  0  171  101   57  277  72  12  37  29   5  15  59  27   \n",
       "2018-02-05/2018-02-11  1  162   96   51  269  71  18  36  31  12  20  62  32   \n",
       "\n",
       "Region                                               BG                       \\\n",
       "Event                  13 14 15 16  17  18  19 20    01   02   03    04   05   \n",
       "2018-01-08/2018-01-14  12  2  0  8  53  14  54  0  1092  453  433  1436  437   \n",
       "2018-01-15/2018-01-21  37  0  6  6  58  28  55  1  1316  486  645  1909  587   \n",
       "2018-01-22/2018-01-28  31  2  1  8  88  46  33  0  1185  571  637  2049  481   \n",
       "2018-01-29/2018-02-04   4  5  1  8  62  16  34  0  1063  429  471  1797  514   \n",
       "2018-02-05/2018-02-11  25  2  0  4  61  20  40  0  1211  550  645  2294  578   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                   06   07   08   09   10   11   12  13   14  15  16   \n",
       "2018-01-08/2018-01-14  169  204  394  106   96  385  172  43   93  24  32   \n",
       "2018-01-15/2018-01-21  167  278  408  115  133  409  160  89  116  28  62   \n",
       "2018-01-22/2018-01-28  138  260  540  103  148  513  129  77   85  17  55   \n",
       "2018-01-29/2018-02-04  161  186  374  149  114  341  120  61   80  17  44   \n",
       "2018-02-05/2018-02-11  163  366  373  117  157  529  148  60  145  74  38   \n",
       "\n",
       "Region                                      BH                               \\\n",
       "Event                   17   18   19   20   01  02  03   04  05  06  07  08   \n",
       "2018-01-08/2018-01-14  334  148  455   73  161  84  57  259  78  25  31  31   \n",
       "2018-01-15/2018-01-21  359  151  467   69  147  62  61  207  62  18  23  30   \n",
       "2018-01-22/2018-01-28  381  198  400  107  123  58  37  209  30  24  17  33   \n",
       "2018-01-29/2018-02-04  476  110  332   52  149  76  56  345  52  15  26  26   \n",
       "2018-02-05/2018-02-11  901  132  476   75  172  54  54  332  54   8  11  21   \n",
       "\n",
       "Region                                                               BK      \\\n",
       "Event                  09  10  11  12  13 14  15 16  17  18   19 20  01  02   \n",
       "2018-01-08/2018-01-14  11   7  57  15  37  3   2  1  42  17   54  0  47  16   \n",
       "2018-01-15/2018-01-21  11  15  56  17   4  5   1  4  57  23   61  0  20   7   \n",
       "2018-01-22/2018-01-28  21   6  41  28   4  7  12  2  45  12  125  0  16  13   \n",
       "2018-01-29/2018-02-04  22  22  62  16   4  6   2  3  67  20  125  0  20  11   \n",
       "2018-02-05/2018-02-11  20   5  54  11   1  2   2  5  46  24   75  1  27  21   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  03  04  05  06  07 08 09 10  11 12 13 14 15 16  17  18   \n",
       "2018-01-08/2018-01-14   7  50  41   2   1  2  5  3   8  7  3  0  1  1  27  17   \n",
       "2018-01-15/2018-01-21   9  47  22   1  11  5  1  2   4  2  3  0  0  2   4   2   \n",
       "2018-01-22/2018-01-28   5  45  18   0   3  3  0  7   4  6  2  3  0  0  10   0   \n",
       "2018-01-29/2018-02-04  10  56  24  16   2  1  4  2  17  1  0  1  0  2  29   1   \n",
       "2018-02-05/2018-02-11   6  52   8   3   2  6  2  3  10  4  3  2  0  6  24   8   \n",
       "\n",
       "Region                        BL                                              \\\n",
       "Event                  19 20  01  02  03   04  05  06  07  08  09 10  11  12   \n",
       "2018-01-08/2018-01-14  10  0  25  20  29   54  31   0   9   6   4  0  15   5   \n",
       "2018-01-15/2018-01-21   9  1  47  30  33  226  43  18   4  10  14  0  23  15   \n",
       "2018-01-22/2018-01-28  10  2  69  24  24  149  45  17  11  15  10  0  23   4   \n",
       "2018-01-29/2018-02-04   7  0  26  19  34  135  26   6  15  14   6  1  15  20   \n",
       "2018-02-05/2018-02-11   8  0  59  29  15   64  23   8  11   4  11  0  32   6   \n",
       "\n",
       "Region                                                BM                      \\\n",
       "Event                  13  14 15  16  17  18  19 20   01   02   03   04   05   \n",
       "2018-01-08/2018-01-14   5   6  0   4  15   5  24  0  316  180  112  324  112   \n",
       "2018-01-15/2018-01-21   4  11  0  10  38   9  15  0  371  138  112  499  129   \n",
       "2018-01-22/2018-01-28  10   3  3   7  28  18  36  3  408  146  160  570  142   \n",
       "2018-01-29/2018-02-04   1   0  0   4   8   4  24  0  354  112   87  441   73   \n",
       "2018-02-05/2018-02-11   1   3  0   2  10  15  26  0  335  128  155  577  123   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  06  07   08  09  10   11  12  13  14 15  16   17  18   \n",
       "2018-01-08/2018-01-14  36  51  144  42  38  170  98   8  21  3   9  215  43   \n",
       "2018-01-15/2018-01-21  45  47   99  75  20  139  84  23  19  3  13  161  57   \n",
       "2018-01-22/2018-01-28  32  37  217  36  58  162  64   8  25  1  15  119  23   \n",
       "2018-01-29/2018-02-04  56  55  100  49  29  123  62  14   9  8   6  170  54   \n",
       "2018-02-05/2018-02-11  64  99   93  69  25  109  88  20  29  3  12  184  61   \n",
       "\n",
       "Region                           BN                                            \\\n",
       "Event                   19  20   01   02  03   04  05  06  07  08  09  10  11   \n",
       "2018-01-08/2018-01-14  226  48   66   18  37   98  36   7  22   9   3   3  24   \n",
       "2018-01-15/2018-01-21  309  75   59   30  19  100  19  10  13  10  12  10  13   \n",
       "2018-01-22/2018-01-28  207  71  102   25  22  127  33   2  12  17  21   9   8   \n",
       "2018-01-29/2018-02-04  227  38  103   41  46  266  36  37  14  27  32   7  25   \n",
       "2018-02-05/2018-02-11  349  46  118  101  36  241  37  25  32  70  25   3  44   \n",
       "\n",
       "Region                                                   BO                \\\n",
       "Event                  12  13  14 15 16  17  18  19 20   01  02   03   04   \n",
       "2018-01-08/2018-01-14   6  10   4  1  1  37   9  20  0  202  47   60  229   \n",
       "2018-01-15/2018-01-21  20   1   7  2  0  16   5  18  0  204  78  134  406   \n",
       "2018-01-22/2018-01-28   8   3   8  6  2  30  21  54  0  187  67  140  400   \n",
       "2018-01-29/2018-02-04  17  21   9  2  1  18  12  37  0  200  89  151  604   \n",
       "2018-02-05/2018-02-11   9  11  10  1  6  23  23  45  0  128  59   58  290   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   05  06  07  08  09  10  11  12  13  14  15  16  17 18   \n",
       "2018-01-08/2018-01-14   74  25  16  21  18  15  47  12  35   9  17   2  21  2   \n",
       "2018-01-15/2018-01-21   88  22  28  29  14  16  35  32   4  11   7  10  25  2   \n",
       "2018-01-22/2018-01-28  166  53  34  39  13  18  44  28   4   8   5  12  68  4   \n",
       "2018-01-29/2018-02-04  158  45  31  36  17  22  29  18   9   2   1  16  40  4   \n",
       "2018-02-05/2018-02-11  142  25  29  35  16   9  36  12   0  11   8   4  35  9   \n",
       "\n",
       "Region                        BP                                             \\\n",
       "Event                  19 20  01  02  03  04  05 06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14  21  0  11  12  11  25   6  1  5  1  2  0  4  2  0  0   \n",
       "2018-01-15/2018-01-21  22  0  20  15  11  37  21  2  8  2  7  0  7  5  0  0   \n",
       "2018-01-22/2018-01-28  36  0  26   7   6  34  11  4  8  6  3  1  6  3  1  0   \n",
       "2018-01-29/2018-02-04  43  0  32  13  18  46  10  4  7  0  3  0  7  5  1  1   \n",
       "2018-02-05/2018-02-11  22  0  31  22  14  91  15  3  0  4  1  2  1  6  5  0   \n",
       "\n",
       "Region                                   BQ                                   \\\n",
       "Event                 15 16  17 18 19 20 01 02 03 04 05 06 07 08 09 10 11 12   \n",
       "2018-01-08/2018-01-14  0  0   6  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  1  0  15  4  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0   4  3  9  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  3   0  2  8  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0   7  3  4  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                          BR                           \\\n",
       "Event                 13 14 15 16 17 18 19 20   01   02   03   04   05   06   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  298  132  196  576  170  129   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  370  181  222  697  207   77   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  547  289  220  859  194  106   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  436  177  271  697  222  121   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  383  191  245  746  231  119   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                   07   08  09  10   11  12  13  14  15  16   17  18   \n",
       "2018-01-08/2018-01-14   79   95  49  28  130  51  16  14   4   7  130  33   \n",
       "2018-01-15/2018-01-21  119  130  68  29  157  74  31  23  10  14  175  38   \n",
       "2018-01-22/2018-01-28  119  104  74  33  211  77  47  38  14  32  411  51   \n",
       "2018-01-29/2018-02-04   83   87  62  39  171  79  32  10   7  17  170  21   \n",
       "2018-02-05/2018-02-11  109  126  61  27  132  81  19  19  21   6  163  17   \n",
       "\n",
       "Region                         BT                                            \\\n",
       "Event                   19 20  01  02  03   04  05  06  07  08 09 10  11 12   \n",
       "2018-01-08/2018-01-14  109  0  43  15  33  104  37   4   8   9  2  3  16  3   \n",
       "2018-01-15/2018-01-21  163  0  15  13  10   37  30   6   5   2  3  1   3  2   \n",
       "2018-01-22/2018-01-28  310  2  47  24  26   62  30   6  11   7  4  8  20  4   \n",
       "2018-01-29/2018-02-04  217  1  60  18  27  163  50  19  15   7  3  3  23  7   \n",
       "2018-02-05/2018-02-11  177  0  83  20  38   89  40  13  19  10  6  2  22  6   \n",
       "\n",
       "Region                                            BU                          \\\n",
       "Event                 13 14 15 16  17 18  19 20   01   02   03   04   05  06   \n",
       "2018-01-08/2018-01-14  1  3  5  4   8  2  14  1  341  134  203  576  151  21   \n",
       "2018-01-15/2018-01-21  1  3  5  0  11  4   2  0  303  120  192  670  201  27   \n",
       "2018-01-22/2018-01-28  2  0  0  0   9  5   5  1  306  156  126  411  170  32   \n",
       "2018-01-29/2018-02-04  0  0  1  0  16  9   3  0  229   95  119  315  116  26   \n",
       "2018-02-05/2018-02-11  4  3  0  0   7  0  12  0  311  111  198  463  115  36   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  07  08  09  10  11  12  13  14 15  16  17  18  19 20   \n",
       "2018-01-08/2018-01-14  55  45  22  41  73  90  33  30  2  11  50  11  68  0   \n",
       "2018-01-15/2018-01-21  48  70  29  19  78  65  25  14  5   9  45  45  57  0   \n",
       "2018-01-22/2018-01-28  54  31  25  33  89  43  25  18  0   6  55   5  34  0   \n",
       "2018-01-29/2018-02-04  42  39  21  11  70  67  13  16  5   7  44   7  52  0   \n",
       "2018-02-05/2018-02-11  40  23  16  27  85  44  12  16  2   4  48  12  31  0   \n",
       "\n",
       "Region                BV                                                     \\\n",
       "Event                 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18   \n",
       "2018-01-08/2018-01-14  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                        BX                                             \\\n",
       "Event                 19 20   01  02   03   04   05  06  07  08  09  10  11   \n",
       "2018-01-08/2018-01-14  0  0   86  24   28  164   46   8  18  38  12   6  29   \n",
       "2018-01-15/2018-01-21  0  0   69  20   46  162   39  13  14  33   0   4  58   \n",
       "2018-01-22/2018-01-28  0  0  122  49  233  500  318  20   9  26  20   9  48   \n",
       "2018-01-29/2018-02-04  0  0  120  29   31  112   42  20  13  17  10   4  15   \n",
       "2018-02-05/2018-02-11  0  0   83  26   44  200   56  11   8  20  36  10  38   \n",
       "\n",
       "Region                                                 BY                      \\\n",
       "Event                  12  13 14  15 16  17 18  19 20  01  02  03  04  05  06   \n",
       "2018-01-08/2018-01-14   7   3  5  30  0  28  2   6  0  27   7   5  16  11  15   \n",
       "2018-01-15/2018-01-21   5  20  0   2  0   9  1  13  0  22  13  17  51  12  18   \n",
       "2018-01-22/2018-01-28  19  12  2  12  0  13  1  27  0  62  21   6  88  39  13   \n",
       "2018-01-29/2018-02-04  14   6  3  12  6  18  1  14  0  28   9  10  77  20   4   \n",
       "2018-02-05/2018-02-11   5  16  3   5  0  10  0  45  0  40  47   3  51  21  10   \n",
       "\n",
       "Region                                                                     CA  \\\n",
       "Event                  07  08  09 10  11  12 13 14 15 16  17  18  19 20    01   \n",
       "2018-01-08/2018-01-14   7  32   4  2  10   2  1  1  0  0   5   2   9  1  4924   \n",
       "2018-01-15/2018-01-21   8  47  27  9   8   9  0  2  1  6  18  16  40  0  5629   \n",
       "2018-01-22/2018-01-28  14  26   4  4  10  14  2  9  0  3  10   2  27  0  5658   \n",
       "2018-01-29/2018-02-04  23  37   0  3   8   1  5  0  1  1   4   2  21  0  5398   \n",
       "2018-02-05/2018-02-11  21  35  10  4  39  17  5  9  0  2   7   2  21  0  5840   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                    02    03    04    05    06    07    08   09   10   \n",
       "2018-01-08/2018-01-14  2658  2144  6553  2182   809  1132  1467  772  331   \n",
       "2018-01-15/2018-01-21  3059  2654  8906  2582   919  1359  1762  872  456   \n",
       "2018-01-22/2018-01-28  2946  2930  9058  2997  1014  1445  1760  856  487   \n",
       "2018-01-29/2018-02-04  3032  2433  7499  2607   923  1250  1506  951  383   \n",
       "2018-02-05/2018-02-11  3167  2477  8090  2631   961  1194  1690  759  365   \n",
       "\n",
       "Region                                                                    CB  \\\n",
       "Event                    11   12   13   14  15   16    17   18    19 20   01   \n",
       "2018-01-08/2018-01-14  1961  770  496  182  38  291  1418  370  1341  3  290   \n",
       "2018-01-15/2018-01-21  2272  882  613  282  53  242  1841  655  1834  9  275   \n",
       "2018-01-22/2018-01-28  2037  967  537  202  45  206  1393  483  1769  6  281   \n",
       "2018-01-29/2018-02-04  2204  924  445  213  50  200  1341  536  1710  5  367   \n",
       "2018-02-05/2018-02-11  2190  817  571  257  25  287  1756  493  1808  1  313   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   02   03   04   05  06  07   08  09  10   11  12  13   \n",
       "2018-01-08/2018-01-14  154  237  669  257  52  83   65  62  24  110  51  27   \n",
       "2018-01-15/2018-01-21  102  133  454  177  75  68   64  29  34  123  60  25   \n",
       "2018-01-22/2018-01-28  111  215  641  211  83  34   71  38  45  177  40  21   \n",
       "2018-01-29/2018-02-04  103  157  500  249  81  63   81  42  15  284  84  26   \n",
       "2018-02-05/2018-02-11  142  107  519  133  65  30  190  63  46  188  42  33   \n",
       "\n",
       "Region                                               CD                        \\\n",
       "Event                  14  15  16   17  18   19 20   01  02  03   04   05  06   \n",
       "2018-01-08/2018-01-14   6   1  14  273  20   58  0   93  33  40  168   57   5   \n",
       "2018-01-15/2018-01-21   8   3  12  142  15   29  0   61  70  30  238  114  22   \n",
       "2018-01-22/2018-01-28  13  21   7  254  12   90  0  122  50  47  262   98  33   \n",
       "2018-01-29/2018-02-04   7  14  12  408   8  154  1   93  56  86  191   56  12   \n",
       "2018-02-05/2018-02-11   6   1  17  246   6  110  0   97  42  28  179   43  10   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                  07  08  09  10  11  12 13  14  15 16  17  18  19 20   \n",
       "2018-01-08/2018-01-14  47  30  13   4  21  16  3   1   0  7  54   9  61  0   \n",
       "2018-01-15/2018-01-21  46  14  19   5  34   2  9  11   0  3  68   2  69  0   \n",
       "2018-01-22/2018-01-28  52  36  16   3  33  14  5   5   0  6  59  19  68  0   \n",
       "2018-01-29/2018-02-04  46  46  17   5  39  14  5   3  11  1  65   8  97  0   \n",
       "2018-02-05/2018-02-11  51  36  24  13  29  13  5  14   0  5  60  14  42  0   \n",
       "\n",
       "Region                  CE                                                   \\\n",
       "Event                   01   02   03   04   05   06   07   08   09  10   11   \n",
       "2018-01-08/2018-01-14  414  249  283  836  246  146   86  160   62  48  184   \n",
       "2018-01-15/2018-01-21  379  192  268  876  242  150   76  178   78  60  198   \n",
       "2018-01-22/2018-01-28  308  233  238  902  277  148   71  137   65  38  167   \n",
       "2018-01-29/2018-02-04  470  279  242  907  352  111   97  198   61  75  293   \n",
       "2018-02-05/2018-02-11  576  308  251  973  378  125  134  201  113  83  283   \n",
       "\n",
       "Region                                                        CF           \\\n",
       "Event                   12  13  14  15  16   17  18   19 20   01   02  03   \n",
       "2018-01-08/2018-01-14   55  24  23   0  25  168  34  150  7   66   45  31   \n",
       "2018-01-15/2018-01-21   78  24  27   6  31  223  43  139  0  165   69  15   \n",
       "2018-01-22/2018-01-28   66  24  33   1  23  155  46   95  3  145  100  42   \n",
       "2018-01-29/2018-02-04  153  43  38   1  33  276  80  151  4   96   59  49   \n",
       "2018-02-05/2018-02-11  141  56  67  37  38  320  49  140  2  109   58  27   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   04  05  06  07  08  09  10   11  12  13  14 15  16   \n",
       "2018-01-08/2018-01-14  149  32  10   4  48  10  21   45  10  44  26  1   5   \n",
       "2018-01-15/2018-01-21  170  50  14  24  45  16   8  113  28  14  38  2   6   \n",
       "2018-01-22/2018-01-28  123  36  17  29  73  22  10   94  23  27  46  3   4   \n",
       "2018-01-29/2018-02-04  147  41   8  14  41  12  12   50  22  12  17  1   1   \n",
       "2018-02-05/2018-02-11  130  44  21  25  45   7   7   68  20  25   5  2  21   \n",
       "\n",
       "Region                                 CG                                      \\\n",
       "Event                  17  18   19 20  01  02  03  04  05  06  07  08  09  10   \n",
       "2018-01-08/2018-01-14  26  12   63  3  36  46  10  47  23   4  13  35   4   0   \n",
       "2018-01-15/2018-01-21  53  27  105  1  73  68   6  51  19  14   8  23  14  10   \n",
       "2018-01-22/2018-01-28  77  14  114  3  77  94  17  54  11   5  12  41  17  19   \n",
       "2018-01-29/2018-02-04  50  18   59  2  59  38  10  69  17   1  12  29   7   0   \n",
       "2018-02-05/2018-02-11  33  25   44  0  51  32   6  96  19   7  17  24   0   1   \n",
       "\n",
       "Region                                                         CH              \\\n",
       "Event                  11  12  13  14  15 16  17  18  19 20    01    02    03   \n",
       "2018-01-08/2018-01-14  26  12  31  16   0  2  12  13  28  0  4342  1923  2432   \n",
       "2018-01-15/2018-01-21  43  11   7  32  18  0  51  40  80  0  4362  1988  2019   \n",
       "2018-01-22/2018-01-28  74  15   6  57  14  1  84  17  92  0  4518  2072  2411   \n",
       "2018-01-29/2018-02-04  37  11   4   6   0  2  25  11  30  2  4176  2069  2735   \n",
       "2018-02-05/2018-02-11  41   7   6   4   0  0  23  15  44  0  4204  1999  2288   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                    04    05    06    07    08   09   10    11   12   13   \n",
       "2018-01-08/2018-01-14  7696  2973  1246  1192  1070  642  313  1865  621  468   \n",
       "2018-01-15/2018-01-21  6611  2387   986  1042   981  802  340  2100  721  520   \n",
       "2018-01-22/2018-01-28  6837  2695  1255   963  1045  591  378  2119  685  441   \n",
       "2018-01-29/2018-02-04  7762  2747  1074  1002  1020  630  355  1740  711  563   \n",
       "2018-02-05/2018-02-11  7653  3146  1219   955  1043  659  404  1710  700  525   \n",
       "\n",
       "Region                                                      CI            \\\n",
       "Event                   14   15   16    17   18    19  20   01   02   03   \n",
       "2018-01-08/2018-01-14  186  148  260  1162  169  1039   7  384  128  372   \n",
       "2018-01-15/2018-01-21  152  159  262  1332  239   874  16  968  343  675   \n",
       "2018-01-22/2018-01-28  148  110  276  1194  173   829  10  449  143  200   \n",
       "2018-01-29/2018-02-04  146  121  279  1278  188   790   1  351  169  194   \n",
       "2018-02-05/2018-02-11  169  166  236  1286  239  1030   1  351  117  123   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                    04   05  06   07   08   09  10   11   12  13   14   \n",
       "2018-01-08/2018-01-14   893  179  50   35   71   60  20  202   38  34   46   \n",
       "2018-01-15/2018-01-21  2558  554  50  106  123   51  50  558  131  87  142   \n",
       "2018-01-22/2018-01-28  1109  664  55   68   88   32  38  334   71  20   26   \n",
       "2018-01-29/2018-02-04   715  325  42  105   83  101  33  190   76  18   11   \n",
       "2018-02-05/2018-02-11   544   93  42   52   56   54   7  214   54   6   20   \n",
       "\n",
       "Region                                          CJ                           \\\n",
       "Event                  15  16   17  18   19 20  01  02  03   04  05  06  07   \n",
       "2018-01-08/2018-01-14  17  15  108  42  143  0  21   3  16   30  14   1  12   \n",
       "2018-01-15/2018-01-21  25  22  322  61  212  0  46  12  28   78  19  11  15   \n",
       "2018-01-22/2018-01-28   5  15  114  19   82  0  27   9  18   66  14   8   4   \n",
       "2018-01-29/2018-02-04   3  17   97  22   55  0  20   9  29  112  20   5   2   \n",
       "2018-02-05/2018-02-11   0   3   56  12   59  0  25  16   9   47  26   2   7   \n",
       "\n",
       "Region                                                             CM       \\\n",
       "Event                 08 09 10  11  12 13 14 15 16  17 18  19 20   01   02   \n",
       "2018-01-08/2018-01-14  5  2  1   6   6  3  0  0  0  10  0   9  0  137   68   \n",
       "2018-01-15/2018-01-21  7  1  9   5   8  1  1  0  0   7  1  23  0  117   43   \n",
       "2018-01-22/2018-01-28  2  4  0   5   4  1  0  0  0   0  0   5  0  131   63   \n",
       "2018-01-29/2018-02-04  2  0  0  11  14  0  0  0  1   4  0   4  0  151  130   \n",
       "2018-02-05/2018-02-11  2  6  1   2  12  3  0  0  0   9  7   7  0  206  138   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  03   04   05  06  07  08  09  10   11  12  13  14 15   \n",
       "2018-01-08/2018-01-14  53  231   66  20  35  95   3  27   66  29  26  28  2   \n",
       "2018-01-15/2018-01-21  26  192   55  18  29  39  19  11   85   6  15  13  4   \n",
       "2018-01-22/2018-01-28  48  228   62  13  36  52  11  17   75   9  14  13  7   \n",
       "2018-01-29/2018-02-04  81  306  105  69  45  94   8  16   68  16  30  36  8   \n",
       "2018-02-05/2018-02-11  70  420  137  50  69  94  27  32  103  38  26  18  6   \n",
       "\n",
       "Region                                     CN                                \\\n",
       "Event                  16   17  18   19 20 01 02 03 04 05 06 07 08 09 10 11   \n",
       "2018-01-08/2018-01-14  14  107  33  110  0  5  2  1  0  2  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21   2   62  31   86  0  1  1  0  8  3  1  1  0  5  0  1   \n",
       "2018-01-22/2018-01-28   6   42  12   84  8  2  3  1  4  2  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04   2  162  16  218  0  0  0  0  9  2  0  0  2  0  0  3   \n",
       "2018-02-05/2018-02-11   6   84  18   98  1  0  0  5  0  4  2  0  0  0  0  0   \n",
       "\n",
       "Region                                              CO                      \\\n",
       "Event                 12 13 14 15  16 17 18 19 20   01   02   03   04   05   \n",
       "2018-01-08/2018-01-14  0  0  0  3   0  6  0  6  0  207   95  231  641  227   \n",
       "2018-01-15/2018-01-21  0  0  0  0  12  0  0  0  0  211   89  119  579  155   \n",
       "2018-01-22/2018-01-28  0  0  0  0   0  0  0  0  0  215  121  113  332  119   \n",
       "2018-01-29/2018-02-04  0  0  0  0   0  0  0  0  0  272  117  233  589  185   \n",
       "2018-02-05/2018-02-11  1  0  0  0   0  3  0  0  0  314  162  184  634  211   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  06  07  08  09  10   11  12  13  14  15  16   17  18   \n",
       "2018-01-08/2018-01-14  50  55  52  29  15  146  27  29  24  15  24   95  45   \n",
       "2018-01-15/2018-01-21  63  52  72  39  13  121  45  22  14  10  15  107  44   \n",
       "2018-01-22/2018-01-28  47  46  67  32  17  136  33  25  14   7  17  103  38   \n",
       "2018-01-29/2018-02-04  50  49  94  28  20   90  32  20  10   4  46   53  76   \n",
       "2018-02-05/2018-02-11  50  80  74  33  39  204  42  42  34   9  29  144  51   \n",
       "\n",
       "Region                        CQ                                               \\\n",
       "Event                   19 20 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16   \n",
       "2018-01-08/2018-01-14  181  0  0  1  3  4  0  0  0  0  2  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  165  1  5  1  6  6  6  0  1  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  179  1  6  0  1  2  3  0  0  0  0  0  0  0  0  0  0  1   \n",
       "2018-01-29/2018-02-04  241  0  1  2  0  0  8  0  0  0  0  0  0  0  1  0  0  0   \n",
       "2018-02-05/2018-02-11  144  4  4  0  0  9  0  0  3  0  0  0  1  0  0  0  0  0   \n",
       "\n",
       "Region                            CR                                         \\\n",
       "Event                 17 18 19 20 01 02 03 04 05 06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                   CS                                   \\\n",
       "Event                 15 16 17 18 19 20  01  02  03   04  05  06  07  08  09   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  70  48  20  284  68  14  17  14  20   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  67  46  31  175  67  16  17  19  10   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  69  28  42  179  59   9  21   8  15   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  73  40  52  176  43   6  21  41  13   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  69  44  62  103  42   3   9  24  13   \n",
       "\n",
       "Region                                                       CT               \\\n",
       "Event                  10  11  12 13 14 15 16  17 18  19 20  01  02  03   04   \n",
       "2018-01-08/2018-01-14   5  22  56  9  3  0  4  17  5  38  0  64  42  11   19   \n",
       "2018-01-15/2018-01-21   6  11  33  3  0  0  4  17  8  18  0  35  33   2  153   \n",
       "2018-01-22/2018-01-28   8  10  14  7  0  1  1  14  2  11  0  17  14   5   65   \n",
       "2018-01-29/2018-02-04  14  63   5  7  1  1  2  21  4  12  0  43  25   6   29   \n",
       "2018-02-05/2018-02-11   9  29  17  7  7  0  2  28  5  21  0  33  14   6   46   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                  05 06  07  08  09 10  11 12  13 14 15  16  17  18  19   \n",
       "2018-01-08/2018-01-14   6  3   9  18  13  0   8  4   7  7  1   3   9   3  18   \n",
       "2018-01-15/2018-01-21  11  5  39  28   2  2  43  4  22  6  3   1  15  16  62   \n",
       "2018-01-22/2018-01-28  22  2  13  26   4  0  10  0   3  0  0   2  24   8  59   \n",
       "2018-01-29/2018-02-04  29  5  14  23   1  5  29  3   5  3  0  14   8   4  44   \n",
       "2018-02-05/2018-02-11  21  4  15  20   3  4  13  5   0  8  1   4  10  14  39   \n",
       "\n",
       "Region                     CU                                                 \\\n",
       "Event                 20   01   02   03   04   05  06  07   08   09  10   11   \n",
       "2018-01-08/2018-01-14  0  341  143  145  773  229  51  81   93  102  55  213   \n",
       "2018-01-15/2018-01-21  0  175  105   82  614  153  27  43   61   14  21   92   \n",
       "2018-01-22/2018-01-28  0  236   98  108  580  204  46  59   82   28  23   86   \n",
       "2018-01-29/2018-02-04  0  411  161  132  866  229  51  70  104   40  44  119   \n",
       "2018-02-05/2018-02-11  0  251  116  114  789  193  42  48   58   23  27  167   \n",
       "\n",
       "Region                                                     CV                  \\\n",
       "Event                  12  13  14  15  16   17  18   19 20 01 02 03  04 05 06   \n",
       "2018-01-08/2018-01-14  70  42  12  29  49   94  29  137  0  1  2  0  14  1  0   \n",
       "2018-01-15/2018-01-21  25  14   9   0  24   73  23   61  1  1  1  0   4  1  0   \n",
       "2018-01-22/2018-01-28  51   7  19   4   7  116  14   42  0  6  0  0  10  0  1   \n",
       "2018-01-29/2018-02-04  60  32  24   5  37  162  30  143  0  9  8  6   9  3  0   \n",
       "2018-02-05/2018-02-11  43  13  16  13  26   53  17   87  1  1  2  0  15  5  0   \n",
       "\n",
       "Region                                                           CW         \\\n",
       "Event                 07 08 09 10 11 12 13 14 15 16 17 18 19 20  01 02  03   \n",
       "2018-01-08/2018-01-14  0  0  3  0  0  0  0  0  0  0  0  0  0  0  24  8   3   \n",
       "2018-01-15/2018-01-21  1  0  0  0  1  0  0  0  0  0  0  0  0  0  15  8  12   \n",
       "2018-01-22/2018-01-28  1  2  4  0  0  2  0  0  0  0  2  0  0  0   9  6  13   \n",
       "2018-01-29/2018-02-04  1  1  1  0  1  1  0  0  0  0  0  0  0  0   7  3   5   \n",
       "2018-02-05/2018-02-11  0  2  1  0  0  0  0  0  0  0  0  0  0  0   8  5   7   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   04  05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   \n",
       "2018-01-08/2018-01-14   49  13  0  3  2  2  0  2  1  0  0  0  0  1  0  3  0   \n",
       "2018-01-15/2018-01-21   21   5  4  4  5  1  1  7  0  0  0  0  2  0  1  2  0   \n",
       "2018-01-22/2018-01-28   18   8  0  2  2  0  0  2  2  1  0  0  0  7  0  3  0   \n",
       "2018-01-29/2018-02-04  112   3  2  3  1  0  0  4  0  0  0  0  0  1  1  0  0   \n",
       "2018-02-05/2018-02-11   44   8  1  1  0  1  1  0  2  1  0  0  0  5  0  0  0   \n",
       "\n",
       "Region                  CY                                                     \\\n",
       "Event                   01  02  03   04   05  06  07  08  09  10   11  12  13   \n",
       "2018-01-08/2018-01-14  106  32  39  238   67  23  26  13  16   6   37  15   4   \n",
       "2018-01-15/2018-01-21   93  78  72  309  151  16  20  21   5  12   53  26   1   \n",
       "2018-01-22/2018-01-28  136  95  61  188  100  13  24  37  19  23  110  24  11   \n",
       "2018-01-29/2018-02-04  134  64  47  230  145  49  27  34   7  25   69  33  17   \n",
       "2018-02-05/2018-02-11  104  53  63  193  126  26  25  19  50  20   73  36  31   \n",
       "\n",
       "Region                                            DA                          \\\n",
       "Event                  14 15  16  17  18  19 20   01   02   03   04   05  06   \n",
       "2018-01-08/2018-01-14   3  0  25  38   9  27  0  172   86   91  386  109  60   \n",
       "2018-01-15/2018-01-21   5  4  12  37   5  28  0  275  102  121  464   96  57   \n",
       "2018-01-22/2018-01-28  13  0  11  31   6  24  0  229  178  176  625  138  26   \n",
       "2018-01-29/2018-02-04   5  4  19  28  11  49  0  206  115  113  369  103  35   \n",
       "2018-02-05/2018-02-11   7  0  16  29   1  22  0  232   94  116  356  121  48   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                  07  08  09  10   11  12  13  14  15  16   17  18   19   \n",
       "2018-01-08/2018-01-14  69  58  34   8   73  37  25  13   3  12   61   5   23   \n",
       "2018-01-15/2018-01-21  82  48  36  16   96  28  25   8  10   9  152  52  234   \n",
       "2018-01-22/2018-01-28  60  90  22  33  116  72  12  10   1  31  113  33  114   \n",
       "2018-01-29/2018-02-04  79  43  38  11   73  41  11   3   1   3   66  15   27   \n",
       "2018-02-05/2018-02-11  85  53  15  14   69  36  25   8   1   5   52   7   41   \n",
       "\n",
       "Region                    DJ                                                \\\n",
       "Event                 20  01  02  03   04  05  06  07  08 09  10  11 12 13   \n",
       "2018-01-08/2018-01-14  0  59  24  43  175  55  20  11  26  7  11  46  4  1   \n",
       "2018-01-15/2018-01-21  1  31  16  26   92  23  16   9  19  3   0  14  9  2   \n",
       "2018-01-22/2018-01-28  0  25  14  15  159  22  19  13   9  3   0   7  2  8   \n",
       "2018-01-29/2018-02-04  0  38  17  10   68  27  15   6   5  2   0   4  4  1   \n",
       "2018-02-05/2018-02-11  0  22  28  25  104  15  15   7  13  8   1   4  3  2   \n",
       "\n",
       "Region                                          DO                           \\\n",
       "Event                  14  15 16  17 18  19 20  01 02  03  04  05  06 07 08   \n",
       "2018-01-08/2018-01-14  18   1  4  11  0  16  0   5  3   2  17   3  14  3  0   \n",
       "2018-01-15/2018-01-21   0  10  4   4  8  21  0   6  2   4  13   6   3  3  0   \n",
       "2018-01-22/2018-01-28   0   3  0   7  1  21  0   7  5   4  12   3   1  4  1   \n",
       "2018-01-29/2018-02-04   0   1  2   3  0   8  0   7  3  10  22   9   2  2  1   \n",
       "2018-02-05/2018-02-11   0  24  3   6  1  20  0  16  5  32  56  16   3  7  0   \n",
       "\n",
       "Region                                                    DQ                 \\\n",
       "Event                 09 10 11 12 13 14 15 16 17 18 19 20 01 02 03 04 05 06   \n",
       "2018-01-08/2018-01-14  0  1  2  1  0  0  0  0  0  0  1  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  1  6  0  0  0  0  1  1  0  3  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  2  0  0  0  2  2  0  2  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  2  0  0  0  0  0  0  0  1  0  0  2  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  1  0  1  1  0  0  0  0  1  1  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                                           DR           \\\n",
       "Event                 07 08 09 10 11 12 13 14 15 16 17 18 19 20  01  02   03   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  38  29   36   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  88  30  109   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  59  16   15   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  83  19   18   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  60  33   22   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   04  05  06  07  08 09  10  11  12 13 14 15 16  17  18   \n",
       "2018-01-08/2018-01-14  205  39   4  10  17  6   5  27   4  0  1  0  3  36   3   \n",
       "2018-01-15/2018-01-21  173  30  10  22  17  4  10  29  13  4  1  0  6  74   8   \n",
       "2018-01-22/2018-01-28  134  32   7  20  21  6   2  13   3  8  3  0  4  28  13   \n",
       "2018-01-29/2018-02-04  258  48  10  15   9  6   9  22   8  4  3  1  2  16   5   \n",
       "2018-02-05/2018-02-11  118  58   7   7  11  3   3  25  16  1  3  1  4  25   2   \n",
       "\n",
       "Region                         EC                                          \\\n",
       "Event                  19 20   01   02   03   04  05  06   07  08  09  10   \n",
       "2018-01-08/2018-01-14  19  1  238  178  135  332  89  25  130  56  14   7   \n",
       "2018-01-15/2018-01-21  22  0   84   56   36  182  49  18   22  31  10   7   \n",
       "2018-01-22/2018-01-28   4  0  179   69   80  222  59  25   65  62  18  20   \n",
       "2018-01-29/2018-02-04  12  0   85   60   45  206  92  26   31  26   7  10   \n",
       "2018-02-05/2018-02-11   4  0  120   79   65  128  87  24   42  48  12  15   \n",
       "\n",
       "Region                                                           EG            \\\n",
       "Event                   11   12  13 14 15  16   17  18  19 20    01   02   03   \n",
       "2018-01-08/2018-01-14  110  110  15  2  1   7  125  34  41  0   734  376  727   \n",
       "2018-01-15/2018-01-21   41   35   8  3  2  13   22   4  15  0   933  447  880   \n",
       "2018-01-22/2018-01-28   49   42  11  5  5  11   66  32  26  0   917  434  499   \n",
       "2018-01-29/2018-02-04   54   31   6  4  0  13   60  21  42  0  1038  487  562   \n",
       "2018-02-05/2018-02-11   70   83  10  1  0   9   86  21  38  0   785  311  613   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                    04   05   06   07   08   09   10   11   12   13   14   \n",
       "2018-01-08/2018-01-14  1908  618  191  150  311  140  115  462  199   90  131   \n",
       "2018-01-15/2018-01-21  3131  700  141  168  278  114   98  537  256  161   68   \n",
       "2018-01-22/2018-01-28  2320  651  152  167  302  108  102  587  184   93   82   \n",
       "2018-01-29/2018-02-04  1766  685  214  151  295  132   66  526  152  125   94   \n",
       "2018-02-05/2018-02-11  1973  597  166  174  226   91   70  423  148  147   49   \n",
       "\n",
       "Region                                               EI                       \\\n",
       "Event                   15   16   17   18   19 20    01   02   03    04   05   \n",
       "2018-01-08/2018-01-14   44  150  466  114  601  7  1509  766  689  2174  759   \n",
       "2018-01-15/2018-01-21   77  208  456  136  436  2  1552  699  637  2290  764   \n",
       "2018-01-22/2018-01-28   16  121  513  174  418  2  1679  740  834  2469  811   \n",
       "2018-01-29/2018-02-04   15  128  484  215  546  1  1474  716  662  2331  762   \n",
       "2018-02-05/2018-02-11  106  110  435  128  691  0  1450  723  851  2662  747   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                   06   07   08   09   10   11   12   13  14  15  16   \n",
       "2018-01-08/2018-01-14  172  255  253  206  102  437  247  121  47  18  41   \n",
       "2018-01-15/2018-01-21  162  260  303  143  123  577  231  149  97   4  33   \n",
       "2018-01-22/2018-01-28  198  334  277  198  111  453  224  140  41   8  43   \n",
       "2018-01-29/2018-02-04  229  279  328  227  107  514  214  134  53  17  51   \n",
       "2018-02-05/2018-02-11  173  295  369  214  106  446  249  178  52   9  67   \n",
       "\n",
       "Region                                   EK                                   \\\n",
       "Event                   17   18   19 20  01  02  03   04  05  06 07 08 09 10   \n",
       "2018-01-08/2018-01-14  412   98  391  4  23   2  12   44  12   0  6  5  1  1   \n",
       "2018-01-15/2018-01-21  345  131  333  2  29  14   8   52  16   6  6  9  3  0   \n",
       "2018-01-22/2018-01-28  365  122  322  0  55  19  19  121  35  12  2  6  3  2   \n",
       "2018-01-29/2018-02-04  421  118  423  0  37  20   8   30  24   1  2  6  4  3   \n",
       "2018-02-05/2018-02-11  361   97  350  0  10  16  23   19  34   1  0  4  2  3   \n",
       "\n",
       "Region                                                     EN               \\\n",
       "Event                  11  12 13 14 15 16  17  18  19 20   01  02  03   04   \n",
       "2018-01-08/2018-01-14   6   5  0  0  0  2  14   1   7  0  112  51  81  360   \n",
       "2018-01-15/2018-01-21  25  14  0  1  1  1  26  17  13  0   71  46  69  194   \n",
       "2018-01-22/2018-01-28  15   7  3  0  0  1  38   1   3  0   73  29  35  190   \n",
       "2018-01-29/2018-02-04  12   7  1  0  0  0   7   0  12  0   83  41  71  145   \n",
       "2018-02-05/2018-02-11  12  15  2  0  0  4  21   7   2  0   44  41  95  219   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   05  06  07  08  09  10  11  12  13 14  15 16  17 18   \n",
       "2018-01-08/2018-01-14  136  21  38  14   5   5  14   7  10  1   6  1   7  1   \n",
       "2018-01-15/2018-01-21   90  29  31  11  35   5  23   8   2  0   2  2  15  8   \n",
       "2018-01-22/2018-01-28   31  18  20  11  10   6  27  11   6  1   5  1   8  2   \n",
       "2018-01-29/2018-02-04   62   7  17   8   4  14  26   6   0  4   2  4  11  1   \n",
       "2018-02-05/2018-02-11   75  11  18  25  12  12  22  14   7  8  12  0  23  1   \n",
       "\n",
       "Region                        ER                                            \\\n",
       "Event                  19 20  01  02  03   04   05  06  07  08  09  10  11   \n",
       "2018-01-08/2018-01-14   7  0  63  40  23  230   29   7  20  27   2   5  12   \n",
       "2018-01-15/2018-01-21  28  0  91  18  18  125   37  17  36  24   4   6  63   \n",
       "2018-01-22/2018-01-28  11  0  62  44   6   88   25   5  12  13   8  16  31   \n",
       "2018-01-29/2018-02-04  11  0  47  29  12  134  203  27  52  15   6   8  19   \n",
       "2018-02-05/2018-02-11  13  0  40  43   7  268   29  13  14  17  12  12  25   \n",
       "\n",
       "Region                                                     ES                 \\\n",
       "Event                  12  13  14  15  16  17  18  19 20   01   02   03   04   \n",
       "2018-01-08/2018-01-14  37  13   0  30  16  12   6   9  5  601  304  227  972   \n",
       "2018-01-15/2018-01-21  29   8   4  22  23  56  13  25  0  244  140   72  519   \n",
       "2018-01-22/2018-01-28   8   9  17   2  10  13   7  15  0  110   66   40  249   \n",
       "2018-01-29/2018-02-04  18  10   2   5   4  27   8  89  0  137   59   48  291   \n",
       "2018-02-05/2018-02-11  17   6   0   1   4  43   9  28  0   65   21   27  188   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                   05  06   07   08  09  10   11   12  13  14 15  16   \n",
       "2018-01-08/2018-01-14  228  51  117  160  44  63  311  113  94  32  4  69   \n",
       "2018-01-15/2018-01-21  106  30   82   72  46  18  147   40  29   7  1  18   \n",
       "2018-01-22/2018-01-28   63  27   24   27   6  15   72   14   4   4  0  12   \n",
       "2018-01-29/2018-02-04   62  15   26   27  16  15   37   17  10   3  2  10   \n",
       "2018-02-05/2018-02-11   26   9    7    8   2   8   43    8   7   2  0   9   \n",
       "\n",
       "Region                                    ET                               \\\n",
       "Event                   17  18   19  20   01   02   03    04   05  06  07   \n",
       "2018-01-08/2018-01-14  138  54  165   2  327  120  184   656  191  50  69   \n",
       "2018-01-15/2018-01-21   84  20   92  10  392  191  223   952  387  62  82   \n",
       "2018-01-22/2018-01-28   61  10   56   1  359  209  305  1342  272  28  84   \n",
       "2018-01-29/2018-02-04   63  36  129   0  401  181  367  1083  445  39  80   \n",
       "2018-02-05/2018-02-11   34  13   20   0  259  115  187   516  248  37  47   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   08  09  10   11  12  13  14  15  16   17  18   19 20   \n",
       "2018-01-08/2018-01-14  129  26  24  179  46  27  16   9  29  266  26   77  7   \n",
       "2018-01-15/2018-01-21  359  18  37  222  63  37  24  19  26  199  25  108  1   \n",
       "2018-01-22/2018-01-28  162  29  44  191  73  28  35  12  26  145  14  148  0   \n",
       "2018-01-29/2018-02-04   90  29  37  149  87  56  22   2  55   73  16  126  1   \n",
       "2018-02-05/2018-02-11  191   8  21  120  49  14  16   4  42  148  12   74  0   \n",
       "\n",
       "Region                EU                                                     \\\n",
       "Event                 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                        EZ                                           \\\n",
       "Event                 19 20   01   02   03   04   05  06  07   08  09  10   \n",
       "2018-01-08/2018-01-14  0  0  238  108  120  341  146  63  35   59  17  12   \n",
       "2018-01-15/2018-01-21  0  0  262  102  173  557  147  63  60   72  50  11   \n",
       "2018-01-22/2018-01-28  0  0  349  163  163  476  229  50  64  114  24  26   \n",
       "2018-01-29/2018-02-04  0  0  209  109   90  347  187  32  49   41  19  11   \n",
       "2018-02-05/2018-02-11  0  0  145   50   88  273   94  35  22   38  18  12   \n",
       "\n",
       "Region                                                        FG             \\\n",
       "Event                   11   12  13  14 15  16  17  18  19 20 01 02  03  04   \n",
       "2018-01-08/2018-01-14  128   72  21  15  1   6  74  15  51  0  1  0   1   6   \n",
       "2018-01-15/2018-01-21  102   72  24  22  1   5  56  11  62  0  3  8   1   0   \n",
       "2018-01-22/2018-01-28  168  115  29   5  0  17  47   7  84  1  2  4  15  16   \n",
       "2018-01-29/2018-02-04   87   41  29   5  1   4  46  14  66  0  0  2   8  13   \n",
       "2018-02-05/2018-02-11   60   36  11   8  0   2  24   3  25  0  0  0   5  15   \n",
       "\n",
       "Region                                                                  FI  \\\n",
       "Event                 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   01   \n",
       "2018-01-08/2018-01-14  1  0  0  0  0  0  1  0  0  0  0  0  1  0  1  0  119   \n",
       "2018-01-15/2018-01-21  5  0  2  0  0  0  0  1  0  0  0  0  0  0  0  0  142   \n",
       "2018-01-22/2018-01-28  0  0  0  0  6  0  1  0  0  0  0  0  0  0  0  0  157   \n",
       "2018-01-29/2018-02-04  4  0  0  0  0  0  0  0  0  4  0  0  0  0  0  0  142   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  1  0  2  0  147   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                  02   03   04   05  06  07  08  09  10  11  12  13  14   \n",
       "2018-01-08/2018-01-14  64   78  274   77  42  33  20  34   2  61  16   6   0   \n",
       "2018-01-15/2018-01-21  75  138  581  167  35  47  24  22   9  34  11  10   7   \n",
       "2018-01-22/2018-01-28  92   95  263  141  29  46  40  20  12  70  37  11   6   \n",
       "2018-01-29/2018-02-04  63  161  285  130  13  34  33  28   8  39  35  14  15   \n",
       "2018-02-05/2018-02-11  80   82  249   62  23  68  43  26   4  52  20  22   3   \n",
       "\n",
       "Region                                        FJ                              \\\n",
       "Event                 15  16  17  18  19 20   01   02   03   04   05  06  07   \n",
       "2018-01-08/2018-01-14  9   2  23  16  24  0  233  124  113  417  202  43  62   \n",
       "2018-01-15/2018-01-21  2   4  13   6  28  1  234  133  113  418  187  56  42   \n",
       "2018-01-22/2018-01-28  6  10  25   5  23  0  180   89   89  438  173  91  64   \n",
       "2018-01-29/2018-02-04  0   7  26   1  18  0  165  131  114  383  132  57  46   \n",
       "2018-02-05/2018-02-11  4   3  25   4  23  0  243  110  149  540  137  43  66   \n",
       "\n",
       "Region                                                                   FK  \\\n",
       "Event                  08  09  10  11  12  13  14  15  16  17  18  19 20 01   \n",
       "2018-01-08/2018-01-14  64  27  26  60  42  24  16  10  14  45  23  20  0  0   \n",
       "2018-01-15/2018-01-21  42  35  29  67  30  17   4  10   2  54  30  19  0  1   \n",
       "2018-01-22/2018-01-28  51  66  24  54  23  12   9   6   7  52  29  21  0  4   \n",
       "2018-01-29/2018-02-04  42  32  25  82  18  21   2   6   6  56  42  36  0  1   \n",
       "2018-02-05/2018-02-11  47  38   9  72  19  24   5   5   6  52  14  34  0  0   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                 02 03  04  05 06 07 08 09 10 11 12 13 14 15 16 17 18 19   \n",
       "2018-01-08/2018-01-14  0  2   0   0  0  1  0  0  0  0  0  2  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  0   0   0  0  0  0  0  0  0  0  0  0  0  0  1  0  0   \n",
       "2018-01-22/2018-01-28  0  1  15  13  0  0  0  2  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  1  2   5   2  0  1  0  0  1  0  0  0  0  0  0  4  0  2   \n",
       "2018-02-05/2018-02-11  0  2   0   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                    FM                                                  \\\n",
       "Event                 20  01  02 03  04  05 06 07 08 09 10 11 12 13 14 15 16   \n",
       "2018-01-08/2018-01-14  0   3   4  3   4   4  0  3  1  0  0  3  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  16   1  5  24  11  6  5  0  2  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0   5  10  5   8   7  3  6  1  0  3  3  1  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0   2   1  0   6   3  1  2  0  0  0  1  0  1  1  0  0   \n",
       "2018-02-05/2018-02-11  0  11   0  0  25   1  0  0  3  1  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                            FO                                           \\\n",
       "Event                 17 18 19 20 01 02  03  04 05 06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14  0  0  0  0  1  0   1   1  0  1  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  5  0  0  0  0  0   0   1  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  1  3  0  6  3  10  24  0  0  0  5  0  0  0  8  0  1   \n",
       "2018-01-29/2018-02-04  0  0  2  0  2  1   1   8  3  1  0  3  0  0  4  0  0  0   \n",
       "2018-02-05/2018-02-11  7  1  4  0  2  1   3   8  1  1  1  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                   FP                                 \\\n",
       "Event                 15 16 17 18 19 20  01  02  03  04 05 06 07 08  09 10   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0   2   0  11  30  7  1  2  0   0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0   5   1   7  29  0  0  0  3  12  1   \n",
       "2018-01-22/2018-01-28  0  0  2  0  0  0  10  10   1  23  4  2  2  3  12  0   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  13   1   2  17  3  1  1  1   0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0   9   0   2  24  5  2  0  0   0  0   \n",
       "\n",
       "Region                                               FQ                       \\\n",
       "Event                  11 12 13 14 15 16 17 18 19 20 01 02 03 04 05 06 07 08   \n",
       "2018-01-08/2018-01-14   0  1  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21   3  0  0  0  0  0  0  3  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28   5  5  0  0  0  0  4  0  1  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11   0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                                       FR              \\\n",
       "Event                 09 10 11 12 13 14 15 16 17 18 19 20    01    02    03   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  2094  1027  1157   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  2138  1079  1770   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  2300  1165  1403   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  2061   826  1058   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  2178  1080  1113   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                    04    05   06   07   08   09   10    11   12   13   \n",
       "2018-01-08/2018-01-14  3999  1473  345  396  558  328  168  1023  292  218   \n",
       "2018-01-15/2018-01-21  4383  1379  341  462  611  292  140  1052  399  246   \n",
       "2018-01-22/2018-01-28  4864  1373  293  465  553  252  173  1072  397  305   \n",
       "2018-01-29/2018-02-04  3626  1221  318  375  531  367  164  1084  316  226   \n",
       "2018-02-05/2018-02-11  4328  1239  293  456  581  188  182  1106  435  222   \n",
       "\n",
       "Region                                                   GA                \\\n",
       "Event                   14  15   16   17   18   19  20   01  02   03   04   \n",
       "2018-01-08/2018-01-14  135  20  152  572  204  664  15  125  90   70  152   \n",
       "2018-01-15/2018-01-21  167  82   86  676  195  809   2  222  86   98  307   \n",
       "2018-01-22/2018-01-28  159  51  212  656  236  869   3  163  67   45  256   \n",
       "2018-01-29/2018-02-04  138  32  129  662  487  874   5  189  80   80  318   \n",
       "2018-02-05/2018-02-11  128  70  104  766  351  967   1  191  58  102  244   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   05  06  07  08  09  10  11  12  13 14 15 16  17  18   \n",
       "2018-01-08/2018-01-14   79  29  32  18  21  20  52  23  12  2  4  2  14  40   \n",
       "2018-01-15/2018-01-21  214  37  56  75  19  28  81  42  15  6  1  6  62  15   \n",
       "2018-01-22/2018-01-28   76  12  51  27  10  28  60  21  16  7  2  4  85  17   \n",
       "2018-01-29/2018-02-04   90  25  55  79  16  18  68  52  15  7  6  3  70  10   \n",
       "2018-02-05/2018-02-11  113  25  65  31  23  15  64  22   8  4  0  6  49   8   \n",
       "\n",
       "Region                        GB                                               \\\n",
       "Event                  19 20  01  02  03   04  05 06 07  08  09 10  11  12 13   \n",
       "2018-01-08/2018-01-14  51  0  24  18  19   31   7  3  3   3   1  0   2   1  0   \n",
       "2018-01-15/2018-01-21  54  0  53  23  54  115  30  2  1  11  10  1  15  18  3   \n",
       "2018-01-22/2018-01-28  43  0  16  17  26   50  25  1  5   4   3  2   6   5  2   \n",
       "2018-01-29/2018-02-04  57  0  34  16  24   55  22  5  5   9   1  0   5   1  5   \n",
       "2018-02-05/2018-02-11  29  0  13   8  15   17   7  3  1   1   0  0   7   1  4   \n",
       "\n",
       "Region                                        GG                               \\\n",
       "Event                 14 15 16  17 18  19 20  01  02  03   04  05  06  07  08   \n",
       "2018-01-08/2018-01-14  2  2  0   5  0   8  0  55  22  12   98  29  21   3  24   \n",
       "2018-01-15/2018-01-21  3  1  1  40  3  14  0  60  37  19  166  28   8   9  40   \n",
       "2018-01-22/2018-01-28  0  0  0   7  0  10  0  63  39  34  214  69  22  13  26   \n",
       "2018-01-29/2018-02-04  0  0  0   3  0  11  0  80  37  59  291  89  15  16  28   \n",
       "2018-02-05/2018-02-11  1  0  0   2  0  17  0  65  32  26  198  95  16  16  14   \n",
       "\n",
       "Region                                                                GH       \\\n",
       "Event                  09  10  11  12 13  14 15  16  17  18  19 20    01   02   \n",
       "2018-01-08/2018-01-14   0   2  23  16  6   8  2   1  47  16  47  0  1213  627   \n",
       "2018-01-15/2018-01-21  16   9  50   8  3  20  7   7  26  33  33  0  1166  606   \n",
       "2018-01-22/2018-01-28   6  18  31  31  4  11  1   8  30  14  30  0  1232  604   \n",
       "2018-01-29/2018-02-04  17   9  39   5  3   3  1  10  33   7  11  0  1064  454   \n",
       "2018-02-05/2018-02-11   8   6  39  22  5   7  7   7  19   6  24  0  1202  547   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   03    04   05   06   07   08   09   10   11   12   13   \n",
       "2018-01-08/2018-01-14  334  1249  670  167  236  158  125   92  583  161  145   \n",
       "2018-01-15/2018-01-21  340  1205  671  137  231  119  130   90  474  128  127   \n",
       "2018-01-22/2018-01-28  414  1543  658  128  251  190  129  100  459  149   84   \n",
       "2018-01-29/2018-02-04  357  1534  639  151  211  175  123   94  384  140   63   \n",
       "2018-02-05/2018-02-11  336  1223  620  126  210  147   59  101  410   92   68   \n",
       "\n",
       "Region                                              GI                      \\\n",
       "Event                  14  15  16   17  18   19 20  01  02  03   04  05 06   \n",
       "2018-01-08/2018-01-14  26  34  27  293  61  193  0  18   6  25   51  12  9   \n",
       "2018-01-15/2018-01-21  55  13  17  305  77  224  0  26  10  26   63  15  4   \n",
       "2018-01-22/2018-01-28  19  32  36  405  69  364  0  23  12  17   53  14  9   \n",
       "2018-01-29/2018-02-04  26   7  23  339  63  173  0  37  11  48   47  26  4   \n",
       "2018-02-05/2018-02-11  54   6  37  258  43  181  0  25  19  78  134  13  7   \n",
       "\n",
       "Region                                                              GJ      \\\n",
       "Event                  07 08 09 10  11  12 13 14 15 16 17 18 19 20  01  02   \n",
       "2018-01-08/2018-01-14   3  5  2  3   3   8  1  0  0  0  5  0  2  0  36  24   \n",
       "2018-01-15/2018-01-21  16  3  0  0   8   4  1  0  1  1  6  1  0  0  36   8   \n",
       "2018-01-22/2018-01-28   6  3  4  1   8   0  3  0  1  1  4  2  0  0  15   9   \n",
       "2018-01-29/2018-02-04   0  5  1  0   5  13  1  0  0  0  1  1  2  0  55  23   \n",
       "2018-02-05/2018-02-11   6  0  3  2  11   4  4  0  0  0  3  1  1  0  36   3   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  03  04  05  06 07  08 09 10  11 12 13 14 15  16  17 18   \n",
       "2018-01-08/2018-01-14  20  44  33   6  6   0  1  3  17  2  1  2  0   3  10  1   \n",
       "2018-01-15/2018-01-21  16  62   3   7  3  11  9  2  13  4  1  3  0  10  22  7   \n",
       "2018-01-22/2018-01-28  22  93  33   0  3  11  2  8   3  3  0  0  0   0   9  0   \n",
       "2018-01-29/2018-02-04  17  53  32  13  7  18  6  0   9  2  2  1  0   0   8  6   \n",
       "2018-02-05/2018-02-11   6  66  77   0  1  10  3  2   1  6  2  1  0   1   6  5   \n",
       "\n",
       "Region                        GK                                             \\\n",
       "Event                  19 20  01  02  03  04  05 06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14   4  0  12   9   2  16   2  1  7  4  1  0  1  2  2  1   \n",
       "2018-01-15/2018-01-21  15  0  19  10  10  26   4  5  0  0  7  1  7  1  1  0   \n",
       "2018-01-22/2018-01-28   3  0  18  15   4  30  13  1  6  4  0  1  4  3  0  0   \n",
       "2018-01-29/2018-02-04   3  0  18   3   9  14   7  5  4  3  3  1  5  1  1  0   \n",
       "2018-02-05/2018-02-11  10  0   9  11  13  11  11  0  2  2  1  0  4  4  0  0   \n",
       "\n",
       "Region                                   GL                                    \\\n",
       "Event                 15 16 17 18 19 20  01  02  03  04  05 06 07 08 09 10 11   \n",
       "2018-01-08/2018-01-14  0  0  0  0  2  0  23   7  15  32   3  1  5  0  2  1  4   \n",
       "2018-01-15/2018-01-21  0  0  2  0  4  0  26  19  17  59  11  7  6  3  8  1  5   \n",
       "2018-01-22/2018-01-28  0  1  2  1  9  0  27  13  13  70  15  5  4  9  9  0  7   \n",
       "2018-01-29/2018-02-04  0  1  1  0  5  0  27  18  13  84  12  9  4  6  2  0  3   \n",
       "2018-02-05/2018-02-11  0  3  6  0  2  0  32  13  16  64  16  1  9  1  3  0  4   \n",
       "\n",
       "Region                                                GM                   \\\n",
       "Event                 12 13 14 15 16  17 18  19 20    01   02    03    04   \n",
       "2018-01-08/2018-01-14  1  5  0  0  0   2  1   1  0  1711  844  1128  3058   \n",
       "2018-01-15/2018-01-21  8  1  1  0  2   9  2   2  0  1648  800   991  2909   \n",
       "2018-01-22/2018-01-28  4  0  3  1  4  13  1  17  0  1974  865   986  3307   \n",
       "2018-01-29/2018-02-04  4  2  0  0  0  13  1   8  0  1734  783   998  3127   \n",
       "2018-02-05/2018-02-11  5  1  1  0  0   1  0   6  0  1711  812  1127  2722   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                    05   06   07   08   09   10   11   12   13   14  15   \n",
       "2018-01-08/2018-01-14  1288  339  330  447  257  137  708  312  170  101  20   \n",
       "2018-01-15/2018-01-21  1157  360  376  464  257  158  751  350  180  151  22   \n",
       "2018-01-22/2018-01-28  1140  328  402  499  231  158  880  340  212  154  28   \n",
       "2018-01-29/2018-02-04  1112  303  454  464  283  105  884  267  220  187  19   \n",
       "2018-02-05/2018-02-11  1262  350  391  573  274  153  739  290  148  121  30   \n",
       "\n",
       "Region                                       GO                                \\\n",
       "Event                   16   17   18   19 20 01 02 03 04 05 06 07 08 09 10 11   \n",
       "2018-01-08/2018-01-14  124  492  126  640  2  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  132  576  211  744  1  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  120  720  170  854  1  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  157  515  146  677  8  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  111  402  156  575  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                           GP                           \\\n",
       "Event                 12 13 14 15 16 17 18 19 20 01 02 03  04 05 06 07 08 09   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  2  0  0  12  1  1  2  0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0   4  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0   5  1  0  0  0  2   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  2  2  27  0  0  0  0  3   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  4  2  0   0  0  0  0  0  1   \n",
       "\n",
       "Region                                                  GQ                   \\\n",
       "Event                 10 11 12 13 14 15 16 17 18 19 20  01  02  03   04  05   \n",
       "2018-01-08/2018-01-14  1  0  0  0  0  0  0  0  0  1  0  78  39  28   94  26   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  96  39  32  131  40   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  56  44  26   90  23   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  1  0  73  26  28  101  14   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  40  33  27   99  10   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                 06  07  08  09 10  11  12  13 14  15 16  17  18  19 20   \n",
       "2018-01-08/2018-01-14  6  26  29  11  2  37   9  10  4  23  2  20  14  17  0   \n",
       "2018-01-15/2018-01-21  1  21  12  12  4  51  15  11  0  19  2  29   9  16  0   \n",
       "2018-01-22/2018-01-28  3  13  18   5  8  42  14   7  2  13  3  15  13  11  0   \n",
       "2018-01-29/2018-02-04  7  21  15  12  0  21   7  11  0   3  0  10   8  16  0   \n",
       "2018-02-05/2018-02-11  4  10  12   6  2  25   9   2  0   8  1  16   5  10  0   \n",
       "\n",
       "Region                  GR                                                   \\\n",
       "Event                   01   02   03    04   05   06   07   08  09  10   11   \n",
       "2018-01-08/2018-01-14  481  284  307   845  263   63  127  143  67  73  273   \n",
       "2018-01-15/2018-01-21  547  259  333  1072  299   61  116  122  61  44  227   \n",
       "2018-01-22/2018-01-28  546  307  483  1209  311   89  100  119  40  63  221   \n",
       "2018-01-29/2018-02-04  736  289  423  1271  285  106  151  151  67  76  235   \n",
       "2018-02-05/2018-02-11  643  317  360  1423  357  103  122  131  99  62  324   \n",
       "\n",
       "Region                                                        GT               \\\n",
       "Event                   12  13  14  15  16   17  18   19 20   01  02  03   04   \n",
       "2018-01-08/2018-01-14  115  61  92   2  63  144  36  134  0   47  22  37  174   \n",
       "2018-01-15/2018-01-21  109  61  87   1  47  181  52  118  0   75  55  37  184   \n",
       "2018-01-22/2018-01-28  189  38  99   9  33  174  29  152  7   82  43  24  160   \n",
       "2018-01-29/2018-02-04  175  49  89  25  32  265  49  185  0  135  47  33  220   \n",
       "2018-02-05/2018-02-11  143  74  85  18  18  217  83  121  1  119  37  18  266   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   05  06  07  08  09  10  11  12  13  14 15 16   17  18   \n",
       "2018-01-08/2018-01-14   39  11  12  24   6  17  51   8  10   0  0  4   92  12   \n",
       "2018-01-15/2018-01-21   55  22  21  25   8  22  71  14   6  13  3  0   77  13   \n",
       "2018-01-22/2018-01-28   45  10  14  14  20  16  73   7   0  10  0  7   87  20   \n",
       "2018-01-29/2018-02-04  106  12  26  24  31   5  52  12   4   0  2  1   63  18   \n",
       "2018-02-05/2018-02-11  128  29  13  22  14   4  64  11   7   1  1  2  101  12   \n",
       "\n",
       "Region                        GV                                              \\\n",
       "Event                  19 20  01  02  03   04   05  06  07  08  09 10  11 12   \n",
       "2018-01-08/2018-01-14  41  0  27  20  20   82   19   0   1   6  11  0   8  7   \n",
       "2018-01-15/2018-01-21  56  5  33  20  34   65   40   6  13  18  11  0  19  8   \n",
       "2018-01-22/2018-01-28  77  0  60  18  67  180   49   9  38  13   4  2   2  3   \n",
       "2018-01-29/2018-02-04  39  0  42  11  34   86   66   7  10   9   2  9   5  2   \n",
       "2018-02-05/2018-02-11  60  0  85  32  30   45  101  10   3  16   1  7  45  5   \n",
       "\n",
       "Region                                               GY                     \\\n",
       "Event                  13 14 15  16  17  18  19 20   01   02  03   04   05   \n",
       "2018-01-08/2018-01-14   1  1  7   4   4   1  26  0  164   84  60  206   49   \n",
       "2018-01-15/2018-01-21   8  6  0  10  21   9  63  0  179  124  73  208   94   \n",
       "2018-01-22/2018-01-28   9  0  0   7   7   0  12  0  168   98  62  250   45   \n",
       "2018-01-29/2018-02-04   5  6  0   1  10   8  30  0  141   57  41  209  101   \n",
       "2018-02-05/2018-02-11  18  5  3  47  20  20  40  0  197   70  72  225   77   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  06  07  08  09  10  11  12 13  14 15 16  17  18  19 20   \n",
       "2018-01-08/2018-01-14   6  42  37  14  10  55  22  7  13  0  3  67   9  59  0   \n",
       "2018-01-15/2018-01-21  21  19  24  25   8  51  14  9   5  0  1  38   7  45  0   \n",
       "2018-01-22/2018-01-28   9  19  37  24   6  54  10  2   7  5  6  60  12  59  0   \n",
       "2018-01-29/2018-02-04   8   9  29  14   6  68  39  7   2  0  2  82  15  55  1   \n",
       "2018-02-05/2018-02-11  25  32  38  24   7  48  29  6   9  3  2  80   7  53  1   \n",
       "\n",
       "Region                 GZ                                                   \\\n",
       "Event                  01  02  03   04  05  06   07  08 09  10  11  12  13   \n",
       "2018-01-08/2018-01-14  70  34   7  149  48  25   87  16  3   4  43  20  28   \n",
       "2018-01-15/2018-01-21  99  38  32   87  83  23  166   9  3   5  58  22  27   \n",
       "2018-01-22/2018-01-28  37  26  25   62  28  16   95  13  1   4  35   4  20   \n",
       "2018-01-29/2018-02-04  76  39  22   92  37   2   82  25  4  10  43   9  36   \n",
       "2018-02-05/2018-02-11  60  44  25   73  48   8   73  17  7   1  16  16  13   \n",
       "\n",
       "Region                                              HA                       \\\n",
       "Event                  14 15  16  17  18   19 20    01   02   03    04   05   \n",
       "2018-01-08/2018-01-14  18  2   5  11  11  131  0  1266  646  411  1532  608   \n",
       "2018-01-15/2018-01-21  12  3  30  16   7   89  0   853  467  272  1334  479   \n",
       "2018-01-22/2018-01-28  21  0   6  13   0   51  0   377  144  139   647  158   \n",
       "2018-01-29/2018-02-04  29  3  23  19  10   84  0   226   88  118   437  131   \n",
       "2018-02-05/2018-02-11   9  7  16  23   6   51  0   267  102  112   393   96   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  06   07   08   09   10   11   12  13  14 15  16   17   \n",
       "2018-01-08/2018-01-14  86  260  175  107  116  942  281  79  43  2  94  188   \n",
       "2018-01-15/2018-01-21  69  204  122   64   67  736  197  57  76  1  80  148   \n",
       "2018-01-22/2018-01-28  51   70   50   24   21  247   64  22  90  0  49   65   \n",
       "2018-01-29/2018-02-04  22   76   48   23   21   98   38  12  19  4  27   58   \n",
       "2018-02-05/2018-02-11  17  101   56   29   18  112   51  21   5  0  21   62   \n",
       "\n",
       "Region                              HK                                      \\\n",
       "Event                  18   19 20   01   02   03   04   05  06  07  08  09   \n",
       "2018-01-08/2018-01-14  66  111  0  227  112  102  401   92  64  56  53  31   \n",
       "2018-01-15/2018-01-21  54  165  1  221  121  123  471   79  67  44  67  69   \n",
       "2018-01-22/2018-01-28  22   73  0  251  145  117  339  153  34  51  75  46   \n",
       "2018-01-29/2018-02-04  22   30  0  295  170  116  443  185  46  53  65  24   \n",
       "2018-02-05/2018-02-11  36   21  0  247  154  108  382   98  70  52  82  37   \n",
       "\n",
       "Region                                                            HM           \\\n",
       "Event                  10   11  12  13  14 15  16   17  18  19 20 01 02 03 04   \n",
       "2018-01-08/2018-01-14  17  129  44  23  14  2  16   82   5  54  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  14   99  36  20  36  4   7  158  12  51  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  35  107  50  18  37  2  28  123  22  61  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  38  114  53  23  35  3  21  163  15  69  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  36  118  41  40  18  3  25  139  10  80  1  0  0  0  0   \n",
       "\n",
       "Region                                                                  HO  \\\n",
       "Event                 05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   01   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  127   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  112   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  175   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   86   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   93   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  02  03   04  05  06  07  08  09 10   11  12  13  14 15   \n",
       "2018-01-08/2018-01-14  84  41  207  72  10  21  24  13  6   72  25  35  27  7   \n",
       "2018-01-15/2018-01-21  94  49  193  79  19  25  27   6  3   55   7  20  26  8   \n",
       "2018-01-22/2018-01-28  78  32  210  66  22  24  31  11  9  120  19  40  60  8   \n",
       "2018-01-29/2018-02-04  46  18  233  53  16  35  21   4  9   59   9   8  10  0   \n",
       "2018-02-05/2018-02-11  30  32  144  25  21  23  19  18  6   48  12   4  15  0   \n",
       "\n",
       "Region                                     HQ                                \\\n",
       "Event                  16   17  18   19 20 01 02 03 04 05 06 07 08 09 10 11   \n",
       "2018-01-08/2018-01-14  10   46  14   60  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  11   89   8   75  0  0  0  0  2  0  0  0  0  1  0  0   \n",
       "2018-01-22/2018-01-28  14  106  21  111  0  0  0  0  1  0  0  0  0  1  0  0   \n",
       "2018-01-29/2018-02-04   4   51  13   54  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11   7   42  11   45  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                             HR                        \\\n",
       "Event                 12 13 14 15 16 17 18 19 20   01  02  03   04   05  06   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0   41  23  75  116   50   9   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  1  0  1  0   70  18  55  189   56   8   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0   75  33  60  184   29   4   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  153  23  98  224  126  18   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  133  47  49  163  136  11   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                  07  08  09  10  11  12  13 14 15 16  17  18  19 20   \n",
       "2018-01-08/2018-01-14   7  14   3   9  21  20  11  1  1  6  15   5  17  0   \n",
       "2018-01-15/2018-01-21  14  32   3   3  14  10  10  2  0  1  22   4  28  2   \n",
       "2018-01-22/2018-01-28  24  21   7   7  37  12   3  7  0  0  22   3  28  0   \n",
       "2018-01-29/2018-02-04  25  21  24   4  43  11  10  6  2  3  27   7  69  1   \n",
       "2018-02-05/2018-02-11  16  32   5  15  25  18  14  3  0  5  16  10  32  1   \n",
       "\n",
       "Region                  HU                                                   \\\n",
       "Event                   01   02   03   04   05  06  07  08  09  10   11  12   \n",
       "2018-01-08/2018-01-14  237   82  104  367  101  24  65  38  17  20  122  81   \n",
       "2018-01-15/2018-01-21  200  110  144  425  119  39  70  33  21  61  141  52   \n",
       "2018-01-22/2018-01-28  152  135   96  372  162  17  60  53  16  15  117  67   \n",
       "2018-01-29/2018-02-04  191  110  113  565  157  17  52  52  14  23   88  40   \n",
       "2018-02-05/2018-02-11  204   79  128  320  152  68  75  45  20  19   86  37   \n",
       "\n",
       "Region                                                  IC                   \\\n",
       "Event                  13  14  15  16   17  18  19 20   01  02  03   04  05   \n",
       "2018-01-08/2018-01-14  31  12   1   9   43  14  22  0   67  43  79  186  35   \n",
       "2018-01-15/2018-01-21  16  13   6  13  138   4  47  2  141  70  85  318  51   \n",
       "2018-01-22/2018-01-28   9   6   1  15   43  10  35  0   53  50  65  167  32   \n",
       "2018-01-29/2018-02-04  27   7  11   7   52   5  70  0   59  26  47  168  44   \n",
       "2018-02-05/2018-02-11  21  11   2  11   43   3  25  0   74  42  59  160  41   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                  06  07  08  09 10  11  12 13 14 15 16  17 18  19 20   \n",
       "2018-01-08/2018-01-14  17   4  27  15  3  17   6  4  8  1  0  24  3  30  0   \n",
       "2018-01-15/2018-01-21  16  19  30  19  1  54  38  9  9  0  0  53  4  28  0   \n",
       "2018-01-22/2018-01-28   7  15  30   6  1  23  13  2  0  0  0  11  9  17  0   \n",
       "2018-01-29/2018-02-04   7  12  11  12  0  17   8  2  0  0  1  22  8  16  0   \n",
       "2018-02-05/2018-02-11  22  18  17  10  4  30  10  8  1  0  1  34  2  13  0   \n",
       "\n",
       "Region                  ID                                                    \\\n",
       "Event                   01   02   03    04   05   06   07   08   09  10   11   \n",
       "2018-01-08/2018-01-14  488  276  279   962  255  123  132  106   69  28  170   \n",
       "2018-01-15/2018-01-21  649  227  282  1052  264  135  119  129  105  55  204   \n",
       "2018-01-22/2018-01-28  979  468  618  2665  658  230  222  159  104  89  253   \n",
       "2018-01-29/2018-02-04  727  315  413  1451  477  120  160  141  123  80  302   \n",
       "2018-02-05/2018-02-11  615  312  371  1495  414  176  139  168  117  96  233   \n",
       "\n",
       "Region                                                        IM               \\\n",
       "Event                   12   13  14  15  16   17  18   19 20  01 02 03  04 05   \n",
       "2018-01-08/2018-01-14   88   44  30   5  21  313  26  111  2   0  0  1   0  0   \n",
       "2018-01-15/2018-01-21  107   53  33  15  40  277  39  129  1   2  0  4   6  0   \n",
       "2018-01-22/2018-01-28  102   72  54  33  60  400  78  259  6  15  0  0  27  1   \n",
       "2018-01-29/2018-02-04   71   76  36   5  16  300  40  161  0   0  1  0   0  0   \n",
       "2018-02-05/2018-02-11  103  101  20  24  37  377  55  250  5   1  6  2   6  0   \n",
       "\n",
       "Region                                                                IN  \\\n",
       "Event                 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20    01   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  7118   \n",
       "2018-01-15/2018-01-21  0  0  5  0  0  0  0  0  0  0  0  0  0  0  0  7110   \n",
       "2018-01-22/2018-01-28  0  1  0  1  1  0  0  0  0  0  0  0  0  0  0  7080   \n",
       "2018-01-29/2018-02-04  0  0  0  1  0  0  0  0  0  0  0  2  0  2  0  6349   \n",
       "2018-02-05/2018-02-11  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  7196   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                    02    03     04    05    06    07    08    09    10   \n",
       "2018-01-08/2018-01-14  3774  4398  12657  3267  1034  1295  1918  1305   952   \n",
       "2018-01-15/2018-01-21  4254  4714  14132  3672  1012  1440  2319  1244  1077   \n",
       "2018-01-22/2018-01-28  3882  3980  13414  4152   829  1233  2465  1062   904   \n",
       "2018-01-29/2018-02-04  3714  2902  10281  3164   903  1461  1913  1163   825   \n",
       "2018-02-05/2018-02-11  4130  3990  12857  3401  1044  1668  2171  1295   940   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                    11    12   13    14   15   16    17    18    19  20   \n",
       "2018-01-08/2018-01-14  3944  1541  638   789  163  335  2831   857  2742  15   \n",
       "2018-01-15/2018-01-21  3978  1435  753  1031  287  405  2882  1369  3378  24   \n",
       "2018-01-22/2018-01-28  4129  1528  738  1658  417  376  3067  1270  2804  15   \n",
       "2018-01-29/2018-02-04  3806  1273  547   823  199  332  2348  1011  2452  16   \n",
       "2018-02-05/2018-02-11  4492  1455  577  1004  199  402  2590   853  2887   8   \n",
       "\n",
       "Region                IO                                                     \\\n",
       "Event                 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  1  3  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  2  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                      IP                                               \\\n",
       "Event                 19 20 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                               IR                                    \\\n",
       "Event                 17 18 19 20    01    02    03    04    05   06   07   \n",
       "2018-01-08/2018-01-14  0  0  0  0  2593  1251  1164  3384  2417  380  408   \n",
       "2018-01-15/2018-01-21  0  0  0  0  1787   808  1043  2792  1921  234  315   \n",
       "2018-01-22/2018-01-28  0  0  0  0  1419   605   845  2300  1249  292  255   \n",
       "2018-01-29/2018-02-04  0  0  0  0  1327   667   662  2404   930  327  289   \n",
       "2018-02-05/2018-02-11  0  0  0  0  1792   740   675  2643  1254  329  325   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                    08   09   10    11   12   13   14  15   16    17   \n",
       "2018-01-08/2018-01-14  1079  320  359  1889  620  726  661  47  603  1131   \n",
       "2018-01-15/2018-01-21   654  205  146  1006  477  455  271  46  229   707   \n",
       "2018-01-22/2018-01-28   488  159  130   913  345  411  192  80  186   500   \n",
       "2018-01-29/2018-02-04   494  140  127   915  316  334  392  56  144   729   \n",
       "2018-02-05/2018-02-11   442  173  180  1009  473  492  265  55  126   527   \n",
       "\n",
       "Region                                 IS                                     \\\n",
       "Event                   18    19 20    01    02    03    04    05   06    07   \n",
       "2018-01-08/2018-01-14  269   990  1  3214  1688  1941  5680  2050  397   770   \n",
       "2018-01-15/2018-01-21  210   657  2  3582  1807  2729  8122  2893  504  1238   \n",
       "2018-01-22/2018-01-28  132   654  3  3672  1805  2270  8022  2620  345   949   \n",
       "2018-01-29/2018-02-04  131   549  0  3369  1741  1479  5563  2113  501   921   \n",
       "2018-02-05/2018-02-11  214  1305  5  3938  1827  2283  8075  2491  589  1010   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   08   09   10    11    12   13   14   15   16    17   \n",
       "2018-01-08/2018-01-14  762  513  359  2243   938  514  431   61  537  1340   \n",
       "2018-01-15/2018-01-21  897  325  373  2414  1112  646  487   84  795  1364   \n",
       "2018-01-22/2018-01-28  837  327  351  2283   993  601  506   66  708  1212   \n",
       "2018-01-29/2018-02-04  806  385  324  1993   783  712  416   73  735  1257   \n",
       "2018-02-05/2018-02-11  869  498  364  2312   919  947  371  131  390  1417   \n",
       "\n",
       "Region                                  IT                                 \\\n",
       "Event                   18    19  20    01   02   03    04   05   06   07   \n",
       "2018-01-08/2018-01-14  435  3191  43  1037  498  613  2153  614  158  282   \n",
       "2018-01-15/2018-01-21  473  2671  77   867  457  543  1985  562  141  240   \n",
       "2018-01-22/2018-01-28  442  1661  50  1040  451  545  1950  600  154  247   \n",
       "2018-01-29/2018-02-04  386  2296  33  1076  618  675  2135  707  157  287   \n",
       "2018-02-05/2018-02-11  686  4378  36  1132  538  805  2650  689  195  223   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                   08   09  10   11   12   13   14  15  16   17   18   \n",
       "2018-01-08/2018-01-14  268  121  75  401  160   53   48   7  40  368   99   \n",
       "2018-01-15/2018-01-21  204   91  82  458  150   62   72   4  22  283   83   \n",
       "2018-01-22/2018-01-28  208  109  86  492  162   86   53  22  58  326   89   \n",
       "2018-01-29/2018-02-04  204  137  93  364  187   87   45  12  39  411   96   \n",
       "2018-02-05/2018-02-11  247  127  54  540  144  112  119  29  51  512  127   \n",
       "\n",
       "Region                         IV                                            \\\n",
       "Event                   19 20  01  02  03   04  05  06  07 08 09 10  11  12   \n",
       "2018-01-08/2018-01-14  306  4  17  10   9   34   1   5   9  6  4  1   3   1   \n",
       "2018-01-15/2018-01-21  259  0  51  31  16   69  21   5   9  9  3  8  24   3   \n",
       "2018-01-22/2018-01-28  544  1  70  19  71   89  24  10  10  3  2  3  11   8   \n",
       "2018-01-29/2018-02-04  605  1  36  26  17  125  30   7   5  6  5  0   2  10   \n",
       "2018-02-05/2018-02-11  574  8  34  28   9   62  22   8   7  0  4  2   8   3   \n",
       "\n",
       "Region                                             IZ                       \\\n",
       "Event                 13 14 15 16  17  18  19 20   01   02   03    04   05   \n",
       "2018-01-08/2018-01-14  2  4  2  2   6   3  23  0  754  363  235  1423  416   \n",
       "2018-01-15/2018-01-21  2  1  0  2  34   3   3  0  919  389  344  1720  573   \n",
       "2018-01-22/2018-01-28  0  0  0  1  20   1  21  0  778  340  331  1582  444   \n",
       "2018-01-29/2018-02-04  0  0  0  0  23   0   7  0  645  215  241  1061  320   \n",
       "2018-02-05/2018-02-11  2  6  2  0   5  15  20  0  917  392  319  1533  520   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   06   07   08  09   10   11   12   13  14  15  16   17   \n",
       "2018-01-08/2018-01-14  181  187  246  77   90  325  119  113  53  48  45  252   \n",
       "2018-01-15/2018-01-21  170  205  291  75   98  376  170  180  51  56  67  279   \n",
       "2018-01-22/2018-01-28  138  211  234  73   69  307  128  149  26  55  77  312   \n",
       "2018-01-29/2018-02-04  131  176  236  59   70  305  101  113  34  33  61  277   \n",
       "2018-02-05/2018-02-11  193  243  294  61  101  404  123  116  37  73  54  333   \n",
       "\n",
       "Region                                 JA                                  \\\n",
       "Event                   18   19  20    01   02   03    04    05   06   07   \n",
       "2018-01-08/2018-01-14  139  725   7  1586  631  996  3231  1024  369  456   \n",
       "2018-01-15/2018-01-21  180  886  10  1348  619  973  3237   824  316  351   \n",
       "2018-01-22/2018-01-28  152  809  14  1460  629  961  2769   956  441  422   \n",
       "2018-01-29/2018-02-04  102  696   3  1198  549  811  2637   788  366  328   \n",
       "2018-02-05/2018-02-11  123  925   7  1382  564  938  2842   816  357  344   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   08   09   10   11   12   13  14  15   16   17   18   \n",
       "2018-01-08/2018-01-14  427  215  137  630  223   98  68  60   62  496  108   \n",
       "2018-01-15/2018-01-21  372  209   82  531  209  164  51  72  122  435  102   \n",
       "2018-01-22/2018-01-28  361  211   79  541  220  149  52  73  115  347  121   \n",
       "2018-01-29/2018-02-04  436  219   86  470  206  117  59  47   65  434  120   \n",
       "2018-02-05/2018-02-11  374  173   90  515  178  142  35  34   61  336  130   \n",
       "\n",
       "Region                          JE                                         \\\n",
       "Event                   19 20   01   02  03   04   05  06  07  08  09  10   \n",
       "2018-01-08/2018-01-14  407  0  211  137  87  280  113  18  59  59  27  10   \n",
       "2018-01-15/2018-01-21  476  1  309  120  99  387  139  23  79  69  18   8   \n",
       "2018-01-22/2018-01-28  459  0  217  122  98  333   78  36  62  58  32   9   \n",
       "2018-01-29/2018-02-04  505  0  187   89  57  266  102  35  59  63  20  12   \n",
       "2018-02-05/2018-02-11  379  0  199  112  95  235   76  25  61  37  29  12   \n",
       "\n",
       "Region                                                           JM            \\\n",
       "Event                   11  12  13  14 15  16   17  18   19 20   01   02   03   \n",
       "2018-01-08/2018-01-14   93  52  23   6  0   5  123  29  124  0  256   88   77   \n",
       "2018-01-15/2018-01-21  110  54  29   6  0   9   82  22   55  0  362  145  124   \n",
       "2018-01-22/2018-01-28   87  38  24   2  0  12   97  27   70  0  390  155  159   \n",
       "2018-01-29/2018-02-04   72  49  12   1  0  10   78  20   64  0  396  198  234   \n",
       "2018-02-05/2018-02-11   85  32  14  14  0   9   64   8   60  0  330  190  156   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   04   05  06  07  08  09  10   11  12   13  14  15  16   \n",
       "2018-01-08/2018-01-14  798  122  53  51  61  24  20   75  46  139  11   5  14   \n",
       "2018-01-15/2018-01-21  470  210  39  49  56  38  21   55  24   50   1  13  13   \n",
       "2018-01-22/2018-01-28  539  208  49  61  49  41  18   95  26   39   7  15  14   \n",
       "2018-01-29/2018-02-04  736  239  54  88  61  25  26  106  60   25   9   4  31   \n",
       "2018-02-05/2018-02-11  882  182  58  63  59  36  19   92  41   21   5   2  23   \n",
       "\n",
       "Region                                 JN                                      \\\n",
       "Event                   17  18   19 20 01 02 03 04 05 06 07 08 09 10 11 12 13   \n",
       "2018-01-08/2018-01-14   73  41  109  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  134  21  219  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  119  41  170  2  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  180  30  142  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11   93  20  117  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                       JO                                \\\n",
       "Event                 14 15 16 17 18 19 20   01   02   03    04   05  06   07   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  323  129  298   788  193  46   57   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  474  197  416  1312  418  68  135   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  438  248  296  1379  374  58  106   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  440  206  186   965  298  99  133   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  408  157  640  1863  342  84   60   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   08  09  10   11  12  13  14 15  16   17  18   19 20   \n",
       "2018-01-08/2018-01-14   82  44  30  132  34  29  23  7  21  124  16  178  0   \n",
       "2018-01-15/2018-01-21  115  46  49  206  66  49  39  6  65  132  25  258  1   \n",
       "2018-01-22/2018-01-28   87  32  50  178  81  60  30  3  43   95  40  144  4   \n",
       "2018-01-29/2018-02-04  104  41  39  205  42  16  21  6  47  164  57  202  3   \n",
       "2018-02-05/2018-02-11  112  48  22  106  43  19  11  4  21  147  26  135  0   \n",
       "\n",
       "Region                JQ                                                     \\\n",
       "Event                 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                         KE                                           \\\n",
       "Event                 19 20    01   02   03    04   05   06   07   08   09   \n",
       "2018-01-08/2018-01-14  0  0  1061  478  534  1720  458  178  229  202   92   \n",
       "2018-01-15/2018-01-21  0  0  1131  788  505  2039  539  160  285  287  206   \n",
       "2018-01-22/2018-01-28  0  0   864  519  405  1511  425  139  227  226  118   \n",
       "2018-01-29/2018-02-04  0  0  1438  625  414  2004  646   97  239  437  129   \n",
       "2018-02-05/2018-02-11  0  0  1079  508  422  1565  476  239  188  412  130   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                   10   11   12   13   14  15   16   17   18   19 20   \n",
       "2018-01-08/2018-01-14  107  645  200   98   38  23   85  269   84  445  0   \n",
       "2018-01-15/2018-01-21  186  741  214  161   85  29   78  396  146  452  0   \n",
       "2018-01-22/2018-01-28  157  553  178  112   43   8   70  293  109  212  0   \n",
       "2018-01-29/2018-02-04  195  976  388  208  127  28  102  827  133  471  0   \n",
       "2018-02-05/2018-02-11  208  798  296  156  115  26  100  730  119  408  1   \n",
       "\n",
       "Region                  KG                                                  \\\n",
       "Event                   01  02  03   04  05  06  07  08  09  10  11  12 13   \n",
       "2018-01-08/2018-01-14   53  32  34  162  46  15   9  14   5   9  22   1  3   \n",
       "2018-01-15/2018-01-21   96  45  61  318  91  40  20  29   2   6  18  18  7   \n",
       "2018-01-22/2018-01-28  109  59  37  219  74  31  12  14   7  10  18   6  5   \n",
       "2018-01-29/2018-02-04   95  52  58  378  45  17  16  17  14  21  19  21  7   \n",
       "2018-02-05/2018-02-11   63  32  59  322  58  20  21  14  13  11  11  20  6   \n",
       "\n",
       "Region                                           KN                            \\\n",
       "Event                  14 15 16  17 18  19 20    01   02   03    04   05   06   \n",
       "2018-01-08/2018-01-14   1  1  1  30  1  16  0  1243  561  711  2255  767  193   \n",
       "2018-01-15/2018-01-21  11  1  0  31  4  19  0  1049  507  453  1830  509  123   \n",
       "2018-01-22/2018-01-28   2  0  8  16  5  41  0   664  369  316   997  323   92   \n",
       "2018-01-29/2018-02-04   3  0  3  18  3  26  0  1196  427  320  1586  392  185   \n",
       "2018-02-05/2018-02-11   0  1  2  21  2  34  0   859  377  813  2152  539  130   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   07   08   09   10   11   12   13  14   15   16   17   \n",
       "2018-01-08/2018-01-14  107  202   79   80  480  173  417  41   91  185  135   \n",
       "2018-01-15/2018-01-21  116  297   96  127  501  210  486  40  150  220  150   \n",
       "2018-01-22/2018-01-28  120  183  122   72  310  159  250  23   54  235  163   \n",
       "2018-01-29/2018-02-04  182  269  131   91  547  169  330  34   58  155  244   \n",
       "2018-02-05/2018-02-11  153  189   69   83  401  157  268  26   73  164  208   \n",
       "\n",
       "Region                            KQ                                         \\\n",
       "Event                  18   19 20 01 02 03 04 05 06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14  56  333  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  77  374  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  70  294  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  71  315  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  75  246  3  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                   KR                                  \\\n",
       "Event                 15 16 17 18 19 20  01  02  03   04  05  06  07 08  09   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0   5   3   2   10   2   1   0  0   0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0   3  13   2    7   0   0   0  0   1   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  24   7  10   78  12  21  20  3  50   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  58  31  10  207  11   4  32  5  73   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  24   2   1   60   9   1   3  5   6   \n",
       "\n",
       "Region                                                      KS             \\\n",
       "Event                 10  11 12 13 14 15 16  17 18 19 20    01   02    03   \n",
       "2018-01-08/2018-01-14  0   0  0  0  0  0  0   0  0  3  0  1094  502   911   \n",
       "2018-01-15/2018-01-21  0   6  0  0  0  0  0   2  2  0  0   729  438   557   \n",
       "2018-01-22/2018-01-28  1   4  5  0  0  1  0   3  5  1  0   804  414   501   \n",
       "2018-01-29/2018-02-04  2  10  0  1  0  0  6   4  2  8  0   794  364   538   \n",
       "2018-02-05/2018-02-11  2   8  2  2  0  0  1  10  0  1  0  1017  507  1031   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                    04   05   06   07   08   09   10   11   12   13  14   \n",
       "2018-01-08/2018-01-14  2452  755  186  230  203  146  102  344  156  150  35   \n",
       "2018-01-15/2018-01-21  1631  399  187  181  180  101   55  327  127  160  18   \n",
       "2018-01-22/2018-01-28  1380  356  227  194  133  163   56  451  126  132  28   \n",
       "2018-01-29/2018-02-04  1413  471  207  200  164  111   79  339  100  131  18   \n",
       "2018-02-05/2018-02-11  2539  613  193  231  440  128   74  445  188  149  48   \n",
       "\n",
       "Region                                           KT                           \\\n",
       "Event                  15   16   17   18   19 20 01 02 03  04 05 06 07 08 09   \n",
       "2018-01-08/2018-01-14  85   86  277   40  240  1  1  0  0   9  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  42  115  343   50  258  0  4  1  2   4  2  0  0  3  0   \n",
       "2018-01-22/2018-01-28  24  107  270  138  332  2  6  0  0  12  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  53   78  201   58  229  3  0  0  0   0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  59  132  422   98  255  1  0  0  0  11  0  0  0  0  0   \n",
       "\n",
       "Region                                                   KU                 \\\n",
       "Event                 10 11 12 13 14 15 16 17 18 19 20   01   02   03   04   \n",
       "2018-01-08/2018-01-14  0  0  1  0  0  0  2  0  0  0  0  152   82   87  321   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  1  0  0  207   85  136  417   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  2  0  0  0  246  125  136  612   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  243   92  137  399   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  259  166  241  523   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   05  06  07  08  09  10   11  12  13  14  15  16   17   \n",
       "2018-01-08/2018-01-14  112  25  31  45  20  11   59  24  14  16   0   7   77   \n",
       "2018-01-15/2018-01-21  174  60  44  43  34  26   77  26  10   9  10  11   98   \n",
       "2018-01-22/2018-01-28  177  22  61  60  27  39   70  26  20  31  10   9  145   \n",
       "2018-01-29/2018-02-04  152  32  51  51  12  12   43  42   4  10   4  11   90   \n",
       "2018-02-05/2018-02-11  136  47  87  49  48  51  116  30  12   3   7  43  155   \n",
       "\n",
       "Region                             KV                                          \\\n",
       "Event                  18  19 20   01  02  03   04  05 06  07  08  09  10  11   \n",
       "2018-01-08/2018-01-14   6  39  0   54  52  59   73  43  6   9  12  13  21  38   \n",
       "2018-01-15/2018-01-21  12  25  0  193  94  72  220  89  9  19  23  28  38  93   \n",
       "2018-01-22/2018-01-28  19  28  0   53  21  42  123  28  3   4   4   3   8  15   \n",
       "2018-01-29/2018-02-04  16  31  0   55  11  54   86  32  3   8  12   2  10  19   \n",
       "2018-02-05/2018-02-11  24  42  1   60  19  53  175  48  0   4   7   4  22  18   \n",
       "\n",
       "Region                                                    KZ                 \\\n",
       "Event                  12  13 14 15  16  17  18   19 20   01   02   03   04   \n",
       "2018-01-08/2018-01-14  20  12  7  0   9  23  16   13  0  156   45  115  394   \n",
       "2018-01-15/2018-01-21  49  33  4  1  24  99  36  258  5  284  108  230  911   \n",
       "2018-01-22/2018-01-28  18  12  0  3   6   9   0   23  0  150   64   76  410   \n",
       "2018-01-29/2018-02-04  14  14  1  0   1  14   1   26  0  123   53   72  557   \n",
       "2018-02-05/2018-02-11  27   1  7  2  11  16   3   20  1   74   32   77  322   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   05  06  07  08  09  10  11  12  13  14 15  16  17 18   \n",
       "2018-01-08/2018-01-14  131  37  51  30  17   6  56  20   8   5  1   3  14  2   \n",
       "2018-01-15/2018-01-21  187  73  71  60  17  19  64  26  28   8  3  19  58  2   \n",
       "2018-01-22/2018-01-28   98  39  44  38  15   7  27  23   4  10  0   5  26  5   \n",
       "2018-01-29/2018-02-04  135  60  21  11  10   4  33  18   1   1  1   7  28  2   \n",
       "2018-02-05/2018-02-11   84  29  21  21   5  11  33   6   5   2  3   5  20  2   \n",
       "\n",
       "Region                        LA                                               \\\n",
       "Event                  19 20  01  02  03   04  05  06  07  08  09  10  11  12   \n",
       "2018-01-08/2018-01-14  25  0  60  22  54  230  68  11   9   9   2   1  13  14   \n",
       "2018-01-15/2018-01-21  58  0  58  29  70  266  70  29  28  20   6   2  25   8   \n",
       "2018-01-22/2018-01-28  43  0  71  30  60  414  86  29  29   6   5   5  22   5   \n",
       "2018-01-29/2018-02-04  40  0  60  33  32  200  54  24  11  15  12  11  13   5   \n",
       "2018-02-05/2018-02-11  20  0  64  21  36  267  71  23  10  10  16   1  12   5   \n",
       "\n",
       "Region                                               LE                       \\\n",
       "Event                  13 14 15  16  17  18  19 20   01   02   03    04   05   \n",
       "2018-01-08/2018-01-14   0  0  2   1  24   1  12  0  371  143  128   766  203   \n",
       "2018-01-15/2018-01-21  10  1  3   1  40   2  14  0  514  210  195   807  306   \n",
       "2018-01-22/2018-01-28   3  0  1   0  22  17  18  0  514  249  141  1187  295   \n",
       "2018-01-29/2018-02-04   8  0  0  12  17   0  21  1  531  247  202  1125  388   \n",
       "2018-02-05/2018-02-11   3  0  0   2   9  15  14  0  595  235  279  1473  277   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                   06   07   08   09  10   11   12   13  14  15   16   \n",
       "2018-01-08/2018-01-14   71  103  110   58  43  161   69   33  55  11   46   \n",
       "2018-01-15/2018-01-21  110  164  185   89  39  287  107   58  18  10  148   \n",
       "2018-01-22/2018-01-28  110  147   98  106  39  242   46   91  61   3   42   \n",
       "2018-01-29/2018-02-04  110  174  130   67  37  246   98  142  37  25   36   \n",
       "2018-02-05/2018-02-11   96  130  146  101  27  342  129  153  40  27   20   \n",
       "\n",
       "Region                                   LG                                    \\\n",
       "Event                   17  18   19 20   01  02   03   04  05  06  07  08  09   \n",
       "2018-01-08/2018-01-14  137  71  253  3  123  25   66  160  35  17  27  61  11   \n",
       "2018-01-15/2018-01-21  492  66  330  3   80  45   79  188  75  31  27  29   6   \n",
       "2018-01-22/2018-01-28  282  76  310  7   65  32   40  135  64  12  17   8   8   \n",
       "2018-01-29/2018-02-04  136  52  275  0   41  14  119  144  29  16   6   6   6   \n",
       "2018-02-05/2018-02-11  174  57  477  1   78  30   63  252  59  10  24  23   8   \n",
       "\n",
       "Region                                                        LH           \\\n",
       "Event                 10  11  12 13 14 15  16  17 18  19 20   01  02   03   \n",
       "2018-01-08/2018-01-14  5  32  10  2  2  2  11  36  1  10  0   81  43  124   \n",
       "2018-01-15/2018-01-21  2  19   7  4  3  4   1   7  5  11  0  117  44   74   \n",
       "2018-01-22/2018-01-28  2   9   8  3  0  3   1  21  0  17  0   73  31   33   \n",
       "2018-01-29/2018-02-04  6  14  22  3  3  4   0  18  4  11  0   77  50   81   \n",
       "2018-02-05/2018-02-11  3  26  26  0  2  4   1  21  3  10  0   94  41   21   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   04   05  06  07  08  09  10  11  12  13 14 15  16  17   \n",
       "2018-01-08/2018-01-14  305  125  29  43  10  11   4  16  26   3  7  5   2  29   \n",
       "2018-01-15/2018-01-21  258  165  46  21  18  17   4  48  12  10  2  6  14  19   \n",
       "2018-01-22/2018-01-28  141   64  21  25  15   2   3  18   5   6  5  2   1   9   \n",
       "2018-01-29/2018-02-04  241   71  30  24  17  11   6  51  10   6  6  5   5  48   \n",
       "2018-02-05/2018-02-11  149   53   9   9   8   4  23  36  14   1  1  7   0  17   \n",
       "\n",
       "Region                            LI                                          \\\n",
       "Event                 18  19 20   01   02   03   04   05  06  07  08  09  10   \n",
       "2018-01-08/2018-01-14  0  16  0  207   78   92  209  195  54  44  44   5  11   \n",
       "2018-01-15/2018-01-21  1  26  0  264  114  100  381  254  58  34  34  11  46   \n",
       "2018-01-22/2018-01-28  8   8  0  249  112  126  338  158  64  28  32  17  43   \n",
       "2018-01-29/2018-02-04  4  29  0  252  117   77  370  150  47  40  52  21  26   \n",
       "2018-02-05/2018-02-11  1  26  0  197   86   91  374  200  47  37  28  15  17   \n",
       "\n",
       "Region                                                        LO               \\\n",
       "Event                   11  12  13  14 15  16  17  18  19 20  01  02  03   04   \n",
       "2018-01-08/2018-01-14  107  30  19   5  1   6  54  15  11  0  58  15  31   91   \n",
       "2018-01-15/2018-01-21   94  19  17  11  0  11  52   9  33  0  42  16  27   75   \n",
       "2018-01-22/2018-01-28   62  26  14  15  3   9  44  12  25  0  76  19  26  123   \n",
       "2018-01-29/2018-02-04   90  34  29  16  0   1  60  26  21  0  62  20  19  116   \n",
       "2018-02-05/2018-02-11   42  37  14  16  2   4  52  19  11  0  35  14  16   69   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  05  06  07  08 09 10  11  12 13  14 15 16  17 18  19   \n",
       "2018-01-08/2018-01-14  43  10   3  10  0  3  20  10  1  12  0  0  22  1   1   \n",
       "2018-01-15/2018-01-21  31   6   5  13  6  3  11   7  1   0  2  1   6  2   5   \n",
       "2018-01-22/2018-01-28  51   5  17  13  3  0  24  22  0   2  0  3  14  0  17   \n",
       "2018-01-29/2018-02-04  51   8   9  17  4  1  11  12  0   0  1  4   9  1  18   \n",
       "2018-02-05/2018-02-11  35   5   4  10  1  2   5   5  1   6  0  1  14  6  17   \n",
       "\n",
       "Region                   LQ                                                  \\\n",
       "Event                 20 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  1  0  0  1  0  0  0  0  0   \n",
       "\n",
       "Region                         LS                                            \\\n",
       "Event                 18 19 20 01 02 03  04  05  06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14  0  0  0  4  1  1  12   5   0  2  1  0  1  0  2  0  0   \n",
       "2018-01-15/2018-01-21  0  0  0  4  1  4  11   6   0  0  0  1  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  5  0  2   2   1   0  1  1  3  0  2  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  1  3  0  12  13   0  6  1  3  0  7  1  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  5  0  0  17   8  13  4  2  0  0  3  1  0  0   \n",
       "\n",
       "Region                                   LT                                   \\\n",
       "Event                 15 16 17 18 19 20  01  02  03   04  05  06  07  08  09   \n",
       "2018-01-08/2018-01-14  0  0  0  0  1  0  43  19  10   71  10   6  13  13   1   \n",
       "2018-01-15/2018-01-21  0  0  2  0  0  0  49  20   9   59  34   6   5  22   3   \n",
       "2018-01-22/2018-01-28  0  0  0  0  1  0  65  19  13   87  22  13  10   5   9   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  72  43   6   77  35  13   9   5  17   \n",
       "2018-02-05/2018-02-11  0  0  2  0  2  0  49  33  25  122  44   8  16  12   4   \n",
       "\n",
       "Region                                                      LU               \\\n",
       "Event                 10  11  12 13 14 15 16  17 18  19 20  01  02  03   04   \n",
       "2018-01-08/2018-01-14  2  14   1  2  0  0  0   9  5  28  0  70  35  45  101   \n",
       "2018-01-15/2018-01-21  1  26   2  2  1  1  0  16  0  27  0  49  52  57   99   \n",
       "2018-01-22/2018-01-28  1  12   6  4  0  3  2   7  6  38  0  48  25  68  148   \n",
       "2018-01-29/2018-02-04  7  25   6  6  0  7  3  14  4  19  0  54  46  37  113   \n",
       "2018-02-05/2018-02-11  3  14  16  1  2  3  4  18  6  18  0  56  47  35  105   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  05  06  07  08  09 10  11  12 13 14 15 16  17 18  19   \n",
       "2018-01-08/2018-01-14  25  16  40  23  23  5  16  24  3  4  0  4  16  4  17   \n",
       "2018-01-15/2018-01-21  44  18  46  24  13  3  13  11  2  0  3  2  16  0   7   \n",
       "2018-01-22/2018-01-28  40   8  40  19   9  0  13   9  4  5  2  4  18  0   4   \n",
       "2018-01-29/2018-02-04  34   6  43  14  15  0  20  16  1  4  0  0  11  0   7   \n",
       "2018-02-05/2018-02-11  37  10  23   7  13  2  22  10  1  7  1  1   7  1  10   \n",
       "\n",
       "Region                     LY                                                 \\\n",
       "Event                 20   01   02   03   04   05  06   07   08  09  10   11   \n",
       "2018-01-08/2018-01-14  0  312  101  118  767  193  61  146   88  39  36  122   \n",
       "2018-01-15/2018-01-21  0  246   81   80  373  129  48   63   66  33  29  134   \n",
       "2018-01-22/2018-01-28  0  240  116   83  422  134  41   98   62  63  47  217   \n",
       "2018-01-29/2018-02-04  0  260  131   79  616  128  53  138  117  48  17  113   \n",
       "2018-02-05/2018-02-11  0  246  106   65  543  146  57  101   94  39  34  130   \n",
       "\n",
       "Region                                                       MA              \\\n",
       "Event                  12  13  14  15  16   17   18   19 20  01  02  03  04   \n",
       "2018-01-08/2018-01-14  77  54  28   3  41  180   38  118  2  70  16  15  91   \n",
       "2018-01-15/2018-01-21  33  26  15   3  38  120   67  262  0  24  15   3  74   \n",
       "2018-01-22/2018-01-28  21  29  18  10  18  101  147  307  4  42  19  12  57   \n",
       "2018-01-29/2018-02-04  39  32  22   6  25   72   47  149  0  32  12   6  47   \n",
       "2018-02-05/2018-02-11  32  29  12  11  34  109   55  212  0  42  18  11  30   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  05  06  07  08  09 10 11 12  13 14 15 16  17 18  19 20   \n",
       "2018-01-08/2018-01-14  21   1   4   7   9  0  0  5  13  2  0  0   6  0   7  0   \n",
       "2018-01-15/2018-01-21  11  12   2   5  12  0  9  4   5  2  1  2   2  2   4  0   \n",
       "2018-01-22/2018-01-28   7   7   4   6  11  4  7  2   7  3  0  0  12  1   7  0   \n",
       "2018-01-29/2018-02-04  20   6   7   4   5  2  6  5   2  0  0  3   8  0   2  0   \n",
       "2018-02-05/2018-02-11  10   9  14  12   4  3  4  1   1  2  0  0   5  0  12  0   \n",
       "\n",
       "Region                MB                                                      \\\n",
       "Event                 01 02 03  04 05 06 07 08 09 10 11 12 13 14 15 16 17 18   \n",
       "2018-01-08/2018-01-14  1  4  1   0  4  0  2  0  1  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  1  1  1  12  6  3  0  1  2  0  0  0  0  0  0  0  1  0   \n",
       "2018-01-22/2018-01-28  0  2  1   2  1  3  0  0  0  0  0  0  0  0  0  0  1  0   \n",
       "2018-01-29/2018-02-04  0  0  2   3  1  0  1  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0   2  0  0  2  0  0  0  0  1  0  0  0  0  2  0   \n",
       "\n",
       "Region                       MC                                                \\\n",
       "Event                 19 20  01  02  03  04  05 06 07  08  09 10  11 12 13 14   \n",
       "2018-01-08/2018-01-14  0  0  20   9  22  21   5  3  2   2   2  5  16  1  0  0   \n",
       "2018-01-15/2018-01-21  0  0  27  10   1  52   7  5  4   3   5  2  10  5  2  0   \n",
       "2018-01-22/2018-01-28  2  0  15  16   8  46  29  1  4   3   6  0  23  1  2  0   \n",
       "2018-01-29/2018-02-04  0  0  44  20  14  66   7  7  5  12  23  3  19  6  1  0   \n",
       "2018-02-05/2018-02-11  2  0  16  18  12  47   9  4  7  16   5  5  29  4  0  0   \n",
       "\n",
       "Region                                    MD                                  \\\n",
       "Event                 15 16  17 18 19 20  01  02  03   04  05  06  07  08 09   \n",
       "2018-01-08/2018-01-14  0  0   7  0  4  0  37  21  18   64  28  11  14  11  3   \n",
       "2018-01-15/2018-01-21  0  0  11  0  5  0  25  15   8   49  28   9   8   3  6   \n",
       "2018-01-22/2018-01-28  0  1  14  0  7  0  21  24  16  125  16  10   7  10  2   \n",
       "2018-01-29/2018-02-04  0  2   4  1  5  0  17  17  19   45  12  10   5   9  1   \n",
       "2018-02-05/2018-02-11  0  0   3  2  6  0  30  21  25   57  82  13  15   6  8   \n",
       "\n",
       "Region                                                      MF                 \\\n",
       "Event                 10  11  12 13 14 15  16  17 18  19 20 01 02 03 04 05 06   \n",
       "2018-01-08/2018-01-14  8  16   8  2  1  0   1  32  3  17  0  1  0  0  0  1  0   \n",
       "2018-01-15/2018-01-21  5  12   5  0  2  0   0  15  2   5  0  0  0  0  1  0  0   \n",
       "2018-01-22/2018-01-28  9  29   9  4  7  1  11  17  1   5  0  0  0  0  1  0  0   \n",
       "2018-01-29/2018-02-04  3  10   1  1  1  1   2   4  0   2  0  0  0  0  2  0  0   \n",
       "2018-02-05/2018-02-11  7  26  14  5  1  6   0  31  1   3  0  2  0  0  5  0  0   \n",
       "\n",
       "Region                                                           MG          \\\n",
       "Event                 07 08 09 10 11 12 13 14 15 16 17 18 19 20  01  02  03   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  30  26  21   \n",
       "2018-01-15/2018-01-21  0  1  0  0  0  2  0  0  0  0  0  0  0  0  28  16  34   \n",
       "2018-01-22/2018-01-28  1  0  0  0  0  0  0  0  0  0  0  0  0  0  58  26  16   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  7  0  0  0  52  19  18   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  1  0  10  22  11   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   04  05  06  07  08 09 10  11  12 13 14 15 16  17 18   \n",
       "2018-01-08/2018-01-14  127  25  11  13   3  8  0   9  10  4  3  1  1   7  2   \n",
       "2018-01-15/2018-01-21  168  46   4  17  10  6  3   6   5  4  0  0  0   7  0   \n",
       "2018-01-22/2018-01-28  216  60  16  14   5  2  1  13   1  3  1  2  0  13  0   \n",
       "2018-01-29/2018-02-04   63  36  15  21   6  2  2  17   0  0  0  0  0   7  3   \n",
       "2018-02-05/2018-02-11   75  27   3  12   2  3  2  10   0  0  1  0  0   2  3   \n",
       "\n",
       "Region                       MH                                                \\\n",
       "Event                  19 20 01 02 03  04 05 06 07 08 09 10 11 12 13 14 15 16   \n",
       "2018-01-08/2018-01-14   9  0  1  1  1   6  1  0  0  0  0  0  8  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  12  0  0  0  2   0  4  0  0  0  0  0  1  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28   3  0  1  3  6   8  0  0  0  0  0  0  1  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04   5  0  3  0  5   6  0  0  3  0  0  0  4  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11   1  0  7  3  9  12  5  0  3  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                              MI                                         \\\n",
       "Event                 17 18 19 20   01   02  03   04   05  06  07  08  09  10   \n",
       "2018-01-08/2018-01-14  0  0  0  0  277  101  39  269  158  20  67  55  18  15   \n",
       "2018-01-15/2018-01-21  0  0  0  0  182   90  75  207  103  22  36  28  13  23   \n",
       "2018-01-22/2018-01-28  0  0  0  0  265  125  74  282   71  30  53  37  18  11   \n",
       "2018-01-29/2018-02-04  0  0  0  0  203  123  96  347  112  19  56  34   9  16   \n",
       "2018-02-05/2018-02-11  0  0  0  0  259  117  69  218  156  30  95  45  19  15   \n",
       "\n",
       "Region                                                        MJ              \\\n",
       "Event                   11  12  13  14 15 16   17  18  19 20  01  02  03  04   \n",
       "2018-01-08/2018-01-14  116  21  10  13  4  9  139  29  30  0  16   8   6  19   \n",
       "2018-01-15/2018-01-21   91  26  12   1  0  6  106   7  23  0  22  11   7  41   \n",
       "2018-01-22/2018-01-28   82  18  11  15  1  2  115  21  33  0  17   0  12  29   \n",
       "2018-01-29/2018-02-04   87  32  27   1  4  7  126  15  43  0  29   5   6  42   \n",
       "2018-02-05/2018-02-11   62  20  12   2  0  8   94  12  29  0  35   2  14  44   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                  05 06  07 08 09 10  11 12 13 14 15 16 17 18  19 20   \n",
       "2018-01-08/2018-01-14   8  3  11  2  0  0   4  2  3  1  1  0  3  0   2  0   \n",
       "2018-01-15/2018-01-21  27  4   4  0  2  1  20  5  2  2  1  4  4  0  31  0   \n",
       "2018-01-22/2018-01-28  12  5   3  4  1  3   4  0  1  2  0  1  3  2   3  0   \n",
       "2018-01-29/2018-02-04  27  5   7  5  3  2   8  0  1  2  0  1  0  0   1  0   \n",
       "2018-02-05/2018-02-11  18  0   0  5  2  6  15  1  7  0  0  0  2  1  13  0   \n",
       "\n",
       "Region                  MK                                                    \\\n",
       "Event                   01  02   03   04   05  06  07  08 09  10  11  12  13   \n",
       "2018-01-08/2018-01-14   95  38   87  147   48   2  15  35  1  16  60  30  12   \n",
       "2018-01-15/2018-01-21  187  92  146  281  172   9  16  16  6  26  48  82  16   \n",
       "2018-01-22/2018-01-28  124  81  113  430   65  16  10  43  6  22  70  84  17   \n",
       "2018-01-29/2018-02-04  159  67  108  242   76   9  11  15  8  18  39  83  26   \n",
       "2018-02-05/2018-02-11  100  66  105  173   86   7  18   9  8  13  44  54  29   \n",
       "\n",
       "Region                                             ML                       \\\n",
       "Event                  14  15  16  17  18  19 20   01  02  03   04  05  06   \n",
       "2018-01-08/2018-01-14   4   4  11   9   2  13  0   49  14   7   64  23   4   \n",
       "2018-01-15/2018-01-21  33   0   9  34   7  16  0   37  47  44   83  31  12   \n",
       "2018-01-22/2018-01-28  50   1  22  26   3  23  0  174  26  55  103  96  39   \n",
       "2018-01-29/2018-02-04  44   3   6  15  11  18  0   73  19  44   90  63  15   \n",
       "2018-02-05/2018-02-11  60  10   9  20   8  14  0   37  14  29   75  34  14   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                  07  08  09 10  11  12  13 14  15 16  17  18   19 20   \n",
       "2018-01-08/2018-01-14   9  21   3  4  15   9  20  0   3  7  18  10   43  0   \n",
       "2018-01-15/2018-01-21  17  17  11  4   8   4   8  0  13  0   4   9   23  0   \n",
       "2018-01-22/2018-01-28  23  27  11  3  68  12  10  2  40  2  49  19  230  0   \n",
       "2018-01-29/2018-02-04  17  20  10  3   7   3   9  0   7  0  27  14   64  0   \n",
       "2018-02-05/2018-02-11   7   7  16  4  17   8   4  0   1  1  11  14   47  0   \n",
       "\n",
       "Region                 MN                                                      \\\n",
       "Event                  01  02  03  04  05  06  07  08  09 10  11  12 13 14 15   \n",
       "2018-01-08/2018-01-14  31  18  23  84  18   1   7  12  13  0  18  12  1  0  1   \n",
       "2018-01-15/2018-01-21  14  14  17  54  24   1   8   2   1  1  11   5  0  0  0   \n",
       "2018-01-22/2018-01-28  30  17  25  72  22   1   6   8   0  0   6   1  3  1  0   \n",
       "2018-01-29/2018-02-04  36  23  19  99  42   0   2   5   1  1   9   5  3  0  1   \n",
       "2018-02-05/2018-02-11  30   8  18  66  13  10  10   5   0  3  12  14  0  0  0   \n",
       "\n",
       "Region                                  MO                                     \\\n",
       "Event                 16 17 18  19 20   01  02   03   04   05  06  07  08  09   \n",
       "2018-01-08/2018-01-14  0  6  1  15  0  174  64   99  431  157  27  37  44  15   \n",
       "2018-01-15/2018-01-21  0  4  2   6  0  101  76   54  507  126  35  31  36  19   \n",
       "2018-01-22/2018-01-28  1  4  2   6  0  131  88   57  493  163  54  27  33  22   \n",
       "2018-01-29/2018-02-04  0  1  1   8  0  149  91  107  351   96  37  36  16  30   \n",
       "2018-02-05/2018-02-11  4  2  0   9  0  127  63   84  347  124  37  27  26  18   \n",
       "\n",
       "Region                                                           MP          \\\n",
       "Event                  10  11  12  13  14 15  16  17  18  19 20  01  02  03   \n",
       "2018-01-08/2018-01-14   4  53  25  13  10  0  10  35  20  27  0  25  12  15   \n",
       "2018-01-15/2018-01-21   4  51  20  16  18  7   8  55  10  67  0  48  23  21   \n",
       "2018-01-22/2018-01-28  10  65  16   4   3  0  12  39  23  68  0  28  15  27   \n",
       "2018-01-29/2018-02-04  12  46  24  12  20  6   2  76  11  99  0  42  17  10   \n",
       "2018-02-05/2018-02-11  10  89  32  11  19  7   2  37  21  85  0  38  21  35   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   04  05  06  07  08  09 10  11  12 13  14 15 16 17 18   \n",
       "2018-01-08/2018-01-14   71  13  13  11   6   3  2  12   3  1   0  0  1  3  0   \n",
       "2018-01-15/2018-01-21   64  17   6   8  12  20  0  13   3  2   1  0  2  3  0   \n",
       "2018-01-22/2018-01-28  155  26   5  11  14   0  6   9   6  1   2  1  1  8  1   \n",
       "2018-01-29/2018-02-04   27  42   5  10   5   2  0  21  10  1   0  3  4  5  3   \n",
       "2018-02-05/2018-02-11   66  33  21  14  14   6  3  23   8  2  14  0  3  2  0   \n",
       "\n",
       "Region                       MQ                                               \\\n",
       "Event                  19 20 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16   \n",
       "2018-01-08/2018-01-14  11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  16  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28   2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04   3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11   3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                             MR                                        \\\n",
       "Event                 17 18 19 20  01  02  03   04  05  06 07  08 09 10  11   \n",
       "2018-01-08/2018-01-14  0  0  0  0  20  10   5   31  34   1  2   0  2  5   7   \n",
       "2018-01-15/2018-01-21  0  0  0  0  17   8   9  140  16   5  5   2  2  0  11   \n",
       "2018-01-22/2018-01-28  0  0  0  0  28  10   6   51  77  12  6   0  3  5   5   \n",
       "2018-01-29/2018-02-04  0  0  0  0  13  11  12   53  60   1  6  27  0  5   9   \n",
       "2018-02-05/2018-02-11  0  0  0  0  23  11  24   92  26   7  1   7  0  7  14   \n",
       "\n",
       "Region                                               MT                      \\\n",
       "Event                 12 13 14 15 16  17 18  19 20   01   02   03   04   05   \n",
       "2018-01-08/2018-01-14  4  2  1  0  4   4  2  28  0  222  112   81  360  112   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0   6  1  18  1  248  131   94  484   87   \n",
       "2018-01-22/2018-01-28  0  1  0  0  0   9  1  12  0  192  141  121  290  145   \n",
       "2018-01-29/2018-02-04  0  0  8  5  0  25  1  15  0  259  145  108  465  129   \n",
       "2018-02-05/2018-02-11  1  0  3  0  0   5  2   8  0  279  128   93  371  135   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  06  07  08  09  10   11  12  13  14 15  16  17  18  19   \n",
       "2018-01-08/2018-01-14  32  39  36  30  23   77  71  21   5  2  22  48  17  69   \n",
       "2018-01-15/2018-01-21  30  35  48  20  21  114  49  17   2  2  18  61  27  67   \n",
       "2018-01-22/2018-01-28  25  39  56  28  27   93  30   7   8  2   6  41  17  41   \n",
       "2018-01-29/2018-02-04  34  47  33  60  30   75  48  27   5  0  16  77  17  41   \n",
       "2018-02-05/2018-02-11  49  42  53  36  29   90  53  18  21  2  11  96   8  46   \n",
       "\n",
       "Region                     MU                                                  \\\n",
       "Event                 20   01  02   03    04   05  06  07  08  09  10  11  12   \n",
       "2018-01-08/2018-01-14  0   81  47   94   274  170  16  39  16  11   2  19  13   \n",
       "2018-01-15/2018-01-21  0  108  70  151   419  119  42  50  18  14   8  34  13   \n",
       "2018-01-22/2018-01-28  0  126  62  256   541  118  36  37  15   9   8  23  17   \n",
       "2018-01-29/2018-02-04  0  121  57  175   303  185  45  47   9  17  15  27  12   \n",
       "2018-02-05/2018-02-11  0  142  63  561  1033  243  33  64  26  10   9  49  18   \n",
       "\n",
       "Region                                              MV                       \\\n",
       "Event                  13 14 15 16  17  18  19 20   01   02   03    04   05   \n",
       "2018-01-08/2018-01-14   6  2  2  3  17   2  27  0   66   40  131   221   69   \n",
       "2018-01-15/2018-01-21  14  3  8  0  38  14  15  0   36   14   33   149   25   \n",
       "2018-01-22/2018-01-28   8  9  9  7  36   2  20  0   59   23   22   135   41   \n",
       "2018-01-29/2018-02-04   2  6  2  8  42   6  28  0  203  155   93   228  156   \n",
       "2018-02-05/2018-02-11  12  2  9  6  23   1   8  0  567  481  204  1166  187   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                  06   07   08  09   10   11   12   13  14  15  16   17   \n",
       "2018-01-08/2018-01-14  27    8   11  13   10   42   12    6   1   6   0   18   \n",
       "2018-01-15/2018-01-21  22    1   11  31    3   24   14    8   2   0   4    7   \n",
       "2018-01-22/2018-01-28  15   11   17  15    4   48    9   11   6   3   3   41   \n",
       "2018-01-29/2018-02-04  19   17  241  34   61  122   64   51  59   6  26  157   \n",
       "2018-02-05/2018-02-11  72  129  355  60  192  286  166  118  52  25  74  616   \n",
       "\n",
       "Region                               MX                                      \\\n",
       "Event                  18   19 20    01   02   03    04   05   06   07   08   \n",
       "2018-01-08/2018-01-14   4   11  0  1111  529  618  2071  516  245  296  366   \n",
       "2018-01-15/2018-01-21   3   18  0  1247  581  549  1952  634  313  324  244   \n",
       "2018-01-22/2018-01-28   4   21  0  1036  573  771  2231  569  311  305  354   \n",
       "2018-01-29/2018-02-04   4  115  0  1100  612  884  2591  571  221  232  274   \n",
       "2018-02-05/2018-02-11  30  115  0  1008  503  460  1854  423  257  256  219   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   09   10   11   12   13  14  15   16   17   18   19 20   \n",
       "2018-01-08/2018-01-14  144   98  467  206  271  27  13  152  493  114  634  3   \n",
       "2018-01-15/2018-01-21  114   79  577  259  156  65  22   80  433  106  537  1   \n",
       "2018-01-22/2018-01-28  162   69  553  265  128  44  17   87  460   87  533  0   \n",
       "2018-01-29/2018-02-04  137  105  496  195  108  45  30   78  429  107  443  1   \n",
       "2018-02-05/2018-02-11  127   56  485  212  136  23  66   77  412  124  549  0   \n",
       "\n",
       "Region                   MY                                                \\\n",
       "Event                    01   02   03    04   05   06   07   08   09   10   \n",
       "2018-01-08/2018-01-14  1084  486  650  1608  639  245  178  324  235  127   \n",
       "2018-01-15/2018-01-21  1173  553  529  1581  669  249  250  198  128  132   \n",
       "2018-01-22/2018-01-28  1120  511  430  1606  478  215  214  197  197  128   \n",
       "2018-01-29/2018-02-04  1001  550  376  1568  409  175  176  185  120  166   \n",
       "2018-02-05/2018-02-11  1092  524  556  1328  521  213  245  200  194  130   \n",
       "\n",
       "Region                                                            MZ           \\\n",
       "Event                   11   12  13  14  15  16   17  18   19 20  01  02   03   \n",
       "2018-01-08/2018-01-14  477  222  78  20  22  98  525  53  310  1  55  23   52   \n",
       "2018-01-15/2018-01-21  486  163  89  66  10  52  377  70  269  0  55  45  105   \n",
       "2018-01-22/2018-01-28  568  172  86  29  11  37  355  76  195  5  71  35   43   \n",
       "2018-01-29/2018-02-04  478  201  69  36  13  24  426  95  200  3  63  24   39   \n",
       "2018-02-05/2018-02-11  430  165  75  17   8  26  472  34  149  7  93  34   39   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   04   05  06  07  08  09 10  11  12 13 14 15 16  17   \n",
       "2018-01-08/2018-01-14  152   70  38  26  13   4  1  16  20  5  5  0  0  25   \n",
       "2018-01-15/2018-01-21  188  139  34  21  14   9  6  28   9  9  4  2  1  20   \n",
       "2018-01-22/2018-01-28   93   93   7  28   6  13  4  16   6  6  4  0  5  46   \n",
       "2018-01-29/2018-02-04   93   47  15  14  21  16  3  17   4  4  3  0  3  21   \n",
       "2018-02-05/2018-02-11  112   52  27  36  11  12  2  20   9  4  1  3  3  20   \n",
       "\n",
       "Region                           NC                                            \\\n",
       "Event                  18  19 20 01  02  03  04 05 06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14   6  38  1  3   2  10   2  0  0  1  0  0  0  0  2  0  0   \n",
       "2018-01-15/2018-01-21  19  54  0  3   1   7  10  0  0  2  0  0  0  2  3  0  0   \n",
       "2018-01-22/2018-01-28  11  22  0  6   2   0  28  1  0  0  5  0  0  0  1  0  0   \n",
       "2018-01-29/2018-02-04   1  17  0  4  10   3  11  5  4  1  0  0  1  0  2  0  0   \n",
       "2018-02-05/2018-02-11   3  12  0  2   0   4  23  0  1  0  1  0  0  0  0  0  0   \n",
       "\n",
       "Region                                  NE                                   \\\n",
       "Event                 15 16 17 18 19 20 01 02 03 04 05 06 07 08 09 10 11 12   \n",
       "2018-01-08/2018-01-14  0  0  4  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0   \n",
       "2018-01-15/2018-01-21  0  0  1  0  0  0  0  2  1  3  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  3  2  2  0  0  0  0  2  0  0  0  1  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  1  0  4  0  3  3  0  8  4  0  0  0  0  0  2  0   \n",
       "2018-02-05/2018-02-11  0  1  0  0  2  0  3  2  2  3  0  0  2  0  1  0  0  0   \n",
       "\n",
       "Region                                        NF                              \\\n",
       "Event                 13 14 15 16 17 18 19 20 01 02 03  04 05 06 07 08 09 10   \n",
       "2018-01-08/2018-01-14  0  0  0  0  1  0  0  0  1  0  0  10  0  0  1  0  0  0   \n",
       "2018-01-15/2018-01-21  1  0  0  0  0  0  0  0  0  0  0   0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0   0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  2  0  0  0  0  0  0  0  0  0  2   0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0   0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                                NG                    \\\n",
       "Event                 11 12 13 14 15 16 17 18 19 20   01  02   03   04  05   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  120  69   45  144  40   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  135  20   18  198  42   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  116  25   30  221  68   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  143  80  143  278  92   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  138  88   41  185  53   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  06  07  08  09  10  11  12  13  14 15 16  17  18  19   \n",
       "2018-01-08/2018-01-14   4  18  22  24  10  56  19   4   6  7  3  27  17  52   \n",
       "2018-01-15/2018-01-21  34  27  18  11   5  32   3  12   2  9  3  49  33  92   \n",
       "2018-01-22/2018-01-28  13  24  32   8  10  36  21  30  10  5  6  42  22  59   \n",
       "2018-01-29/2018-02-04  32  23  24  12   5  31  20  16   1  5  4  27  27  78   \n",
       "2018-02-05/2018-02-11  12  26  15   6   5  39  19  14  15  2  1  17  12  73   \n",
       "\n",
       "Region                    NH                                                  \\\n",
       "Event                 20  01  02  03  04  05 06  07 08 09 10  11 12 13 14 15   \n",
       "2018-01-08/2018-01-14  0  18   8   7  19  21  8   8  1  1  0   2  1  0  0  0   \n",
       "2018-01-15/2018-01-21  2  23  19  15  71  15  5   8  5  1  0   6  2  0  0  2   \n",
       "2018-01-22/2018-01-28  1  31  13  11  33   9  4   6  0  3  0  12  3  2  0  0   \n",
       "2018-01-29/2018-02-04  0  31  14  12  52  32  0   5  1  1  2   5  1  2  2  0   \n",
       "2018-02-05/2018-02-11  0  12  15  12  19  30  3  13  2  0  1   5  1  0  1  4   \n",
       "\n",
       "Region                                  NI                                     \\\n",
       "Event                 16 17 18 19 20    01    02    03    04    05   06    07   \n",
       "2018-01-08/2018-01-14  0  3  0  1  0  5305  2773  1506  6327  2354  512   930   \n",
       "2018-01-15/2018-01-21  1  0  0  0  0  5410  2733  1398  6942  2622  440   912   \n",
       "2018-01-22/2018-01-28  1  6  1  2  0  5697  3080  1405  7015  2617  508   982   \n",
       "2018-01-29/2018-02-04  0  8  0  0  0  5770  2917  1524  7037  2683  539  1039   \n",
       "2018-02-05/2018-02-11  3  3  0  2  0  6554  3241  1761  7673  3399  567  1180   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                    08   09   10    11    12   13   14   15   16    17   \n",
       "2018-01-08/2018-01-14  1200  470  585  2695   895  830  375  313  169  1703   \n",
       "2018-01-15/2018-01-21  1288  390  706  2693  1027  696  246  171  169  1632   \n",
       "2018-01-22/2018-01-28   998  374  613  2739   903  687  331   91  177  1416   \n",
       "2018-01-29/2018-02-04   925  446  513  2734   966  616  181  158  174  1432   \n",
       "2018-02-05/2018-02-11  1254  514  773  2872   907  685  291  226  141  1736   \n",
       "\n",
       "Region                                 NL                                      \\\n",
       "Event                   18    19  20   01   02   03    04   05   06   07   08   \n",
       "2018-01-08/2018-01-14  578  2688  99  590  273  268   991  260  139  134  134   \n",
       "2018-01-15/2018-01-21  942  2441  77  533  273  272  1014  345   92  109  148   \n",
       "2018-01-22/2018-01-28  554  1858  64  557  257  298  1038  258  119  128  134   \n",
       "2018-01-29/2018-02-04  550  2159  12  601  288  320  1077  226  151  165  133   \n",
       "2018-02-05/2018-02-11  634  1903  35  547  284  334  1207  287   95  146  130   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                   09  10   11   12  13  14  15  16   17  18   19 20   \n",
       "2018-01-08/2018-01-14   60  52  235  119  29  10   7   9  159  31  137  1   \n",
       "2018-01-15/2018-01-21   84  45  202   66  52  29  10  83  121  38  166  1   \n",
       "2018-01-22/2018-01-28   83  32  185  104  37  20   3  19  192  46  170  0   \n",
       "2018-01-29/2018-02-04   90  31  230   96  39  37  12  20  168  26  180  0   \n",
       "2018-02-05/2018-02-11  105  58  212  112  30  33   3  95  203  39  222  0   \n",
       "\n",
       "Region                  NO                                                  \\\n",
       "Event                   01   02   03    04   05  06   07   08  09  10   11   \n",
       "2018-01-08/2018-01-14  636  386  273  1375  314  84  171   85  80  52  315   \n",
       "2018-01-15/2018-01-21  612  276  291   990  295  71  177  119  50  44  256   \n",
       "2018-01-22/2018-01-28  283  182  189   712  234  79  111   73  40  20  122   \n",
       "2018-01-29/2018-02-04  317  160  348   850  231  80   83   57  46  25   71   \n",
       "2018-02-05/2018-02-11  269  116  179   502  222  61   65   61  39  17   76   \n",
       "\n",
       "Region                                                        NP            \\\n",
       "Event                   12  13  14  15  16   17  18   19 20   01   02   03   \n",
       "2018-01-08/2018-01-14  137  46   8   8  24   81  12   90  5  335  142  120   \n",
       "2018-01-15/2018-01-21  121  38  16  13  34  137  26  110  5  278  147  176   \n",
       "2018-01-22/2018-01-28   95  46  11  14  16   88  14   60  0  255  132  141   \n",
       "2018-01-29/2018-02-04   58  11   7   6  20  128  23   43  0  274  201  280   \n",
       "2018-02-05/2018-02-11   43  20   7   6  15   49   2   64  0  284  165  221   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   04   05   06   07  08  09  10   11  12  13  14  15   \n",
       "2018-01-08/2018-01-14  597  178   39   71  84  36  71  115  39  11  47   4   \n",
       "2018-01-15/2018-01-21  576  185   46   36  69  45  33   87  37  22  26   9   \n",
       "2018-01-22/2018-01-28  542  242   42  109  87  40  28   77  17   9  12  14   \n",
       "2018-01-29/2018-02-04  881  193   56   51  62  45  23   96  51  29   9  19   \n",
       "2018-02-05/2018-02-11  810  204  137   79  69  37  28   82  51  13  20   4   \n",
       "\n",
       "Region                                     NR                                  \\\n",
       "Event                  16   17  18  19 20  01  02  03  04  05 06  07 08 09 10   \n",
       "2018-01-08/2018-01-14  19  150  32  73  0   8   5   1   9   1  0   1  2  0  0   \n",
       "2018-01-15/2018-01-21  13  131  31  92  0   4   4   1  14   0  0   0  0  0  2   \n",
       "2018-01-22/2018-01-28   5  125  20  91  0  18  11  19  46   3  0   4  1  1  0   \n",
       "2018-01-29/2018-02-04  27  131  30  56  0  28  15  14  80  17  1  11  4  0  3   \n",
       "2018-02-05/2018-02-11  14  111  33  74  0  15   5  17  35   8  5   3  4  1  0   \n",
       "\n",
       "Region                                              NS                       \\\n",
       "Event                 11 12 13 14 15 16 17 18 19 20 01 02  03  04  05 06 07   \n",
       "2018-01-08/2018-01-14  0  2  2  2  0  5  4  0  2  0  1  3   0  28   1  0  0   \n",
       "2018-01-15/2018-01-21  1  0  0  0  0  0  5  0  0  0  4  0   1  22   5  9  1   \n",
       "2018-01-22/2018-01-28  2  4  0  2  0  0  8  1  1  0  5  2   7   3  21  3  1   \n",
       "2018-01-29/2018-02-04  9  5  2  2  0  0  7  1  1  0  9  5   9   9   7  1  0   \n",
       "2018-02-05/2018-02-11  6  4  1  4  0  0  5  3  0  0  4  7  17   9   3  2  0   \n",
       "\n",
       "Region                                                       NT               \\\n",
       "Event                 08 09 10 11 12 13 14 15 16 17 18 19 20 01 02 03  04 05   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  1  0  1  3  0  0  0  0  0  0   1  0   \n",
       "2018-01-15/2018-01-21  1  2  0  0  0  0  0  0  0  2  0  0  0  0  1  0  10  4   \n",
       "2018-01-22/2018-01-28  6  0  0  1  0  0  2  0  0  0  1  1  0  0  0  0   0  0   \n",
       "2018-01-29/2018-02-04  0  3  2  2  3  1  0  0  0  3  0  1  0  0  0  0   1  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  1  4  0  0  0  0  0  0  0  0  0  0   4  0   \n",
       "\n",
       "Region                                                              NU      \\\n",
       "Event                 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20  01  02   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  43  19   \n",
       "2018-01-15/2018-01-21  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0  16  15   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  27  47   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  32  25   \n",
       "2018-02-05/2018-02-11  0  0  1  0  0  0  0  0  0  0  0  2  0  0  0  23  10   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  03   04  05  06  07  08 09  10  11  12 13 14 15 16  17   \n",
       "2018-01-08/2018-01-14  32  109  62   0  23   9  1   7  26   7  8  6  0  1   4   \n",
       "2018-01-15/2018-01-21   9   65  18  12  15  10  1   1  12   9  7  4  0  0  13   \n",
       "2018-01-22/2018-01-28  39  110  55  24   9  14  6   1  12   8  0  3  0  7   9   \n",
       "2018-01-29/2018-02-04  26  109  46   4  33  25  7  17  42  20  4  0  0  5  20   \n",
       "2018-02-05/2018-02-11  12  125  22   2   8   3  9   2  20   3  6  0  0  2   4   \n",
       "\n",
       "Region                              NZ                                      \\\n",
       "Event                  18  19 20    01   02   03    04   05   06   07   08   \n",
       "2018-01-08/2018-01-14   1   7  0   668  354  248  1198  349  101  169  133   \n",
       "2018-01-15/2018-01-21   6  16  0   770  407  403  1403  448  103  215  154   \n",
       "2018-01-22/2018-01-28   2  11  0   810  394  529  1523  762  144  240  165   \n",
       "2018-01-29/2018-02-04   6   6  0  1002  587  555  1848  692  158  263  224   \n",
       "2018-02-05/2018-02-11  12  10  0  1001  527  499  1662  536  139  249  249   \n",
       "\n",
       "Region                                                                     OD  \\\n",
       "Event                   09  10   11   12  13  14 15  16   17  18   19 20   01   \n",
       "2018-01-08/2018-01-14   84  30  236   79  46  24  1  25  182  73  157  0  142   \n",
       "2018-01-15/2018-01-21   80  46  270  109  48  22  6  27  191  73  195  0  108   \n",
       "2018-01-22/2018-01-28  109  56  233  119  79  54  4  42  178  55  216  0  178   \n",
       "2018-01-29/2018-02-04  208  59  283  132  74  24  6  25  281  31  310  0  155   \n",
       "2018-02-05/2018-02-11   92  71  316  113  80  25  1  41  248  43  233  2  172   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   02  03   04   05  06  07  08  09  10   11  12  13  14   \n",
       "2018-01-08/2018-01-14   34  31  106   35  20  12  22  12  10   55  10  32   2   \n",
       "2018-01-15/2018-01-21   50  46  224   59  35  38  39  27  20   87  24  29   4   \n",
       "2018-01-22/2018-01-28  100  59  305  103  52  30  30  26  12   92  18  24   5   \n",
       "2018-01-29/2018-02-04  100  54  294  116  28  35  30  11  21  109  11  28   2   \n",
       "2018-02-05/2018-02-11  125  46  299  103  43  33  63   7  31  109  38  32  14   \n",
       "\n",
       "Region                                        PA                             \\\n",
       "Event                 15  16  17  18   19 20  01  02  03  04  05  06 07  08   \n",
       "2018-01-08/2018-01-14  4   8   9   4  108  6  23   3   9  49  10  10  3  12   \n",
       "2018-01-15/2018-01-21  2   2  25  12  100  5  17   6   4  65   6  12  3   7   \n",
       "2018-01-22/2018-01-28  6  19  28  16   96  0  20   4   5  41  12   4  1   3   \n",
       "2018-01-29/2018-02-04  8  69  34  16   55  0  10   9  10  48  11  12  3   5   \n",
       "2018-02-05/2018-02-11  1  85  25  26   84  3  30  10   9  50  26   5  8  28   \n",
       "\n",
       "Region                                                       PC               \\\n",
       "Event                 09  10  11 12 13 14 15 16  17 18 19 20 01 02  03 04 05   \n",
       "2018-01-08/2018-01-14  8   0  10  9  3  0  0  0  23  4  8  0  1  1   2  0  1   \n",
       "2018-01-15/2018-01-21  2   0   6  3  1  0  0  0   0  4  7  0  6  1   2  0  0   \n",
       "2018-01-22/2018-01-28  0   1   7  2  1  0  0  0  12  5  5  0  2  0   0  0  0   \n",
       "2018-01-29/2018-02-04  1   0   3  3  6  0  0  0   4  2  2  0  1  0   0  2  2   \n",
       "2018-02-05/2018-02-11  1  16   1  0  3  1  0  0   9  9  4  0  0  0  20  2  3   \n",
       "\n",
       "Region                                                               PE       \\\n",
       "Event                 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   01   02   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  1  0  0  0  0  1  0  0  0  280  126   \n",
       "2018-01-15/2018-01-21  2  0  0  0  0  0  0  0  0  0  0  1  0  0  0  632  296   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  284  173   \n",
       "2018-01-29/2018-02-04  1  0  0  0  0  0  0  0  0  0  0  2  0  0  0  207   98   \n",
       "2018-02-05/2018-02-11  0  1  0  0  0  0  0  0  0  0  0  1  0  0  0  222   85   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   03    04   05  06  07   08  09  10   11  12  13  14   \n",
       "2018-01-08/2018-01-14  301   662  142  70  87  128  50  30  128  35  23  17   \n",
       "2018-01-15/2018-01-21  494  1921  371  39  52  134  42  40  263  56  49  47   \n",
       "2018-01-22/2018-01-28  142   871  176  21  50   78  25  26  197  69  24  16   \n",
       "2018-01-29/2018-02-04  234   508   99  45  32  124  49  23  116  44  16  16   \n",
       "2018-02-05/2018-02-11  189   517  108  40  51   43  36   8   73  35  23   8   \n",
       "\n",
       "Region                                        PF                             \\\n",
       "Event                 15  16   17  18   19 20 01 02 03 04 05 06 07 08 09 10   \n",
       "2018-01-08/2018-01-14  0  15  111  19   77  0  0  0  0  0  1  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  16  149  44  133  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  4  11  128  27   73  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  3  13  113  20   35  0  0  0  0  0  0  0  0  0  1  0   \n",
       "2018-02-05/2018-02-11  1   5   83  14   52  2  0  0  1  4  0  0  0  0  0  0   \n",
       "\n",
       "Region                                              PG                       \\\n",
       "Event                 11 12 13 14 15 16 17 18 19 20 01 02 03 04 05 06 07 08   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  2  1  0  2  0  0  0  0   \n",
       "2018-01-15/2018-01-21  2  0  2  0  0  0  0  0  0  0  0  0  0  1  0  4  3  0   \n",
       "2018-01-22/2018-01-28  0  0  0  3  0  0  0  0  0  0  1  0  2  0  0  0  0  2   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  2  0  0  2  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  3  3  0  0  3  0  0  1   \n",
       "\n",
       "Region                                                       PK              \\\n",
       "Event                 09 10 11 12 13 14 15 16 17 18 19 20    01    02    03   \n",
       "2018-01-08/2018-01-14  0  0  1  0  0  0  0  0  0  0  1  0  3161  1433   950   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  3487  1691  1127   \n",
       "2018-01-22/2018-01-28  0  0  0  0  1  0  0  0  0  0  0  0  3459  1661  1104   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  3350  1513  1040   \n",
       "2018-02-05/2018-02-11  2  0  1  1  0  0  3  0  1  0  2  0  3995  1566  1214   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                    04    05   06   07   08   09   10    11   12   13   \n",
       "2018-01-08/2018-01-14  4438  1414  471  574  909  398  412  1939  635  370   \n",
       "2018-01-15/2018-01-21  5084  1300  452  654  943  502  601  2381  576  391   \n",
       "2018-01-22/2018-01-28  5085  1330  413  690  880  531  526  2090  574  313   \n",
       "2018-01-29/2018-02-04  4813  1381  459  604  758  538  454  2049  631  285   \n",
       "2018-02-05/2018-02-11  5089  1738  522  647  885  416  506  2235  609  346   \n",
       "\n",
       "Region                                                     PL                  \\\n",
       "Event                   14  15   16    17   18    19  20   01   02   03    04   \n",
       "2018-01-08/2018-01-14  512  51  575  1067  882  2127   5  353  257  296   799   \n",
       "2018-01-15/2018-01-21  515  98  307  1300  975  2933  15  323  183  215   872   \n",
       "2018-01-22/2018-01-28  371  38  350  1314  734  2518   2  673  263  327  1154   \n",
       "2018-01-29/2018-02-04  370  56  244  1391  632  2300   5  952  533  374  1490   \n",
       "2018-02-05/2018-02-11  414  69  340  1726  546  3138  10  732  412  425  1044   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   05   06   07   08  09  10   11   12   13  14  15   16   \n",
       "2018-01-08/2018-01-14  243  117   97  100  30  40  155  132   38  39   6   53   \n",
       "2018-01-15/2018-01-21  246   67   96   80  41  27  150   97   30  22  11   32   \n",
       "2018-01-22/2018-01-28  421   60  167  133  53  29  388  126   91  30  11   31   \n",
       "2018-01-29/2018-02-04  481  106  144  153  60  71  686  246  104  41   8  137   \n",
       "2018-02-05/2018-02-11  462   55  106  126  39  50  773  158   73  69  11  125   \n",
       "\n",
       "Region                                    PM                                   \\\n",
       "Event                   17  18   19  20   01  02  03   04  05  06  07  08  09   \n",
       "2018-01-08/2018-01-14  120  30  148   6  193  43  94  317  77  22  26  63  21   \n",
       "2018-01-15/2018-01-21  167  19  149   4  107  85  46  195  82  21  34  35  25   \n",
       "2018-01-22/2018-01-28  209  37  242   4  132  72  63  170  99  26  23  27  28   \n",
       "2018-01-29/2018-02-04  245  68  477  35  110  63  80  237  59  17  19  21  46   \n",
       "2018-02-05/2018-02-11  260  63  274  11  101  76  54  219  67   8  21  15  22   \n",
       "\n",
       "Region                                                             PO      \\\n",
       "Event                  10   11  12  13  14 15  16  17  18  19 20   01  02   \n",
       "2018-01-08/2018-01-14  17   79  34   9   7  1   8  54  13  28  0  165  71   \n",
       "2018-01-15/2018-01-21  10  114  37  10  19  0  24  49   6  36  0  183  54   \n",
       "2018-01-22/2018-01-28  11   91  28   7   6  2  16  63   9  37  0  143  66   \n",
       "2018-01-29/2018-02-04  16   80  35   5   3  0  10  90  17  25  0  158  66   \n",
       "2018-02-05/2018-02-11  17   70  21   6   3  1   6  47   3  52  0  107  59   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   03   04   05  06  07  08  09  10  11  12  13 14 15   \n",
       "2018-01-08/2018-01-14  107  316  162  30  36  49  15   8  43  37   6  4  1   \n",
       "2018-01-15/2018-01-21  172  328  120  22  44  35  16   2  48  21   6  5  1   \n",
       "2018-01-22/2018-01-28  105  437  132  28  34  32   8   7  49  35  17  4  0   \n",
       "2018-01-29/2018-02-04   84  356  149  31  25  41  33  18  57  22   9  9  2   \n",
       "2018-02-05/2018-02-11  102  303  110  34  51  59  22  12  53  74   7  6  7   \n",
       "\n",
       "Region                                     PP                               \\\n",
       "Event                  16  17  18  19 20   01  02  03   04  05  06  07  08   \n",
       "2018-01-08/2018-01-14   6  63  11  54  0  101  37  30  131  26  33  31   6   \n",
       "2018-01-15/2018-01-21  14  89  12  47  0   55  19  29   91  44  14  39   9   \n",
       "2018-01-22/2018-01-28  11  67  16  29  0   89  35  59  210  34  11  39  17   \n",
       "2018-01-29/2018-02-04   3  55  10  45  0   67  34  35  106  33  15  22   5   \n",
       "2018-02-05/2018-02-11   3  54  11  46  0  107  52  82  260  60  14  24  18   \n",
       "\n",
       "Region                                                             PS         \\\n",
       "Event                  09 10  11  12 13  14 15  16  17  18  19 20  01 02  03   \n",
       "2018-01-08/2018-01-14  17  8  29   4  3   3  0  18   8   7  29  0  11  5   1   \n",
       "2018-01-15/2018-01-21  11  2  31   9  4   7  1  20  10   9  19  0  11  3  11   \n",
       "2018-01-22/2018-01-28   6  4  36   9  3  19  3   0  35  16  18  0   2  1   1   \n",
       "2018-01-29/2018-02-04   4  3  16   9  4   1  0   2  10  13  23  0   3  3   1   \n",
       "2018-02-05/2018-02-11  14  7  30  11  4   3  2   0  27  10  40  0   6  1   0   \n",
       "\n",
       "Region                                                                     PU  \\\n",
       "Event                  04  05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20 01   \n",
       "2018-01-08/2018-01-14  25  10  0  5  9  0  0  7  3  1  0  0  0  2  1  0  0  5   \n",
       "2018-01-15/2018-01-21  12   8  0  4  0  0  0  3  0  0  0  0  7  3  0  0  0  9   \n",
       "2018-01-22/2018-01-28  16   2  0  4  0  0  0  2  0  0  0  0  8  2  0  2  0  5   \n",
       "2018-01-29/2018-02-04   1  17  0  0  1  0  1  2  9  0  0  0  0  0  0  0  0  2   \n",
       "2018-02-05/2018-02-11   5   1  2  3  2  2  0  1  1  0  0  0  2  4  0  4  0  2   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                 02 03  04  05 06 07 08 09 10 11 12 13 14 15  16 17 18   \n",
       "2018-01-08/2018-01-14  0  0  12   1  0  0  1  6  0  1  0  1  0  0   0  0  1   \n",
       "2018-01-15/2018-01-21  3  4  33  10  0  1  0  0  0  0  0  2  0  0   0  1  3   \n",
       "2018-01-22/2018-01-28  3  4  11   2  0  0  0  1  0  1  0  0  0  0   0  3  0   \n",
       "2018-01-29/2018-02-04  7  6  15   7  0  1  0  1  1  0  1  0  0  0   0  0  0   \n",
       "2018-02-05/2018-02-11  3  0   1  10  5  0  0  0  2  4  2  0  0  0  16  0  0   \n",
       "\n",
       "Region                        QA                                             \\\n",
       "Event                 19 20   01   02   03    04   05   06   07  08  09  10   \n",
       "2018-01-08/2018-01-14  0  0  269  123  175   559  224   88   95  56  33  24   \n",
       "2018-01-15/2018-01-21  1  0  442  178  235   928  406   70  122  62  52  31   \n",
       "2018-01-22/2018-01-28  0  0  332  111  185   676  307   41   93  51   9  12   \n",
       "2018-01-29/2018-02-04  0  0  408  181  378  1091  618  124  118  74  13  19   \n",
       "2018-02-05/2018-02-11  0  0  251  127  190   781  395  116  123  33  14  15   \n",
       "\n",
       "Region                                                           RE         \\\n",
       "Event                   11   12  13  14  15  16   17  18  19 20  01 02  03   \n",
       "2018-01-08/2018-01-14  306   83  15  20  23  61   68  10  55  0  11  3   3   \n",
       "2018-01-15/2018-01-21  552  141  33   5  25  76  125   9  52  0   9  6  16   \n",
       "2018-01-22/2018-01-28  408   56  17   3   7  30   56  12  24  0  10  1   4   \n",
       "2018-01-29/2018-02-04  300   78  18  10  28  64   47  16  56  0   5  1   3   \n",
       "2018-02-05/2018-02-11  162   43  14  12   6  53   53   5  25  0   4  1   5   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                  04 05 06 07 08 09 10 11 12 13 14 15 16 17 18  19 20   \n",
       "2018-01-08/2018-01-14  12  2  1  1  1  2  1  2  0  1  0  0  0  2  0   3  0   \n",
       "2018-01-15/2018-01-21  28  5  5  1  1  0  0  0  0  0  0  0  0  0  0   5  0   \n",
       "2018-01-22/2018-01-28  11  7  3  3  0  0  1  4  2  0  0  0  0  4  6  14  0   \n",
       "2018-01-29/2018-02-04   8  5  0  0  0  1  0  1  0  0  0  0  0  1  8   5  0   \n",
       "2018-02-05/2018-02-11  38  8  2  2  1  0  0  3  2  0  1  0  0  2  0   5  0   \n",
       "\n",
       "Region                  RI                                                    \\\n",
       "Event                   01  02  03   04  05  06  07  08  09  10   11  12  13   \n",
       "2018-01-08/2018-01-14   58  20  68  160  59   3   8  14  10  14   32  26   9   \n",
       "2018-01-15/2018-01-21  153  45  75  245  98  11  14  20  24  56  106  19  10   \n",
       "2018-01-22/2018-01-28   71  38  46  129  42  26  14  12   8   9   20  14   4   \n",
       "2018-01-29/2018-02-04   81  19  74  132  57  23  21  10   8   3   24  10  12   \n",
       "2018-02-05/2018-02-11   95  48  78  175  72  45   6  28   5  16   29  27   3   \n",
       "\n",
       "Region                                           RM                            \\\n",
       "Event                 14 15  16  17  18   19 20  01 02 03  04 05 06 07  08 09   \n",
       "2018-01-08/2018-01-14  2  2   6  20   9   13  1  10  5  8  43  7  2  2  17  0   \n",
       "2018-01-15/2018-01-21  5  1  16  86  14  200  0  13  6  5  14  2  2  2   7  8   \n",
       "2018-01-22/2018-01-28  3  0   4  24   0   14  0   2  3  3  10  6  3  2   1  0   \n",
       "2018-01-29/2018-02-04  6  0   3   9   1   15  0  17  7  1  32  5  2  2   0  3   \n",
       "2018-02-05/2018-02-11  5  1   2  16   1   18  2  27  4  3  11  2  2  7   5  1   \n",
       "\n",
       "Region                                                   RN                    \\\n",
       "Event                 10  11 12 13 14 15 16 17 18 19 20  01 02 03 04 05 06 07   \n",
       "2018-01-08/2018-01-14  0  12  4  0  2  0  2  2  3  9  0   1  0  0  2  1  0  0   \n",
       "2018-01-15/2018-01-21  1   7  1  3  0  0  1  7  1  9  0   0  0  0  2  0  0  0   \n",
       "2018-01-22/2018-01-28  0   0  0  1  0  0  2  3  0  0  0   1  0  0  7  0  0  0   \n",
       "2018-01-29/2018-02-04  1   9  0  4  1  0  1  4  2  3  0  13  5  1  3  2  2  0   \n",
       "2018-02-05/2018-02-11  1   6  3  4  0  0  1  3  0  2  0   0  1  1  0  2  0  0   \n",
       "\n",
       "Region                                                          RO            \\\n",
       "Event                 08 09 10 11 12 13 14 15 16  17 18 19 20   01   02   03   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  1  0  0   0  0  0  0  259  187  159   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0   0  1  0  0  324  195  289   \n",
       "2018-01-22/2018-01-28  2  0  0  0  0  0  0  0  0   2  0  0  0  219  129  135   \n",
       "2018-01-29/2018-02-04  3  0  3  2  2  0  0  0  1  12  0  0  0  231  125  221   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0   3  0  0  0  234   82  176   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                    04   05  06  07   08  09  10   11  12  13  14 15  16   \n",
       "2018-01-08/2018-01-14   494  170  43  47  119  24  57  214  62  14  21  3  10   \n",
       "2018-01-15/2018-01-21  1076  231  94  44  176  35  26  132  59  35  92  4  39   \n",
       "2018-01-22/2018-01-28   523  115  30  64   71  21  44   81  37  41  43  4   6   \n",
       "2018-01-29/2018-02-04   638  194  68  95  109  45  32  125  51  33  25  1   7   \n",
       "2018-02-05/2018-02-11   428  113  60  43   58  34  24   72  49  21  24  1  16   \n",
       "\n",
       "Region                                  RP                                   \\\n",
       "Event                  17  18  19 20    01    02   03    04    05   06   07   \n",
       "2018-01-08/2018-01-14  73  30  12  0  1730   889  815  2290  1015  267  311   \n",
       "2018-01-15/2018-01-21  46  19  78  0  1821   988  614  2249   944  218  317   \n",
       "2018-01-22/2018-01-28  33  11  70  0  2162  1067  973  3428  1122  381  414   \n",
       "2018-01-29/2018-02-04  55  12  43  0  2040   972  733  2636   870  247  403   \n",
       "2018-02-05/2018-02-11  84   9  48  0  2116  1116  930  2593  1097  265  369   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   08   09   10    11   12   13   14  15   16   17   18   \n",
       "2018-01-08/2018-01-14  416  249  257   951  251  243  106  78   66  571  101   \n",
       "2018-01-15/2018-01-21  601  248  247   995  353  281  106  24  134  623   86   \n",
       "2018-01-22/2018-01-28  534  244  283  1064  318  277   92  39  154  945  187   \n",
       "2018-01-29/2018-02-04  646  319  342  1154  386  240   84  30  105  811  142   \n",
       "2018-02-05/2018-02-11  408  470  304  1095  293  316   71  45  157  631  114   \n",
       "\n",
       "Region                           RQ                                           \\\n",
       "Event                    19 20   01   02   03   04   05  06   07  08  09  10   \n",
       "2018-01-08/2018-01-14   765  5  224  125  106  441  146  70   65  45  44  13   \n",
       "2018-01-15/2018-01-21   692  1  171  121  102  320  105  43  108  43  31  14   \n",
       "2018-01-22/2018-01-28  1196  7  208  101  158  388  101  46   93  27  11   9   \n",
       "2018-01-29/2018-02-04   577  4  233  129  132  414   64  46  151  57  31  19   \n",
       "2018-02-05/2018-02-11   806  1  221  115  192  284  113  53  172  57  29   6   \n",
       "\n",
       "Region                                                         RS              \\\n",
       "Event                  11  12  13  14 15  16  17  18  19 20    01    02    03   \n",
       "2018-01-08/2018-01-14  61  25  13   2  4   7  47   8  55  0  3485  1584  1396   \n",
       "2018-01-15/2018-01-21  92  28  24   2  2   3  45  12  35  0  3982  1675  1599   \n",
       "2018-01-22/2018-01-28  65  43   5   5  2   3  37   8  39  0  4261  2070  1938   \n",
       "2018-01-29/2018-02-04  91  27  14  12  1  23  46   7  49  0  5227  2143  1853   \n",
       "2018-02-05/2018-02-11  72  52  20   3  8  10  33  15  28  0  4207  1782  1585   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                    04    05   06   07    08    09   10    11    12   13   \n",
       "2018-01-08/2018-01-14  5145  1722  659  649  1003   713  266  1907   715  469   \n",
       "2018-01-15/2018-01-21  6135  2015  543  631   933   694  365  1994   734  733   \n",
       "2018-01-22/2018-01-28  6839  2054  731  676  1221   822  435  2199   952  759   \n",
       "2018-01-29/2018-02-04  7801  2211  899  736  1864  1104  495  2676  1096  848   \n",
       "2018-02-05/2018-02-11  6399  2092  730  646  1208   856  370  2178   801  615   \n",
       "\n",
       "Region                                                      RW                 \\\n",
       "Event                   14   15   16    17   18    19  20   01   02   03   04   \n",
       "2018-01-08/2018-01-14  165  118  303   859  268  1727   4  190  105  139  487   \n",
       "2018-01-15/2018-01-21  176  165  425   910  336  1485  14  142   55  151  580   \n",
       "2018-01-22/2018-01-28  261  214  586  1197  260  1378  15  300  149  232  741   \n",
       "2018-01-29/2018-02-04  266  250  666  1260  279  2054   6  284  124  151  530   \n",
       "2018-02-05/2018-02-11  125  163  315  1124  271  2232  21  157   96   69  410   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   05  06   07  08  09  10  11  12  13  14 15  16   17   \n",
       "2018-01-08/2018-01-14  129  67   46  35  11   4  70  21  18  11  6  15   58   \n",
       "2018-01-15/2018-01-21  140  28   48  45   4  15  56  12  14   7  2   4   33   \n",
       "2018-01-22/2018-01-28  220  51   70  35  21  29  85  49  30  23  3  21   76   \n",
       "2018-01-29/2018-02-04  163  55   34  90  17  13  79  51  20  16  0  15   77   \n",
       "2018-02-05/2018-02-11  126  77  105  44  12  23  92  54  23  32  4   9  108   \n",
       "\n",
       "Region                             SA                                      \\\n",
       "Event                  18  19 20   01   02   03    04   05   06   07   08   \n",
       "2018-01-08/2018-01-14  13  83  4  716  359  497  1619  595  179  200  290   \n",
       "2018-01-15/2018-01-21  20  74  1  677  376  474  1467  597  192  199  256   \n",
       "2018-01-22/2018-01-28  10  41  2  924  409  566  2092  515  171  228  419   \n",
       "2018-01-29/2018-02-04  28  63  2  743  421  372  1394  573  213  212  330   \n",
       "2018-02-05/2018-02-11  18  32  0  679  341  420  1753  564  181  191  208   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   09   10   11   12  13  14  15   16   17   18   19 20   \n",
       "2018-01-08/2018-01-14   96  112  427  137  75  79  18   89  390   90  536  0   \n",
       "2018-01-15/2018-01-21   68  118  411  150  64  37  37  114  383   54  523  0   \n",
       "2018-01-22/2018-01-28   74   66  470  138  96  32  16   81  459  100  595  0   \n",
       "2018-01-29/2018-02-04  104  104  371  166  59  40  22   56  384   42  636  0   \n",
       "2018-02-05/2018-02-11  102   73  381  121  78  22   7   61  404   88  485  0   \n",
       "\n",
       "Region                SB                                                     \\\n",
       "Event                 01 02 03 04 05 06 07 08 09 10 11 12 13 14 15 16 17 18   \n",
       "2018-01-08/2018-01-14  2  0  1  2  0  0  0  0  0  0  0  0  0  0  0  0  2  0   \n",
       "2018-01-15/2018-01-21  2  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  6  1  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                      SC                                                \\\n",
       "Event                 19 20 01 02 03  04 05 06 07 08 09 10 11 12 13 14 15 16   \n",
       "2018-01-08/2018-01-14  0  0  3  0  0  11  2  0  0  2  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  0  1  4  0   8  0  0  0  0  0  0  0  1  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  4   4  3  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  2  0   4  3  0  0  0  0  0  0  0  0  0  0  1   \n",
       "2018-02-05/2018-02-11  0  0  2  0  0   6  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                             SE                                         \\\n",
       "Event                 17 18 19 20  01 02   03   04  05  06 07 08 09 10 11 12   \n",
       "2018-01-08/2018-01-14  0  0  0  0  24  4   21  130   9   4  0  5  1  2  8  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  20  8    8   93  11   1  5  5  0  2  7  1   \n",
       "2018-01-22/2018-01-28  0  0  0  0  36  4   26   77  78  10  5  4  1  0  4  2   \n",
       "2018-01-29/2018-02-04  3  0  0  0  28  7  117   45  33   0  6  8  0  0  3  4   \n",
       "2018-02-05/2018-02-11  0  0  0  0  22  6   26  129   9   3  0  4  1  1  2  1   \n",
       "\n",
       "Region                                            SF                          \\\n",
       "Event                 13 14 15 16 17 18  19 20    01    02    03    04    05   \n",
       "2018-01-08/2018-01-14  1  0  0  0  3  0  11  0  2251  1216  1048  3804  1030   \n",
       "2018-01-15/2018-01-21  3  2  0  0  4  2   0  0  2231  1254   974  3732  1138   \n",
       "2018-01-22/2018-01-28  0  0  0  6  1  0   1  0  2357  1187  1152  4017  1069   \n",
       "2018-01-29/2018-02-04  1  3  1  2  2  0   1  0  2458  1193  1059  3681  1044   \n",
       "2018-02-05/2018-02-11  1  0  1  1  6  0   4  0  2814  1444  1271  4715  1262   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                   06   07   08   09   10    11   12   13   14  15   16   \n",
       "2018-01-08/2018-01-14  344  408  437  359  212  1095  317  179  221  38  130   \n",
       "2018-01-15/2018-01-21  320  386  552  307  221  1114  410  223  339  25  125   \n",
       "2018-01-22/2018-01-28  344  459  517  315  232   983  315  211  268  28  133   \n",
       "2018-01-29/2018-02-04  319  522  479  352  214   968  355  242  155  25  120   \n",
       "2018-02-05/2018-02-11  299  524  976  262  261  1164  502  241  225  17  227   \n",
       "\n",
       "Region                                    SG                                   \\\n",
       "Event                   17   18   19  20  01  02  03   04  05  06  07  08  09   \n",
       "2018-01-08/2018-01-14  530  306  937   1  98  62  17  226  23  19  42  35  31   \n",
       "2018-01-15/2018-01-21  765  281  942   4  68  26  36  171  62  11  31   6  16   \n",
       "2018-01-22/2018-01-28  799  240  739   5  53  20  26  128  38   8  18  12  22   \n",
       "2018-01-29/2018-02-04  690  255  765   4  78  42  98  374  64  18  33  17  10   \n",
       "2018-02-05/2018-02-11  698  285  947  10  56  22  44  166  51  20  35  20  11   \n",
       "\n",
       "Region                                                            SH        \\\n",
       "Event                 10   11  12 13  14  15  16  17  18  19  20  01 02 03   \n",
       "2018-01-08/2018-01-14  3  103   5  7   5   5  10  42  18  94   1   4  0  6   \n",
       "2018-01-15/2018-01-21  4   38   8  2   3   2   4  34  12  24  11   2  4  5   \n",
       "2018-01-22/2018-01-28  7   32  14  1   1  14   0  39  34  45   2   7  4  0   \n",
       "2018-01-29/2018-02-04  6   49  17  8  12   6   2  17  10  36   0   4  3  5   \n",
       "2018-02-05/2018-02-11  4   10   3  0   2   0   3  27   2  16   0  10  0  4   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                  04  05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   \n",
       "2018-01-08/2018-01-14  14  11  0  0  0  0  0  1  0  0  1  0  0  0  0  2  0   \n",
       "2018-01-15/2018-01-21  24   2  0  1  0  0  0  0  0  0  0  2  0  0  0  1  0   \n",
       "2018-01-22/2018-01-28  30   9  0  2  0  0  0  0  0  0  2  0  1  1  0  0  0   \n",
       "2018-01-29/2018-02-04  20   4  3  1  0  0  0  3  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  24   8  0  0  0  0  0  0  0  0  3  0  0  1  0  0  0   \n",
       "\n",
       "Region                 SI                                                     \\\n",
       "Event                  01  02  03   04  05  06  07  08  09 10  11  12  13 14   \n",
       "2018-01-08/2018-01-14  17   9  25   66  20   7   7   7   0  5  17   5   0  1   \n",
       "2018-01-15/2018-01-21  34  14  24   62  29  10  12  10   3  3  22   4  10  8   \n",
       "2018-01-22/2018-01-28  58  25  23   46  32   2   9   4  12  8  22  12   4  9   \n",
       "2018-01-29/2018-02-04  53   7  36  135  38  12   3  10  10  0  16   7   8  0   \n",
       "2018-02-05/2018-02-11  26  14  20  103  17   7  10   6   3  6   6   7  10  0   \n",
       "\n",
       "Region                                       SL                               \\\n",
       "Event                 15  16  17 18  19 20   01  02  03   04  05  06  07  08   \n",
       "2018-01-08/2018-01-14  4   1   4  0   5  0  100  31  31  194  54   8  14  35   \n",
       "2018-01-15/2018-01-21  0   0   5  0   6  0  130  45  59  184  63  10  31  51   \n",
       "2018-01-22/2018-01-28  0   1  10  1   8  0  105  47  46  130  64  15  24  24   \n",
       "2018-01-29/2018-02-04  2   3   6  0  11  0  104  59  71  184  67  20  20  24   \n",
       "2018-02-05/2018-02-11  0  10   9  0   6  0  124  57  68  228  99  12  16  16   \n",
       "\n",
       "Region                                                             SM        \\\n",
       "Event                  09  10  11  12 13  14  15 16  17  18  19 20 01 02 03   \n",
       "2018-01-08/2018-01-14  10  14  34  22  8  13  20  3  18   5   9  0  1  0  0   \n",
       "2018-01-15/2018-01-21   5  25  51  11  7  12   0  5  51  25  14  0  1  0  4   \n",
       "2018-01-22/2018-01-28  10   9  82  25  2   3   0  4  19  14  15  0  1  4  0   \n",
       "2018-01-29/2018-02-04   9   6  44  20  8   2   0  8  23  13  16  0  5  0  0   \n",
       "2018-02-05/2018-02-11   4   8  56  24  5   1   0  4  52  11  21  0  3  0  3   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                 04  05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   \n",
       "2018-01-08/2018-01-14  2   0  1  1  0  1  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  6  36  0  0  0  0  0  1  0  0  0  0  0  4  0  0  0   \n",
       "2018-01-22/2018-01-28  9   4  0  2  0  1  0  1  0  0  0  0  0  3  3  0  0   \n",
       "2018-01-29/2018-02-04  7   4  3  1  0  5  0  1  0  0  0  0  0  0  1  0  0   \n",
       "2018-02-05/2018-02-11  0   0  0  0  1  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                  SN                                                   \\\n",
       "Event                   01   02   03    04   05   06   07   08  09  10   11   \n",
       "2018-01-08/2018-01-14  369  183  247   842  238   94  106   93  42  34  165   \n",
       "2018-01-15/2018-01-21  394  220  336   919  281  139  122   95  51  31  174   \n",
       "2018-01-22/2018-01-28  429  227  360  1181  393  175  130  118  32  30  142   \n",
       "2018-01-29/2018-02-04  438  224  273   783  285   81  150  111  49  26  166   \n",
       "2018-02-05/2018-02-11  584  247  360  1251  381  182  177  128  96  41  179   \n",
       "\n",
       "Region                                                       SO            \\\n",
       "Event                   12  13  14  15  16   17  18  19 20   01   02   03   \n",
       "2018-01-08/2018-01-14  111  23   7   7  23  161  11  94  0  300  153   85   \n",
       "2018-01-15/2018-01-21   56  48  11   5  13  205  16  67  1  341  107  100   \n",
       "2018-01-22/2018-01-28   69  40  16   8  19  185  15  59  0  282  185  136   \n",
       "2018-01-29/2018-02-04   59  34  12  10  57  155  30  47  0  214  130  124   \n",
       "2018-02-05/2018-02-11   83  31  12  19  32  143  22  93  0  226   82   94   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   04   05  06   07   08  09  10   11  12  13  14  15   \n",
       "2018-01-08/2018-01-14  717  232  63   72  125  26  24  217  40  35   9  39   \n",
       "2018-01-15/2018-01-21  633  209  71  119  165  28  27  183  57  34  17  25   \n",
       "2018-01-22/2018-01-28  778  187  61  113  115  19  25  133  37  37   6  19   \n",
       "2018-01-29/2018-02-04  719  169  37   61   78  33  30  130  28  51   4  13   \n",
       "2018-02-05/2018-02-11  536  199  40   76  107  26  19  128  21  19  16  10   \n",
       "\n",
       "Region                                        SP                            \\\n",
       "Event                  16   17  18   19  20   01   02   03    04   05   06   \n",
       "2018-01-08/2018-01-14  15  135  89  388   2  643  340  452  1563  507  117   \n",
       "2018-01-15/2018-01-21  14  125  86  434   0  752  339  547  1835  728  176   \n",
       "2018-01-22/2018-01-28  16   96  43  357   0  982  546  490  2171  558  127   \n",
       "2018-01-29/2018-02-04  12   76  44  211   2  850  386  360  1564  540  150   \n",
       "2018-02-05/2018-02-11  19  127  74  278  11  745  403  551  1412  378  131   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   07   08   09   10   11   12   13  14  15   16   17   \n",
       "2018-01-08/2018-01-14  209  241  124   62  331  158   65  43  10   32  264   \n",
       "2018-01-15/2018-01-21  192  216  117  100  381  169  109  36  10   49  301   \n",
       "2018-01-22/2018-01-28  183  247  120  192  490  209   66  57  23  104  459   \n",
       "2018-01-29/2018-02-04  204  244  112   75  330  174   87  60  24   52  350   \n",
       "2018-02-05/2018-02-11  150  184   87   66  310  122   54  44  13   16  310   \n",
       "\n",
       "Region                              ST                                        \\\n",
       "Event                   18   19 20  01 02   03  04  05 06  07 08 09 10 11 12   \n",
       "2018-01-08/2018-01-14   61  225  0  25  7    5  16  10  2   2  1  0  1  4  0   \n",
       "2018-01-15/2018-01-21   67  304  4  23  5    9  29  13  5  23  2  1  1  4  2   \n",
       "2018-01-22/2018-01-28  125  301  6  15  6   11  14  16  4  28  4  0  1  3  0   \n",
       "2018-01-29/2018-02-04   63  270  0  10  5  112  31  14  2   6  2  1  0  1  1   \n",
       "2018-02-05/2018-02-11   72  237  0  37  5    8  47  17  1   6  2  0  0  1  7   \n",
       "\n",
       "Region                                           SU                           \\\n",
       "Event                 13 14 15  16 17 18 19 20   01   02   03    04   05  06   \n",
       "2018-01-08/2018-01-14  1  0  0   4  4  0  1  0  326  159  167   915  323  55   \n",
       "2018-01-15/2018-01-21  0  0  0  12  2  0  0  0  355  182  153   998  265  48   \n",
       "2018-01-22/2018-01-28  0  0  0   4  0  1  9  0  339  165  280   812  318  49   \n",
       "2018-01-29/2018-02-04  1  0  0   0  0  0  2  0  387  249  405  1006  345  73   \n",
       "2018-02-05/2018-02-11  1  0  0   5  0  0  3  0  328  225  220   794  193  47   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                  07   08  09  10   11  12  13   14  15   16   17  18   \n",
       "2018-01-08/2018-01-14  72  123  24  37  278  79  46  126  26   88  205  51   \n",
       "2018-01-15/2018-01-21  92  166  23  35  226  75  72  150  52   97  309  50   \n",
       "2018-01-22/2018-01-28  74  123  14  53  223  86  40   59  16   52  173  72   \n",
       "2018-01-29/2018-02-04  86  219  34  40  211  47  50   81   9  126  210  59   \n",
       "2018-02-05/2018-02-11  75  143  31  51  213  71  50   84  12  108  204  49   \n",
       "\n",
       "Region                        SV                                             \\\n",
       "Event                   19 20 01 02 03  04 05 06 07 08 09 10 11 12 13 14 15   \n",
       "2018-01-08/2018-01-14  225  3  5  2  6   4  4  0  0  0  0  0  2  0  0  0  0   \n",
       "2018-01-15/2018-01-21  162  0  2  0  4   7  0  0  0  0  0  0  0  1  0  0  0   \n",
       "2018-01-22/2018-01-28   91  0  1  0  0  19  0  0  0  0  0  0  1  0  0  0  0   \n",
       "2018-01-29/2018-02-04  135  0  0  0  0   4  4  0  4  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  159  2  0  1  0   7  0  0  0  0  0  1  0  0  0  0  0   \n",
       "\n",
       "Region                                 SW                                     \\\n",
       "Event                 16 17 18 19 20   01   02   03    04   05  06   07   08   \n",
       "2018-01-08/2018-01-14  2  0  0  0  0  393  218  193   576  238  60  106  177   \n",
       "2018-01-15/2018-01-21  2  0  0  0  0  383  124  246   713  145  40   95   80   \n",
       "2018-01-22/2018-01-28  0  2  0  1  0  568  255  191   796  239  79  126  159   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  439  222  507  1007  257  78  103  118   \n",
       "2018-02-05/2018-02-11  0  0  0  1  0  533  268  204   784  235  81  107  145   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                  09  10   11   12  13  14  15  16   17   18   19 20   \n",
       "2018-01-08/2018-01-14  41  37  141   85  21  27  14  34  158  126  123  0   \n",
       "2018-01-15/2018-01-21  66  13  148   59  31  34  57   6  141   66  204  2   \n",
       "2018-01-22/2018-01-28  77  54  190   60  28  20  13  15  181  105  146  0   \n",
       "2018-01-29/2018-02-04  80  31  209   86  39   9   7  35   96   45  131  0   \n",
       "2018-02-05/2018-02-11  53  51  197  103  37  46   2  21  202  146   82  0   \n",
       "\n",
       "Region                   SY                                                    \\\n",
       "Event                    01    02    03    04    05   06   07    08   09   10   \n",
       "2018-01-08/2018-01-14  1218   539   480  2114   696  183  323   660  225  158   \n",
       "2018-01-15/2018-01-21  1986   957   952  3510  1229  250  693   766  167  287   \n",
       "2018-01-22/2018-01-28  2494  1295  1161  4556  1379  304  558  1012  200  353   \n",
       "2018-01-29/2018-02-04  2026   970   786  3948  1058  268  526   827  301  234   \n",
       "2018-02-05/2018-02-11  2433  1138   872  4590  1279  440  610   911  422  317   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                    11   12    13   14   15   16   17   18    19  20   \n",
       "2018-01-08/2018-01-14   716  235   408  145   84   72  390  350  2820   1   \n",
       "2018-01-15/2018-01-21  1340  510  1014  204  321  218  514  412  3004  25   \n",
       "2018-01-22/2018-01-28  1727  641   718  177  223  327  643  443  3624  21   \n",
       "2018-01-29/2018-02-04  1387  524   566  141  117  252  684  400  3660  20   \n",
       "2018-02-05/2018-02-11  1965  515   846  163  171  187  770  570  6002  11   \n",
       "\n",
       "Region                   SZ                                                  \\\n",
       "Event                    01   02    03    04    05   06   07   08   09   10   \n",
       "2018-01-08/2018-01-14   827  289   735  1748   389  105  228  158   94   47   \n",
       "2018-01-15/2018-01-21   771  336   727  2096   335  160  172  165   63   43   \n",
       "2018-01-22/2018-01-28  2280  941  1799  7134  1063  178  353  400  115  184   \n",
       "2018-01-29/2018-02-04   917  398   508  1796   378   88  199  180   95   57   \n",
       "2018-02-05/2018-02-11   689  314   399  1478   419  125  229  123   54   55   \n",
       "\n",
       "Region                                                                 TD      \\\n",
       "Event                   11   12   13   14  15   16   17   18   19 20   01  02   \n",
       "2018-01-08/2018-01-14  264  147   40  101  11   30  217   47  147  7  178  68   \n",
       "2018-01-15/2018-01-21  279   88   49   64   3   39  136   37  145  3  128  74   \n",
       "2018-01-22/2018-01-28  712  341  281  246  32  195  243   52  249  0  150  82   \n",
       "2018-01-29/2018-02-04  379  176   39   28   7   38  190  106  108  0  130  92   \n",
       "2018-02-05/2018-02-11  232  110   35   24   7   36  125   57   87  2  201  74   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                  03   04   05  06  07  08  09  10  11  12  13  14 15   \n",
       "2018-01-08/2018-01-14  53  326  136  21  25  21  15  11  49  14  13  13  3   \n",
       "2018-01-15/2018-01-21  61  215   67  14  19  15  21  14  43  16   5   3  2   \n",
       "2018-01-22/2018-01-28  47  312   47  31  18  19  24  12  66  25   5   5  1   \n",
       "2018-01-29/2018-02-04  53  295   81  24  24  54  25  18  63  14   8   4  2   \n",
       "2018-02-05/2018-02-11  58  265   91  14  22  37  58  20  53  11  20   3  0   \n",
       "\n",
       "Region                                   TE                                   \\\n",
       "Event                  16  17  18  19 20 01 02 03 04 05 06 07 08 09 10 11 12   \n",
       "2018-01-08/2018-01-14  10  67  20  86  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21   3  41   8  84  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28   6  43  21  67  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04   2  66   9  83  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  13  85  15  44  1  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                          TH                            \\\n",
       "Event                 13 14 15 16 17 18 19 20   01   02   03    04   05   06   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  571  284  585  1178  268  114   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  496  264  358  1117  278  128   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  636  240  409  1583  363  152   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  526  251  289  1033  320  143   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  624  324  510  1221  283  152   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   07   08   09  10   11  12  13  14  15  16   17  18   \n",
       "2018-01-08/2018-01-14  137  184  130  85  333  59  30  31   8  22  445  50   \n",
       "2018-01-15/2018-01-21   98  162  128  54  305  84  47  31  15  19  504  72   \n",
       "2018-01-22/2018-01-28  129  197   98  42  303  73  49  45  27  20  357  72   \n",
       "2018-01-29/2018-02-04  117  169   93  75  279  80  49  54  22  34  397  35   \n",
       "2018-02-05/2018-02-11   75  173  115  61  276  72  54  47   8  17  474  69   \n",
       "\n",
       "Region                          TI                                            \\\n",
       "Event                   19  20  01  02  03   04  05  06  07  08 09 10  11 12   \n",
       "2018-01-08/2018-01-14  210   3  23  15  61  133  30  19   6  14  2  0  13  8   \n",
       "2018-01-15/2018-01-21  238   4  26   9  12  109  30  11  12   4  2  1   7  1   \n",
       "2018-01-22/2018-01-28  213   7  39  22   8   99  14  22   5  14  3  0  15  1   \n",
       "2018-01-29/2018-02-04  154   2  33  14  45  275  49  23  14  14  1  0  24  5   \n",
       "2018-02-05/2018-02-11  186  14  58  16  12  189  32   7  11  28  5  4   7  9   \n",
       "\n",
       "Region                                           TK                           \\\n",
       "Event                 13 14 15  16  17 18  19 20 01 02 03  04 05 06 07 08 09   \n",
       "2018-01-08/2018-01-14  2  2  0   2   4  0   4  0  0  0  4   1  5  1  1  0  0   \n",
       "2018-01-15/2018-01-21  2  0  2  13   6  0   4  0  0  0  2   0  3  0  0  0  0   \n",
       "2018-01-22/2018-01-28  1  0  2   2  32  2  16  0  1  0  7  10  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  2  3  1   2  26  1  15  0  1  0  0   0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  2  0  1  11  21  1  14  0  1  0  0   1  1  0  0  0  1   \n",
       "\n",
       "Region                                                 TL                    \\\n",
       "Event                 10 11 12 13 14 15 16 17 18 19 20 01 02 03 04 05 06 07   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  1  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  0  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  1  0  0  0  0  0  4  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  4   \n",
       "2018-02-05/2018-02-11  0  2  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0  1   \n",
       "\n",
       "Region                                                        TN               \\\n",
       "Event                 08 09 10 11 12 13 14 15 16 17 18 19 20  01  02  03   04   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  19  18   6   57   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  21  15  13   49   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0  0  0  0  0  20   6  22   30   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  18   6   7  152   \n",
       "2018-02-05/2018-02-11  1  1  0  0  0  0  0  0  0  0  0  0  0  15  13   6   61   \n",
       "\n",
       "Region                                                                   TO  \\\n",
       "Event                  05 06  07 08 09 10 11 12 13 14 15 16 17 18 19 20  01   \n",
       "2018-01-08/2018-01-14  10  5   4  3  3  0  1  1  0  0  0  0  1  3  7  0  19   \n",
       "2018-01-15/2018-01-21  12  6   4  4  5  3  6  4  0  4  4  3  0  0  3  0  12   \n",
       "2018-01-22/2018-01-28  20  7   7  5  0  1  4  0  1  1  2  0  5  1  4  0   8   \n",
       "2018-01-29/2018-02-04  12  4  11  3  1  0  7  5  5  2  0  1  6  0  3  0  35   \n",
       "2018-02-05/2018-02-11  17  2   3  1  2  0  6  3  4  0  2  1  7  1  5  0  19   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                  02  03  04  05 06 07  08 09 10  11 12 13  14 15 16  17   \n",
       "2018-01-08/2018-01-14  13  17  20  18  2  5   3  2  0  12  0  1   7  0  0   3   \n",
       "2018-01-15/2018-01-21   6  46  64   7  8  5  12  1  1   9  0  0   6  3  0  11   \n",
       "2018-01-22/2018-01-28   2   2  18   1  0  2   2  2  2   5  0  0   0  0  0   6   \n",
       "2018-01-29/2018-02-04   9  19  53  23  2  4   2  1  2   7  1  4  17  0  6  14   \n",
       "2018-02-05/2018-02-11   4   9  49  15  3  1   0  0  3  10  7  5   3  0  0   0   \n",
       "\n",
       "Region                          TP                                            \\\n",
       "Event                  18 19 20 01  02 03  04  05 06 07 08 09 10 11 12 13 14   \n",
       "2018-01-08/2018-01-14   6  7  0  1   0  6   5   1  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  10  8  0  8   5  1  62  27  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28   3  4  0  1   0  0   2   6  0  1  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04   2  0  0  0  12  0   1   6  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11   0  0  0  0   0  0   1   0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                    TS                                   \\\n",
       "Event                 15 16 17 18 19 20   01   02   03   04   05  06  07   08   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  361  165  124  389  113  11  53  104   \n",
       "2018-01-15/2018-01-21  0  3  0  0  0  0  198   79  114  193   75   5  18   30   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  103   48   42  185  194  12  26   12   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  166   57  116  324   91  24  25   35   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0   71   54   53  166   66  14   9   11   \n",
       "\n",
       "Region                                                                     TT  \\\n",
       "Event                  09  10   11  12  13   14  15  16   17   18   19 20  01   \n",
       "2018-01-08/2018-01-14  13  32  238  85  47  411  29  10  374  143  341  0   8   \n",
       "2018-01-15/2018-01-21   8   2   55  38  24  105  10   4   97   12  124  3   6   \n",
       "2018-01-22/2018-01-28  11   3   40   1  18   60   8   6   69   13   85  0  10   \n",
       "2018-01-29/2018-02-04  17  18   44  19  20   57   3   9   60   14   29  1   3   \n",
       "2018-02-05/2018-02-11   2   9   42  19   4    1   6  17   34   13   47  0   9   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                 02  03  04  05 06 07 08 09 10 11 12 13 14 15 16 17 18   \n",
       "2018-01-08/2018-01-14  2   9  30   0  2  1  0  0  0  4  1  0  0  0  1  1  2   \n",
       "2018-01-15/2018-01-21  2   6   5   0  0  2  0  0  0  0  0  1  0  0  0  2  1   \n",
       "2018-01-22/2018-01-28  2  19  59  16  4  2  4  0  3  2  6  2  0  0  4  1  3   \n",
       "2018-01-29/2018-02-04  7  11  32   3  1  5  2  2  0  1  2  0  0  0  1  3  0   \n",
       "2018-02-05/2018-02-11  1   6  21   4  2  3  3  0  0  0  1  0  0  0  0  2  0   \n",
       "\n",
       "Region                         TU                                              \\\n",
       "Event                 19 20    01    02    03    04    05   06   07   08   09   \n",
       "2018-01-08/2018-01-14  2  0  1197   589   783  2625  1002  219  247  453  141   \n",
       "2018-01-15/2018-01-21  3  0  2190   815  1000  3727  1290  285  361  550  185   \n",
       "2018-01-22/2018-01-28  0  0  2785  1415  1128  4095  1690  307  419  665  215   \n",
       "2018-01-29/2018-02-04  0  0  1645   672   925  3294   971  407  335  536  166   \n",
       "2018-02-05/2018-02-11  0  0  1643   736  1311  4330  1129  360  308  507  155   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   10    11   12    13   14   15   16   17   18    19   \n",
       "2018-01-08/2018-01-14  182   910  295   289   55  112  128  666  125   750   \n",
       "2018-01-15/2018-01-21  265  1469  480  1109  181  224  235  723  240  1970   \n",
       "2018-01-22/2018-01-28  347  1880  545   736  283  188  253  885  384  3052   \n",
       "2018-01-29/2018-02-04  249  1468  373   465  106  102  147  904  386  2226   \n",
       "2018-02-05/2018-02-11  206  1292  388   433  137  118  202  897  233  1788   \n",
       "\n",
       "Region                    TV                                                   \\\n",
       "Event                  20 01 02 03  04 05 06 07 08 09 10 11 12 13 14 15 16 17   \n",
       "2018-01-08/2018-01-14   0  0  0  1   0  5  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21   9  1  3  0   2  2  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  21  0  0  3   0  2  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  16  0  1  0  12  5  0  3  0  0  0  0  1  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  16  9  2  6  10  5  0  6  0  5  1  0  1  3  0  0  0  0   \n",
       "\n",
       "Region                           TW                                         \\\n",
       "Event                 18 19 20   01   02   03   04   05   06   07   08  09   \n",
       "2018-01-08/2018-01-14  0  0  0  325  212  174  459  231   69   74   56  41   \n",
       "2018-01-15/2018-01-21  0  0  0  232  134  115  430  170   44   54   47  27   \n",
       "2018-01-22/2018-01-28  0  0  0  283  112  176  441  151   81   57   46  41   \n",
       "2018-01-29/2018-02-04  0  0  0  290  217  230  758  208   84   62  101  51   \n",
       "2018-02-05/2018-02-11  0  1  0  426  182  246  672  299  106  259  104  85   \n",
       "\n",
       "Region                                                               TX      \\\n",
       "Event                  10   11  12  13  14  15  16   17  18   19 20  01  02   \n",
       "2018-01-08/2018-01-14  36  155  67  26  37  15  16   63  19   44  0  23   8   \n",
       "2018-01-15/2018-01-21  25   99  53  31  20  11  19   92   7   76  0  47   7   \n",
       "2018-01-22/2018-01-28  19  138  67  25  16   4  13   87  10   74  0  33   8   \n",
       "2018-01-29/2018-02-04  33  144  56  52  18  15  93  124   7   68  0  25  18   \n",
       "2018-02-05/2018-02-11  22  145  47  37  22  18  24  102  34  100  0  21  10   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                  03   04  05  06  07  08 09  10  11 12 13 14 15 16  17   \n",
       "2018-01-08/2018-01-14  18   83  39   5   0   3  1   3   3  6  0  1  0  3  22   \n",
       "2018-01-15/2018-01-21  22  140  34  10   3   6  5   6   8  1  0  0  1  0  22   \n",
       "2018-01-22/2018-01-28  11  127  60  10  10  10  2  13  10  0  1  1  0  0   8   \n",
       "2018-01-29/2018-02-04  36  181  53  20   0   9  4   6   7  1  0  4  0  4   6   \n",
       "2018-02-05/2018-02-11  10  103  31   4   2   3  0   1   7  1  0  2  1  1   3   \n",
       "\n",
       "Region                            TZ                                          \\\n",
       "Event                 18  19 20   01   02   03   04   05  06  07  08  09  10   \n",
       "2018-01-08/2018-01-14  3   5  0  183  110  109  415  122  41  22  36  20  16   \n",
       "2018-01-15/2018-01-21  0   6  0  166   72   97  291   83  30  63  22  27  30   \n",
       "2018-01-22/2018-01-28  2  19  1  185   86   78  349  116  30  60  29  37  29   \n",
       "2018-01-29/2018-02-04  1  15  0  149  119   78  246   90  20  56  43  15  19   \n",
       "2018-02-05/2018-02-11  0   3  0  183  120   72  256  135  41  30  55   9  16   \n",
       "\n",
       "Region                                                         UG            \\\n",
       "Event                  11  12  13  14 15  16   17  18  19 20   01   02   03   \n",
       "2018-01-08/2018-01-14  74  16  33   5  0   7  104   9  72  0  497  239  121   \n",
       "2018-01-15/2018-01-21  86  33  19   9  0  11  106  28  79  0  454  243  118   \n",
       "2018-01-22/2018-01-28  83  28  31  14  0   9  103  13  92  0  522  284  182   \n",
       "2018-01-29/2018-02-04  65  21  13   5  1   4   56   8  44  0  469  283  155   \n",
       "2018-02-05/2018-02-11  82  17  24   6  4  13   71   9  70  0  593  265  206   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                    04   05   06   07   08  09  10   11  12  13  14  15   \n",
       "2018-01-08/2018-01-14   718  214   77  104  132  44  37  215  89  22  19   5   \n",
       "2018-01-15/2018-01-21   765  222  100  138  180  52  35  319  89  60  13  12   \n",
       "2018-01-22/2018-01-28   898  344   74  114  130  74  77  280  78  37  26  14   \n",
       "2018-01-29/2018-02-04  1022  291   80  182  109  38  73  219  63  73  19   6   \n",
       "2018-02-05/2018-02-11   952  311  115  184  166  78  64  260  85  46  11   8   \n",
       "\n",
       "Region                                        UK                           \\\n",
       "Event                  16   17  18   19 20    01    02    03     04    05   \n",
       "2018-01-08/2018-01-14  35  153  57  185  0  8128  4171  4051  13385  4880   \n",
       "2018-01-15/2018-01-21  20  240  63  170  0  8290  4441  4685  13545  4417   \n",
       "2018-01-22/2018-01-28  29  256  65  218  3  8418  4098  4274  14483  4710   \n",
       "2018-01-29/2018-02-04  41  225  55  214  1  8294  4360  4356  12954  4495   \n",
       "2018-02-05/2018-02-11  19  210  62  128  0  8455  4561  4283  13050  4593   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                    06    07    08    09   10    11    12    13   14   \n",
       "2018-01-08/2018-01-14  1200  1719  2080  1135  676  3696  1454   892  356   \n",
       "2018-01-15/2018-01-21  1217  1869  1859  1299  733  3437  1327   932  346   \n",
       "2018-01-22/2018-01-28  1054  1831  1917  1060  686  3656  1295  1060  356   \n",
       "2018-01-29/2018-02-04  1252  1902  1797  1284  737  3570  1301  1014  394   \n",
       "2018-02-05/2018-02-11  1214  1999  2206  1312  777  3709  1509  1016  470   \n",
       "\n",
       "Region                                                 UP                  \\\n",
       "Event                   15   16    17   18    19  20   01   02   03    04   \n",
       "2018-01-08/2018-01-14  120  613  2450  702  2402  27  516  174  239   822   \n",
       "2018-01-15/2018-01-21  114  499  2323  848  2345  13  579  222  345  1039   \n",
       "2018-01-22/2018-01-28  122  490  2294  762  2428  11  564  176  393  1071   \n",
       "2018-01-29/2018-02-04   84  385  2466  827  2371   7  425  225  208  1078   \n",
       "2018-02-05/2018-02-11  115  488  2882  841  2248  16  583  248  299   837   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   05   06   07   08  09  10   11   12  13  14  15  16   \n",
       "2018-01-08/2018-01-14  290  128  108  118  58  41  190   91  46  28  15  60   \n",
       "2018-01-15/2018-01-21  340  120  121  128  41  52  231  155  73  43  38  65   \n",
       "2018-01-22/2018-01-28  289  146   99  124  48  39  188   96  59  30  27  60   \n",
       "2018-01-29/2018-02-04  299  145  117  131  66  42  223   80  40  37  26  40   \n",
       "2018-02-05/2018-02-11  432  112  129  126  76  53  298  128  43  33  17  21   \n",
       "\n",
       "Region                                      US                              \\\n",
       "Event                   17  18   19  20     01     02     03     04     05   \n",
       "2018-01-08/2018-01-14  147  41  320   1  61559  33163  30229  87743  30530   \n",
       "2018-01-15/2018-01-21  158  55  381   3  60048  31165  28045  90734  30159   \n",
       "2018-01-22/2018-01-28  178  61  383   1  61676  32423  29077  91994  29578   \n",
       "2018-01-29/2018-02-04  112  20  296  18  61920  32114  29224  91856  29239   \n",
       "2018-02-05/2018-02-11  211  28  310   4  58154  31163  27435  87506  28738   \n",
       "\n",
       "Region                                                                     \\\n",
       "Event                    06     07     08    09    10     11     12    13   \n",
       "2018-01-08/2018-01-14  8138  16698  16986  9693  4860  27084  11439  5413   \n",
       "2018-01-15/2018-01-21  8178  16976  15079  8982  4148  26144  10801  5161   \n",
       "2018-01-22/2018-01-28  8423  17333  15835  9053  4421  26226  10984  5237   \n",
       "2018-01-29/2018-02-04  8608  16672  17134  9658  4351  25975  10960  5088   \n",
       "2018-02-05/2018-02-11  8340  16209  14962  8736  4217  25077  10227  4775   \n",
       "\n",
       "Region                                                          UV          \\\n",
       "Event                    14   15    16     17    18     19  20  01  02  03   \n",
       "2018-01-08/2018-01-14  2056  514  3657  22324  6238  22093  72  17   8   9   \n",
       "2018-01-15/2018-01-21  3145  671  3376  21459  7305  22013  41  17  23   8   \n",
       "2018-01-22/2018-01-28  2379  513  3631  22263  7108  24139  79  28  19  19   \n",
       "2018-01-29/2018-02-04  1976  559  3245  22586  6915  23598  74  36  10  19   \n",
       "2018-02-05/2018-02-11  2152  605  2706  21819  6488  23614  79  22  11   5   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   04  05 06  07 08 09  10 11  12 13 14 15 16  17 18  19   \n",
       "2018-01-08/2018-01-14   51   5  3   6  3  9   0  5   1  2  0  0  1   8  0  15   \n",
       "2018-01-15/2018-01-21  344  27  7   7  9  3  12  7   5  0  5  0  2   7  2  22   \n",
       "2018-01-22/2018-01-28  102  24  3   7  6  3  14  9  10  3  0  1  4  12  0  70   \n",
       "2018-01-29/2018-02-04   97  27  0   6  2  2   5  5   4  0  0  0  3   6  6  34   \n",
       "2018-02-05/2018-02-11   68   8  6  12  2  1   5  9   3  0  2  1  0   3  5  29   \n",
       "\n",
       "Region                    UY                                                \\\n",
       "Event                 20  01  02  03   04   05  06  07  08 09 10  11 12 13   \n",
       "2018-01-08/2018-01-14  0  26  41  21   58   16   6  11  35  3  0  13  6  1   \n",
       "2018-01-15/2018-01-21  0  35  13  17   90   18  11  10   3  6  0   6  8  1   \n",
       "2018-01-22/2018-01-28  0  15   8   6   54   33  18   1  10  5  2   6  2  0   \n",
       "2018-01-29/2018-02-04  0  35  13  11  132   19   2   6   5  0  2  10  9  2   \n",
       "2018-02-05/2018-02-11  0  17   5  35   87  106  11   6  11  1  2   6  1  0   \n",
       "\n",
       "Region                                        UZ                               \\\n",
       "Event                  14 15 16 17 18 19 20   01  02  03   04  05  06  07  08   \n",
       "2018-01-08/2018-01-14   1  0  0  6  0  6  0   56  17  36  151  64  22   6  15   \n",
       "2018-01-15/2018-01-21   0  0  0  3  1  0  0   85  32  43  312  36  23  17  10   \n",
       "2018-01-22/2018-01-28  12  0  0  6  1  7  0   63  25  44  210  88  16  12   8   \n",
       "2018-01-29/2018-02-04   5  1  6  5  2  3  0  103  35  42  204  77  36   9  12   \n",
       "2018-02-05/2018-02-11   5  0  0  8  0  1  0   52  15  67  274  62  66  11  31   \n",
       "\n",
       "Region                                                            VC          \\\n",
       "Event                 09  10  11  12 13 14 15  16  17  18  19 20  01  02  03   \n",
       "2018-01-08/2018-01-14  0   1  12   7  6  0  3   2   8   8  27  0  34  16  11   \n",
       "2018-01-15/2018-01-21  5   6  18   5  2  7  0  14  28  10  53  0  16  11  14   \n",
       "2018-01-22/2018-01-28  3  10  25   6  3  2  1  15  19   4  11  0  37  20  11   \n",
       "2018-01-29/2018-02-04  9   8  64  10  5  0  0   6  33   3  32  0  27   9   6   \n",
       "2018-02-05/2018-02-11  4  22  13   3  5  1  1   6  34   6  46  0  24   9  30   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  04  05  06  07 08 09 10 11 12 13 14 15 16  17 18  19   \n",
       "2018-01-08/2018-01-14  41  64   2   1  4  1  1  4  1  0  1  0  0   1  0   2   \n",
       "2018-01-15/2018-01-21  52   7   3   3  6  0  0  1  4  2  0  0  0   3  0   2   \n",
       "2018-01-22/2018-01-28  27  21   4   2  8  2  1  9  6  0  0  0  0  11  6  10   \n",
       "2018-01-29/2018-02-04  45  60   1   1  8  1  1  5  0  0  0  0  0   4  0   8   \n",
       "2018-02-05/2018-02-11  51  16  10  11  4  1  0  7  6  0  0  0  0   9  2   4   \n",
       "\n",
       "Region                     VE                                                \\\n",
       "Event                 20   01   02   03   04   05   06  07  08  09  10   11   \n",
       "2018-01-08/2018-01-14  0  250  142  129  469   78   66  47  91  66  19  106   \n",
       "2018-01-15/2018-01-21  0  312  167   89  320  107   58  62  65  46  42  263   \n",
       "2018-01-22/2018-01-28  0  282  161  118  430  108   34  45  63  34  41  302   \n",
       "2018-01-29/2018-02-04  0  369  166  212  598  197   77  60  65  19  32  318   \n",
       "2018-02-05/2018-02-11  0  378  278  115  689  211  133  66  68  63  40  250   \n",
       "\n",
       "Region                                                        VI             \\\n",
       "Event                   12  13  14  15   16   17  18   19 20  01  02 03  04   \n",
       "2018-01-08/2018-01-14   54  16  26   0   43  135  36  109  0  13  12  7   8   \n",
       "2018-01-15/2018-01-21   90  44  44   5   56  166  40  345  0  19  10  2  42   \n",
       "2018-01-22/2018-01-28  117  63  40   6  129  195  58  199  0  11  13  0  14   \n",
       "2018-01-29/2018-02-04  106  55  31   1   70  147  49   84  0  15   1  9   6   \n",
       "2018-02-05/2018-02-11  110  63  16  14   93  184  46  151  0   8   7  8   7   \n",
       "\n",
       "Region                                                                   VM  \\\n",
       "Event                  05 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   01   \n",
       "2018-01-08/2018-01-14   4  2  8  2  0  0  7  1  2  0  0  0  0  0  4  0  558   \n",
       "2018-01-15/2018-01-21   1  1  2  4  2  0  1  2  1  0  0  0  4  0  0  0  602   \n",
       "2018-01-22/2018-01-28   3  2  3  1  1  1  4  0  0  0  0  0  0  0  0  0  794   \n",
       "2018-01-29/2018-02-04  12  3  1  2  1  0  3  1  0  0  0  0  3  1  0  0  662   \n",
       "2018-02-05/2018-02-11   1  3  3  2  2  4  5  2  0  0  1  1  1  0  3  0  545   \n",
       "\n",
       "Region                                                                         \\\n",
       "Event                   02    03    04   05   06   07   08   09  10   11   12   \n",
       "2018-01-08/2018-01-14  305   648  1679  793  204  168  138   74  55  243  127   \n",
       "2018-01-15/2018-01-21  316   592  1605  549  209  217  151   79  52  277   92   \n",
       "2018-01-22/2018-01-28  399  1002  3338  995  238  235  203   86  60  301  130   \n",
       "2018-01-29/2018-02-04  296   637  1817  490  235  207  207  106  41  285  104   \n",
       "2018-02-05/2018-02-11  272   324  1623  599  215  179  165   80  36  217  109   \n",
       "\n",
       "Region                                                  VQ                  \\\n",
       "Event                  13  14  15  16   17  18   19 20  01  02  03  04  05   \n",
       "2018-01-08/2018-01-14  47  33  25  10  311  43  195  0  15  10   8  29   5   \n",
       "2018-01-15/2018-01-21  56  33  12  26  290  51  264  4  24  17  13  32  13   \n",
       "2018-01-22/2018-01-28  49  32  32  24  359  87  471  0  26  13   5  40   5   \n",
       "2018-01-29/2018-02-04  48  26  48  23  305  79  701  1  28  11  13  34  15   \n",
       "2018-02-05/2018-02-11  24  36  19  27  284  91  334  2  41  16  21  60  10   \n",
       "\n",
       "Region                                                                  VT  \\\n",
       "Event                  06  07 08 09 10  11 12 13 14 15 16  17 18 19 20  01   \n",
       "2018-01-08/2018-01-14   2   7  1  4  0   2  2  0  0  0  0   5  2  5  0  70   \n",
       "2018-01-15/2018-01-21   3  11  6  3  1   9  2  4  0  0  0   5  4  4  0  44   \n",
       "2018-01-22/2018-01-28   4   5  7  4  1  12  2  2  0  0  1   7  0  0  0  43   \n",
       "2018-01-29/2018-02-04  23  10  2  0  5   3  5  2  0  0  0   7  1  7  0  65   \n",
       "2018-02-05/2018-02-11   2  16  3  0  1   5  7  0  0  0  2  12  1  3  0  63   \n",
       "\n",
       "Region                                                                        \\\n",
       "Event                  02   03   04  05 06 07  08 09  10  11  12 13 14 15 16   \n",
       "2018-01-08/2018-01-14  34   40  108  63  6  7   6  1   6  26   5  2  1  0  2   \n",
       "2018-01-15/2018-01-21  21   21   60  29  1  6   8  1   1  17   5  1  2  0  0   \n",
       "2018-01-22/2018-01-28  14    5   84  12  1  4   8  1   2  10   5  0  0  0  5   \n",
       "2018-01-29/2018-02-04  26   12   67  49  6  9  24  3  15  32  21  3  2  0  2   \n",
       "2018-02-05/2018-02-11  31  123  216  71  3  4  19  4   5  20  23  4  3  0  1   \n",
       "\n",
       "Region                               WA                                        \\\n",
       "Event                  17 18 19 20   01  02  03   04   05  06  07  08  09  10   \n",
       "2018-01-08/2018-01-14   8  0  3  0  221  96  95  313   83  42  35  30  29   6   \n",
       "2018-01-15/2018-01-21   4  0  6  0  247  86  85  298  100  32  47  33  18  16   \n",
       "2018-01-22/2018-01-28   3  0  4  0  177  69  66  239   54  31  24  30  19  21   \n",
       "2018-01-29/2018-02-04  14  1  1  0  186  77  48  299   74  28  30  41  31   9   \n",
       "2018-02-05/2018-02-11  19  3  8  2  136  72  44  252  108  20  28  15  20   7   \n",
       "\n",
       "Region                                                       WE            \\\n",
       "Event                  11  12  13 14 15  16  17  18  19 20   01   02   03   \n",
       "2018-01-08/2018-01-14  80  36  13  2  0  23  24  14  51  0  303  171  136   \n",
       "2018-01-15/2018-01-21  62  17   5  6  4   7  23   5  40  0  252  171  173   \n",
       "2018-01-22/2018-01-28  45  17  15  1  1   9  55  14  69  0  212  126  157   \n",
       "2018-01-29/2018-02-04  52  28  10  3  4  19  77   7  28  0  316  172   96   \n",
       "2018-02-05/2018-02-11  76  14  14  9  3   1  35   8  23  0  423  189  383   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                    04   05  06   07   08  09  10   11  12  13  14  15   \n",
       "2018-01-08/2018-01-14   546  167  57  147   70  76  25  162  66  23  65   9   \n",
       "2018-01-15/2018-01-21   554  172  78  264   80  56  35  182  78  40  57   5   \n",
       "2018-01-22/2018-01-28   362  156  32  174   49  30  28  159  60  26  64   5   \n",
       "2018-01-29/2018-02-04   537  137  50  157  124  73  22  140  27  34  74   7   \n",
       "2018-02-05/2018-02-11  1220  234  26  154   64  59  24  189  38  31  84  10   \n",
       "\n",
       "Region                                      WF                                \\\n",
       "Event                  16   17   18   19 20 01 02 03 04 05 06 07 08 09 10 11   \n",
       "2018-01-08/2018-01-14  36  189   69  690  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-15/2018-01-21  67  193   68  463  1  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-22/2018-01-28  51  129   42  256  1  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-01-29/2018-02-04  24  204   96  449  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "2018-02-05/2018-02-11  24  164  142  681  0  0  0  0  0  0  0  0  0  0  0  0   \n",
       "\n",
       "Region                                            WI                           \\\n",
       "Event                 12 13 14 15 16 17 18 19 20  01  02  03  04  05 06 07 08   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  31  15  18  30  19  2  4  0   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0   2   3   0   5   4  1  0  1   \n",
       "2018-01-22/2018-01-28  0  0  0  0  0  0  0  0  0   0   4   4  31   6  2  0  0   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0   9   7   0  15   7  2  2  3   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0   5   1   4  48   7  2  0  1   \n",
       "\n",
       "Region                                                      WQ               \\\n",
       "Event                 09 10  11 12 13 14 15 16 17 18  19 20 01 02  03 04 05   \n",
       "2018-01-08/2018-01-14  1  4  10  4  0  2  0  5  0  0  16  0  2  0   0  1  0   \n",
       "2018-01-15/2018-01-21  0  1   2  4  1  0  0  0  0  0   0  0  0  0   0  0  0   \n",
       "2018-01-22/2018-01-28  2  0   2  1  1  0  0  0  0  0   5  0  1  0   0  0  0   \n",
       "2018-01-29/2018-02-04  3  0   2  2  3  0  0  0  7  1   0  0  0  0   0  0  0   \n",
       "2018-02-05/2018-02-11  0  2   2  0  0  0  0  0  2  0   0  0  4  8  10  0  0   \n",
       "\n",
       "Region                                                               WS      \\\n",
       "Event                 06 07 08 09 10 11 12 13 14 15 16 17 18 19 20   01  02   \n",
       "2018-01-08/2018-01-14  0  0  0  0  0  0  0  0  0  0  0  0  0  1  0  141  37   \n",
       "2018-01-15/2018-01-21  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  158  43   \n",
       "2018-01-22/2018-01-28  0  0  1  0  0  1  0  0  0  0  0  0  0  0  0  128  38   \n",
       "2018-01-29/2018-02-04  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  130  49   \n",
       "2018-02-05/2018-02-11  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  135  42   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                  03   04  05  06  07  08  09  10  11  12  13 14 15 16   \n",
       "2018-01-08/2018-01-14  15  147  23  11  11  17   8   3  40  14  10  5  0  3   \n",
       "2018-01-15/2018-01-21  23  162  35  11  18  24   8  15  33   7  18  3  0  3   \n",
       "2018-01-22/2018-01-28  22  254  52   7  18  21  21  11  24   5   8  2  1  1   \n",
       "2018-01-29/2018-02-04  24  136  53  10  12  15  12   3  20  22   3  0  0  1   \n",
       "2018-02-05/2018-02-11  47  175  56  16  23  14  22   6  12   5  12  0  0  3   \n",
       "\n",
       "Region                               WZ                                        \\\n",
       "Event                  17 18  19 20  01  02  03  04  05  06  07 08  09 10  11   \n",
       "2018-01-08/2018-01-14  12  4  15  0  36   9  11  31  10  11  17  6   1  4   5   \n",
       "2018-01-15/2018-01-21  19  0  15  0  25   7  15  49   2   4  10  3   2  0   5   \n",
       "2018-01-22/2018-01-28  15  1   8  0  29   9   8  65  49   3  14  1   1  0   4   \n",
       "2018-01-29/2018-02-04  19  4  10  0  23  19   8  50   1   2   2  6  10  3   6   \n",
       "2018-02-05/2018-02-11  26  8  25  0  33   2   9  48  11   3   5  1   3  2  11   \n",
       "\n",
       "Region                                               YM                      \\\n",
       "Event                 12 13 14 15 16  17 18  19 20   01   02   03   04   05   \n",
       "2018-01-08/2018-01-14  5  1  0  3  0  11  2   3  0  285  142  128  523  173   \n",
       "2018-01-15/2018-01-21  3  0  1  0  3   4  1   9  0  223  200   74  455  179   \n",
       "2018-01-22/2018-01-28  5  0  0  0  0  25  5  19  0  434  195  160  690  262   \n",
       "2018-01-29/2018-02-04  6  0  0  0  0   5  2  12  0  411  200  117  621  262   \n",
       "2018-02-05/2018-02-11  5  4  3  0  7   5  6   8  0  276  100   47  510  153   \n",
       "\n",
       "Region                                                                      \\\n",
       "Event                   06   07   08  09  10   11  12  13  14  15  16   17   \n",
       "2018-01-08/2018-01-14   44   92   91  18  34  161  44  94  40  15  25   67   \n",
       "2018-01-15/2018-01-21  129  138   92  18  24  171  53  64  18  10  19  148   \n",
       "2018-01-22/2018-01-28   75  136  145  11  31  228  39  55  28  15  31  145   \n",
       "2018-01-29/2018-02-04   82  125  105  31  38  296  51  67  51  15  28  111   \n",
       "2018-02-05/2018-02-11   31  108   63  38  42  129  47  55  15  14  25   94   \n",
       "\n",
       "Region                              ZA                                       \\\n",
       "Event                  18   19 20   01   02   03   04   05  06   07  08  09   \n",
       "2018-01-08/2018-01-14  61  599  0  402  196  110  390  196  55  104  53  21   \n",
       "2018-01-15/2018-01-21  73  420  3  273  142  179  412  173  57   58  66  19   \n",
       "2018-01-22/2018-01-28  46  638  3  380  178   98  409  143  30   71  51  25   \n",
       "2018-01-29/2018-02-04  60  840  0  242  181   76  266  133  31   59  28  34   \n",
       "2018-02-05/2018-02-11  62  539  2  277  173  128  297  183  31   77  27  36   \n",
       "\n",
       "Region                                                                ZI       \\\n",
       "Event                  10   11  12  13  14  15  16   17  18   19 20   01   02   \n",
       "2018-01-08/2018-01-14  44  157  52  15  23  10  32  236  14  120  0  725  381   \n",
       "2018-01-15/2018-01-21  28   83  28  36  15   5   9   82   9   53  0  903  351   \n",
       "2018-01-22/2018-01-28  47  116  42  30   9   0   5   86   5   27  0  902  372   \n",
       "2018-01-29/2018-02-04  29   76  16  28   5   4  11   75  17   19  0  964  450   \n",
       "2018-02-05/2018-02-11  37   95  16  24   8   2   8   53   4   32  0  722  337   \n",
       "\n",
       "Region                                                                       \\\n",
       "Event                   03    04   05   06   07   08  09   10   11   12  13   \n",
       "2018-01-08/2018-01-14  275   869  379  109  130  207  55  106  417  128  59   \n",
       "2018-01-15/2018-01-21  351  1306  377  131  157  198  90  107  356   98  62   \n",
       "2018-01-22/2018-01-28  423  1263  393  129  165  193  70  100  338  134  82   \n",
       "2018-01-29/2018-02-04  475  1450  487  110  146  199  75  100  389   85  80   \n",
       "2018-02-05/2018-02-11  349   999  453  118  152  152  55   89  315  204  51   \n",
       "\n",
       "Region                                               \n",
       "Event                  14  15  16   17   18   19 20  \n",
       "2018-01-08/2018-01-14  35  12  45  265   79  192  2  \n",
       "2018-01-15/2018-01-21  32  18  45  210   87  192  2  \n",
       "2018-01-22/2018-01-28  30  13  51  245   45  191  9  \n",
       "2018-01-29/2018-02-04  39  15  26  264  139  175  3  \n",
       "2018-02-05/2018-02-11  41  11  31  205   73  150  0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "truth.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d95fa58",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt=truth.index[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56911103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2018-02-05/2018-02-11', 'W-SUN')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "52ba3654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2018-02-12/2018-02-18', 'W-SUN')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "869c9675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2018-02-19/2018-02-25', 'W-SUN')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bf06f2c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2018-02-26/2018-03-04', 'W-SUN')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt+3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37462f90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Period('2018-03-05/2018-03-11', 'W-SUN')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt+4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e1adc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd2022",
   "language": "python",
   "name": "atd2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
