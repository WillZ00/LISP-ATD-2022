{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fc83ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import atd2022\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from atd_informer.atd_informer_V2 import ATD_Informer_V2\n",
    "from utils.tools import dotdict\n",
    "from atd_wrapper_V2 import InformerForcaster_V2\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    MeanAbsoluteScaledError,\n",
    "    MeanSquaredError,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ade0840",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7e00f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557ef19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7fbef327",
   "metadata": {},
   "source": [
    "# Testing ATD Protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc6ba570",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = atd2022.io.read_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e9538b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "args.enc_in = 5200 # encoder input size\n",
    "args.dec_in = 5200 # decoder input size\n",
    "args.c_out = 5200 # output size\n",
    "args.factor = 6 # probsparse attn factor\n",
    "args.d_model = 1500 # dimension of model\n",
    "args.n_heads = 18 # num of heads\n",
    "args.e_layers = 6 # num of encoder layers\n",
    "args.d_layers = 6 # num of decoder layers\n",
    "args.d_ff = 3000 # dimension of fcn in model\n",
    "args.dropout = 0.005 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = True # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'w'\n",
    "args.inverse=False\n",
    "args.timeenc=1\n",
    "\n",
    "\n",
    "args.use_gpu=True\n",
    "\n",
    "#args.cols=1\n",
    "args.checkpoints = \"/scratch/wzong/atd_tmp\"\n",
    "\n",
    "\n",
    "\n",
    "args.seq_len=90\n",
    "args.label_len=25\n",
    "args.pred_len=25\n",
    "\n",
    "\n",
    "args.batch_size = 32\n",
    "args.learning_rate = 0.001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type2'\n",
    "args.use_amp = False\n",
    "\n",
    "args.itr=1\n",
    "args.train_epochs=20\n",
    "args.patience=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef0033e",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = InformerForcaster_V2(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6303fab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:None\n",
      "Use GPU: cuda:None\n",
      ">>>>>>>start training : None_None_ftNone_sl90_ll25_pl25_dm1500_nh18_el6_dl6_df3000_atprob_fc6_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "Epoch: 1 cost time: 0.925755500793457\n",
      "Epoch: 1, Steps: 2 | Train Loss: 4893475.5000000\n",
      "Validation loss decreased (inf --> 4893475.500000).  Saving model ...\n",
      "Epoch: 2 cost time: 0.8893454074859619\n",
      "Epoch: 2, Steps: 2 | Train Loss: 4891720.5000000\n",
      "Validation loss decreased (4893475.500000 --> 4891720.500000).  Saving model ...\n",
      "Epoch: 3 cost time: 0.9147562980651855\n",
      "Epoch: 3, Steps: 2 | Train Loss: 4890465.5000000\n",
      "Validation loss decreased (4891720.500000 --> 4890465.500000).  Saving model ...\n",
      "Epoch: 4 cost time: 0.9834203720092773\n",
      "Epoch: 4, Steps: 2 | Train Loss: 4889153.0000000\n",
      "Validation loss decreased (4890465.500000 --> 4889153.000000).  Saving model ...\n",
      "Epoch: 5 cost time: 1.0104670524597168\n",
      "Epoch: 5, Steps: 2 | Train Loss: 4887764.5000000\n",
      "Validation loss decreased (4889153.000000 --> 4887764.500000).  Saving model ...\n",
      "Epoch: 6 cost time: 0.9333093166351318\n",
      "Epoch: 6, Steps: 2 | Train Loss: 4886332.2500000\n",
      "Validation loss decreased (4887764.500000 --> 4886332.250000).  Saving model ...\n",
      "Epoch: 7 cost time: 0.9134430885314941\n",
      "Epoch: 7, Steps: 2 | Train Loss: 4884851.5000000\n",
      "Validation loss decreased (4886332.250000 --> 4884851.500000).  Saving model ...\n",
      "Epoch: 8 cost time: 0.9261741638183594\n",
      "Epoch: 8, Steps: 2 | Train Loss: 4883337.7500000\n",
      "Validation loss decreased (4884851.500000 --> 4883337.750000).  Saving model ...\n",
      "Epoch: 9 cost time: 0.9398155212402344\n",
      "Epoch: 9, Steps: 2 | Train Loss: 4881795.5000000\n",
      "Validation loss decreased (4883337.750000 --> 4881795.500000).  Saving model ...\n",
      "Epoch: 10 cost time: 0.9148354530334473\n",
      "Epoch: 10, Steps: 2 | Train Loss: 4880226.0000000\n",
      "Validation loss decreased (4881795.500000 --> 4880226.000000).  Saving model ...\n",
      "Updating learning rate to 0.00098\n",
      "Epoch: 11 cost time: 0.9060108661651611\n",
      "Epoch: 11, Steps: 2 | Train Loss: 4878646.7500000\n",
      "Validation loss decreased (4880226.000000 --> 4878646.750000).  Saving model ...\n",
      "Epoch: 12 cost time: 0.9391086101531982\n",
      "Epoch: 12, Steps: 2 | Train Loss: 4877070.0000000\n",
      "Validation loss decreased (4878646.750000 --> 4877070.000000).  Saving model ...\n",
      "Epoch: 13 cost time: 0.9220502376556396\n",
      "Epoch: 13, Steps: 2 | Train Loss: 4875478.2500000\n",
      "Validation loss decreased (4877070.000000 --> 4875478.250000).  Saving model ...\n",
      "Epoch: 14 cost time: 0.8890402317047119\n",
      "Epoch: 14, Steps: 2 | Train Loss: 4873870.0000000\n",
      "Validation loss decreased (4875478.250000 --> 4873870.000000).  Saving model ...\n",
      "Epoch: 15 cost time: 0.9179120063781738\n",
      "Epoch: 15, Steps: 2 | Train Loss: 4872246.7500000\n",
      "Validation loss decreased (4873870.000000 --> 4872246.750000).  Saving model ...\n",
      "Epoch: 16 cost time: 0.9215176105499268\n",
      "Epoch: 16, Steps: 2 | Train Loss: 4870611.5000000\n",
      "Validation loss decreased (4872246.750000 --> 4870611.500000).  Saving model ...\n",
      "Epoch: 17 cost time: 0.9266676902770996\n",
      "Epoch: 17, Steps: 2 | Train Loss: 4868961.7500000\n",
      "Validation loss decreased (4870611.500000 --> 4868961.750000).  Saving model ...\n",
      "Epoch: 18 cost time: 0.9281582832336426\n",
      "Epoch: 18, Steps: 2 | Train Loss: 4867299.7500000\n",
      "Validation loss decreased (4868961.750000 --> 4867299.750000).  Saving model ...\n",
      "Epoch: 19 cost time: 0.9354603290557861\n",
      "Epoch: 19, Steps: 2 | Train Loss: 4865625.7500000\n",
      "Validation loss decreased (4867299.750000 --> 4865625.750000).  Saving model ...\n",
      "Epoch: 20 cost time: 0.9324841499328613\n",
      "Epoch: 20, Steps: 2 | Train Loss: 4863939.0000000\n",
      "Validation loss decreased (4865625.750000 --> 4863939.000000).  Saving model ...\n",
      "Updating learning rate to 0.00095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<atd_wrapper_V2.InformerForcaster_V2 at 0x7f029b5ea130>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp.fit(truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef3328d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11.075557   4.925238   9.84288   ... 58.52229   59.10325    1.252343 ]\n",
      " [11.075587   4.9252944  9.842885  ... 58.522232  59.103256   1.2522944]\n",
      " [11.075691   4.9253592  9.842965  ... 58.522186  59.10328    1.2523042]\n",
      " ...\n",
      " [11.075613   4.9252825  9.8429575 ... 58.522236  59.103264   1.2524289]\n",
      " [11.075799   4.9254265  9.84302   ... 58.5222    59.103157   1.2522799]\n",
      " [11.075542   4.9253078  9.842912  ... 58.522182  59.10333    1.2523167]]\n",
      "<class 'numpy.ndarray'>\n",
      "(25, 5200)\n"
     ]
    }
   ],
   "source": [
    "test=exp.predict([1,2,3,4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7f837a",
   "metadata": {},
   "source": [
    "# Full Scaled Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f98d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "\n",
    "args.enc_in = 5200 # encoder input size\n",
    "args.dec_in = 5200 # decoder input size\n",
    "args.c_out = 5200 # output size\n",
    "args.factor = 6 # probsparse attn factor\n",
    "args.d_model = 4096 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 5 # num of encoder layers\n",
    "args.d_layers = 5 # num of decoder layers\n",
    "args.d_ff = 4096 # dimension of fcn in model\n",
    "args.dropout = 0.005 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = True # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'w'\n",
    "args.inverse=False\n",
    "args.timeenc=1\n",
    "\n",
    "\n",
    "args.use_gpu=True\n",
    "\n",
    "#args.cols=1\n",
    "args.checkpoints = \"/scratch/wzong/atd_tmp\"\n",
    "\n",
    "\n",
    "\n",
    "args.seq_len=60\n",
    "args.label_len=20\n",
    "args.pred_len=1\n",
    "\n",
    "\n",
    "args.batch_size = 60\n",
    "args.learning_rate = 0.001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type2'\n",
    "args.use_amp = False\n",
    "\n",
    "args.itr=1\n",
    "args.train_epochs=2\n",
    "args.patience=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8d1ba8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lispDL = InformerForcaster_V2(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95029219",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load data\n",
    "truth = atd2022.io.read_csv()\n",
    "\n",
    "# Subset the data for the sake of making this fast.\n",
    "# Remove if you want to run the example on the entire dataset.\n",
    "truth = truth.head(108)\n",
    "\n",
    "# Experiment Parameters\n",
    "window = 100\n",
    "num_predict = 4\n",
    "gap = 0\n",
    "slide = 1\n",
    "\n",
    "# Create a dataset `Splitter` object for generating train/test splits\n",
    "splitter = atd2022.backtest.Splitter(\n",
    "    truth,\n",
    "    window,\n",
    "    num_predict,\n",
    "    gap,\n",
    "    slide,\n",
    "    expanding=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f79ab47",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Populate a list of models that support the `atd2022.forecasters.Forecaster` protocol\n",
    "# with which will we generate historical forecasts\n",
    "models = [\n",
    "    lispDL,\n",
    "    atd2022.forecasters.PredictMeanForecaster(),\n",
    "    atd2022.forecasters.ExponentiallyWeightedMovingAverage(),\n",
    "]\n",
    "\n",
    "# Compute historical forecasts for all models\n",
    "predictions = [\n",
    "    atd2022.backtest.historical_forecast(model, splitter, verbose=True)\n",
    "    for model in models\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd2022",
   "language": "python",
   "name": "atd2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
