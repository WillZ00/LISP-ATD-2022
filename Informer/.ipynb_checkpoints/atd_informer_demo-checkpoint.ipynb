{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d1a5591",
   "metadata": {},
   "source": [
    "# Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92827dcf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import atd2022\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from atd_informer import atd_informer\n",
    "from utils.tools import dotdict\n",
    "from atd_wrapper import InformerForcaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08dd46b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'atd_informer.atd_informer' from '/Users/will/LISP-ATD-2022/Informer/atd_informer/atd_informer.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(atd_informer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc9b8a5",
   "metadata": {},
   "source": [
    "# Model Parameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d86b3b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "\n",
    "args.enc_in = 20 # encoder input size\n",
    "args.dec_in = 20 # decoder input size\n",
    "args.c_out = 20 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'w'\n",
    "args.inverse=False\n",
    "args.timeenc=0\n",
    "\n",
    "args.cols=1\n",
    "args.checkpoints = \"/Users/will/Desktop/tmp\"\n",
    "\n",
    "\n",
    "\n",
    "args.seq_len=5\n",
    "args.label_len=3\n",
    "args.pred_len=1\n",
    "\n",
    "\n",
    "args.batch_size = 1\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False\n",
    "\n",
    "args.itr=1\n",
    "args.train_epochs=1\n",
    "args.patience=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64fedfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "informer = atd_informer.ATD_Informer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a523c5",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d38fc8f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'W'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m setting \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ft\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_sl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_ll\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_pl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dm\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_nh\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_el\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dl\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_df\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_at\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_fc\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_eb\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_dt\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_mx\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mmodel, args\u001b[38;5;241m.\u001b[39mdata, args\u001b[38;5;241m.\u001b[39mfeatures, \n\u001b[1;32m      4\u001b[0m             args\u001b[38;5;241m.\u001b[39mseq_len, args\u001b[38;5;241m.\u001b[39mlabel_len, args\u001b[38;5;241m.\u001b[39mpred_len,\n\u001b[1;32m      5\u001b[0m             args\u001b[38;5;241m.\u001b[39md_model, args\u001b[38;5;241m.\u001b[39mn_heads, args\u001b[38;5;241m.\u001b[39me_layers, args\u001b[38;5;241m.\u001b[39md_layers, args\u001b[38;5;241m.\u001b[39md_ff, args\u001b[38;5;241m.\u001b[39mattn, args\u001b[38;5;241m.\u001b[39mfactor, args\u001b[38;5;241m.\u001b[39membed, args\u001b[38;5;241m.\u001b[39mdistil, args\u001b[38;5;241m.\u001b[39mmix, args\u001b[38;5;241m.\u001b[39mdes, ii)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# set experiments\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m exp \u001b[38;5;241m=\u001b[39m \u001b[43minformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# train\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m>>>>>>>start training : \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m>>>>>>>>>>>>>>>>>>>>>>>>>>\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(setting))\n",
      "File \u001b[0;32m~/LISP-ATD-2022/Informer/atd_informer/atd_informer.py:21\u001b[0m, in \u001b[0;36mATD_Informer.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mATD_Informer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/LISP-ATD-2022/Informer/atd_informer/exp_basic.py:9\u001b[0m, in \u001b[0;36mExp_Basic.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_acquire_device()\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m~/LISP-ATD-2022/Informer/atd_informer/atd_informer.py:26\u001b[0m, in \u001b[0;36mATD_Informer._build_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_build_model\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;66;03m#print(self.args.dropout)\u001b[39;00m\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;66;03m#print(type(self.args.dropout))\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mInformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menc_in\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdec_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpred_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43me_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_ff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistil\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/LISP-ATD-2022/Informer/models/model.py:23\u001b[0m, in \u001b[0;36mInformer.__init__\u001b[0;34m(self, enc_in, dec_in, c_out, seq_len, label_len, out_len, factor, d_model, n_heads, e_layers, d_layers, d_ff, dropout, attn, embed, freq, activation, output_attention, distil, mix, device)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_attention \u001b[38;5;241m=\u001b[39m output_attention\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menc_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mDataEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43menc_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdec_embedding \u001b[38;5;241m=\u001b[39m DataEmbedding(dec_in, d_model, embed, freq, dropout)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Attention\u001b[39;00m\n",
      "File \u001b[0;32m~/LISP-ATD-2022/Informer/models/embed.py:113\u001b[0m, in \u001b[0;36mDataEmbedding.__init__\u001b[0;34m(self, c_in, d_model, embed_type, freq, dropout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_embedding \u001b[38;5;241m=\u001b[39m TokenEmbedding(c_in\u001b[38;5;241m=\u001b[39mc_in, d_model\u001b[38;5;241m=\u001b[39md_model)\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mposition_embedding \u001b[38;5;241m=\u001b[39m PositionalEmbedding(d_model\u001b[38;5;241m=\u001b[39md_model)\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtemporal_embedding \u001b[38;5;241m=\u001b[39m TemporalEmbedding(d_model\u001b[38;5;241m=\u001b[39md_model, embed_type\u001b[38;5;241m=\u001b[39membed_type, freq\u001b[38;5;241m=\u001b[39mfreq) \u001b[38;5;28;01mif\u001b[39;00m embed_type\u001b[38;5;241m!=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeF\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mTimeFeatureEmbedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43md_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(p\u001b[38;5;241m=\u001b[39mdropout)\n",
      "File \u001b[0;32m~/LISP-ATD-2022/Informer/models/embed.py:96\u001b[0m, in \u001b[0;36mTimeFeatureEmbedding.__init__\u001b[0;34m(self, d_model, embed_type, freq)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28msuper\u001b[39m(TimeFeatureEmbedding, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     95\u001b[0m freq_map \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m6\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m3\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;241m3\u001b[39m}\n\u001b[0;32m---> 96\u001b[0m d_inp \u001b[38;5;241m=\u001b[39m \u001b[43mfreq_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(d_inp, d_model)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'W'"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "    setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                args.seq_len, args.label_len, args.pred_len,\n",
    "                args.d_model, args.n_heads, args.e_layers, args.d_layers, args.d_ff, args.attn, args.factor, args.embed, args.distil, args.mix, args.des, ii)\n",
    "\n",
    "    # set experiments\n",
    "    exp = informer(args)\n",
    "    \n",
    "    # train\n",
    "    print('>>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>'.format(setting))\n",
    "    exp.train(setting)\n",
    "    \n",
    "    # test\n",
    "    print('>>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<'.format(setting))\n",
    "    exp.test(setting)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580a47a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bdde6edb",
   "metadata": {},
   "source": [
    "# Generate Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31c023c",
   "metadata": {},
   "source": [
    "#setting = '{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_at{}_fc{}_eb{}_dt{}_mx{}_{}_{}'.format(args.model, args.data, args.features, \n",
    "                                                                                                     args.seq_len, args.label_len, args.pred_len,\n",
    "                                                                                                     args.d_model, args.n_heads, args.e_layers, args.d_layers,\n",
    "                                                                                                     args.d_ff, args.attn, args.factor, args.embed, args.distil, \n",
    "                                                                                                     args.mix, args.des)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71a6966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "setting= \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64271710",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeenc, freq 1 W\n",
      "check data_x shape (30, 20)\n",
      "check data_y shape (30, 20)\n",
      "data_x [[  1   1   4  11   4   2   1   1   0   0   1   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [ 19   5   3   5   8   0   0   5   0   0   1   2   0   0   0   0   0   0\n",
      "    2   0]\n",
      " [ 14   1  10  11   4   7   2   9   1   1   1   4   1   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 14   3   1  21   6   6   2   3   3   0   5   4   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]\n",
      " [ 36   9  13  97  16   3   5   4  10   0   1   0   1   0   0   0  12   3\n",
      "   13   0]\n",
      " [ 12   4   0  32   4   4   0   2   0   1   2   0   0   0   0   0   3   0\n",
      "    4   0]\n",
      " [ 10   4   4  21   4   1   5   1   0   3   4   1   3   0   0   2   1   0\n",
      "    4   0]\n",
      " [ 13   5   1  14   1   4   2   0   0   1   0   0   0   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 14   5   5  12   3   0   5   4  13   2   0   1   3   0   0   0   3   0\n",
      "    2   0]\n",
      " [  6   2   6   8   7   0  12   0   4   1   1   3   0   0   0   1   0   0\n",
      "    3   0]\n",
      " [ 24   6   1  40   2   0   6   9   4   0   5   5   0   0   0   0   9   0\n",
      "    6   0]\n",
      " [ 12   7   1  16   3   0   0   0   1   0   4   0   0   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 29   7  16  22  11   9   2   5   1   0   7   0   5   1   0   1   2   0\n",
      "    1   0]\n",
      " [  7   0   0  19   3   1   2   0   1   0   3   3   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [  8   2  19  20   4   2   3   0   2   1   2   1   1   0   0   0   2   0\n",
      "    4   0]\n",
      " [  7   1   4   3   2   3   1   0   0   0   1   1   0   0   0   0   2   0\n",
      "    5   0]\n",
      " [  4   2  10  13   4   1   1   3   1   1   3   0   0   1   0   0   5   0\n",
      "    1   0]\n",
      " [ 10   5   8  37   8   5   0   4   1   0   6   0   0   0   0   0   1   0\n",
      "    2   0]\n",
      " [  7   0   8  16   3   0   2   3   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [ 13   1   7   2   3   0   1   1   0   0   2   0   0   0   0   0   0   1\n",
      "    0   0]\n",
      " [ 11   1  10  27   2   0   2   2   5   0   5   1   0   1   0   0   1   1\n",
      "    2   0]\n",
      " [  3   3   4  21   3   4   0   5   0   0   0   0   0   0   0   0  14   0\n",
      "    1   0]\n",
      " [  0   0   0   3   0   0   2   1   0   0   0   1   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  7   5  26  22   5   0   0   1   1   0   3   1   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [ 13   9  20  85   4   2   0   2   8   0   0   0   0   0   0   1   8   0\n",
      "    0   0]\n",
      " [ 14   5   8  10   1   0   0   1   0   1   0   1   0   0   0   0   3   0\n",
      "    2   0]\n",
      " [  5   2   0  23   3   2   5   1   0   0   2   0   0   0   0   3   4   0\n",
      "    4   0]\n",
      " [  3   2   7  37   6  10   4   7   0   1   1   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  7   6   7  49   6   0   1   2   2   2   6   0   1   0   0   0   2   0\n",
      "    4   0]]\n",
      "data_y [[  1   1   4  11   4   2   1   1   0   0   1   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [ 19   5   3   5   8   0   0   5   0   0   1   2   0   0   0   0   0   0\n",
      "    2   0]\n",
      " [ 14   1  10  11   4   7   2   9   1   1   1   4   1   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 14   3   1  21   6   6   2   3   3   0   5   4   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]\n",
      " [ 36   9  13  97  16   3   5   4  10   0   1   0   1   0   0   0  12   3\n",
      "   13   0]\n",
      " [ 12   4   0  32   4   4   0   2   0   1   2   0   0   0   0   0   3   0\n",
      "    4   0]\n",
      " [ 10   4   4  21   4   1   5   1   0   3   4   1   3   0   0   2   1   0\n",
      "    4   0]\n",
      " [ 13   5   1  14   1   4   2   0   0   1   0   0   0   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 14   5   5  12   3   0   5   4  13   2   0   1   3   0   0   0   3   0\n",
      "    2   0]\n",
      " [  6   2   6   8   7   0  12   0   4   1   1   3   0   0   0   1   0   0\n",
      "    3   0]\n",
      " [ 24   6   1  40   2   0   6   9   4   0   5   5   0   0   0   0   9   0\n",
      "    6   0]\n",
      " [ 12   7   1  16   3   0   0   0   1   0   4   0   0   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 29   7  16  22  11   9   2   5   1   0   7   0   5   1   0   1   2   0\n",
      "    1   0]\n",
      " [  7   0   0  19   3   1   2   0   1   0   3   3   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [  8   2  19  20   4   2   3   0   2   1   2   1   1   0   0   0   2   0\n",
      "    4   0]\n",
      " [  7   1   4   3   2   3   1   0   0   0   1   1   0   0   0   0   2   0\n",
      "    5   0]\n",
      " [  4   2  10  13   4   1   1   3   1   1   3   0   0   1   0   0   5   0\n",
      "    1   0]\n",
      " [ 10   5   8  37   8   5   0   4   1   0   6   0   0   0   0   0   1   0\n",
      "    2   0]\n",
      " [  7   0   8  16   3   0   2   3   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [ 13   1   7   2   3   0   1   1   0   0   2   0   0   0   0   0   0   1\n",
      "    0   0]\n",
      " [ 11   1  10  27   2   0   2   2   5   0   5   1   0   1   0   0   1   1\n",
      "    2   0]\n",
      " [  3   3   4  21   3   4   0   5   0   0   0   0   0   0   0   0  14   0\n",
      "    1   0]\n",
      " [  0   0   0   3   0   0   2   1   0   0   0   1   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [  7   5  26  22   5   0   0   1   1   0   3   1   0   0   0   0   1   0\n",
      "    0   0]\n",
      " [ 13   9  20  85   4   2   0   2   8   0   0   0   0   0   0   1   8   0\n",
      "    0   0]\n",
      " [ 14   5   8  10   1   0   0   1   0   1   0   1   0   0   0   0   3   0\n",
      "    2   0]\n",
      " [  5   2   0  23   3   2   5   1   0   0   2   0   0   0   0   3   4   0\n",
      "    4   0]\n",
      " [  3   2   7  37   6  10   4   7   0   1   1   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [  7   6   7  49   6   0   1   2   2   2   6   0   1   0   0   0   2   0\n",
      "    4   0]]\n",
      "check_indices 0 5 2 6\n",
      "check_r_sizes 2 6 3 1\n",
      "x,y [[  1   1   4  11   4   2   1   1   0   0   1   1   0   0   0   0   0   0\n",
      "    0   0]\n",
      " [ 19   5   3   5   8   0   0   5   0   0   1   2   0   0   0   0   0   0\n",
      "    2   0]\n",
      " [ 14   1  10  11   4   7   2   9   1   1   1   4   1   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 14   3   1  21   6   6   2   3   3   0   5   4   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]] [[ 14   1  10  11   4   7   2   9   1   1   1   4   1   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 14   3   1  21   6   6   2   3   3   0   5   4   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]] [[ 0.03333333  0.03846154]\n",
      " [ 0.26666667  0.05769231]\n",
      " [ 0.5         0.07692308]\n",
      " [-0.3         0.09615385]\n",
      " [-0.06666667  0.11538462]] [[ 0.5         0.07692308]\n",
      " [-0.3         0.09615385]\n",
      " [-0.06666667  0.11538462]\n",
      " [ 0.16666667  0.13461538]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 1 6 3 7\n",
      "check_r_sizes 3 7 3 1\n",
      "x,y [[ 19   5   3   5   8   0   0   5   0   0   1   2   0   0   0   0   0   0\n",
      "    2   0]\n",
      " [ 14   1  10  11   4   7   2   9   1   1   1   4   1   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 14   3   1  21   6   6   2   3   3   0   5   4   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]\n",
      " [ 36   9  13  97  16   3   5   4  10   0   1   0   1   0   0   0  12   3\n",
      "   13   0]] [[ 14   3   1  21   6   6   2   3   3   0   5   4   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]\n",
      " [ 36   9  13  97  16   3   5   4  10   0   1   0   1   0   0   0  12   3\n",
      "   13   0]] [[ 0.26666667  0.05769231]\n",
      " [ 0.5         0.07692308]\n",
      " [-0.3         0.09615385]\n",
      " [-0.06666667  0.11538462]\n",
      " [ 0.16666667  0.13461538]] [[-0.3         0.09615385]\n",
      " [-0.06666667  0.11538462]\n",
      " [ 0.16666667  0.13461538]\n",
      " [ 0.4         0.15384615]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 2 7 4 8\n",
      "check_r_sizes 4 8 3 1\n",
      "x,y [[ 14   1  10  11   4   7   2   9   1   1   1   4   1   0   0   0   2   0\n",
      "    0   0]\n",
      " [ 14   3   1  21   6   6   2   3   3   0   5   4   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]\n",
      " [ 36   9  13  97  16   3   5   4  10   0   1   0   1   0   0   0  12   3\n",
      "   13   0]\n",
      " [ 12   4   0  32   4   4   0   2   0   1   2   0   0   0   0   0   3   0\n",
      "    4   0]] [[ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]\n",
      " [ 36   9  13  97  16   3   5   4  10   0   1   0   1   0   0   0  12   3\n",
      "   13   0]\n",
      " [ 12   4   0  32   4   4   0   2   0   1   2   0   0   0   0   0   3   0\n",
      "    4   0]] [[ 0.5         0.07692308]\n",
      " [-0.3         0.09615385]\n",
      " [-0.06666667  0.11538462]\n",
      " [ 0.16666667  0.13461538]\n",
      " [ 0.4         0.15384615]] [[-0.06666667  0.11538462]\n",
      " [ 0.16666667  0.13461538]\n",
      " [ 0.4         0.15384615]\n",
      " [-0.4         0.17307692]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 3 8 5 9\n",
      "check_r_sizes 5 9 3 1\n",
      "x,y [[ 14   3   1  21   6   6   2   3   3   0   5   4   0   0   0   0   3   0\n",
      "    0   0]\n",
      " [ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]\n",
      " [ 36   9  13  97  16   3   5   4  10   0   1   0   1   0   0   0  12   3\n",
      "   13   0]\n",
      " [ 12   4   0  32   4   4   0   2   0   1   2   0   0   0   0   0   3   0\n",
      "    4   0]\n",
      " [ 10   4   4  21   4   1   5   1   0   3   4   1   3   0   0   2   1   0\n",
      "    4   0]] [[36  9 13 97 16  3  5  4 10  0  1  0  1  0  0  0 12  3 13  0]\n",
      " [12  4  0 32  4  4  0  2  0  1  2  0  0  0  0  0  3  0  4  0]\n",
      " [10  4  4 21  4  1  5  1  0  3  4  1  3  0  0  2  1  0  4  0]] [[-0.3         0.09615385]\n",
      " [-0.06666667  0.11538462]\n",
      " [ 0.16666667  0.13461538]\n",
      " [ 0.4         0.15384615]\n",
      " [-0.4         0.17307692]] [[ 0.16666667  0.13461538]\n",
      " [ 0.4         0.15384615]\n",
      " [-0.4         0.17307692]\n",
      " [-0.16666667  0.19230769]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 4 9 6 10\n",
      "check_r_sizes 6 10 3 1\n",
      "x,y [[ 33   9   8 119  12   2   3  13  14   1   6   3   0   0   0   1  11   1\n",
      "   11   0]\n",
      " [ 36   9  13  97  16   3   5   4  10   0   1   0   1   0   0   0  12   3\n",
      "   13   0]\n",
      " [ 12   4   0  32   4   4   0   2   0   1   2   0   0   0   0   0   3   0\n",
      "    4   0]\n",
      " [ 10   4   4  21   4   1   5   1   0   3   4   1   3   0   0   2   1   0\n",
      "    4   0]\n",
      " [ 13   5   1  14   1   4   2   0   0   1   0   0   0   0   0   0   2   0\n",
      "    0   0]] [[12  4  0 32  4  4  0  2  0  1  2  0  0  0  0  0  3  0  4  0]\n",
      " [10  4  4 21  4  1  5  1  0  3  4  1  3  0  0  2  1  0  4  0]\n",
      " [13  5  1 14  1  4  2  0  0  1  0  0  0  0  0  0  2  0  0  0]] [[-0.06666667  0.11538462]\n",
      " [ 0.16666667  0.13461538]\n",
      " [ 0.4         0.15384615]\n",
      " [-0.4         0.17307692]\n",
      " [-0.16666667  0.19230769]] [[ 0.4         0.15384615]\n",
      " [-0.4         0.17307692]\n",
      " [-0.16666667  0.19230769]\n",
      " [ 0.06666667  0.21153846]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 5 10 7 11\n",
      "check_r_sizes 7 11 3 1\n",
      "x,y [[36  9 13 97 16  3  5  4 10  0  1  0  1  0  0  0 12  3 13  0]\n",
      " [12  4  0 32  4  4  0  2  0  1  2  0  0  0  0  0  3  0  4  0]\n",
      " [10  4  4 21  4  1  5  1  0  3  4  1  3  0  0  2  1  0  4  0]\n",
      " [13  5  1 14  1  4  2  0  0  1  0  0  0  0  0  0  2  0  0  0]\n",
      " [14  5  5 12  3  0  5  4 13  2  0  1  3  0  0  0  3  0  2  0]] [[10  4  4 21  4  1  5  1  0  3  4  1  3  0  0  2  1  0  4  0]\n",
      " [13  5  1 14  1  4  2  0  0  1  0  0  0  0  0  0  2  0  0  0]\n",
      " [14  5  5 12  3  0  5  4 13  2  0  1  3  0  0  0  3  0  2  0]] [[ 0.16666667  0.13461538]\n",
      " [ 0.4         0.15384615]\n",
      " [-0.4         0.17307692]\n",
      " [-0.16666667  0.19230769]\n",
      " [ 0.06666667  0.21153846]] [[-0.4         0.17307692]\n",
      " [-0.16666667  0.19230769]\n",
      " [ 0.06666667  0.21153846]\n",
      " [ 0.3         0.23076923]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 6 11 8 12\n",
      "check_r_sizes 8 12 3 1\n",
      "x,y [[12  4  0 32  4  4  0  2  0  1  2  0  0  0  0  0  3  0  4  0]\n",
      " [10  4  4 21  4  1  5  1  0  3  4  1  3  0  0  2  1  0  4  0]\n",
      " [13  5  1 14  1  4  2  0  0  1  0  0  0  0  0  0  2  0  0  0]\n",
      " [14  5  5 12  3  0  5  4 13  2  0  1  3  0  0  0  3  0  2  0]\n",
      " [ 6  2  6  8  7  0 12  0  4  1  1  3  0  0  0  1  0  0  3  0]] [[13  5  1 14  1  4  2  0  0  1  0  0  0  0  0  0  2  0  0  0]\n",
      " [14  5  5 12  3  0  5  4 13  2  0  1  3  0  0  0  3  0  2  0]\n",
      " [ 6  2  6  8  7  0 12  0  4  1  1  3  0  0  0  1  0  0  3  0]] [[ 0.4         0.15384615]\n",
      " [-0.4         0.17307692]\n",
      " [-0.16666667  0.19230769]\n",
      " [ 0.06666667  0.21153846]\n",
      " [ 0.3         0.23076923]] [[-0.16666667  0.19230769]\n",
      " [ 0.06666667  0.21153846]\n",
      " [ 0.3         0.23076923]\n",
      " [-0.46666667  0.25      ]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 7 12 9 13\n",
      "check_r_sizes 9 13 3 1\n",
      "x,y [[10  4  4 21  4  1  5  1  0  3  4  1  3  0  0  2  1  0  4  0]\n",
      " [13  5  1 14  1  4  2  0  0  1  0  0  0  0  0  0  2  0  0  0]\n",
      " [14  5  5 12  3  0  5  4 13  2  0  1  3  0  0  0  3  0  2  0]\n",
      " [ 6  2  6  8  7  0 12  0  4  1  1  3  0  0  0  1  0  0  3  0]\n",
      " [24  6  1 40  2  0  6  9  4  0  5  5  0  0  0  0  9  0  6  0]] [[14  5  5 12  3  0  5  4 13  2  0  1  3  0  0  0  3  0  2  0]\n",
      " [ 6  2  6  8  7  0 12  0  4  1  1  3  0  0  0  1  0  0  3  0]\n",
      " [24  6  1 40  2  0  6  9  4  0  5  5  0  0  0  0  9  0  6  0]] [[-0.4         0.17307692]\n",
      " [-0.16666667  0.19230769]\n",
      " [ 0.06666667  0.21153846]\n",
      " [ 0.3         0.23076923]\n",
      " [-0.46666667  0.25      ]] [[ 0.06666667  0.21153846]\n",
      " [ 0.3         0.23076923]\n",
      " [-0.46666667  0.25      ]\n",
      " [-0.23333333  0.26923077]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 8 13 10 14\n",
      "check_r_sizes 10 14 3 1\n",
      "x,y [[13  5  1 14  1  4  2  0  0  1  0  0  0  0  0  0  2  0  0  0]\n",
      " [14  5  5 12  3  0  5  4 13  2  0  1  3  0  0  0  3  0  2  0]\n",
      " [ 6  2  6  8  7  0 12  0  4  1  1  3  0  0  0  1  0  0  3  0]\n",
      " [24  6  1 40  2  0  6  9  4  0  5  5  0  0  0  0  9  0  6  0]\n",
      " [12  7  1 16  3  0  0  0  1  0  4  0  0  0  0  0  2  0  0  0]] [[ 6  2  6  8  7  0 12  0  4  1  1  3  0  0  0  1  0  0  3  0]\n",
      " [24  6  1 40  2  0  6  9  4  0  5  5  0  0  0  0  9  0  6  0]\n",
      " [12  7  1 16  3  0  0  0  1  0  4  0  0  0  0  0  2  0  0  0]] [[-0.16666667  0.19230769]\n",
      " [ 0.06666667  0.21153846]\n",
      " [ 0.3         0.23076923]\n",
      " [-0.46666667  0.25      ]\n",
      " [-0.23333333  0.26923077]] [[ 0.3         0.23076923]\n",
      " [-0.46666667  0.25      ]\n",
      " [-0.23333333  0.26923077]\n",
      " [ 0.          0.28846154]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 9 14 11 15\n",
      "check_r_sizes 11 15 3 1\n",
      "x,y [[14  5  5 12  3  0  5  4 13  2  0  1  3  0  0  0  3  0  2  0]\n",
      " [ 6  2  6  8  7  0 12  0  4  1  1  3  0  0  0  1  0  0  3  0]\n",
      " [24  6  1 40  2  0  6  9  4  0  5  5  0  0  0  0  9  0  6  0]\n",
      " [12  7  1 16  3  0  0  0  1  0  4  0  0  0  0  0  2  0  0  0]\n",
      " [29  7 16 22 11  9  2  5  1  0  7  0  5  1  0  1  2  0  1  0]] [[24  6  1 40  2  0  6  9  4  0  5  5  0  0  0  0  9  0  6  0]\n",
      " [12  7  1 16  3  0  0  0  1  0  4  0  0  0  0  0  2  0  0  0]\n",
      " [29  7 16 22 11  9  2  5  1  0  7  0  5  1  0  1  2  0  1  0]] [[ 0.06666667  0.21153846]\n",
      " [ 0.3         0.23076923]\n",
      " [-0.46666667  0.25      ]\n",
      " [-0.23333333  0.26923077]\n",
      " [ 0.          0.28846154]] [[-0.46666667  0.25      ]\n",
      " [-0.23333333  0.26923077]\n",
      " [ 0.          0.28846154]\n",
      " [ 0.23333333  0.30769231]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 10 15 12 16\n",
      "check_r_sizes 12 16 3 1\n",
      "x,y [[ 6  2  6  8  7  0 12  0  4  1  1  3  0  0  0  1  0  0  3  0]\n",
      " [24  6  1 40  2  0  6  9  4  0  5  5  0  0  0  0  9  0  6  0]\n",
      " [12  7  1 16  3  0  0  0  1  0  4  0  0  0  0  0  2  0  0  0]\n",
      " [29  7 16 22 11  9  2  5  1  0  7  0  5  1  0  1  2  0  1  0]\n",
      " [ 7  0  0 19  3  1  2  0  1  0  3  3  0  0  0  0  3  0  0  0]] [[12  7  1 16  3  0  0  0  1  0  4  0  0  0  0  0  2  0  0  0]\n",
      " [29  7 16 22 11  9  2  5  1  0  7  0  5  1  0  1  2  0  1  0]\n",
      " [ 7  0  0 19  3  1  2  0  1  0  3  3  0  0  0  0  3  0  0  0]] [[ 0.3         0.23076923]\n",
      " [-0.46666667  0.25      ]\n",
      " [-0.23333333  0.26923077]\n",
      " [ 0.          0.28846154]\n",
      " [ 0.23333333  0.30769231]] [[-0.23333333  0.26923077]\n",
      " [ 0.          0.28846154]\n",
      " [ 0.23333333  0.30769231]\n",
      " [ 0.46666667  0.32692308]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 11 16 13 17\n",
      "check_r_sizes 13 17 3 1\n",
      "x,y [[24  6  1 40  2  0  6  9  4  0  5  5  0  0  0  0  9  0  6  0]\n",
      " [12  7  1 16  3  0  0  0  1  0  4  0  0  0  0  0  2  0  0  0]\n",
      " [29  7 16 22 11  9  2  5  1  0  7  0  5  1  0  1  2  0  1  0]\n",
      " [ 7  0  0 19  3  1  2  0  1  0  3  3  0  0  0  0  3  0  0  0]\n",
      " [ 8  2 19 20  4  2  3  0  2  1  2  1  1  0  0  0  2  0  4  0]] [[29  7 16 22 11  9  2  5  1  0  7  0  5  1  0  1  2  0  1  0]\n",
      " [ 7  0  0 19  3  1  2  0  1  0  3  3  0  0  0  0  3  0  0  0]\n",
      " [ 8  2 19 20  4  2  3  0  2  1  2  1  1  0  0  0  2  0  4  0]] [[-0.46666667  0.25      ]\n",
      " [-0.23333333  0.26923077]\n",
      " [ 0.          0.28846154]\n",
      " [ 0.23333333  0.30769231]\n",
      " [ 0.46666667  0.32692308]] [[ 0.          0.28846154]\n",
      " [ 0.23333333  0.30769231]\n",
      " [ 0.46666667  0.32692308]\n",
      " [-0.33333333  0.34615385]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 12 17 14 18\n",
      "check_r_sizes 14 18 3 1\n",
      "x,y [[12  7  1 16  3  0  0  0  1  0  4  0  0  0  0  0  2  0  0  0]\n",
      " [29  7 16 22 11  9  2  5  1  0  7  0  5  1  0  1  2  0  1  0]\n",
      " [ 7  0  0 19  3  1  2  0  1  0  3  3  0  0  0  0  3  0  0  0]\n",
      " [ 8  2 19 20  4  2  3  0  2  1  2  1  1  0  0  0  2  0  4  0]\n",
      " [ 7  1  4  3  2  3  1  0  0  0  1  1  0  0  0  0  2  0  5  0]] [[ 7  0  0 19  3  1  2  0  1  0  3  3  0  0  0  0  3  0  0  0]\n",
      " [ 8  2 19 20  4  2  3  0  2  1  2  1  1  0  0  0  2  0  4  0]\n",
      " [ 7  1  4  3  2  3  1  0  0  0  1  1  0  0  0  0  2  0  5  0]] [[-0.23333333  0.26923077]\n",
      " [ 0.          0.28846154]\n",
      " [ 0.23333333  0.30769231]\n",
      " [ 0.46666667  0.32692308]\n",
      " [-0.33333333  0.34615385]] [[ 0.23333333  0.30769231]\n",
      " [ 0.46666667  0.32692308]\n",
      " [-0.33333333  0.34615385]\n",
      " [-0.1         0.36538462]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 13 18 15 19\n",
      "check_r_sizes 15 19 3 1\n",
      "x,y [[29  7 16 22 11  9  2  5  1  0  7  0  5  1  0  1  2  0  1  0]\n",
      " [ 7  0  0 19  3  1  2  0  1  0  3  3  0  0  0  0  3  0  0  0]\n",
      " [ 8  2 19 20  4  2  3  0  2  1  2  1  1  0  0  0  2  0  4  0]\n",
      " [ 7  1  4  3  2  3  1  0  0  0  1  1  0  0  0  0  2  0  5  0]\n",
      " [ 4  2 10 13  4  1  1  3  1  1  3  0  0  1  0  0  5  0  1  0]] [[ 8  2 19 20  4  2  3  0  2  1  2  1  1  0  0  0  2  0  4  0]\n",
      " [ 7  1  4  3  2  3  1  0  0  0  1  1  0  0  0  0  2  0  5  0]\n",
      " [ 4  2 10 13  4  1  1  3  1  1  3  0  0  1  0  0  5  0  1  0]] [[ 0.          0.28846154]\n",
      " [ 0.23333333  0.30769231]\n",
      " [ 0.46666667  0.32692308]\n",
      " [-0.33333333  0.34615385]\n",
      " [-0.1         0.36538462]] [[ 0.46666667  0.32692308]\n",
      " [-0.33333333  0.34615385]\n",
      " [-0.1         0.36538462]\n",
      " [ 0.13333333  0.38461538]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 14 19 16 20\n",
      "check_r_sizes 16 20 3 1\n",
      "x,y [[ 7  0  0 19  3  1  2  0  1  0  3  3  0  0  0  0  3  0  0  0]\n",
      " [ 8  2 19 20  4  2  3  0  2  1  2  1  1  0  0  0  2  0  4  0]\n",
      " [ 7  1  4  3  2  3  1  0  0  0  1  1  0  0  0  0  2  0  5  0]\n",
      " [ 4  2 10 13  4  1  1  3  1  1  3  0  0  1  0  0  5  0  1  0]\n",
      " [10  5  8 37  8  5  0  4  1  0  6  0  0  0  0  0  1  0  2  0]] [[ 7  1  4  3  2  3  1  0  0  0  1  1  0  0  0  0  2  0  5  0]\n",
      " [ 4  2 10 13  4  1  1  3  1  1  3  0  0  1  0  0  5  0  1  0]\n",
      " [10  5  8 37  8  5  0  4  1  0  6  0  0  0  0  0  1  0  2  0]] [[ 0.23333333  0.30769231]\n",
      " [ 0.46666667  0.32692308]\n",
      " [-0.33333333  0.34615385]\n",
      " [-0.1         0.36538462]\n",
      " [ 0.13333333  0.38461538]] [[-0.33333333  0.34615385]\n",
      " [-0.1         0.36538462]\n",
      " [ 0.13333333  0.38461538]\n",
      " [ 0.36666667  0.40384615]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 15 20 17 21\n",
      "check_r_sizes 17 21 3 1\n",
      "x,y [[ 8  2 19 20  4  2  3  0  2  1  2  1  1  0  0  0  2  0  4  0]\n",
      " [ 7  1  4  3  2  3  1  0  0  0  1  1  0  0  0  0  2  0  5  0]\n",
      " [ 4  2 10 13  4  1  1  3  1  1  3  0  0  1  0  0  5  0  1  0]\n",
      " [10  5  8 37  8  5  0  4  1  0  6  0  0  0  0  0  1  0  2  0]\n",
      " [ 7  0  8 16  3  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0]] [[ 4  2 10 13  4  1  1  3  1  1  3  0  0  1  0  0  5  0  1  0]\n",
      " [10  5  8 37  8  5  0  4  1  0  6  0  0  0  0  0  1  0  2  0]\n",
      " [ 7  0  8 16  3  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0]] [[ 0.46666667  0.32692308]\n",
      " [-0.33333333  0.34615385]\n",
      " [-0.1         0.36538462]\n",
      " [ 0.13333333  0.38461538]\n",
      " [ 0.36666667  0.40384615]] [[-0.1         0.36538462]\n",
      " [ 0.13333333  0.38461538]\n",
      " [ 0.36666667  0.40384615]\n",
      " [-0.4         0.42307692]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 16 21 18 22\n",
      "check_r_sizes 18 22 3 1\n",
      "x,y [[ 7  1  4  3  2  3  1  0  0  0  1  1  0  0  0  0  2  0  5  0]\n",
      " [ 4  2 10 13  4  1  1  3  1  1  3  0  0  1  0  0  5  0  1  0]\n",
      " [10  5  8 37  8  5  0  4  1  0  6  0  0  0  0  0  1  0  2  0]\n",
      " [ 7  0  8 16  3  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13  1  7  2  3  0  1  1  0  0  2  0  0  0  0  0  0  1  0  0]] [[10  5  8 37  8  5  0  4  1  0  6  0  0  0  0  0  1  0  2  0]\n",
      " [ 7  0  8 16  3  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13  1  7  2  3  0  1  1  0  0  2  0  0  0  0  0  0  1  0  0]] [[-0.33333333  0.34615385]\n",
      " [-0.1         0.36538462]\n",
      " [ 0.13333333  0.38461538]\n",
      " [ 0.36666667  0.40384615]\n",
      " [-0.4         0.42307692]] [[ 0.13333333  0.38461538]\n",
      " [ 0.36666667  0.40384615]\n",
      " [-0.4         0.42307692]\n",
      " [-0.16666667  0.44230769]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 17 22 19 23\n",
      "check_r_sizes 19 23 3 1\n",
      "x,y [[ 4  2 10 13  4  1  1  3  1  1  3  0  0  1  0  0  5  0  1  0]\n",
      " [10  5  8 37  8  5  0  4  1  0  6  0  0  0  0  0  1  0  2  0]\n",
      " [ 7  0  8 16  3  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13  1  7  2  3  0  1  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
      " [11  1 10 27  2  0  2  2  5  0  5  1  0  1  0  0  1  1  2  0]] [[ 7  0  8 16  3  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13  1  7  2  3  0  1  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
      " [11  1 10 27  2  0  2  2  5  0  5  1  0  1  0  0  1  1  2  0]] [[-0.1         0.36538462]\n",
      " [ 0.13333333  0.38461538]\n",
      " [ 0.36666667  0.40384615]\n",
      " [-0.4         0.42307692]\n",
      " [-0.16666667  0.44230769]] [[ 0.36666667  0.40384615]\n",
      " [-0.4         0.42307692]\n",
      " [-0.16666667  0.44230769]\n",
      " [ 0.06666667  0.46153846]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 18 23 20 24\n",
      "check_r_sizes 20 24 3 1\n",
      "x,y [[10  5  8 37  8  5  0  4  1  0  6  0  0  0  0  0  1  0  2  0]\n",
      " [ 7  0  8 16  3  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13  1  7  2  3  0  1  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
      " [11  1 10 27  2  0  2  2  5  0  5  1  0  1  0  0  1  1  2  0]\n",
      " [ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]] [[13  1  7  2  3  0  1  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
      " [11  1 10 27  2  0  2  2  5  0  5  1  0  1  0  0  1  1  2  0]\n",
      " [ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]] [[ 0.13333333  0.38461538]\n",
      " [ 0.36666667  0.40384615]\n",
      " [-0.4         0.42307692]\n",
      " [-0.16666667  0.44230769]\n",
      " [ 0.06666667  0.46153846]] [[-0.4         0.42307692]\n",
      " [-0.16666667  0.44230769]\n",
      " [ 0.06666667  0.46153846]\n",
      " [ 0.3         0.48076923]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 19 24 21 25\n",
      "check_r_sizes 21 25 3 1\n",
      "x,y [[ 7  0  8 16  3  0  2  3  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [13  1  7  2  3  0  1  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
      " [11  1 10 27  2  0  2  2  5  0  5  1  0  1  0  0  1  1  2  0]\n",
      " [ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]] [[11  1 10 27  2  0  2  2  5  0  5  1  0  1  0  0  1  1  2  0]\n",
      " [ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]] [[ 0.36666667  0.40384615]\n",
      " [-0.4         0.42307692]\n",
      " [-0.16666667  0.44230769]\n",
      " [ 0.06666667  0.46153846]\n",
      " [ 0.3         0.48076923]] [[-0.16666667  0.44230769]\n",
      " [ 0.06666667  0.46153846]\n",
      " [ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 20 25 22 26\n",
      "check_r_sizes 22 26 3 1\n",
      "x,y [[13  1  7  2  3  0  1  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
      " [11  1 10 27  2  0  2  2  5  0  5  1  0  1  0  0  1  1  2  0]\n",
      " [ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]] [[ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]] [[-0.4         0.42307692]\n",
      " [-0.16666667  0.44230769]\n",
      " [ 0.06666667  0.46153846]\n",
      " [ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]] [[ 0.06666667  0.46153846]\n",
      " [ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 21 26 23 27\n",
      "check_r_sizes 23 27 3 1\n",
      "x,y [[11  1 10 27  2  0  2  2  5  0  5  1  0  1  0  0  1  1  2  0]\n",
      " [ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]] [[ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]] [[-0.16666667  0.44230769]\n",
      " [ 0.06666667  0.46153846]\n",
      " [ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]] [[ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 22 27 24 28\n",
      "check_r_sizes 24 28 3 1\n",
      "x,y [[ 3  3  4 21  3  4  0  5  0  0  0  0  0  0  0  0 14  0  1  0]\n",
      " [ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]] [[ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]] [[ 0.06666667  0.46153846]\n",
      " [ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]] [[-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 23 28 25 29\n",
      "check_r_sizes 25 29 3 1\n",
      "x,y [[ 0  0  0  3  0  0  2  1  0  0  0  1  0  0  0  0  1  0  0  0]\n",
      " [ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]] [[13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]] [[ 0.3         0.48076923]\n",
      " [-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]] [[-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 24 29 26 30\n",
      "check_r_sizes 26 30 3 1\n",
      "x,y [[ 7  5 26 22  5  0  0  1  1  0  3  1  0  0  0  0  1  0  0  0]\n",
      " [13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]] [[14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]] [[-0.5        -0.5       ]\n",
      " [-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]] [[-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]\n",
      " [-0.36666667 -0.40384615]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (4, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 4, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 4, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 4, 512])\n",
      "check_indices 25 30 27 31\n",
      "check_r_sizes 27 31 3 1\n",
      "x,y [[13  9 20 85  4  2  0  2  8  0  0  0  0  0  0  1  8  0  0  0]\n",
      " [14  5  8 10  1  0  0  1  0  1  0  1  0  0  0  0  3  0  2  0]\n",
      " [ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 7  6  7 49  6  0  1  2  2  2  6  0  1  0  0  0  2  0  4  0]] [[ 5  2  0 23  3  2  5  1  0  0  2  0  0  0  0  3  4  0  4  0]\n",
      " [ 3  2  7 37  6 10  4  7  0  1  1  1  0  0  0  0  0  0  0  0]\n",
      " [ 7  6  7 49  6  0  1  2  2  2  6  0  1  0  0  0  2  0  4  0]] [[-0.26666667 -0.48076923]\n",
      " [-0.03333333 -0.46153846]\n",
      " [ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]\n",
      " [-0.36666667 -0.40384615]] [[ 0.2        -0.44230769]\n",
      " [ 0.43333333 -0.42307692]\n",
      " [-0.36666667 -0.40384615]]\n",
      "check sizes (5, 20) (3, 20) (5, 2) (3, 2)\n",
      "bxm-shape torch.Size([1, 5, 20]) torch.Size([1, 5, 2]) torch.Size([1, 3, 20]) torch.Size([1, 3, 2])\n",
      "dec_inp_pre torch.Size([1, 1, 20])\n",
      "dec_inp_post torch.Size([1, 4, 20])\n",
      "triggered\n",
      "ln1 torch.Size([1, 5, 20]) torch.Size([1, 5, 2])\n",
      "ln2 torch.Size([1, 5, 512]) torch.Size([1, 5, 512]) torch.Size([1, 5, 512])\n",
      "triggered\n",
      "ln1 torch.Size([1, 4, 20]) torch.Size([1, 3, 2])\n",
      "ln2 torch.Size([1, 4, 512]) torch.Size([1, 4, 512]) torch.Size([1, 3, 512])\n"
     ]
    }
   ],
   "source": [
    "exp.predict(setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f28d869e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26, 1, 20)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = np.load('./results/'+setting+'/real_prediction.npy')\n",
    "\n",
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4aeba6af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 6.47144556e+00,  3.96514559e+00,  5.70830107e+00,\n",
       "          1.36275320e+01,  4.42497826e+00,  1.75820732e+00,\n",
       "          1.58704543e+00,  2.12797689e+00,  1.80754888e+00,\n",
       "          1.39501905e+00,  1.80107427e+00,  1.10726678e+00,\n",
       "          1.00372803e+00,  3.98397297e-01,  8.97772312e-02,\n",
       "          5.74410677e-01,  1.22303319e+00,  4.49589849e-01,\n",
       "          1.19408405e+00,  1.42222002e-01]],\n",
       "\n",
       "       [[ 6.45516682e+00,  4.01353550e+00,  5.65772104e+00,\n",
       "          1.37231798e+01,  4.42058992e+00,  1.73948324e+00,\n",
       "          1.55589366e+00,  2.10548544e+00,  1.79255426e+00,\n",
       "          1.41143858e+00,  1.69436467e+00,  1.12260139e+00,\n",
       "          9.80181336e-01,  3.61670166e-01,  1.50871873e-01,\n",
       "          6.14625692e-01,  1.17329812e+00,  4.66475785e-01,\n",
       "          1.20155275e+00,  1.05955541e-01]],\n",
       "\n",
       "       [[ 6.55603075e+00,  4.13451481e+00,  5.63561153e+00,\n",
       "          1.36011152e+01,  4.57715702e+00,  1.60231221e+00,\n",
       "          1.52540195e+00,  2.09848928e+00,  1.76306033e+00,\n",
       "          1.36013806e+00,  1.62752378e+00,  1.15659499e+00,\n",
       "          8.63600135e-01,  2.75240183e-01,  8.33303928e-02,\n",
       "          5.54096818e-01,  1.26770961e+00,  5.02619088e-01,\n",
       "          1.21633625e+00,  9.28219929e-02]],\n",
       "\n",
       "       [[ 6.57494831e+00,  4.16479731e+00,  5.64546585e+00,\n",
       "          1.35575304e+01,  4.58182955e+00,  1.56876934e+00,\n",
       "          1.52541029e+00,  2.13105249e+00,  1.76857686e+00,\n",
       "          1.34815562e+00,  1.65157413e+00,  1.15582252e+00,\n",
       "          8.35338235e-01,  3.01640809e-01,  8.96745920e-02,\n",
       "          5.62364817e-01,  1.26700878e+00,  4.89516914e-01,\n",
       "          1.28342628e+00,  1.39325798e-01]],\n",
       "\n",
       "       [[ 6.54601192e+00,  4.15018225e+00,  5.62720203e+00,\n",
       "          1.36406479e+01,  4.57745218e+00,  1.60151398e+00,\n",
       "          1.57157636e+00,  2.08494854e+00,  1.77189445e+00,\n",
       "          1.35234284e+00,  1.64570463e+00,  1.17506325e+00,\n",
       "          8.48423302e-01,  2.92426467e-01,  1.37066588e-01,\n",
       "          6.07965231e-01,  1.25477815e+00,  4.61465061e-01,\n",
       "          1.23449445e+00,  1.53662637e-01]],\n",
       "\n",
       "       [[ 6.54947138e+00,  4.10507298e+00,  5.68440199e+00,\n",
       "          1.36314125e+01,  4.52740240e+00,  1.67259097e+00,\n",
       "          1.55080020e+00,  2.18036890e+00,  1.78813815e+00,\n",
       "          1.46079934e+00,  1.71887636e+00,  1.16469860e+00,\n",
       "          8.99953902e-01,  2.71536916e-01,  1.06309250e-01,\n",
       "          5.61709642e-01,  1.24061918e+00,  4.38101292e-01,\n",
       "          1.23493397e+00,  1.51984721e-01]],\n",
       "\n",
       "       [[ 6.47225618e+00,  4.13084412e+00,  5.64679623e+00,\n",
       "          1.36389437e+01,  4.59826851e+00,  1.57524729e+00,\n",
       "          1.60975397e+00,  2.10176229e+00,  1.72773814e+00,\n",
       "          1.35072637e+00,  1.62719095e+00,  1.21207750e+00,\n",
       "          8.99510384e-01,  2.70726502e-01,  1.55442581e-01,\n",
       "          5.32999396e-01,  1.10230732e+00,  3.91756058e-01,\n",
       "          1.17558861e+00,  1.84031248e-01]],\n",
       "\n",
       "       [[ 6.42158365e+00,  3.99207854e+00,  5.71068335e+00,\n",
       "          1.36701212e+01,  4.45331764e+00,  1.75148070e+00,\n",
       "          1.59853518e+00,  2.14178228e+00,  1.80051219e+00,\n",
       "          1.36667037e+00,  1.80183947e+00,  1.13734853e+00,\n",
       "          9.77547526e-01,  2.96074331e-01,  1.07350469e-01,\n",
       "          5.27748108e-01,  1.21238863e+00,  4.54104543e-01,\n",
       "          1.20879555e+00,  1.51319012e-01]],\n",
       "\n",
       "       [[ 6.42834806e+00,  4.03189564e+00,  5.72442007e+00,\n",
       "          1.37020893e+01,  4.43387747e+00,  1.73728716e+00,\n",
       "          1.57997811e+00,  2.07348537e+00,  1.76124918e+00,\n",
       "          1.33469236e+00,  1.64543045e+00,  1.05342758e+00,\n",
       "          1.00677252e+00,  2.38160878e-01,  1.07884496e-01,\n",
       "          5.57830334e-01,  1.18926406e+00,  4.98108238e-01,\n",
       "          1.23437607e+00,  2.33062297e-01]],\n",
       "\n",
       "       [[ 6.38157749e+00,  4.13504410e+00,  5.73834515e+00,\n",
       "          1.36919556e+01,  4.58774471e+00,  1.65613735e+00,\n",
       "          1.56432629e+00,  2.06874514e+00,  1.76872635e+00,\n",
       "          1.36041594e+00,  1.67346752e+00,  1.20648575e+00,\n",
       "          8.33256245e-01,  2.85513312e-01,  1.00084737e-01,\n",
       "          6.07742667e-01,  1.24671173e+00,  3.69900793e-01,\n",
       "          1.25117314e+00,  1.70920864e-01]],\n",
       "\n",
       "       [[ 6.42925739e+00,  4.08388805e+00,  5.69288397e+00,\n",
       "          1.37227001e+01,  4.49849796e+00,  1.66292441e+00,\n",
       "          1.56519258e+00,  2.09492874e+00,  1.75036538e+00,\n",
       "          1.29880512e+00,  1.60140204e+00,  1.14428782e+00,\n",
       "          8.70738149e-01,  3.82149905e-01,  9.08444673e-02,\n",
       "          5.30325294e-01,  1.20976102e+00,  4.33600128e-01,\n",
       "          1.20324028e+00,  8.06961954e-02]],\n",
       "\n",
       "       [[ 6.44975853e+00,  4.11665440e+00,  5.81848812e+00,\n",
       "          1.36907549e+01,  4.45916080e+00,  1.73785007e+00,\n",
       "          1.61244607e+00,  2.01569223e+00,  1.81431878e+00,\n",
       "          1.36037004e+00,  1.66248226e+00,  1.09083200e+00,\n",
       "          8.51339996e-01,  3.57985407e-01,  8.67132545e-02,\n",
       "          6.04947448e-01,  1.15527964e+00,  3.13628733e-01,\n",
       "          1.30663657e+00,  1.53166190e-01]],\n",
       "\n",
       "       [[ 6.43186092e+00,  4.18733692e+00,  5.70552588e+00,\n",
       "          1.36372061e+01,  4.61169386e+00,  1.67854750e+00,\n",
       "          1.58058262e+00,  1.99225807e+00,  1.75261950e+00,\n",
       "          1.35233653e+00,  1.59803069e+00,  1.17930806e+00,\n",
       "          9.00310993e-01,  3.24258685e-01,  1.30246058e-01,\n",
       "          5.92634082e-01,  1.24891424e+00,  4.08825278e-01,\n",
       "          1.22825062e+00,  6.27160072e-02]],\n",
       "\n",
       "       [[ 6.57837439e+00,  4.07800961e+00,  5.72066593e+00,\n",
       "          1.35893984e+01,  4.53916407e+00,  1.73256326e+00,\n",
       "          1.61405730e+00,  2.13007259e+00,  1.77448344e+00,\n",
       "          1.34602547e+00,  1.74199724e+00,  1.11721504e+00,\n",
       "          9.73666847e-01,  2.84583211e-01, -7.88616017e-05,\n",
       "          5.80787897e-01,  1.34541321e+00,  3.46198678e-01,\n",
       "          1.33670127e+00,  1.99680060e-01]],\n",
       "\n",
       "       [[ 6.41379738e+00,  4.02115774e+00,  5.76373339e+00,\n",
       "          1.36536398e+01,  4.49797392e+00,  1.78586709e+00,\n",
       "          1.59114814e+00,  2.12033987e+00,  1.77759397e+00,\n",
       "          1.35708570e+00,  1.72673833e+00,  1.13667083e+00,\n",
       "          1.02144897e+00,  3.57665271e-01,  1.75471976e-01,\n",
       "          6.27112508e-01,  1.23403263e+00,  3.91538888e-01,\n",
       "          1.18105602e+00,  1.12337127e-01]],\n",
       "\n",
       "       [[ 6.45022726e+00,  4.10436296e+00,  5.69241142e+00,\n",
       "          1.37236032e+01,  4.50323915e+00,  1.69616568e+00,\n",
       "          1.58477652e+00,  2.06877565e+00,  1.79417646e+00,\n",
       "          1.38159752e+00,  1.59995437e+00,  1.13152039e+00,\n",
       "          1.00328445e+00,  2.76185274e-01,  1.08737558e-01,\n",
       "          5.35775781e-01,  1.17923784e+00,  4.76283252e-01,\n",
       "          1.18488586e+00,  1.99197799e-01]],\n",
       "\n",
       "       [[ 6.49780178e+00,  4.21301603e+00,  5.65129185e+00,\n",
       "          1.36157150e+01,  4.54365826e+00,  1.60019422e+00,\n",
       "          1.53274846e+00,  2.06175709e+00,  1.81274796e+00,\n",
       "          1.35548198e+00,  1.62710643e+00,  1.19335651e+00,\n",
       "          8.28713715e-01,  2.59491175e-01,  5.99857904e-02,\n",
       "          5.64532280e-01,  1.27665567e+00,  4.06911641e-01,\n",
       "          1.27883899e+00,  1.32002518e-01]],\n",
       "\n",
       "       [[ 6.47546291e+00,  4.05405331e+00,  5.74295759e+00,\n",
       "          1.36757421e+01,  4.51320362e+00,  1.71129632e+00,\n",
       "          1.64702106e+00,  2.13766623e+00,  1.76724458e+00,\n",
       "          1.38167310e+00,  1.74232507e+00,  1.15067220e+00,\n",
       "          9.14595902e-01,  4.00902838e-01,  1.39960367e-02,\n",
       "          5.93412399e-01,  1.24741185e+00,  3.92932832e-01,\n",
       "          1.27041495e+00,  1.50521204e-01]],\n",
       "\n",
       "       [[ 6.45099449e+00,  3.97656727e+00,  5.74119711e+00,\n",
       "          1.36245623e+01,  4.44933891e+00,  1.78783202e+00,\n",
       "          1.54382169e+00,  2.05832815e+00,  1.81080616e+00,\n",
       "          1.39690030e+00,  1.81161201e+00,  1.05571747e+00,\n",
       "          9.99829948e-01,  4.00646180e-01,  1.22210950e-01,\n",
       "          5.14581561e-01,  1.23239017e+00,  3.75700980e-01,\n",
       "          1.23479450e+00,  1.28924802e-01]],\n",
       "\n",
       "       [[ 6.58114433e+00,  4.18211222e+00,  5.65979481e+00,\n",
       "          1.35504560e+01,  4.54658556e+00,  1.56428671e+00,\n",
       "          1.50033510e+00,  2.07803011e+00,  1.75865960e+00,\n",
       "          1.36643386e+00,  1.67862368e+00,  1.13431656e+00,\n",
       "          8.81685376e-01,  2.40094066e-01,  4.22189273e-02,\n",
       "          5.17123461e-01,  1.27849281e+00,  4.89701569e-01,\n",
       "          1.26631498e+00,  1.25154525e-01]],\n",
       "\n",
       "       [[ 6.42992306e+00,  4.06891823e+00,  5.77061653e+00,\n",
       "          1.35871296e+01,  4.52959681e+00,  1.69002056e+00,\n",
       "          1.55562246e+00,  2.10774565e+00,  1.80394387e+00,\n",
       "          1.32700396e+00,  1.80576551e+00,  1.17944872e+00,\n",
       "          1.01180792e+00,  3.46306354e-01,  4.46386226e-02,\n",
       "          6.13101959e-01,  1.24128568e+00,  4.03302372e-01,\n",
       "          1.25526619e+00,  1.73738018e-01]],\n",
       "\n",
       "       [[ 6.43548727e+00,  3.89657640e+00,  5.71396923e+00,\n",
       "          1.35708961e+01,  4.42672062e+00,  1.81090808e+00,\n",
       "          1.55824959e+00,  2.11984873e+00,  1.84428287e+00,\n",
       "          1.43898118e+00,  1.85045183e+00,  1.09852517e+00,\n",
       "          1.03127515e+00,  4.29461181e-01,  1.01777837e-01,\n",
       "          6.06427193e-01,  1.24608004e+00,  4.27948982e-01,\n",
       "          1.19787920e+00,  7.61563927e-02]],\n",
       "\n",
       "       [[ 6.41003180e+00,  4.08976603e+00,  5.61341381e+00,\n",
       "          1.37171984e+01,  4.47247362e+00,  1.61421549e+00,\n",
       "          1.55117881e+00,  2.05416012e+00,  1.78177488e+00,\n",
       "          1.38278472e+00,  1.54415870e+00,  1.08849287e+00,\n",
       "          9.48629498e-01,  2.05898523e-01,  9.55546498e-02,\n",
       "          5.42621017e-01,  1.17560589e+00,  4.90459085e-01,\n",
       "          1.24706984e+00,  1.80398837e-01]],\n",
       "\n",
       "       [[ 6.61197042e+00,  4.12707186e+00,  5.58506107e+00,\n",
       "          1.35119762e+01,  4.59987402e+00,  1.57915854e+00,\n",
       "          1.49803042e+00,  2.17276525e+00,  1.79112673e+00,\n",
       "          1.35876477e+00,  1.73399115e+00,  1.16414309e+00,\n",
       "          8.97972286e-01,  2.47370392e-01,  2.55348422e-02,\n",
       "          5.42243600e-01,  1.35758436e+00,  5.35126328e-01,\n",
       "          1.32133579e+00,  1.41379401e-01]],\n",
       "\n",
       "       [[ 6.46624660e+00,  4.02840471e+00,  5.79636049e+00,\n",
       "          1.36593723e+01,  4.51396942e+00,  1.71798289e+00,\n",
       "          1.54975414e+00,  2.09635925e+00,  1.76485515e+00,\n",
       "          1.37151003e+00,  1.68187952e+00,  1.13930261e+00,\n",
       "          9.64661360e-01,  3.51411432e-01,  1.58425555e-01,\n",
       "          5.81326604e-01,  1.16606629e+00,  4.22819138e-01,\n",
       "          1.17781270e+00,  1.70290068e-01]],\n",
       "\n",
       "       [[ 6.52038097e+00,  3.99587178e+00,  5.70344591e+00,\n",
       "          1.36504936e+01,  4.50292015e+00,  1.76075172e+00,\n",
       "          1.59165835e+00,  2.09852028e+00,  1.73877168e+00,\n",
       "          1.42517817e+00,  1.71489525e+00,  1.13430977e+00,\n",
       "          1.01732373e+00,  3.51170123e-01,  8.89258385e-02,\n",
       "          6.14717126e-01,  1.19553649e+00,  4.63611454e-01,\n",
       "          1.23032153e+00,  1.10968307e-01]]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea93532",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7baa46f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117be555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a643a39",
   "metadata": {},
   "source": [
    "# Test ATD Protocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab035e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "\n",
    "args.enc_in = 20 # encoder input size\n",
    "args.dec_in = 20 # decoder input size\n",
    "args.c_out = 20 # output size\n",
    "args.factor = 5 # probsparse attn factor\n",
    "args.d_model = 512 # dimension of model\n",
    "args.n_heads = 8 # num of heads\n",
    "args.e_layers = 2 # num of encoder layers\n",
    "args.d_layers = 1 # num of decoder layers\n",
    "args.d_ff = 2048 # dimension of fcn in model\n",
    "args.dropout = 0.05 # dropout\n",
    "args.attn = 'prob' # attention used in encoder, options:[prob, full]\n",
    "args.embed = 'timeF' # time features encoding, options:[timeF, fixed, learned]\n",
    "args.activation = 'gelu' # activation\n",
    "args.distil = True # whether to use distilling in encoder\n",
    "args.output_attention = False # whether to output attention in ecoder\n",
    "args.mix = True\n",
    "args.padding = 0\n",
    "args.freq = 'w'\n",
    "args.inverse=False\n",
    "args.timeenc=0\n",
    "\n",
    "#args.cols=1\n",
    "args.checkpoints = \"/Users/will/Desktop/tmp\"\n",
    "\n",
    "\n",
    "\n",
    "args.seq_len=5\n",
    "args.label_len=3\n",
    "args.pred_len=1\n",
    "\n",
    "\n",
    "args.batch_size = 1\n",
    "args.learning_rate = 0.0001\n",
    "args.loss = 'mse'\n",
    "args.lradj = 'type1'\n",
    "args.use_amp = False\n",
    "\n",
    "args.itr=1\n",
    "args.train_epochs=1\n",
    "args.patience=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34f9aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = InformerForcaster(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f091a46e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 58.1230278\n",
      "\tspeed: 0.0413s/iter; left time: 1.9018s\n",
      "Epoch: 1 cost time: 5.990779876708984\n",
      "Epoch: 1, Steps: 145 | Train Loss: 56.0341204 Vali Loss: 124.8868713 Test Loss: 30.6210442\n",
      "Validation loss decreased (inf --> 124.886871).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:30.621047973632812, mae:2.3326239585876465\n",
      "1\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 32.0206261\n",
      "\tspeed: 0.0403s/iter; left time: 1.8526s\n",
      "Epoch: 1 cost time: 5.889307975769043\n",
      "Epoch: 1, Steps: 145 | Train Loss: 272.4690017 Vali Loss: 779.2390137 Test Loss: 320.9321289\n",
      "Validation loss decreased (inf --> 779.239014).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:320.93212890625, mae:7.831811428070068\n",
      "21\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 167395.5156250\n",
      "\tspeed: 0.0384s/iter; left time: 1.7648s\n",
      "Epoch: 1 cost time: 5.6079511642456055\n",
      "Epoch: 1, Steps: 145 | Train Loss: 117484.9867996 Vali Loss: 124035.7812500 Test Loss: 119813.2656250\n",
      "Validation loss decreased (inf --> 124035.781250).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:119813.265625, mae:191.3630828857422\n",
      "41\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1459188.6250000\n",
      "\tspeed: 0.0386s/iter; left time: 1.7770s\n",
      "Epoch: 1 cost time: 5.700084924697876\n",
      "Epoch: 1, Steps: 145 | Train Loss: 937420.6983836 Vali Loss: 821714.5625000 Test Loss: 626956.1250000\n",
      "Validation loss decreased (inf --> 821714.562500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:626956.125, mae:483.2742614746094\n",
      "61\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 13847.8281250\n",
      "\tspeed: 0.0392s/iter; left time: 1.8015s\n",
      "Epoch: 1 cost time: 5.725273132324219\n",
      "Epoch: 1, Steps: 145 | Train Loss: 11761.1922591 Vali Loss: 5868.6552734 Test Loss: 2729.2849121\n",
      "Validation loss decreased (inf --> 5868.655273).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2729.28515625, mae:29.087350845336914\n",
      "81\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 5734.2426758\n",
      "\tspeed: 0.0379s/iter; left time: 1.7453s\n",
      "Epoch: 1 cost time: 5.551341772079468\n",
      "Epoch: 1, Steps: 145 | Train Loss: 82240.3015726 Vali Loss: 57742.6523438 Test Loss: 55555.1796875\n",
      "Validation loss decreased (inf --> 57742.652344).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:55555.171875, mae:109.30499267578125\n",
      "101\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 312.0559692\n",
      "\tspeed: 0.0377s/iter; left time: 1.7330s\n",
      "Epoch: 1 cost time: 5.541543960571289\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3950.3355045 Vali Loss: 5000.4082031 Test Loss: 3231.5371094\n",
      "Validation loss decreased (inf --> 5000.408203).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3231.536865234375, mae:31.321441650390625\n",
      "121\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 14936.6328125\n",
      "\tspeed: 0.0391s/iter; left time: 1.7978s\n",
      "Epoch: 1 cost time: 5.682394742965698\n",
      "Epoch: 1, Steps: 145 | Train Loss: 109358.3425141 Vali Loss: 56030.1523438 Test Loss: 46223.8632812\n",
      "Validation loss decreased (inf --> 56030.152344).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:46223.86328125, mae:105.70606994628906\n",
      "141\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1.9862391\n",
      "\tspeed: 0.0374s/iter; left time: 1.7185s\n",
      "Epoch: 1 cost time: 5.475644111633301\n",
      "Epoch: 1, Steps: 145 | Train Loss: 15.3546466 Vali Loss: 5.9594207 Test Loss: 6.0074821\n",
      "Validation loss decreased (inf --> 5.959421).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:6.007481098175049, mae:1.1646112203598022\n",
      "161\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 148.0463409\n",
      "\tspeed: 0.0371s/iter; left time: 1.7063s\n",
      "Epoch: 1 cost time: 5.42484712600708\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4730.1545378 Vali Loss: 4434.3437500 Test Loss: 4626.1694336\n",
      "Validation loss decreased (inf --> 4434.343750).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:4626.16845703125, mae:32.37160873413086\n",
      "181\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3.4890416\n",
      "\tspeed: 0.0369s/iter; left time: 1.6974s\n",
      "Epoch: 1 cost time: 5.3678879737854\n",
      "Epoch: 1, Steps: 145 | Train Loss: 41.1567567 Vali Loss: 13.7327719 Test Loss: 7.0561090\n",
      "Validation loss decreased (inf --> 13.732772).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:7.056107997894287, mae:1.6782068014144897\n",
      "201\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 10956.3574219\n",
      "\tspeed: 0.0367s/iter; left time: 1.6897s\n",
      "Epoch: 1 cost time: 5.347666025161743\n",
      "Epoch: 1, Steps: 145 | Train Loss: 26279.4542767 Vali Loss: 17108.7050781 Test Loss: 32110.6093750\n",
      "Validation loss decreased (inf --> 17108.705078).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:32110.60546875, mae:88.22661590576172\n",
      "221\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1942633.2500000\n",
      "\tspeed: 0.0364s/iter; left time: 1.6762s\n",
      "Epoch: 1 cost time: 5.296069145202637\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Steps: 145 | Train Loss: 5730478.7827586 Vali Loss: 6778974.5000000 Test Loss: 5112271.5000000\n",
      "Validation loss decreased (inf --> 6778974.500000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:5112271.0, mae:1450.6563720703125\n",
      "241\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 16281.0810547\n",
      "\tspeed: 0.0377s/iter; left time: 1.7356s\n",
      "Epoch: 1 cost time: 5.480774164199829\n",
      "Epoch: 1, Steps: 145 | Train Loss: 79871.8792009 Vali Loss: 38232.9218750 Test Loss: 29116.8593750\n",
      "Validation loss decreased (inf --> 38232.921875).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:29116.85546875, mae:96.10032653808594\n",
      "261\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 30.5974846\n",
      "\tspeed: 0.0383s/iter; left time: 1.7638s\n",
      "Epoch: 1 cost time: 5.586963891983032\n",
      "Epoch: 1, Steps: 145 | Train Loss: 13.3110798 Vali Loss: 142.7170868 Test Loss: 37.4018784\n",
      "Validation loss decreased (inf --> 142.717087).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:37.40188217163086, mae:2.134122133255005\n",
      "281\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 112.0058441\n",
      "\tspeed: 0.0375s/iter; left time: 1.7247s\n",
      "Epoch: 1 cost time: 5.491036891937256\n",
      "Epoch: 1, Steps: 145 | Train Loss: 208.4605546 Vali Loss: 318.3847046 Test Loss: 157.4050903\n",
      "Validation loss decreased (inf --> 318.384705).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:157.40509033203125, mae:5.9319000244140625\n",
      "301\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 38916.6171875\n",
      "\tspeed: 0.0371s/iter; left time: 1.7087s\n",
      "Epoch: 1 cost time: 5.428685903549194\n",
      "Epoch: 1, Steps: 145 | Train Loss: 17139.7449766 Vali Loss: 24938.7070312 Test Loss: 14660.1904297\n",
      "Validation loss decreased (inf --> 24938.707031).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:14660.1904296875, mae:65.47528076171875\n",
      "321\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3093.5332031\n",
      "\tspeed: 0.0361s/iter; left time: 1.6599s\n",
      "Epoch: 1 cost time: 5.256005764007568\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1950.2644631 Vali Loss: 3087.1374512 Test Loss: 1891.4996338\n",
      "Validation loss decreased (inf --> 3087.137451).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1891.49951171875, mae:23.20928192138672\n",
      "341\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 161.7444458\n",
      "\tspeed: 0.0352s/iter; left time: 1.6188s\n",
      "Epoch: 1 cost time: 5.114051103591919\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4442.2090625 Vali Loss: 3806.9313965 Test Loss: 3689.4851074\n",
      "Validation loss decreased (inf --> 3806.931396).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3689.485107421875, mae:30.812461853027344\n",
      "361\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 292.9754028\n",
      "\tspeed: 0.0351s/iter; left time: 1.6136s\n",
      "Epoch: 1 cost time: 5.130476951599121\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1005.1253625 Vali Loss: 975.0590210 Test Loss: 941.2619019\n",
      "Validation loss decreased (inf --> 975.059021).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:941.2618408203125, mae:15.991188049316406\n",
      "381\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 90396.4140625\n",
      "\tspeed: 0.0360s/iter; left time: 1.6555s\n",
      "Epoch: 1 cost time: 5.313304901123047\n",
      "Epoch: 1, Steps: 145 | Train Loss: 304981.9383217 Vali Loss: 183990.9843750 Test Loss: 150983.5781250\n",
      "Validation loss decreased (inf --> 183990.984375).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:150983.59375, mae:216.30747985839844\n",
      "401\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2507.6015625\n",
      "\tspeed: 0.0361s/iter; left time: 1.6604s\n",
      "Epoch: 1 cost time: 5.357356071472168\n",
      "Epoch: 1, Steps: 145 | Train Loss: 13142.0974845 Vali Loss: 9533.6240234 Test Loss: 5957.4267578\n",
      "Validation loss decreased (inf --> 9533.624023).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:5957.42626953125, mae:44.11551284790039\n",
      "421\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 191064.4062500\n",
      "\tspeed: 0.0372s/iter; left time: 1.7094s\n",
      "Epoch: 1 cost time: 5.403357982635498\n",
      "Epoch: 1, Steps: 145 | Train Loss: 254517.8963968 Vali Loss: 344077.1562500 Test Loss: 576440.3750000\n",
      "Validation loss decreased (inf --> 344077.156250).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:576440.3125, mae:469.26641845703125\n",
      "441\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1770.7531738\n",
      "\tspeed: 0.0367s/iter; left time: 1.6866s\n",
      "Epoch: 1 cost time: 5.39492392539978\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3686.9518121 Vali Loss: 6135.8125000 Test Loss: 4969.4965820\n",
      "Validation loss decreased (inf --> 6135.812500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:4969.4970703125, mae:38.108699798583984\n",
      "461\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 128.6991272\n",
      "\tspeed: 0.0371s/iter; left time: 1.7076s\n",
      "Epoch: 1 cost time: 5.328705072402954\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1581.7163773 Vali Loss: 439.2954407 Test Loss: 3036.3940430\n",
      "Validation loss decreased (inf --> 439.295441).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:3036.39404296875, mae:16.303085327148438\n",
      "481\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 690.4435425\n",
      "\tspeed: 0.0358s/iter; left time: 1.6474s\n",
      "Epoch: 1 cost time: 5.230547904968262\n",
      "Epoch: 1, Steps: 145 | Train Loss: 5224.5477839 Vali Loss: 1087.6628418 Test Loss: 1593.0310059\n",
      "Validation loss decreased (inf --> 1087.662842).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1593.03125, mae:18.198097229003906\n",
      "501\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 11979.7382812\n",
      "\tspeed: 0.0349s/iter; left time: 1.6055s\n",
      "Epoch: 1 cost time: 5.13327693939209\n",
      "Epoch: 1, Steps: 145 | Train Loss: 40120.5351664 Vali Loss: 45243.1679688 Test Loss: 106951.3203125\n",
      "Validation loss decreased (inf --> 45243.167969).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:106951.3125, mae:170.09645080566406\n",
      "521\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 553.8285522\n",
      "\tspeed: 0.0358s/iter; left time: 1.6479s\n",
      "Epoch: 1 cost time: 5.252267837524414\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2334.0327826 Vali Loss: 1480.1566162 Test Loss: 2202.3176270\n",
      "Validation loss decreased (inf --> 1480.156616).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2202.318115234375, mae:23.540613174438477\n",
      "541\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 6938.7436523\n",
      "\tspeed: 0.0398s/iter; left time: 1.8325s\n",
      "Epoch: 1 cost time: 5.8305370807647705\n",
      "Epoch: 1, Steps: 145 | Train Loss: 36242.5548626 Vali Loss: 27864.9765625 Test Loss: 19275.3691406\n",
      "Validation loss decreased (inf --> 27864.976562).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:19275.3671875, mae:67.6421890258789\n",
      "561\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 28.2406826\n",
      "\tspeed: 0.0378s/iter; left time: 1.7369s\n",
      "Epoch: 1 cost time: 5.4547200202941895\n",
      "Epoch: 1, Steps: 145 | Train Loss: 245.0052182 Vali Loss: 259.6869202 Test Loss: 208.4105988\n",
      "Validation loss decreased (inf --> 259.686920).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:208.41058349609375, mae:5.796147346496582\n",
      "581\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0264492\n",
      "\tspeed: 0.0369s/iter; left time: 1.6988s\n",
      "Epoch: 1 cost time: 5.351593971252441\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0550966 Vali Loss: 0.0318380 Test Loss: 0.0322476\n",
      "Validation loss decreased (inf --> 0.031838).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.032247599214315414, mae:0.125135600566864\n",
      "601\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 51107.2890625\n",
      "\tspeed: 0.0369s/iter; left time: 1.6954s\n",
      "Epoch: 1 cost time: 5.3092968463897705\n",
      "Epoch: 1, Steps: 145 | Train Loss: 106719.4213834 Vali Loss: 89665.0937500 Test Loss: 49112.9726562\n",
      "Validation loss decreased (inf --> 89665.093750).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:49112.98046875, mae:140.671630859375\n",
      "621\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 280.3911743\n",
      "\tspeed: 0.0366s/iter; left time: 1.6826s\n",
      "Epoch: 1 cost time: 5.41461181640625\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2620.1638007 Vali Loss: 2454.3308105 Test Loss: 1282.1778564\n",
      "Validation loss decreased (inf --> 2454.330811).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1282.177978515625, mae:15.62777042388916\n",
      "641\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 8940.7832031\n",
      "\tspeed: 0.0357s/iter; left time: 1.6400s\n",
      "Epoch: 1 cost time: 5.221089124679565\n",
      "Epoch: 1, Steps: 145 | Train Loss: 27181.7542043 Vali Loss: 10361.2636719 Test Loss: 14807.5029297\n",
      "Validation loss decreased (inf --> 10361.263672).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:14807.501953125, mae:63.21943283081055\n",
      "661\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.1685609\n",
      "\tspeed: 0.0359s/iter; left time: 1.6518s\n",
      "Epoch: 1 cost time: 5.275300025939941\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0618363 Vali Loss: 0.0520357 Test Loss: 0.0526317\n",
      "Validation loss decreased (inf --> 0.052036).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.052631676197052, mae:0.14021167159080505\n",
      "681\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3948.8703613\n",
      "\tspeed: 0.0355s/iter; left time: 1.6340s\n",
      "Epoch: 1 cost time: 5.17791485786438\n",
      "Epoch: 1, Steps: 145 | Train Loss: 15642.8644545 Vali Loss: 8467.8144531 Test Loss: 5663.9365234\n",
      "Validation loss decreased (inf --> 8467.814453).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:5663.93701171875, mae:29.423032760620117\n",
      "701\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 23151.9394531\n",
      "\tspeed: 0.0345s/iter; left time: 1.5883s\n",
      "Epoch: 1 cost time: 5.070131301879883\n",
      "Epoch: 1, Steps: 145 | Train Loss: 9923.8861826 Vali Loss: 1592.9168701 Test Loss: 792.0990601\n",
      "Validation loss decreased (inf --> 1592.916870).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:792.09912109375, mae:15.288116455078125\n",
      "721\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1930894.3750000\n",
      "\tspeed: 0.0356s/iter; left time: 1.6394s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 5.258013963699341\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4464667.7655172 Vali Loss: 5652206.0000000 Test Loss: 6132699.5000000\n",
      "Validation loss decreased (inf --> 5652206.000000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:6132699.5, mae:1608.3099365234375\n",
      "741\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 18952.0839844\n",
      "\tspeed: 0.0356s/iter; left time: 1.6384s\n",
      "Epoch: 1 cost time: 5.164387226104736\n",
      "Epoch: 1, Steps: 145 | Train Loss: 29365.8641130 Vali Loss: 29034.6367188 Test Loss: 25320.9609375\n",
      "Validation loss decreased (inf --> 29034.636719).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:25320.9609375, mae:95.57488250732422\n",
      "761\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2767.7705078\n",
      "\tspeed: 0.0359s/iter; left time: 1.6494s\n",
      "Epoch: 1 cost time: 5.338697910308838\n",
      "Epoch: 1, Steps: 145 | Train Loss: 7210.1979904 Vali Loss: 3730.4062500 Test Loss: 5277.3842773\n",
      "Validation loss decreased (inf --> 3730.406250).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:5277.384765625, mae:39.890541076660156\n",
      "781\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 46683.9335938\n",
      "\tspeed: 0.0410s/iter; left time: 1.8864s\n",
      "Epoch: 1 cost time: 5.980952024459839\n",
      "Epoch: 1, Steps: 145 | Train Loss: 146734.7637392 Vali Loss: 92880.4140625 Test Loss: 65576.0468750\n",
      "Validation loss decreased (inf --> 92880.414062).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:65576.046875, mae:159.52928161621094\n",
      "801\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 910.5391846\n",
      "\tspeed: 0.0387s/iter; left time: 1.7794s\n",
      "Epoch: 1 cost time: 5.664992094039917\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3267.1410746 Vali Loss: 3812.5273438 Test Loss: 3226.4951172\n",
      "Validation loss decreased (inf --> 3812.527344).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3226.494873046875, mae:34.33905792236328\n",
      "821\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 27.2070580\n",
      "\tspeed: 0.0375s/iter; left time: 1.7233s\n",
      "Epoch: 1 cost time: 5.461889982223511\n",
      "Epoch: 1, Steps: 145 | Train Loss: 885.1541393 Vali Loss: 1575.5219727 Test Loss: 875.7890625\n",
      "Validation loss decreased (inf --> 1575.521973).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:875.7890014648438, mae:16.216976165771484\n",
      "841\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4188557.2500000\n",
      "\tspeed: 0.0389s/iter; left time: 1.7906s\n",
      "Epoch: 1 cost time: 5.672051906585693\n",
      "Epoch: 1, Steps: 145 | Train Loss: 6759334.6741379 Vali Loss: 7897774.0000000 Test Loss: 5552039.5000000\n",
      "Validation loss decreased (inf --> 7897774.000000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:5552039.0, mae:1440.67724609375\n",
      "861\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3710.5786133\n",
      "\tspeed: 0.0379s/iter; left time: 1.7447s\n",
      "Epoch: 1 cost time: 5.510097980499268\n",
      "Epoch: 1, Steps: 145 | Train Loss: 12206.3649801 Vali Loss: 17312.0644531 Test Loss: 38576.1328125\n",
      "Validation loss decreased (inf --> 17312.064453).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:38576.1328125, mae:84.1654052734375\n",
      "881\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 286.3590393\n",
      "\tspeed: 0.0395s/iter; left time: 1.8168s\n",
      "Epoch: 1 cost time: 5.687659025192261\n",
      "Epoch: 1, Steps: 145 | Train Loss: 364.7192370 Vali Loss: 229.9622040 Test Loss: 147.2666168\n",
      "Validation loss decreased (inf --> 229.962204).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:147.26658630371094, mae:5.796518325805664\n",
      "901\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3414.4133301\n",
      "\tspeed: 0.0380s/iter; left time: 1.7482s\n",
      "Epoch: 1 cost time: 5.505197048187256\n",
      "Epoch: 1, Steps: 145 | Train Loss: 8849.6891235 Vali Loss: 5053.4716797 Test Loss: 6625.1308594\n",
      "Validation loss decreased (inf --> 5053.471680).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:6625.1298828125, mae:49.76632308959961\n",
      "921\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4.2144690\n",
      "\tspeed: 0.0365s/iter; left time: 1.6769s\n",
      "Epoch: 1 cost time: 5.4356369972229\n",
      "Epoch: 1, Steps: 145 | Train Loss: 18.3832166 Vali Loss: 32.9427452 Test Loss: 7.4326310\n",
      "Validation loss decreased (inf --> 32.942745).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:7.4326324462890625, mae:1.6459277868270874\n",
      "941\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 19189.7167969\n",
      "\tspeed: 0.0369s/iter; left time: 1.6973s\n",
      "Epoch: 1 cost time: 5.376075029373169\n",
      "Epoch: 1, Steps: 145 | Train Loss: 33932.3511348 Vali Loss: 38776.6718750 Test Loss: 23981.9238281\n",
      "Validation loss decreased (inf --> 38776.671875).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:23981.92578125, mae:97.0711669921875\n",
      "961\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 12.8755217\n",
      "\tspeed: 0.0379s/iter; left time: 1.7416s\n",
      "Epoch: 1 cost time: 5.531217098236084\n",
      "Epoch: 1, Steps: 145 | Train Loss: 10224.7246477 Vali Loss: 2.1257606 Test Loss: 1.7171049\n",
      "Validation loss decreased (inf --> 2.125761).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:1.7171049118041992, mae:0.8734380602836609\n",
      "981\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0389067\n",
      "\tspeed: 0.0369s/iter; left time: 1.6987s\n",
      "Epoch: 1 cost time: 5.342714071273804\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0401957 Vali Loss: 0.1451486 Test Loss: 0.0111815\n",
      "Validation loss decreased (inf --> 0.145149).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.011181537993252277, mae:0.08270551264286041\n",
      "1001\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 18608.0585938\n",
      "\tspeed: 0.0359s/iter; left time: 1.6491s\n",
      "Epoch: 1 cost time: 5.296220064163208\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2309.7941751 Vali Loss: 2454.4958496 Test Loss: 2433.8134766\n",
      "Validation loss decreased (inf --> 2454.495850).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2433.813720703125, mae:23.543861389160156\n",
      "1021\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4867.4614258\n",
      "\tspeed: 0.0358s/iter; left time: 1.6476s\n",
      "Epoch: 1 cost time: 5.182628154754639\n",
      "Epoch: 1, Steps: 145 | Train Loss: 5651.7353985 Vali Loss: 1675.6430664 Test Loss: 808.6785889\n",
      "Validation loss decreased (inf --> 1675.643066).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:808.678466796875, mae:13.971659660339355\n",
      "1041\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 149142.6406250\n",
      "\tspeed: 0.0356s/iter; left time: 1.6371s\n",
      "Epoch: 1 cost time: 5.1897499561309814\n",
      "Epoch: 1, Steps: 145 | Train Loss: 250058.5884294 Vali Loss: 211113.4062500 Test Loss: 51359.6718750\n",
      "Validation loss decreased (inf --> 211113.406250).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:51359.67578125, mae:129.0869903564453\n",
      "1061\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 5.9457746\n",
      "\tspeed: 0.0356s/iter; left time: 1.6384s\n",
      "Epoch: 1 cost time: 5.190437078475952\n",
      "Epoch: 1, Steps: 145 | Train Loss: 23.5213228 Vali Loss: 59.2812119 Test Loss: 8.5620317\n",
      "Validation loss decreased (inf --> 59.281212).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:8.562031745910645, mae:1.6596744060516357\n",
      "1081\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 35.1318474\n",
      "\tspeed: 0.0356s/iter; left time: 1.6373s\n",
      "Epoch: 1 cost time: 5.298592805862427\n",
      "Epoch: 1, Steps: 145 | Train Loss: 79.2538820 Vali Loss: 69.6770325 Test Loss: 61.0271416\n",
      "Validation loss decreased (inf --> 69.677032).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:61.027137756347656, mae:3.212989330291748\n",
      "1101\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 5989.9248047\n",
      "\tspeed: 0.0366s/iter; left time: 1.6815s\n",
      "Epoch: 1 cost time: 5.3208911418914795\n",
      "Epoch: 1, Steps: 145 | Train Loss: 18638.5642696 Vali Loss: 18459.0859375 Test Loss: 6015.6630859\n",
      "Validation loss decreased (inf --> 18459.085938).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:6015.6630859375, mae:38.415775299072266\n",
      "1121\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 5990.6708984\n",
      "\tspeed: 0.0356s/iter; left time: 1.6380s\n",
      "Epoch: 1 cost time: 5.179443120956421\n",
      "Epoch: 1, Steps: 145 | Train Loss: 24940.4231765 Vali Loss: 19714.7441406 Test Loss: 16443.4550781\n",
      "Validation loss decreased (inf --> 19714.744141).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:16443.455078125, mae:70.52545928955078\n",
      "1141\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1464.8392334\n",
      "\tspeed: 0.0352s/iter; left time: 1.6203s\n",
      "Epoch: 1 cost time: 5.149075984954834\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2374.9826967 Vali Loss: 2181.9509277 Test Loss: 1593.4840088\n",
      "Validation loss decreased (inf --> 2181.950928).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1593.484130859375, mae:16.16702651977539\n",
      "1161\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 39.3708801\n",
      "\tspeed: 0.0348s/iter; left time: 1.6007s\n",
      "Epoch: 1 cost time: 5.085969924926758\n",
      "Epoch: 1, Steps: 145 | Train Loss: 113.0187090 Vali Loss: 43.3020554 Test Loss: 1821.1634521\n",
      "Validation loss decreased (inf --> 43.302055).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1821.1634521484375, mae:12.6910982131958\n",
      "1181\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0391724\n",
      "\tspeed: 0.0354s/iter; left time: 1.6280s\n",
      "Epoch: 1 cost time: 5.151916980743408\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0969833 Vali Loss: 0.0471435 Test Loss: 0.0283575\n",
      "Validation loss decreased (inf --> 0.047144).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.028357481583952904, mae:0.11962459236383438\n",
      "1201\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 350.3959961\n",
      "\tspeed: 0.0357s/iter; left time: 1.6410s\n",
      "Epoch: 1 cost time: 5.191226959228516\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2263.6046280 Vali Loss: 2085.5153809 Test Loss: 1776.5025635\n",
      "Validation loss decreased (inf --> 2085.515381).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1776.50244140625, mae:19.105836868286133\n",
      "1221\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1813.6480713\n",
      "\tspeed: 0.0353s/iter; left time: 1.6223s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 5.1401519775390625\n",
      "Epoch: 1, Steps: 145 | Train Loss: 12506.7186092 Vali Loss: 4126.5874023 Test Loss: 2085.3583984\n",
      "Validation loss decreased (inf --> 4126.587402).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2085.3583984375, mae:23.77265167236328\n",
      "1241\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 331682.0000000\n",
      "\tspeed: 0.0356s/iter; left time: 1.6386s\n",
      "Epoch: 1 cost time: 5.224826097488403\n",
      "Epoch: 1, Steps: 145 | Train Loss: 647303.0717672 Vali Loss: 407926.5937500 Test Loss: 450469.0937500\n",
      "Validation loss decreased (inf --> 407926.593750).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:450469.09375, mae:404.0880432128906\n",
      "1261\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 263785.5625000\n",
      "\tspeed: 0.0355s/iter; left time: 1.6322s\n",
      "Epoch: 1 cost time: 5.161993026733398\n",
      "Epoch: 1, Steps: 145 | Train Loss: 564602.2171875 Vali Loss: 764715.9375000 Test Loss: 541488.5625000\n",
      "Validation loss decreased (inf --> 764715.937500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:541488.5625, mae:453.10626220703125\n",
      "1281\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 5.4206095\n",
      "\tspeed: 0.0349s/iter; left time: 1.6060s\n",
      "Epoch: 1 cost time: 5.078580856323242\n",
      "Epoch: 1, Steps: 145 | Train Loss: 313.9131893 Vali Loss: 443.4349365 Test Loss: 182.3097687\n",
      "Validation loss decreased (inf --> 443.434937).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:182.30975341796875, mae:6.038862705230713\n",
      "1301\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 373.8303833\n",
      "\tspeed: 0.0349s/iter; left time: 1.6038s\n",
      "Epoch: 1 cost time: 5.085047960281372\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4584.9511615 Vali Loss: 8212.0751953 Test Loss: 3903.0539551\n",
      "Validation loss decreased (inf --> 8212.075195).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3903.053466796875, mae:29.666234970092773\n",
      "1321\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 408.7751160\n",
      "\tspeed: 0.0349s/iter; left time: 1.6034s\n",
      "Epoch: 1 cost time: 5.09796404838562\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1951.4244295 Vali Loss: 753.5859375 Test Loss: 903.8499146\n",
      "Validation loss decreased (inf --> 753.585938).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:903.8499145507812, mae:13.965826034545898\n",
      "1341\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1836.4912109\n",
      "\tspeed: 0.0350s/iter; left time: 1.6115s\n",
      "Epoch: 1 cost time: 5.098462820053101\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2862.4107239 Vali Loss: 2553.8232422 Test Loss: 6505.4741211\n",
      "Validation loss decreased (inf --> 2553.823242).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:6505.47265625, mae:33.9450569152832\n",
      "1361\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 49962.2226562\n",
      "\tspeed: 0.0356s/iter; left time: 1.6361s\n",
      "Epoch: 1 cost time: 5.153826951980591\n",
      "Epoch: 1, Steps: 145 | Train Loss: 45152.4697400 Vali Loss: 30041.2441406 Test Loss: 34486.8984375\n",
      "Validation loss decreased (inf --> 30041.244141).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:34486.8984375, mae:99.54881286621094\n",
      "1381\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0167209\n",
      "\tspeed: 0.0350s/iter; left time: 1.6114s\n",
      "Epoch: 1 cost time: 5.094736099243164\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0543476 Vali Loss: 0.0279283 Test Loss: 0.0238229\n",
      "Validation loss decreased (inf --> 0.027928).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.023822952061891556, mae:0.12335098534822464\n",
      "1401\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3536.3132324\n",
      "\tspeed: 0.0367s/iter; left time: 1.6880s\n",
      "Epoch: 1 cost time: 5.4464967250823975\n",
      "Epoch: 1, Steps: 145 | Train Loss: 17885.1522596 Vali Loss: 13788.4072266 Test Loss: 12642.1699219\n",
      "Validation loss decreased (inf --> 13788.407227).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:12642.169921875, mae:64.09481048583984\n",
      "1421\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4.5092411\n",
      "\tspeed: 0.0363s/iter; left time: 1.6689s\n",
      "Epoch: 1 cost time: 5.273470878601074\n",
      "Epoch: 1, Steps: 145 | Train Loss: 8.6164197 Vali Loss: 14.7583961 Test Loss: 9.3871555\n",
      "Validation loss decreased (inf --> 14.758396).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:9.387155532836914, mae:1.1902368068695068\n",
      "1441\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1263.9074707\n",
      "\tspeed: 0.0355s/iter; left time: 1.6353s\n",
      "Epoch: 1 cost time: 5.162343263626099\n",
      "Epoch: 1, Steps: 145 | Train Loss: 8307.0522528 Vali Loss: 11999.3623047 Test Loss: 10961.3564453\n",
      "Validation loss decreased (inf --> 11999.362305).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:10961.35546875, mae:51.805667877197266\n",
      "1461\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4814.1845703\n",
      "\tspeed: 0.0366s/iter; left time: 1.6858s\n",
      "Epoch: 1 cost time: 5.335760831832886\n",
      "Epoch: 1, Steps: 145 | Train Loss: 14577.5928644 Vali Loss: 13093.5966797 Test Loss: 15418.2109375\n",
      "Validation loss decreased (inf --> 13093.596680).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:15418.2119140625, mae:68.61713409423828\n",
      "1481\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.6053947\n",
      "\tspeed: 0.0366s/iter; left time: 1.6836s\n",
      "Epoch: 1 cost time: 5.312263011932373\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1.2925454 Vali Loss: 3.8727508 Test Loss: 2.5493197\n",
      "Validation loss decreased (inf --> 3.872751).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2.5493199825286865, mae:0.858364999294281\n",
      "1501\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 12.2724943\n",
      "\tspeed: 0.0353s/iter; left time: 1.6229s\n",
      "Epoch: 1 cost time: 5.16767692565918\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1082.5556138 Vali Loss: 12.0929928 Test Loss: 24.0830708\n",
      "Validation loss decreased (inf --> 12.092993).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:24.083072662353516, mae:2.5376968383789062\n",
      "1521\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2.9694028\n",
      "\tspeed: 0.0359s/iter; left time: 1.6494s\n",
      "Epoch: 1 cost time: 5.176799058914185\n",
      "Epoch: 1, Steps: 145 | Train Loss: 12.9832017 Vali Loss: 4.0334225 Test Loss: 4.9759040\n",
      "Validation loss decreased (inf --> 4.033422).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:4.9759039878845215, mae:1.2438822984695435\n",
      "1541\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4.8971934\n",
      "\tspeed: 0.0347s/iter; left time: 1.5957s\n",
      "Epoch: 1 cost time: 5.069705009460449\n",
      "Epoch: 1, Steps: 145 | Train Loss: 25.7205052 Vali Loss: 38.3010864 Test Loss: 13.4635639\n",
      "Validation loss decreased (inf --> 38.301086).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:13.463563919067383, mae:1.8302448987960815\n",
      "1561\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.2119065\n",
      "\tspeed: 0.0349s/iter; left time: 1.6039s\n",
      "Epoch: 1 cost time: 5.084456920623779\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0590856 Vali Loss: 0.0451870 Test Loss: 0.0845291\n",
      "Validation loss decreased (inf --> 0.045187).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.08452913165092468, mae:0.13438306748867035\n",
      "1581\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1617485.8750000\n",
      "\tspeed: 0.0346s/iter; left time: 1.5936s\n",
      "Epoch: 1 cost time: 5.044120788574219\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2983942.6663793 Vali Loss: 2172714.5000000 Test Loss: 1457263.3750000\n",
      "Validation loss decreased (inf --> 2172714.500000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1457263.25, mae:739.1453247070312\n",
      "1601\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2720.7365723\n",
      "\tspeed: 0.0350s/iter; left time: 1.6099s\n",
      "Epoch: 1 cost time: 5.141716957092285\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4423.3854677 Vali Loss: 39121.1718750 Test Loss: 4892.8085938\n",
      "Validation loss decreased (inf --> 39121.171875).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:4892.80810546875, mae:40.86062240600586\n",
      "1621\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 19.6560040\n",
      "\tspeed: 0.0348s/iter; left time: 1.5991s\n",
      "Epoch: 1 cost time: 5.06244969367981\n",
      "Epoch: 1, Steps: 145 | Train Loss: 602.7387053 Vali Loss: 130.9830780 Test Loss: 101.0915909\n",
      "Validation loss decreased (inf --> 130.983078).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:101.0915756225586, mae:4.745728492736816\n",
      "1641\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1362.1300049\n",
      "\tspeed: 0.0347s/iter; left time: 1.5974s\n",
      "Epoch: 1 cost time: 5.057286739349365\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4429.9800495 Vali Loss: 4463.6459961 Test Loss: 3349.1628418\n",
      "Validation loss decreased (inf --> 4463.645996).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3349.1630859375, mae:28.230905532836914\n",
      "1661\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 59481.2304688\n",
      "\tspeed: 0.0348s/iter; left time: 1.5989s\n",
      "Epoch: 1 cost time: 5.064844131469727\n",
      "Epoch: 1, Steps: 145 | Train Loss: 176635.8562500 Vali Loss: 287862.1250000 Test Loss: 230757.6718750\n",
      "Validation loss decreased (inf --> 287862.125000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:230757.671875, mae:300.3689880371094\n",
      "1681\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 46.5351028\n",
      "\tspeed: 0.0350s/iter; left time: 1.6117s\n",
      "Epoch: 1 cost time: 5.102682113647461\n",
      "Epoch: 1, Steps: 145 | Train Loss: 268.4642463 Vali Loss: 1389.1843262 Test Loss: 140.4195251\n",
      "Validation loss decreased (inf --> 1389.184326).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:140.4195098876953, mae:5.404726505279541\n",
      "1701\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 33.3387985\n",
      "\tspeed: 0.0348s/iter; left time: 1.5999s\n",
      "Epoch: 1 cost time: 5.058605909347534\n",
      "Epoch: 1, Steps: 145 | Train Loss: 313.7902568 Vali Loss: 439.4856567 Test Loss: 329.2661743\n",
      "Validation loss decreased (inf --> 439.485657).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:329.2662353515625, mae:8.510477066040039\n",
      "1721\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 94.0110931\n",
      "\tspeed: 0.0348s/iter; left time: 1.5988s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 5.057054281234741\n",
      "Epoch: 1, Steps: 145 | Train Loss: 111.0613032 Vali Loss: 117.7272720 Test Loss: 41.3934631\n",
      "Validation loss decreased (inf --> 117.727272).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:41.393463134765625, mae:3.2221896648406982\n",
      "1741\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 51.9140129\n",
      "\tspeed: 0.0348s/iter; left time: 1.6006s\n",
      "Epoch: 1 cost time: 5.060110807418823\n",
      "Epoch: 1, Steps: 145 | Train Loss: 194.8851543 Vali Loss: 204.3827515 Test Loss: 92.8278427\n",
      "Validation loss decreased (inf --> 204.382751).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:92.82782745361328, mae:4.635118007659912\n",
      "1761\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 616783.8750000\n",
      "\tspeed: 0.0347s/iter; left time: 1.5961s\n",
      "Epoch: 1 cost time: 5.047638893127441\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1525451.5064655 Vali Loss: 2055137.8750000 Test Loss: 980673.2500000\n",
      "Validation loss decreased (inf --> 2055137.875000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:980673.375, mae:631.4860229492188\n",
      "1781\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0160767\n",
      "\tspeed: 0.0346s/iter; left time: 1.5903s\n",
      "Epoch: 1 cost time: 5.033677101135254\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0407950 Vali Loss: 0.0226353 Test Loss: 0.0179311\n",
      "Validation loss decreased (inf --> 0.022635).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.017931059002876282, mae:0.10788664221763611\n",
      "1801\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2.0638041\n",
      "\tspeed: 0.0350s/iter; left time: 1.6111s\n",
      "Epoch: 1 cost time: 5.105988264083862\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4.7010196 Vali Loss: 100.1020279 Test Loss: 223.2798309\n",
      "Validation loss decreased (inf --> 100.102028).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:223.27987670898438, mae:2.640346050262451\n",
      "1821\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1725.2255859\n",
      "\tspeed: 0.0346s/iter; left time: 1.5898s\n",
      "Epoch: 1 cost time: 5.029212951660156\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1935.8994099 Vali Loss: 8058.1171875 Test Loss: 1049.8591309\n",
      "Validation loss decreased (inf --> 8058.117188).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1049.8592529296875, mae:17.07330894470215\n",
      "1841\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 100022.0468750\n",
      "\tspeed: 0.0352s/iter; left time: 1.6208s\n",
      "Epoch: 1 cost time: 5.134111166000366\n",
      "Epoch: 1, Steps: 145 | Train Loss: 434457.7091191 Vali Loss: 157102.1875000 Test Loss: 108882.7500000\n",
      "Validation loss decreased (inf --> 157102.187500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:108882.765625, mae:191.69525146484375\n",
      "1861\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 9989.1181641\n",
      "\tspeed: 0.0349s/iter; left time: 1.6034s\n",
      "Epoch: 1 cost time: 5.083920001983643\n",
      "Epoch: 1, Steps: 145 | Train Loss: 5118.6107134 Vali Loss: 4262.3642578 Test Loss: 4325.0258789\n",
      "Validation loss decreased (inf --> 4262.364258).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:4325.02734375, mae:32.99700927734375\n",
      "1881\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1682.4212646\n",
      "\tspeed: 0.0346s/iter; left time: 1.5919s\n",
      "Epoch: 1 cost time: 5.040194749832153\n",
      "Epoch: 1, Steps: 145 | Train Loss: 6076.5905568 Vali Loss: 1640.7093506 Test Loss: 687.6093140\n",
      "Validation loss decreased (inf --> 1640.709351).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:687.6092529296875, mae:13.514413833618164\n",
      "1901\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4458.1083984\n",
      "\tspeed: 0.0351s/iter; left time: 1.6142s\n",
      "Epoch: 1 cost time: 5.099179029464722\n",
      "Epoch: 1, Steps: 145 | Train Loss: 6862.1380727 Vali Loss: 10573.6601562 Test Loss: 3910.2358398\n",
      "Validation loss decreased (inf --> 10573.660156).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3910.236328125, mae:36.01411819458008\n",
      "1921\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 9036.6933594\n",
      "\tspeed: 0.0349s/iter; left time: 1.6050s\n",
      "Epoch: 1 cost time: 5.08049201965332\n",
      "Epoch: 1, Steps: 145 | Train Loss: 9286.2919176 Vali Loss: 880.4101562 Test Loss: 2326.0053711\n",
      "Validation loss decreased (inf --> 880.410156).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2326.005126953125, mae:24.424291610717773\n",
      "1941\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3286.6081543\n",
      "\tspeed: 0.0353s/iter; left time: 1.6221s\n",
      "Epoch: 1 cost time: 5.1194798946380615\n",
      "Epoch: 1, Steps: 145 | Train Loss: 15806.1355637 Vali Loss: 10008.2216797 Test Loss: 30346.4492188\n",
      "Validation loss decreased (inf --> 10008.221680).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:30346.447265625, mae:71.2796859741211\n",
      "1961\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 17160.8769531\n",
      "\tspeed: 0.0346s/iter; left time: 1.5898s\n",
      "Epoch: 1 cost time: 5.066427230834961\n",
      "Epoch: 1, Steps: 145 | Train Loss: 30817.2300916 Vali Loss: 25108.3359375 Test Loss: 14344.1757812\n",
      "Validation loss decreased (inf --> 25108.335938).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:14344.173828125, mae:72.53207397460938\n",
      "1981\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0240730\n",
      "\tspeed: 0.0348s/iter; left time: 1.6029s\n",
      "Epoch: 1 cost time: 5.09221076965332\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0391534 Vali Loss: 0.0146399 Test Loss: 0.0131337\n",
      "Validation loss decreased (inf --> 0.014640).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.01313371118158102, mae:0.09461464732885361\n",
      "2001\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 746.6866455\n",
      "\tspeed: 0.0349s/iter; left time: 1.6058s\n",
      "Epoch: 1 cost time: 5.108068943023682\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3554.3765212 Vali Loss: 2006.9774170 Test Loss: 5794.2944336\n",
      "Validation loss decreased (inf --> 2006.977417).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:5794.29443359375, mae:41.89143753051758\n",
      "2021\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 13.8522921\n",
      "\tspeed: 0.0351s/iter; left time: 1.6144s\n",
      "Epoch: 1 cost time: 5.195555210113525\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2.6949650 Vali Loss: 1.6584690 Test Loss: 0.1730745\n",
      "Validation loss decreased (inf --> 1.658469).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.17307445406913757, mae:0.26585450768470764\n",
      "2041\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1725.7099609\n",
      "\tspeed: 0.0347s/iter; left time: 1.5972s\n",
      "Epoch: 1 cost time: 5.059571981430054\n",
      "Epoch: 1, Steps: 145 | Train Loss: 8462.4373501 Vali Loss: 3956.1857910 Test Loss: 3564.9628906\n",
      "Validation loss decreased (inf --> 3956.185791).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3564.96240234375, mae:28.365140914916992\n",
      "2061\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4317.3876953\n",
      "\tspeed: 0.0350s/iter; left time: 1.6096s\n",
      "Epoch: 1 cost time: 5.0977699756622314\n",
      "Epoch: 1, Steps: 145 | Train Loss: 42705.6573099 Vali Loss: 24285.7675781 Test Loss: 14261.1298828\n",
      "Validation loss decreased (inf --> 24285.767578).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:14261.130859375, mae:66.4709701538086\n",
      "2081\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2091.5944824\n",
      "\tspeed: 0.0346s/iter; left time: 1.5929s\n",
      "Epoch: 1 cost time: 5.129249095916748\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3354.5680496 Vali Loss: 2989.7067871 Test Loss: 2424.9150391\n",
      "Validation loss decreased (inf --> 2989.706787).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2424.914794921875, mae:23.56499671936035\n",
      "2101\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 114574.3359375\n",
      "\tspeed: 0.0352s/iter; left time: 1.6181s\n",
      "Epoch: 1 cost time: 5.123418092727661\n",
      "Epoch: 1, Steps: 145 | Train Loss: 318750.2843211 Vali Loss: 291834.6562500 Test Loss: 167821.5625000\n",
      "Validation loss decreased (inf --> 291834.656250).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:167821.5625, mae:242.4907684326172\n",
      "2121\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1.1361749\n",
      "\tspeed: 0.0345s/iter; left time: 1.5866s\n",
      "Epoch: 1 cost time: 5.037613153457642\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4.1665167 Vali Loss: 4.4053588 Test Loss: 14.1136951\n",
      "Validation loss decreased (inf --> 4.405359).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:14.11369514465332, mae:1.3652076721191406\n",
      "2141\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 9449646.0000000\n",
      "\tspeed: 0.0345s/iter; left time: 1.5872s\n",
      "Epoch: 1 cost time: 5.027886152267456\n",
      "Epoch: 1, Steps: 145 | Train Loss: 12929552.7586207 Vali Loss: 16255674.0000000 Test Loss: 13391942.0000000\n",
      "Validation loss decreased (inf --> 16255674.000000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:13391943.0, mae:2449.049072265625\n",
      "2161\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0637272\n",
      "\tspeed: 0.0346s/iter; left time: 1.5921s\n",
      "Epoch: 1 cost time: 5.033504962921143\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1.6058614 Vali Loss: 2.7303741 Test Loss: 0.5280484\n",
      "Validation loss decreased (inf --> 2.730374).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.5280482769012451, mae:0.3736659586429596\n",
      "2181\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0197278\n",
      "\tspeed: 0.0347s/iter; left time: 1.5943s\n",
      "Epoch: 1 cost time: 5.048685789108276\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0482719 Vali Loss: 0.0185260 Test Loss: 0.0110359\n",
      "Validation loss decreased (inf --> 0.018526).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.011035861447453499, mae:0.08534689247608185\n",
      "2201\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1627355.7500000\n",
      "\tspeed: 0.0344s/iter; left time: 1.5839s\n",
      "Epoch: 1 cost time: 5.1497790813446045\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2013592.6799569 Vali Loss: 1434255.2500000 Test Loss: 1628735.7500000\n",
      "Validation loss decreased (inf --> 1434255.250000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1628735.75, mae:844.7822875976562\n",
      "2221\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3485809.2500000\n",
      "\tspeed: 0.0347s/iter; left time: 1.5961s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 5.051563024520874\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3973755.4387931 Vali Loss: 3861795.7500000 Test Loss: 4283612.5000000\n",
      "Validation loss decreased (inf --> 3861795.750000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:4283612.5, mae:1340.32958984375\n",
      "2241\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 228203.5312500\n",
      "\tspeed: 0.0348s/iter; left time: 1.5987s\n",
      "Epoch: 1 cost time: 5.051724910736084\n",
      "Epoch: 1, Steps: 145 | Train Loss: 588722.9831897 Vali Loss: 667483.2500000 Test Loss: 400044.5625000\n",
      "Validation loss decreased (inf --> 667483.250000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:400044.5625, mae:370.47332763671875\n",
      "2261\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 303.4955444\n",
      "\tspeed: 0.0351s/iter; left time: 1.6155s\n",
      "Epoch: 1 cost time: 5.104424238204956\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2145.8891761 Vali Loss: 1774.3696289 Test Loss: 2689.7045898\n",
      "Validation loss decreased (inf --> 1774.369629).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2689.705078125, mae:16.403480529785156\n",
      "2281\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1872596.3750000\n",
      "\tspeed: 0.0351s/iter; left time: 1.6154s\n",
      "Epoch: 1 cost time: 5.120291233062744\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1575925.7182112 Vali Loss: 822117.0000000 Test Loss: 566517.6875000\n",
      "Validation loss decreased (inf --> 822117.000000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:566517.625, mae:471.29193115234375\n",
      "2301\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 767018.8750000\n",
      "\tspeed: 0.0347s/iter; left time: 1.5968s\n",
      "Epoch: 1 cost time: 5.060858249664307\n",
      "Epoch: 1, Steps: 145 | Train Loss: 975406.8697198 Vali Loss: 1194651.5000000 Test Loss: 930842.3125000\n",
      "Validation loss decreased (inf --> 1194651.500000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:930842.1875, mae:543.5263671875\n",
      "2321\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4322.7099609\n",
      "\tspeed: 0.0349s/iter; left time: 1.6049s\n",
      "Epoch: 1 cost time: 5.071963787078857\n",
      "Epoch: 1, Steps: 145 | Train Loss: 8294.8969348 Vali Loss: 7576.8066406 Test Loss: 6813.0913086\n",
      "Validation loss decreased (inf --> 7576.806641).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:6813.091796875, mae:51.97392272949219\n",
      "2341\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 9209.9941406\n",
      "\tspeed: 0.0346s/iter; left time: 1.5904s\n",
      "Epoch: 1 cost time: 5.033191919326782\n",
      "Epoch: 1, Steps: 145 | Train Loss: 26822.2614359 Vali Loss: 27847.9296875 Test Loss: 22825.9257812\n",
      "Validation loss decreased (inf --> 27847.929688).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:22825.927734375, mae:84.43818664550781\n",
      "2361\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0439087\n",
      "\tspeed: 0.0345s/iter; left time: 1.5880s\n",
      "Epoch: 1 cost time: 5.028237819671631\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0540832 Vali Loss: 0.0735991 Test Loss: 0.0178545\n",
      "Validation loss decreased (inf --> 0.073599).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.017854535952210426, mae:0.09315598011016846\n",
      "2381\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 30827.7500000\n",
      "\tspeed: 0.0377s/iter; left time: 1.7322s\n",
      "Epoch: 1 cost time: 5.504444122314453\n",
      "Epoch: 1, Steps: 145 | Train Loss: 105422.5403085 Vali Loss: 90165.0312500 Test Loss: 73047.9296875\n",
      "Validation loss decreased (inf --> 90165.031250).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:73047.9296875, mae:137.87799072265625\n",
      "2401\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.4398523\n",
      "\tspeed: 0.0347s/iter; left time: 1.5942s\n",
      "Epoch: 1 cost time: 5.0896172523498535\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0919148 Vali Loss: 0.0518254 Test Loss: 0.0806273\n",
      "Validation loss decreased (inf --> 0.051825).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.08062729239463806, mae:0.15644116699695587\n",
      "2421\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 125678.4375000\n",
      "\tspeed: 0.0348s/iter; left time: 1.6004s\n",
      "Epoch: 1 cost time: 5.059623956680298\n",
      "Epoch: 1, Steps: 145 | Train Loss: 337695.6242996 Vali Loss: 336404.4687500 Test Loss: 262003.1406250\n",
      "Validation loss decreased (inf --> 336404.468750).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:262003.140625, mae:341.6965026855469\n",
      "2441\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1143.0725098\n",
      "\tspeed: 0.0344s/iter; left time: 1.5845s\n",
      "Epoch: 1 cost time: 5.042346954345703\n",
      "Epoch: 1, Steps: 145 | Train Loss: 6437.0815943 Vali Loss: 7988.2910156 Test Loss: 4288.5708008\n",
      "Validation loss decreased (inf --> 7988.291016).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:4288.5703125, mae:30.042184829711914\n",
      "2461\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 36972.2421875\n",
      "\tspeed: 0.0345s/iter; left time: 1.5855s\n",
      "Epoch: 1 cost time: 5.07245397567749\n",
      "Epoch: 1, Steps: 145 | Train Loss: 102272.0003199 Vali Loss: 806526.1875000 Test Loss: 712523.1250000\n",
      "Validation loss decreased (inf --> 806526.187500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:712523.125, mae:516.417236328125\n",
      "2481\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0258783\n",
      "\tspeed: 0.0349s/iter; left time: 1.6072s\n",
      "Epoch: 1 cost time: 5.077124834060669\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0521687 Vali Loss: 0.0153834 Test Loss: 0.0132143\n",
      "Validation loss decreased (inf --> 0.015383).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.013214275240898132, mae:0.08380436897277832\n",
      "2501\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 16.7156258\n",
      "\tspeed: 0.0346s/iter; left time: 1.5904s\n",
      "Epoch: 1 cost time: 5.033359050750732\n",
      "Epoch: 1, Steps: 145 | Train Loss: 75.6553883 Vali Loss: 17.3524303 Test Loss: 169.0387878\n",
      "Validation loss decreased (inf --> 17.352430).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:169.03880310058594, mae:4.2283244132995605\n",
      "2521\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 326603.9687500\n",
      "\tspeed: 0.0358s/iter; left time: 1.6474s\n",
      "Epoch: 1 cost time: 5.17986273765564\n",
      "Epoch: 1, Steps: 145 | Train Loss: 272403.2714170 Vali Loss: 491171.8750000 Test Loss: 401345.6562500\n",
      "Validation loss decreased (inf --> 491171.875000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:401345.6875, mae:366.92327880859375\n",
      "2541\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3.5689189\n",
      "\tspeed: 0.0360s/iter; left time: 1.6575s\n",
      "Epoch: 1 cost time: 5.195982933044434\n",
      "Epoch: 1, Steps: 145 | Train Loss: 36.6648761 Vali Loss: 3.9597244 Test Loss: 3.2406275\n",
      "Validation loss decreased (inf --> 3.959724).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3.2406280040740967, mae:1.0207825899124146\n",
      "2561\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 5763.4448242\n",
      "\tspeed: 0.0349s/iter; left time: 1.6046s\n",
      "Epoch: 1 cost time: 5.0957350730896\n",
      "Epoch: 1, Steps: 145 | Train Loss: 20433.8873956 Vali Loss: 22185.7109375 Test Loss: 14287.2617188\n",
      "Validation loss decreased (inf --> 22185.710938).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:14287.259765625, mae:59.4512939453125\n",
      "2581\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 225.3628387\n",
      "\tspeed: 0.0349s/iter; left time: 1.6048s\n",
      "Epoch: 1 cost time: 5.069917917251587\n",
      "Epoch: 1, Steps: 145 | Train Loss: 922.7174826 Vali Loss: 1012.9393921 Test Loss: 1104.5556641\n",
      "Validation loss decreased (inf --> 1012.939392).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1104.5556640625, mae:16.135730743408203\n",
      "2601\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 5341.0478516\n",
      "\tspeed: 0.0347s/iter; left time: 1.5949s\n",
      "Epoch: 1 cost time: 5.055384159088135\n",
      "Epoch: 1, Steps: 145 | Train Loss: 13180.9874116 Vali Loss: 53822.4726562 Test Loss: 19917.1835938\n",
      "Validation loss decreased (inf --> 53822.472656).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:19917.181640625, mae:59.848567962646484\n",
      "2621\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1320.9669189\n",
      "\tspeed: 0.0345s/iter; left time: 1.5870s\n",
      "Epoch: 1 cost time: 5.0231969356536865\n",
      "Epoch: 1, Steps: 145 | Train Loss: 20202.9235352 Vali Loss: 3753.6821289 Test Loss: 4737.7851562\n",
      "Validation loss decreased (inf --> 3753.682129).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:4737.78515625, mae:25.115610122680664\n",
      "2641\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 81244.2109375\n",
      "\tspeed: 0.0346s/iter; left time: 1.5939s\n",
      "Epoch: 1 cost time: 5.046966075897217\n",
      "Epoch: 1, Steps: 145 | Train Loss: 127267.7431304 Vali Loss: 75988.2890625 Test Loss: 181306.5156250\n",
      "Validation loss decreased (inf --> 75988.289062).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:181306.515625, mae:223.64752197265625\n",
      "2661\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1344.1398926\n",
      "\tspeed: 0.0350s/iter; left time: 1.6091s\n",
      "Epoch: 1 cost time: 5.087286949157715\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3921.2979138 Vali Loss: 2349.5175781 Test Loss: 1967.1527100\n",
      "Validation loss decreased (inf --> 2349.517578).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1967.1527099609375, mae:22.0245304107666\n",
      "2681\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 458.4681091\n",
      "\tspeed: 0.0374s/iter; left time: 1.7186s\n",
      "Epoch: 1 cost time: 5.390520811080933\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3790.5172384 Vali Loss: 3632.8249512 Test Loss: 2319.1582031\n",
      "Validation loss decreased (inf --> 3632.824951).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2319.158447265625, mae:24.62867546081543\n",
      "2701\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4333.4399414\n",
      "\tspeed: 0.0355s/iter; left time: 1.6327s\n",
      "Epoch: 1 cost time: 5.149121046066284\n",
      "Epoch: 1, Steps: 145 | Train Loss: 37450.7561540 Vali Loss: 13347.7871094 Test Loss: 9774.0292969\n",
      "Validation loss decreased (inf --> 13347.787109).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:9774.0283203125, mae:56.86128234863281\n",
      "2721\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 176.4083710\n",
      "\tspeed: 0.0351s/iter; left time: 1.6151s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 5.115487098693848\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1829.0591944 Vali Loss: 892.7867432 Test Loss: 648.7766724\n",
      "Validation loss decreased (inf --> 892.786743).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:648.776611328125, mae:12.230910301208496\n",
      "2741\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.0182289\n",
      "\tspeed: 0.0353s/iter; left time: 1.6223s\n",
      "Epoch: 1 cost time: 5.104961156845093\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0865068 Vali Loss: 0.0489106 Test Loss: 0.0385851\n",
      "Validation loss decreased (inf --> 0.048911).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.03858513757586479, mae:0.144297793507576\n",
      "2761\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 55.7470589\n",
      "\tspeed: 0.0344s/iter; left time: 1.5826s\n",
      "Epoch: 1 cost time: 5.028668165206909\n",
      "Epoch: 1, Steps: 145 | Train Loss: 18.1257796 Vali Loss: 10.6947594 Test Loss: 6.6224961\n",
      "Validation loss decreased (inf --> 10.694759).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:6.622495651245117, mae:1.589572548866272\n",
      "2781\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 46.6841888\n",
      "\tspeed: 0.0351s/iter; left time: 1.6166s\n",
      "Epoch: 1 cost time: 5.133940935134888\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1229.8658601 Vali Loss: 951.0818481 Test Loss: 798.6283569\n",
      "Validation loss decreased (inf --> 951.081848).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:798.6282958984375, mae:16.088706970214844\n",
      "2801\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 255.4881134\n",
      "\tspeed: 0.0360s/iter; left time: 1.6583s\n",
      "Epoch: 1 cost time: 5.239903926849365\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4185.2685936 Vali Loss: 1987.9426270 Test Loss: 1572.1995850\n",
      "Validation loss decreased (inf --> 1987.942627).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1572.19970703125, mae:20.20139503479004\n",
      "2821\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 27363.2128906\n",
      "\tspeed: 0.0356s/iter; left time: 1.6379s\n",
      "Epoch: 1 cost time: 5.188391923904419\n",
      "Epoch: 1, Steps: 145 | Train Loss: 108921.6597926 Vali Loss: 62761.3632812 Test Loss: 49362.2226562\n",
      "Validation loss decreased (inf --> 62761.363281).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:49362.21875, mae:128.17364501953125\n",
      "2841\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 120.3168716\n",
      "\tspeed: 0.0355s/iter; left time: 1.6336s\n",
      "Epoch: 1 cost time: 5.195170164108276\n",
      "Epoch: 1, Steps: 145 | Train Loss: 335.9252280 Vali Loss: 1303.3516846 Test Loss: 562.1989136\n",
      "Validation loss decreased (inf --> 1303.351685).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:562.1989135742188, mae:10.506208419799805\n",
      "2861\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2.3523693\n",
      "\tspeed: 0.0369s/iter; left time: 1.6985s\n",
      "Epoch: 1 cost time: 5.294620990753174\n",
      "Epoch: 1, Steps: 145 | Train Loss: 6.8887584 Vali Loss: 10.4777422 Test Loss: 13.9234333\n",
      "Validation loss decreased (inf --> 10.477742).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:13.923433303833008, mae:1.5237752199172974\n",
      "2881\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 15.3613844\n",
      "\tspeed: 0.0350s/iter; left time: 1.6102s\n",
      "Epoch: 1 cost time: 5.085235834121704\n",
      "Epoch: 1, Steps: 145 | Train Loss: 257.7654535 Vali Loss: 521.0446777 Test Loss: 126.2000427\n",
      "Validation loss decreased (inf --> 521.044678).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:126.20003509521484, mae:5.592368125915527\n",
      "2901\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 50.0242844\n",
      "\tspeed: 0.0360s/iter; left time: 1.6547s\n",
      "Epoch: 1 cost time: 5.178866863250732\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1985.9582550 Vali Loss: 2688.1682129 Test Loss: 1101.1816406\n",
      "Validation loss decreased (inf --> 2688.168213).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1101.181640625, mae:15.175514221191406\n",
      "2921\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.3050369\n",
      "\tspeed: 0.0345s/iter; left time: 1.5865s\n",
      "Epoch: 1 cost time: 5.0416600704193115\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1.7121795 Vali Loss: 0.6590688 Test Loss: 0.8330190\n",
      "Validation loss decreased (inf --> 0.659069).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.8330190181732178, mae:0.5154519081115723\n",
      "2941\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2588.3334961\n",
      "\tspeed: 0.0346s/iter; left time: 1.5934s\n",
      "Epoch: 1 cost time: 5.042738914489746\n",
      "Epoch: 1, Steps: 145 | Train Loss: 3837.4961386 Vali Loss: 1548.8854980 Test Loss: 547.9277344\n",
      "Validation loss decreased (inf --> 1548.885498).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:547.9278564453125, mae:9.749282836914062\n",
      "2961\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1.7517216\n",
      "\tspeed: 0.0347s/iter; left time: 1.5964s\n",
      "Epoch: 1 cost time: 5.051922798156738\n",
      "Epoch: 1, Steps: 145 | Train Loss: 7.7973086 Vali Loss: 5.9794078 Test Loss: 5.1048975\n",
      "Validation loss decreased (inf --> 5.979408).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:5.104897975921631, mae:1.3225005865097046\n",
      "2981\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 5831.8588867\n",
      "\tspeed: 0.0347s/iter; left time: 1.5952s\n",
      "Epoch: 1 cost time: 5.055448055267334\n",
      "Epoch: 1, Steps: 145 | Train Loss: 7539.6164429 Vali Loss: 10486.7880859 Test Loss: 9902.6074219\n",
      "Validation loss decreased (inf --> 10486.788086).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:9902.6064453125, mae:59.874813079833984\n",
      "3001\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 20.7102852\n",
      "\tspeed: 0.0345s/iter; left time: 1.5889s\n",
      "Epoch: 1 cost time: 5.034764051437378\n",
      "Epoch: 1, Steps: 145 | Train Loss: 147.0192828 Vali Loss: 735.6581421 Test Loss: 37.8362236\n",
      "Validation loss decreased (inf --> 735.658142).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:37.83622741699219, mae:3.21285080909729\n",
      "3021\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1193.1676025\n",
      "\tspeed: 0.0343s/iter; left time: 1.5789s\n",
      "Epoch: 1 cost time: 5.110357046127319\n",
      "Epoch: 1, Steps: 145 | Train Loss: 6771.1652289 Vali Loss: 3888.1916504 Test Loss: 2226.8654785\n",
      "Validation loss decreased (inf --> 3888.191650).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2226.86572265625, mae:23.385082244873047\n",
      "3041\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2842.2243652\n",
      "\tspeed: 0.0351s/iter; left time: 1.6169s\n",
      "Epoch: 1 cost time: 5.094498872756958\n",
      "Epoch: 1, Steps: 145 | Train Loss: 12214.3865550 Vali Loss: 4565.8701172 Test Loss: 2152.7236328\n",
      "Validation loss decreased (inf --> 4565.870117).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2152.723876953125, mae:26.17072105407715\n",
      "3061\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 17.7348270\n",
      "\tspeed: 0.0348s/iter; left time: 1.5985s\n",
      "Epoch: 1 cost time: 5.065851926803589\n",
      "Epoch: 1, Steps: 145 | Train Loss: 285.5011827 Vali Loss: 319.0504456 Test Loss: 318.1639099\n",
      "Validation loss decreased (inf --> 319.050446).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:318.1639099121094, mae:8.120835304260254\n",
      "3081\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4432.8681641\n",
      "\tspeed: 0.0344s/iter; left time: 1.5824s\n",
      "Epoch: 1 cost time: 5.032705068588257\n",
      "Epoch: 1, Steps: 145 | Train Loss: 16656.3043541 Vali Loss: 22496.4082031 Test Loss: 11016.2871094\n",
      "Validation loss decreased (inf --> 22496.408203).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:11016.287109375, mae:54.61359405517578\n",
      "3101\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 142.4910126\n",
      "\tspeed: 0.0344s/iter; left time: 1.5819s\n",
      "Epoch: 1 cost time: 5.02317476272583\n",
      "Epoch: 1, Steps: 145 | Train Loss: 722.2888632 Vali Loss: 1006.5144043 Test Loss: 608.7774048\n",
      "Validation loss decreased (inf --> 1006.514404).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:608.7772827148438, mae:10.905411720275879\n",
      "3121\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.2724310\n",
      "\tspeed: 0.0345s/iter; left time: 1.5889s\n",
      "Epoch: 1 cost time: 5.042650938034058\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.0697937 Vali Loss: 0.0156129 Test Loss: 0.0113245\n",
      "Validation loss decreased (inf --> 0.015613).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.01132451556622982, mae:0.08741703629493713\n",
      "3141\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 59.8133926\n",
      "\tspeed: 0.0345s/iter; left time: 1.5871s\n",
      "Epoch: 1 cost time: 5.039793014526367\n",
      "Epoch: 1, Steps: 145 | Train Loss: 500.2338730 Vali Loss: 631.2241821 Test Loss: 206.6677551\n",
      "Validation loss decreased (inf --> 631.224182).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:206.6677703857422, mae:6.862619876861572\n",
      "3161\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 6155.1884766\n",
      "\tspeed: 0.0345s/iter; left time: 1.5875s\n",
      "Epoch: 1 cost time: 5.049925088882446\n",
      "Epoch: 1, Steps: 145 | Train Loss: 17420.9140995 Vali Loss: 20170.9824219 Test Loss: 19046.0449219\n",
      "Validation loss decreased (inf --> 20170.982422).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:19046.044921875, mae:79.08454132080078\n",
      "3181\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3205.8649902\n",
      "\tspeed: 0.0343s/iter; left time: 1.5771s\n",
      "Epoch: 1 cost time: 5.006688117980957\n",
      "Epoch: 1, Steps: 145 | Train Loss: 10078.9288886 Vali Loss: 8340.0947266 Test Loss: 11379.6679688\n",
      "Validation loss decreased (inf --> 8340.094727).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:11379.66796875, mae:46.194087982177734\n",
      "3201\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 727.5206299\n",
      "\tspeed: 0.0346s/iter; left time: 1.5909s\n",
      "Epoch: 1 cost time: 5.062362194061279\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2289.2777624 Vali Loss: 1921.7738037 Test Loss: 8330.0244141\n",
      "Validation loss decreased (inf --> 1921.773804).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:8330.0244140625, mae:31.2052059173584\n",
      "3221\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 402065.0312500\n",
      "\tspeed: 0.0347s/iter; left time: 1.5966s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 5.119541168212891\n",
      "Epoch: 1, Steps: 145 | Train Loss: 431216.2314116 Vali Loss: 612578.0625000 Test Loss: 363963.4375000\n",
      "Validation loss decreased (inf --> 612578.062500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:363963.4375, mae:382.6822204589844\n",
      "3241\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 225701.3281250\n",
      "\tspeed: 0.0357s/iter; left time: 1.6444s\n",
      "Epoch: 1 cost time: 5.165195941925049\n",
      "Epoch: 1, Steps: 145 | Train Loss: 549781.0559267 Vali Loss: 382278.8125000 Test Loss: 276963.3750000\n",
      "Validation loss decreased (inf --> 382278.812500).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:276963.375, mae:344.8622741699219\n",
      "3261\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 565.4225464\n",
      "\tspeed: 0.0349s/iter; left time: 1.6038s\n",
      "Epoch: 1 cost time: 5.084558963775635\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4655.1242036 Vali Loss: 1915.4305420 Test Loss: 1412.9290771\n",
      "Validation loss decreased (inf --> 1915.430542).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1412.9290771484375, mae:20.10970115661621\n",
      "3281\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4.0925431\n",
      "\tspeed: 0.0367s/iter; left time: 1.6902s\n",
      "Epoch: 1 cost time: 5.306671142578125\n",
      "Epoch: 1, Steps: 145 | Train Loss: 20.1358473 Vali Loss: 22.9403553 Test Loss: 23.8886890\n",
      "Validation loss decreased (inf --> 22.940355).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:23.888690948486328, mae:2.2971267700195312\n",
      "3301\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 2.8108530\n",
      "\tspeed: 0.0355s/iter; left time: 1.6317s\n",
      "Epoch: 1 cost time: 5.169042348861694\n",
      "Epoch: 1, Steps: 145 | Train Loss: 8.3388174 Vali Loss: 17.1750202 Test Loss: 3.8560758\n",
      "Validation loss decreased (inf --> 17.175020).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:3.8560750484466553, mae:0.8647971153259277\n",
      "3321\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.1315305\n",
      "\tspeed: 0.0355s/iter; left time: 1.6347s\n",
      "Epoch: 1 cost time: 5.172068119049072\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1.6085561 Vali Loss: 0.5587718 Test Loss: 1.0528820\n",
      "Validation loss decreased (inf --> 0.558772).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1.0528818368911743, mae:0.5376821756362915\n",
      "3341\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 787.2396240\n",
      "\tspeed: 0.0353s/iter; left time: 1.6251s\n",
      "Epoch: 1 cost time: 5.191516160964966\n",
      "Epoch: 1, Steps: 145 | Train Loss: 5369.6850741 Vali Loss: 3714.9296875 Test Loss: 25726.2148438\n",
      "Validation loss decreased (inf --> 3714.929688).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:25726.2109375, mae:68.03629302978516\n",
      "3361\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 36.1639023\n",
      "\tspeed: 0.0375s/iter; left time: 1.7240s\n",
      "Epoch: 1 cost time: 5.436337947845459\n",
      "Epoch: 1, Steps: 145 | Train Loss: 962.4333584 Vali Loss: 270.3618774 Test Loss: 596.8680420\n",
      "Validation loss decreased (inf --> 270.361877).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:596.8681030273438, mae:9.309786796569824\n",
      "3381\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3477220.5000000\n",
      "\tspeed: 0.0361s/iter; left time: 1.6609s\n",
      "Epoch: 1 cost time: 5.193819046020508\n",
      "Epoch: 1, Steps: 145 | Train Loss: 4377340.1193966 Vali Loss: 5817115.5000000 Test Loss: 5003014.5000000\n",
      "Validation loss decreased (inf --> 5817115.500000).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:5003014.5, mae:1474.941162109375\n",
      "3401\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 42161.9765625\n",
      "\tspeed: 0.0346s/iter; left time: 1.5937s\n",
      "Epoch: 1 cost time: 5.066688776016235\n",
      "Epoch: 1, Steps: 145 | Train Loss: 113229.0549098 Vali Loss: 140368.4843750 Test Loss: 79750.2890625\n",
      "Validation loss decreased (inf --> 140368.484375).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:79750.2890625, mae:173.35011291503906\n",
      "3421\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 14625.4453125\n",
      "\tspeed: 0.0375s/iter; left time: 1.7237s\n",
      "Epoch: 1 cost time: 5.390935182571411\n",
      "Epoch: 1, Steps: 145 | Train Loss: 33313.9299855 Vali Loss: 35108.4882812 Test Loss: 36293.7421875\n",
      "Validation loss decreased (inf --> 35108.488281).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:36293.7421875, mae:106.72636413574219\n",
      "3441\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 39774.8710938\n",
      "\tspeed: 0.0363s/iter; left time: 1.6678s\n",
      "Epoch: 1 cost time: 5.199610948562622\n",
      "Epoch: 1, Steps: 145 | Train Loss: 124026.5169248 Vali Loss: 42575.2265625 Test Loss: 26349.7148438\n",
      "Validation loss decreased (inf --> 42575.226562).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:26349.71484375, mae:94.90853881835938\n",
      "3461\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 6.6023154\n",
      "\tspeed: 0.0348s/iter; left time: 1.6022s\n",
      "Epoch: 1 cost time: 5.0608251094818115\n",
      "Epoch: 1, Steps: 145 | Train Loss: 451.4330204 Vali Loss: 212.2310944 Test Loss: 82.4531860\n",
      "Validation loss decreased (inf --> 212.231094).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:82.45319366455078, mae:4.592956066131592\n",
      "3481\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3.9038005\n",
      "\tspeed: 0.0346s/iter; left time: 1.5909s\n",
      "Epoch: 1 cost time: 5.035129070281982\n",
      "Epoch: 1, Steps: 145 | Train Loss: 54.5236698 Vali Loss: 57.4615135 Test Loss: 65.6222382\n",
      "Validation loss decreased (inf --> 57.461514).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:65.62223052978516, mae:2.7977755069732666\n",
      "3501\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.3291302\n",
      "\tspeed: 0.0344s/iter; left time: 1.5822s\n",
      "Epoch: 1 cost time: 5.036524057388306\n",
      "Epoch: 1, Steps: 145 | Train Loss: 0.6737081 Vali Loss: 0.6341149 Test Loss: 0.8362409\n",
      "Validation loss decreased (inf --> 0.634115).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:0.8362409472465515, mae:0.45462486147880554\n",
      "3521\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1744.3658447\n",
      "\tspeed: 0.0344s/iter; left time: 1.5829s\n",
      "Epoch: 1 cost time: 5.026275634765625\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1231.9567957 Vali Loss: 1283.0109863 Test Loss: 1056.5117188\n",
      "Validation loss decreased (inf --> 1283.010986).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1056.5118408203125, mae:15.025076866149902\n",
      "3541\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 71587.3671875\n",
      "\tspeed: 0.0348s/iter; left time: 1.5994s\n",
      "Epoch: 1 cost time: 5.056126832962036\n",
      "Epoch: 1, Steps: 145 | Train Loss: 258082.3207705 Vali Loss: 248017.4531250 Test Loss: 211362.5781250\n",
      "Validation loss decreased (inf --> 248017.453125).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:211362.546875, mae:277.6297302246094\n",
      "3561\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 4567.9941406\n",
      "\tspeed: 0.0345s/iter; left time: 1.5874s\n",
      "Epoch: 1 cost time: 5.0276710987091064\n",
      "Epoch: 1, Steps: 145 | Train Loss: 22904.8164837 Vali Loss: 11151.1132812 Test Loss: 5929.0576172\n",
      "Validation loss decreased (inf --> 11151.113281).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:5929.0576171875, mae:44.80301284790039\n",
      "3581\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 145.0603485\n",
      "\tspeed: 0.0352s/iter; left time: 1.6179s\n",
      "Epoch: 1 cost time: 5.254197835922241\n",
      "Epoch: 1, Steps: 145 | Train Loss: 1964.3040112 Vali Loss: 636.2658081 Test Loss: 188.4354706\n",
      "Validation loss decreased (inf --> 636.265808).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:188.4354705810547, mae:5.849636554718018\n",
      "3601\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 1.1263283\n",
      "\tspeed: 0.0358s/iter; left time: 1.6482s\n",
      "Epoch: 1 cost time: 5.2065629959106445\n",
      "Epoch: 1, Steps: 145 | Train Loss: 2.1087515 Vali Loss: 3.8076873 Test Loss: 2.6949253\n",
      "Validation loss decreased (inf --> 3.807687).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:2.694925308227539, mae:0.9089818596839905\n",
      "3621\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 3722.7805176\n",
      "\tspeed: 0.0352s/iter; left time: 1.6196s\n",
      "Epoch: 1 cost time: 5.136469841003418\n",
      "Epoch: 1, Steps: 145 | Train Loss: 19705.6341443 Vali Loss: 40895.0117188 Test Loss: 27798.5664062\n",
      "Validation loss decreased (inf --> 40895.011719).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:27798.56640625, mae:83.17267608642578\n",
      "3641\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\titers: 100, epoch: 1 | loss: 0.3042442\n",
      "\tspeed: 0.0368s/iter; left time: 1.6932s\n",
      "Epoch: 1 cost time: 5.297600030899048\n",
      "Epoch: 1, Steps: 145 | Train Loss: 29.1758379 Vali Loss: 2.2605276 Test Loss: 1.6118594\n",
      "Validation loss decreased (inf --> 2.260528).  Saving model ...\n",
      "Updating learning rate to 0.0001\n",
      ">>>>>>>testing : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\n",
      "mse:1.6118595600128174, mae:0.8677762150764465\n",
      "3661\n",
      "Use CPU\n",
      ">>>>>>>start training : None_None_ftNone_sl5_ll3_pl1_dm512_nh8_el2_dl1_df2048_atprob_fc5_ebtimeF_dtTrue_mxTrue_None_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n"
     ]
    }
   ],
   "source": [
    "exp.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba2e371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd2022",
   "language": "python",
   "name": "atd2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
