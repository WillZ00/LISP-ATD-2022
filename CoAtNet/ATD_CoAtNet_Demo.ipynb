{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcc862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import atd2022\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None)\n",
    "from sktime.performance_metrics.forecasting import (\n",
    "    MeanAbsoluteScaledError,\n",
    "    MeanSquaredError,\n",
    ")\n",
    "#import  as util\n",
    "from atd_CoAtNet.atd_CoAtNet import ATD_CoAtNet\n",
    "from utils.tools import dotdict\n",
    "from CoAtNet_Forecaster_wrapper import CoAtNetForecaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193775ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e21691",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e313b5d6",
   "metadata": {},
   "source": [
    "# Full Scale Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eb378c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "024b5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dotdict()\n",
    "\n",
    "args.use_gpu=True\n",
    "args.lr=0.00001\n",
    "args.batch_size=40\n",
    "args.train_epochs=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58d51e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "lispDL =  CoAtNetForecaster(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c925101f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Load data\n",
    "truth = atd2022.io.read_csv()\n",
    "\n",
    "# Subset the data for the sake of making this fast.\n",
    "# Remove if you want to run the example on the entire dataset.\n",
    "truth = truth.head(108)\n",
    "\n",
    "# Experiment Parameters\n",
    "window = 100\n",
    "num_predict = 4\n",
    "gap = 0\n",
    "slide = 1\n",
    "\n",
    "# Create a dataset `Splitter` object for generating train/test splits\n",
    "splitter = atd2022.backtest.Splitter(\n",
    "    truth,\n",
    "    window,\n",
    "    num_predict,\n",
    "    gap,\n",
    "    slide,\n",
    "    expanding=True,\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920f2bec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb9a8005e524a12bb4942a64dba0e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "<CoAtNet_Forecaster_wrapper.CoAtNetForecaster object at 0x7fb55c568ee0>:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:None\n",
      "train_loss 22.98277473449707\n",
      "train_loss 12.026276588439941\n",
      "train_loss 2.3046748638153076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/wzong/anaconda3/envs/atd2022/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([40, 1, 20])) that is different to the input size (torch.Size([40, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss 1.968843698501587\n",
      "train_loss 3.844921350479126\n",
      "train_loss 1.6209901571273804\n",
      "train_loss 4.82570743560791\n",
      "train_loss 1.5272374153137207\n",
      "train_loss 21.40088653564453\n",
      "train_loss 4.512599945068359\n",
      "train_loss 13.644489288330078\n",
      "train_loss 3.06559681892395\n",
      "train_loss 2.0622076988220215\n",
      "train_loss 4.728172302246094\n",
      "train_loss 12.350069046020508\n",
      "train_loss 3.6689722537994385\n",
      "train_loss 3.1021406650543213\n",
      "train_loss 1.9661340713500977\n",
      "train_loss 3.897282361984253\n",
      "train_loss 3.708792209625244\n",
      "train_loss 2.920849323272705\n",
      "train_loss 1.5915124416351318\n",
      "train_loss 3.7778642177581787\n",
      "train_loss 1.4060730934143066\n",
      "train_loss 5.103010177612305\n",
      "train_loss 2.6550004482269287\n",
      "train_loss 2.2984440326690674\n",
      "train_loss 3.5145363807678223\n",
      "train_loss 5.039549827575684\n",
      "train_loss 3.9519314765930176\n",
      "train_loss 0.03728920966386795\n",
      "train_loss 2.4112443923950195\n",
      "train_loss 9.62978744506836\n",
      "train_loss 1.5858469009399414\n",
      "train_loss 0.004418368451297283\n",
      "train_loss 4.019041538238525\n",
      "train_loss 4.837618827819824\n",
      "train_loss 1.9415203332901\n",
      "train_loss 1.7516294717788696\n",
      "train_loss 5.945143222808838\n",
      "train_loss 2.518101692199707\n",
      "train_loss 2.1441502571105957\n",
      "train_loss 2.3929624557495117\n",
      "train_loss 2.244657278060913\n",
      "train_loss 1.1537725925445557\n",
      "train_loss 1.5250136852264404\n",
      "train_loss 3.6306471824645996\n",
      "train_loss 31.49747085571289\n",
      "train_loss 2.4168503284454346\n",
      "train_loss 1.6103323698043823\n",
      "train_loss 0.0011633136309683323\n",
      "train_loss 1.8421595096588135\n",
      "train_loss 1.9205511808395386\n",
      "train_loss 11.902670860290527\n",
      "train_loss 18.793842315673828\n",
      "train_loss 5.723262310028076\n",
      "train_loss 8.159710884094238\n",
      "train_loss 4.573422908782959\n",
      "train_loss 3.448529005050659\n",
      "train_loss 7.991242408752441\n",
      "train_loss 0.018841808661818504\n",
      "train_loss 1.2317887544631958\n",
      "train_loss 2.1397545337677\n",
      "train_loss 2.2176403999328613\n",
      "train_loss 1.9112486839294434\n",
      "train_loss 98.65592956542969\n",
      "train_loss 4.071195602416992\n",
      "train_loss 2.1245570182800293\n",
      "train_loss 9.311614990234375\n",
      "train_loss 1.3602964878082275\n",
      "train_loss 0.0017549076583236456\n",
      "train_loss 1.813735008239746\n",
      "train_loss 9.103078842163086\n",
      "train_loss 2.1781821250915527\n",
      "train_loss 2.5339200496673584\n",
      "train_loss 2.191439628601074\n",
      "train_loss 2.1598472595214844\n",
      "train_loss 50.855072021484375\n",
      "train_loss 7.122929096221924\n",
      "train_loss 0.002161540323868394\n",
      "train_loss 3.1284005641937256\n",
      "train_loss 3.927175998687744\n",
      "train_loss 3.083479642868042\n",
      "train_loss 1.402673602104187\n",
      "train_loss 2.7356526851654053\n",
      "train_loss 3.278313159942627\n",
      "train_loss 2.315896987915039\n",
      "train_loss 4.237732410430908\n",
      "train_loss 1.567218542098999\n",
      "train_loss 1.7576717138290405\n",
      "train_loss 0.0028101278003305197\n",
      "train_loss 4.449125289916992\n",
      "train_loss 1.2271404266357422\n",
      "train_loss 3.5933032035827637\n",
      "train_loss 4.514579772949219\n",
      "train_loss 23.072492599487305\n",
      "train_loss 1.611454963684082\n",
      "train_loss 41.89987564086914\n",
      "train_loss 1.8345932960510254\n",
      "train_loss 8.729313850402832\n",
      "train_loss 0.004711480811238289\n",
      "train_loss 13.037707328796387\n",
      "train_loss 7.850895881652832\n",
      "train_loss 1.3410522937774658\n",
      "train_loss 1.8618552684783936\n",
      "train_loss 2.423841953277588\n",
      "train_loss 2.7369565963745117\n",
      "train_loss 7.787447452545166\n",
      "train_loss 2.2673277854919434\n",
      "train_loss 7.053098201751709\n",
      "train_loss 0.005356004927307367\n",
      "train_loss 1.217840552330017\n",
      "train_loss 3.113924026489258\n",
      "train_loss 1.4544678926467896\n",
      "train_loss 3.5202476978302\n",
      "train_loss 12.944958686828613\n",
      "train_loss 1.2363930940628052\n",
      "train_loss 3.0995781421661377\n",
      "train_loss 1.3544107675552368\n",
      "train_loss 0.012693852186203003\n",
      "train_loss 8.589509010314941\n",
      "train_loss 0.013000703416764736\n",
      "train_loss 1.7565720081329346\n",
      "train_loss 1.5265254974365234\n",
      "train_loss 2.7617297172546387\n",
      "train_loss 0.012137188576161861\n",
      "train_loss 6.33761739730835\n",
      "train_loss 1.2502927780151367\n",
      "train_loss 26.073711395263672\n",
      "train_loss 2.3356518745422363\n",
      "train_loss 1.5838701725006104\n",
      "train_loss 2.433828353881836\n",
      "train_loss 2.485696792602539\n",
      "train_loss 1.4842116832733154\n",
      "train_loss 2.293590545654297\n",
      "train_loss 1.342055082321167\n",
      "train_loss 26.807458877563477\n",
      "train_loss 1.8444489240646362\n",
      "train_loss 0.062430787831544876\n",
      "train_loss 5.478847980499268\n",
      "train_loss 62.08430099487305\n",
      "train_loss 7.866877555847168\n",
      "train_loss 2.91003680229187\n",
      "train_loss 3.725748062133789\n",
      "train_loss 9.434706687927246\n",
      "train_loss 19.56961441040039\n",
      "train_loss 4.127187728881836\n",
      "train_loss 3.4722976684570312\n",
      "train_loss 3.9938435554504395\n",
      "train_loss 52.0727424621582\n",
      "train_loss 1.6410388946533203\n",
      "train_loss 2.8556699752807617\n",
      "train_loss 1.9636774063110352\n",
      "train_loss 8.044129371643066\n",
      "train_loss 6.447458267211914\n",
      "train_loss 1.3336807489395142\n",
      "train_loss 9.666170120239258\n",
      "train_loss 0.06653165072202682\n",
      "train_loss 5.514622688293457\n",
      "train_loss 1.3634251356124878\n",
      "train_loss 1.2942222356796265\n",
      "train_loss 4.205580234527588\n",
      "train_loss 1.7563689947128296\n",
      "train_loss 1.7418230772018433\n",
      "train_loss 2.295407772064209\n",
      "train_loss 13.806034088134766\n",
      "train_loss 84.54531860351562\n",
      "train_loss 5.32003927230835\n",
      "train_loss 6.28770637512207\n",
      "train_loss 9.27227783203125\n",
      "train_loss 2.084312677383423\n",
      "train_loss 1.3459246158599854\n",
      "train_loss 0.8528599739074707\n",
      "train_loss 2.0940070152282715\n",
      "train_loss 14.326536178588867\n",
      "train_loss 3.878988027572632\n",
      "train_loss 1.0261784791946411\n",
      "train_loss 1.9675565958023071\n",
      "train_loss 0.9030870199203491\n",
      "train_loss 0.529046893119812\n",
      "train_loss 5.470534324645996\n",
      "train_loss 3.9417216777801514\n",
      "train_loss 1.8812135457992554\n",
      "train_loss 8.20843505859375\n",
      "train_loss 10.8519926071167\n",
      "train_loss 0.7365094423294067\n",
      "train_loss 1.825467824935913\n",
      "train_loss 0.36119183897972107\n",
      "train_loss 2.1647305488586426\n",
      "train_loss 1.8684945106506348\n",
      "train_loss 1.6981374025344849\n",
      "train_loss 28.16667938232422\n",
      "train_loss 1.4618014097213745\n",
      "train_loss 3.6257975101470947\n",
      "train_loss 1.4687516689300537\n",
      "train_loss 2.486631393432617\n",
      "train_loss 3.4811322689056396\n",
      "train_loss 4.0195746421813965\n",
      "train_loss 1.2734235525131226\n",
      "train_loss 1.1045558452606201\n",
      "train_loss 0.8829861879348755\n",
      "train_loss 1.4856045246124268\n",
      "train_loss 0.5956885814666748\n",
      "train_loss 1.7406539916992188\n",
      "train_loss 6.810164928436279\n",
      "train_loss 1.7244943380355835\n",
      "train_loss 0.4605740010738373\n",
      "train_loss 1.101454257965088\n",
      "train_loss 2.9744043350219727\n",
      "train_loss 0.9541587233543396\n",
      "train_loss 8.561928749084473\n",
      "train_loss 10.98315715789795\n",
      "train_loss 0.5399258136749268\n",
      "train_loss 1.0233335494995117\n",
      "train_loss 0.4328650236129761\n",
      "train_loss 3.456920623779297\n",
      "train_loss 0.602857232093811\n",
      "train_loss 6.90397310256958\n",
      "train_loss 0.7336763143539429\n",
      "train_loss 1.0455806255340576\n",
      "train_loss 0.7699217796325684\n",
      "train_loss 0.5608518123626709\n",
      "train_loss 1.156800389289856\n",
      "train_loss 1.3920711278915405\n",
      "train_loss 1.4864959716796875\n",
      "train_loss 2.5104475021362305\n",
      "train_loss 9.13321590423584\n",
      "train_loss 4.067574501037598\n",
      "train_loss 7.845040798187256\n",
      "train_loss 5.949661731719971\n",
      "train_loss 1.1070486307144165\n",
      "train_loss 3.365579128265381\n",
      "train_loss 0.8501312732696533\n",
      "train_loss 27.368459701538086\n",
      "train_loss 0.6842223405838013\n",
      "train_loss 1.1790474653244019\n",
      "train_loss 0.8114385008811951\n",
      "train_loss 0.2806771993637085\n",
      "train_loss 0.3758218586444855\n",
      "train_loss 5.343146800994873\n",
      "train_loss 0.6572542190551758\n",
      "train_loss 99.8775634765625\n",
      "train_loss 3.25150990486145\n",
      "train_loss 1.9402774572372437\n",
      "train_loss 4.428282737731934\n",
      "train_loss 1.1303901672363281\n",
      "train_loss 2.6655383110046387\n",
      "train_loss 5.520270347595215\n",
      "train_loss 0.9588309526443481\n",
      "train_loss 1.6882789134979248\n",
      "train_loss 0.7676202058792114\n",
      "train_loss 9.42262077331543\n",
      "train_loss 1.5584824085235596\n",
      "train_loss 1.4065980911254883\n",
      "train_loss 3.6417086124420166\n",
      "train_loss 0.965764582157135\n",
      "train_loss 1.572641372680664\n",
      "train_loss 2.726871967315674\n",
      "train_loss 5.944375514984131\n",
      "train_loss 0.6013753414154053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/wzong/LISP-ATD-2022/CoAtNet/atd_CoAtNet/atd_CoAtNet.py:154: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  pred = model(torch.tensor(inputs).to(device).float()).cpu().detach().numpy()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use GPU: cuda:None\n",
      "train_loss 23.018508911132812\n",
      "train_loss 12.052367210388184\n",
      "train_loss 2.3271028995513916\n",
      "train_loss 1.9865188598632812\n",
      "train_loss 3.8654768466949463\n",
      "train_loss 1.6401487588882446\n",
      "train_loss 4.849228858947754\n",
      "train_loss 1.5449068546295166\n",
      "train_loss 21.415515899658203\n",
      "train_loss 4.5335164070129395\n",
      "train_loss 13.661408424377441\n",
      "train_loss 3.086364984512329\n",
      "train_loss 2.0784974098205566\n",
      "train_loss 4.755134105682373\n",
      "train_loss 12.373700141906738\n",
      "train_loss 3.6871485710144043\n",
      "train_loss 3.1215147972106934\n",
      "train_loss 1.9812618494033813\n",
      "train_loss 3.9118404388427734\n",
      "train_loss 3.727674961090088\n",
      "train_loss 2.946011781692505\n",
      "train_loss 1.6063963174819946\n",
      "train_loss 3.803231716156006\n",
      "train_loss 1.423923373222351\n",
      "train_loss 5.124417304992676\n",
      "train_loss 2.6764087677001953\n",
      "train_loss 2.323347330093384\n",
      "train_loss 3.5311007499694824\n",
      "train_loss 5.073920249938965\n",
      "train_loss 3.9712765216827393\n",
      "train_loss 0.03808068111538887\n",
      "train_loss 2.4341299533843994\n",
      "train_loss 9.674440383911133\n",
      "train_loss 1.6070706844329834\n",
      "train_loss 0.004377313889563084\n",
      "train_loss 4.0386738777160645\n",
      "train_loss 4.856532573699951\n",
      "train_loss 1.9588277339935303\n",
      "train_loss 1.7745059728622437\n",
      "train_loss 5.9698381423950195\n",
      "train_loss 2.546339273452759\n",
      "train_loss 2.159184694290161\n",
      "train_loss 2.407681941986084\n",
      "train_loss 2.266934871673584\n",
      "train_loss 1.1716755628585815\n",
      "train_loss 1.5368489027023315\n",
      "train_loss 3.652337074279785\n",
      "train_loss 31.52627182006836\n",
      "train_loss 2.442152500152588\n",
      "train_loss 1.6193873882293701\n",
      "train_loss 0.0008155280956998467\n",
      "train_loss 1.8637065887451172\n",
      "train_loss 1.9369585514068604\n",
      "train_loss 11.937649726867676\n",
      "train_loss 18.808727264404297\n",
      "train_loss 5.743819236755371\n",
      "train_loss 8.188126564025879\n",
      "train_loss 4.600242614746094\n",
      "train_loss 3.473658323287964\n",
      "train_loss 8.009687423706055\n",
      "train_loss 0.018644126132130623\n",
      "train_loss 1.253995418548584\n",
      "train_loss 2.1665821075439453\n",
      "train_loss 2.243867874145508\n",
      "train_loss 1.9266680479049683\n",
      "train_loss 98.7339096069336\n",
      "train_loss 4.09611177444458\n",
      "train_loss 2.146641969680786\n",
      "train_loss 9.349236488342285\n",
      "train_loss 1.3824474811553955\n",
      "train_loss 0.0011934173526242375\n",
      "train_loss 1.835957407951355\n",
      "train_loss 9.127655029296875\n",
      "train_loss 2.196957588195801\n",
      "train_loss 2.5561769008636475\n",
      "train_loss 2.2007548809051514\n",
      "train_loss 2.174633264541626\n",
      "train_loss 50.90849685668945\n",
      "train_loss 7.152267932891846\n",
      "train_loss 0.0014611443039029837\n",
      "train_loss 3.154463529586792\n",
      "train_loss 3.9444777965545654\n",
      "train_loss 3.1026251316070557\n",
      "train_loss 1.4215799570083618\n",
      "train_loss 2.7451179027557373\n",
      "train_loss 3.304137945175171\n",
      "train_loss 2.332287549972534\n",
      "train_loss 4.2487664222717285\n",
      "train_loss 1.5848300457000732\n",
      "train_loss 1.7823848724365234\n",
      "train_loss 0.001954125240445137\n",
      "train_loss 4.4695611000061035\n",
      "train_loss 1.243131160736084\n",
      "train_loss 3.6233224868774414\n",
      "train_loss 4.549066543579102\n",
      "train_loss 23.1248722076416\n",
      "train_loss 1.6276154518127441\n",
      "train_loss 41.95060348510742\n",
      "train_loss 1.857216477394104\n",
      "train_loss 8.763153076171875\n",
      "train_loss 0.0036003030836582184\n",
      "train_loss 13.0883207321167\n",
      "train_loss 7.859317302703857\n",
      "train_loss 1.3625154495239258\n",
      "train_loss 1.8822822570800781\n",
      "train_loss 2.451690435409546\n",
      "train_loss 2.7615606784820557\n",
      "train_loss 7.799936771392822\n",
      "train_loss 2.2945973873138428\n",
      "train_loss 7.064207553863525\n",
      "train_loss 0.0039430297911167145\n",
      "train_loss 1.2413769960403442\n",
      "train_loss 3.1405279636383057\n",
      "train_loss 1.4788994789123535\n",
      "train_loss 3.5465641021728516\n",
      "train_loss 12.984624862670898\n",
      "train_loss 1.2576786279678345\n",
      "train_loss 3.1282613277435303\n",
      "train_loss 1.3708875179290771\n",
      "train_loss 0.010708164423704147\n",
      "train_loss 8.63046932220459\n",
      "train_loss 0.011385977268218994\n",
      "train_loss 1.7718825340270996\n",
      "train_loss 1.5492078065872192\n",
      "train_loss 2.7940285205841064\n",
      "train_loss 0.009927122853696346\n",
      "train_loss 6.366342067718506\n",
      "train_loss 1.2763872146606445\n",
      "train_loss 26.13549041748047\n",
      "train_loss 2.3670153617858887\n",
      "train_loss 1.609395146369934\n",
      "train_loss 2.465782403945923\n",
      "train_loss 2.5259010791778564\n",
      "train_loss 1.5142766237258911\n",
      "train_loss 2.3296682834625244\n",
      "train_loss 1.3634626865386963\n",
      "train_loss 26.903400421142578\n",
      "train_loss 1.8702610731124878\n",
      "train_loss 0.05900735780596733\n",
      "train_loss 5.507312774658203\n",
      "train_loss 62.22385025024414\n",
      "train_loss 7.902993202209473\n",
      "train_loss 2.9474048614501953\n",
      "train_loss 3.7677056789398193\n",
      "train_loss 9.500700950622559\n",
      "train_loss 19.643348693847656\n",
      "train_loss 4.157786846160889\n",
      "train_loss 3.4806413650512695\n",
      "train_loss 4.0564422607421875\n",
      "train_loss 52.09238052368164\n",
      "train_loss 1.6656382083892822\n",
      "train_loss 2.901326894760132\n",
      "train_loss 2.0124189853668213\n",
      "train_loss 8.128569602966309\n",
      "train_loss 6.506383895874023\n",
      "train_loss 1.3935043811798096\n",
      "train_loss 9.759077072143555\n",
      "train_loss 0.051348816603422165\n",
      "train_loss 5.602375030517578\n",
      "train_loss 1.408298134803772\n",
      "train_loss 1.3534756898880005\n",
      "train_loss 4.295937538146973\n",
      "train_loss 1.8310023546218872\n",
      "train_loss 1.7954853773117065\n",
      "train_loss 2.3663880825042725\n",
      "train_loss 13.892111778259277\n",
      "train_loss 84.60755157470703\n",
      "train_loss 5.354643821716309\n",
      "train_loss 6.462054252624512\n",
      "train_loss 9.354170799255371\n",
      "train_loss 2.1712427139282227\n",
      "train_loss 1.4310373067855835\n",
      "train_loss 0.9223365187644958\n",
      "train_loss 2.1958770751953125\n",
      "train_loss 14.458568572998047\n",
      "train_loss 3.975433826446533\n",
      "train_loss 0.968168318271637\n",
      "train_loss 2.0573995113372803\n",
      "train_loss 0.984772264957428\n",
      "train_loss 0.5157359838485718\n",
      "train_loss 5.585721969604492\n",
      "train_loss 3.961480140686035\n",
      "train_loss 1.9822179079055786\n",
      "train_loss 8.183984756469727\n",
      "train_loss 10.84796142578125\n",
      "train_loss 0.7732061147689819\n",
      "train_loss 1.89937424659729\n",
      "train_loss 0.3986659049987793\n",
      "train_loss 2.235705614089966\n",
      "train_loss 1.784704327583313\n",
      "train_loss 1.7159134149551392\n",
      "train_loss 28.249435424804688\n",
      "train_loss 1.551787257194519\n",
      "train_loss 3.7233400344848633\n",
      "train_loss 1.4959636926651\n",
      "train_loss 2.4363927841186523\n",
      "train_loss 3.504765748977661\n",
      "train_loss 4.086965084075928\n",
      "train_loss 1.270942211151123\n",
      "train_loss 1.087259292602539\n",
      "train_loss 0.8542278409004211\n",
      "train_loss 1.4945263862609863\n",
      "train_loss 0.6129920482635498\n",
      "train_loss 1.6707285642623901\n",
      "train_loss 6.793211460113525\n",
      "train_loss 1.7214102745056152\n",
      "train_loss 0.45069217681884766\n",
      "train_loss 1.1322039365768433\n",
      "train_loss 3.0789103507995605\n",
      "train_loss 0.9600621461868286\n",
      "train_loss 8.544424057006836\n",
      "train_loss 10.980302810668945\n",
      "train_loss 0.5320421457290649\n",
      "train_loss 1.0275037288665771\n",
      "train_loss 0.43723925948143005\n",
      "train_loss 3.4588630199432373\n",
      "train_loss 0.5946654677391052\n",
      "train_loss 6.927011013031006\n",
      "train_loss 0.7441198825836182\n",
      "train_loss 1.038726806640625\n",
      "train_loss 0.770489513874054\n",
      "train_loss 0.5596859455108643\n",
      "train_loss 1.1158382892608643\n",
      "train_loss 1.3917465209960938\n",
      "train_loss 1.4959439039230347\n",
      "train_loss 2.4974639415740967\n",
      "train_loss 9.14886474609375\n",
      "train_loss 4.049859046936035\n",
      "train_loss 7.777967929840088\n",
      "train_loss 5.963992595672607\n",
      "train_loss 1.1112253665924072\n",
      "train_loss 3.3542354106903076\n",
      "train_loss 0.8671866655349731\n",
      "train_loss 27.26053237915039\n",
      "train_loss 0.6874198317527771\n",
      "train_loss 1.1860837936401367\n",
      "train_loss 0.8281683921813965\n",
      "train_loss 0.2827528119087219\n",
      "train_loss 0.3686407208442688\n",
      "train_loss 5.349951267242432\n",
      "train_loss 0.6508495211601257\n",
      "train_loss 99.73681640625\n",
      "train_loss 3.2267892360687256\n",
      "train_loss 1.9469813108444214\n",
      "train_loss 4.366659164428711\n",
      "train_loss 1.1281161308288574\n",
      "train_loss 2.67787504196167\n",
      "train_loss 5.527658939361572\n",
      "train_loss 0.9341104030609131\n",
      "train_loss 1.685089111328125\n",
      "train_loss 0.7622406482696533\n",
      "train_loss 9.414986610412598\n",
      "train_loss 1.4951167106628418\n",
      "train_loss 1.4133529663085938\n",
      "train_loss 3.699028491973877\n",
      "train_loss 0.9616899490356445\n",
      "train_loss 1.5688822269439697\n",
      "train_loss 2.7158148288726807\n",
      "train_loss 5.955867767333984\n",
      "train_loss 0.5973176956176758\n"
     ]
    }
   ],
   "source": [
    " # Populate a list of models that support the `atd2022.forecasters.Forecaster` protocol\n",
    "# with which will we generate historical forecasts\n",
    "models = [\n",
    "    lispDL,\n",
    "    atd2022.forecasters.PredictMeanForecaster(),\n",
    "    atd2022.forecasters.ExponentiallyWeightedMovingAverage(),\n",
    "]\n",
    "\n",
    "# Compute historical forecasts for all models\n",
    "predictions = [\n",
    "    atd2022.backtest.historical_forecast(model, splitter, verbose=True)\n",
    "    for model in models\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8330a",
   "metadata": {},
   "outputs": [],
   "source": [
    " metric_functions = [\n",
    "    MeanAbsoluteScaledError(),\n",
    "    MeanSquaredError(square_root=True),\n",
    "]\n",
    "\n",
    "metrics_df = atd2022.metrics.compute_metrics(\n",
    "    truth,\n",
    "    predictions,\n",
    "    metric_functions,\n",
    "    models=models,\n",
    "    train=truth,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "metrics_n = atd2022.metrics.compute_nstep_metrics(\n",
    "    truth,\n",
    "    predictions,\n",
    "    metric_functions,\n",
    "    models=models,\n",
    "    train=truth,\n",
    "    verbose=True,\n",
    ")\n",
    "display(metrics_df)\n",
    "display(metrics_n)\n",
    "display(atd2022.viz.plot_nstep_metrics(metrics_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b7cb8e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "atd2022",
   "language": "python",
   "name": "atd2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
